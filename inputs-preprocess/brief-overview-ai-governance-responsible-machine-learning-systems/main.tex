\documentclass{article}
\PassOptionsToPackage{numbers, compress}{natbib}
\usepackage[final]{neurips_2022}


\usepackage[utf8]{inputenc} 
\usepackage[T1]{fontenc}
\usepackage{hyperref} 
\usepackage{url}
\usepackage{booktabs} 
\usepackage{amsfonts} 
\usepackage{nicefrac} 
\usepackage{microtype}
\usepackage{xcolor} 
\usepackage{graphics, graphicx}
\usepackage{comment}
\usepackage{csquotes}
\usepackage[accsupp]{axessibility} 

\title{A Brief Overview of AI Governance for\\Responsible Machine Learning Systems}


\author{
Navdeep Gill \quad Abhishek Mathur \quad Marcos V. Conde\thanks{MC is also with University of Würzburg, CAIDAS. Supported by The Alexander von Humboldt Foundation.}\\
\\
\textbf{H2O.ai}\\
Mountain View, CA\\
\texttt{\{navdeep.gill, abhishek.mathur, marcos.conde\}@h2o.ai} \\
}


\begin{document}


\maketitle


\begin{abstract}
Organizations of all sizes, across all industries and domains are leveraging artificial intelligence (AI) technologies to solve some of their biggest challenges around operations, customer experience, and much more. However, due to the probabilistic nature of AI, the risks associated with it are far greater than traditional technologies. Research has shown that these risks can range anywhere from regulatory, compliance, reputational, and user trust, to financial and even societal risks. Depending on the nature and size of the organization, AI technologies can pose a significant risk, if not used in a responsible way. This position paper seeks to present a brief introduction to AI governance, which is a framework designed to oversee the responsible use of AI with the goal of preventing and mitigating risks. Having such a framework will not only manage risks but also gain maximum value out of AI projects and develop consistency for organization-wide adoption of AI.

\end{abstract}

\section{Introduction}

In this position paper, we share our insights about AI Governance in companies, which enables new connections between various aspects and properties of trustworthy and socially responsible Machine Learning: security, robustness, privacy, fairness, ethics, interpretability, transparency, etc.

For a long time \emph{Artificial intelligence} (AI) was something enterprise organizations adopted due to the huge amounts of resources they have at their fingertips. Today, smaller companies are able to take advantage of AI due to newer technologies, \emph{e.g.} cloud software, which are significantly more affordable than what was available in the past~\cite{alsheibani2018artificial, cubric2020drivers, duan2019artificial, floridi2018ai4people}. AI has been on an upward trajectory in recent years and it will increase significantly over the next several years 
\cite{hbr} \cite{IBM} \cite{PwC}. 
However, every investment has its pros and cons. Unfortunately, the cons associated with AI adoption are caused by its inherent uncertainties, and the builders of such AI systems who do not take the necessary steps to avoid problems down the road~\cite{marr2018artificial, floridi2018ai4people}. Note that, in this work, AI comprises modern \emph{Machine Learning} (ML) and \emph{Deep Learning} (DL) systems, yet not their software around them - which represents other threats and vulnerabilities by itself~\cite{mcgraw2004software, alhazmi2007measuring, dowd2006art} -.


\subsection{Problems within Industry}

Some popular applications of AI are: anomaly detection and forecasting~\cite{makridakis2018statistical, sultani2018real,gordeev2020backtesting}, recommender systems~\cite{portugal2018use}, medical diagnosis~\cite{suzuki2017overview,esteva2021deep}, natural sciences~\cite{conde2021weakly, conde2022few, henkel2021recognizing}, and search engines\cite{conde2022general, conde2021clip}.

However, these applications of AI in industry is still in its infancy. With that said, many problems have arisen since its adoption \cite{ai_incidents, floridi2018ai4people, marr2018artificial}, which can be attributed to several factors:

\textbf{Lack of risk awareness and management:} Too much attention is given to applications of AI and its potential success and not enough attention is given to its potential pitfalls and risks.

\textbf{AI adoption is moving too fast:} According to a survey by KPMG in 2021 \cite{KPMG}, many respondents noted that AI technology is moving too fast for their comfort in industrial manufacturing (55\%), technology (49\%), financial services (37\%), government (37\%), and health care (35\%) sectors.

\textbf{AI adoption needs government intervention:} According to the same survey by KPMG, \cite{KPMG} (see Figure~<a href='#fig-surveys'>fig-surveys</a>), an overwhelming percentage of respondents agreed that governments should be involved in regulating AI technology in the industrial manufacturing (94\%), retail (87\%), financial services (86\%), life sciences (86\%), technology (86\%), health care (84\%), and government (82\%) sectors.

\textbf{Companies are still immature when it comes to adopting AI:} Some companies are not prepared for business conditions to change once a ML model is deployed into the real world.

Many of these problems can be avoided with proper governance mechanisms. AI without such mechanisms is a dangerous game with detrimental outcomes due its inherent uncertainty~\cite{zadeh1986probability, bresina2012planning}. 
With that said, adding governance into applications of AI is imperative to ensure safety in production. 

<div id="fig-surveys" class="flex items-center justify-center">\{\{< image src="images-preprocessed/surveys.png" caption="(Left) ``Rate of AI adoption skyrocketed during COVID-19'' by KPMG~\cite{KPMG}. (Right) IBM Global AI Adoption Index 2022 \cite{IBM}. We refer the reader to the surveys~\cite{KPMG, IBM} for more details." zoomable="true" >\}\}</div>




\section{AI Governance}

\textbf{What is AI Governance (AIG)?}
\begin{displayquote}

AI Governance is a framework to operationalize responsible artificial intelligence at organizations. This framework encourages organizations to curate and use bias-free data, consider societal and end-user impact, and produce unbiased models; the framework also enforces controls on model progression through deployment stages. The potential risks associated with AI need to be considered when designing models, before they affect the quality of models and algorithms. If left unmonitored, AI may not only produce undesirable results, but can also have a significant adverse impact on the organization.

\end{displayquote}

In order for organizations to realize the maximum value out of AI projects and develop consistency for organization-wide adoption of AI, while managing significant risks to their business, they must implement AI governance~\cite{reddy2020governance, dafoe2018ai, gasser2017layered, kuziemski2020ai}; this enables organizations to not only develop AI projects in a responsible way, but also ensure that there is consistency across the entire organization and the business objective is front and center. With the AI governance implemented (as illustrated in Figure~<a href='#fig-teaser'>fig-teaser</a>), the following benefits can be realized:

\textbf{Alignment and Clarity:} teams would be aware and aligned on what the industry, international, regional, local, and organizational policies are that need to be adhered to. 

\textbf{Thoughtfulness and Accountability:} teams would put deliberate effort into justifying the business case for AI projects, and put conscious effort into thinking about end-user experience, adversarial impacts, public safety \& privacy. This also places greater accountability on the teams developing their respective AI projects.

\textbf{Consistency and Organizational Adoption:} teams would have a more consistent way of developing and collaborating on their AI projects, leading to increased tracking and transparency for their projects. This also provides an overarching view of all AI projects going on within the organization, leading to increased visibility and overall adoption.

\textbf{Process, Communication, and Tools:} teams would have complete understanding of what the steps are in order to move the AI project to production to start realizing business value. They would also be able to leverage tools that take them through the defined process, while being able to communicate with the right stakeholders through the tool.

\textbf{Trust and Public Perception:} as teams build out their AI projects more thoughtfully, this will inherently build trust amongst customers and end users, and therefore a positive public perception.

AI governance requires the following: 

\begin{enumerate}
\item A structured organization that gives AIG leaders the correct information they need to establish policies and accountability for AI efforts across their entire organization. For smaller organizations, this might require a more phased approach in which they will work towards the desired structural framework of AIG. For larger organizations, this process might be more attainable due to resources alone, \emph{e.g.} people, IT infrastructure, larger budgets.

\item A concrete and specific AI workflow that collects information needed by AIG leaders will help enforce the constructed policies. This provides information to various parties in a consumable manner. Having such information can be used to minimize mistakes, errors, and bias, amongst other things.
\end{enumerate}


The requirements for AI governance manifest into a framework that an organization must work towards developing. The components of this framework need to be transparent and comprehensive to achieve a successful implementation of AIG. Specifically, this should focus on organizational and use case planning, AI development, and AI ``operationalization'', which come together to make a \textbf{4 stage AI life cycle approach}.

<div id="fig-teaser" class="flex items-center justify-center">\{\{< image src="images-preprocessed/aigov-1.png" caption="Illustration of the AI Governance application towards responsible AI in companies." zoomable="true" >\}\}</div>



\section{Stages of a Governed AI Life cycle}

\subsection{Organizational Planning}

An AI Governance Program~\cite{kuziemski2020ai, dafoe2018ai, wang2018artificial} should be organized in such a way that (a) there is comprehensive understanding of regulations, laws, and policies amongst all team members (b) resources and help available for team members who encounter challenges (c) there is a light weight, yet clear process to assist team members.

\begin{enumerate}
\item \textbf{Regulations, Laws, Policies}

\smallskip Laws and regulations that apply to a specific entity should be identified, documented, and available for others to review and audit. These regulations, laws, and policies vary across industry and sometimes by geographical location. Organizations should, if applicable, develop policies for themselves, which reflect their values and ethical views~\cite{wang2018artificial, mittelstadt2019principles, floridi2018ai4people}; this enables teams to be more autonomous and make decisions with confidence.

\item \textbf{Organization (Center of Competency)}

\smallskip Establishing groups within an organization that provide support to teams with AI projects can prove to be quite beneficial. This includes a group that is knowledgeable with regulations, laws, and policies and can answer any questions that AI teams may have; a group that is able to share best practices across different AI teams within the organization; a group that is able to audit the data, model, process, etc. to ensure there isn't a breach or non-compliance. For more information, we refer the reader to to the survey by Floridi \emph{et al.}~\cite{floridi2018ai4people}.

\item \textbf{Process}

\smallskip Developing a light-weight process that provides guidelines to AI teams can help with their efficiency, rather than hinder their progress and velocity. This involves identifying what the approval process and incident response would be for data, model, deployments, etc.
\end{enumerate}

\subsection{Use Case Planning}
Building use cases involves establishing business value, technology stack, and model usage. The group of people involved in this process can include: subject matter experts, data scientists/analysts/annotators and ML engineers, IT professionals, and finance departments. 

\textbf{Business Value Framework.} The AI team should ensure that the motivation for the AI use case is documented and communicated amongst all stakeholders. This should also include the original hypothesis, and the metrics that would be used for evaluating the experiments.

\textbf{Tools, Technology, Products.} The AI team should either select from a set of pre-approved tools and products from the organization or get a set of tools and products approved before using in an AI user case. If tools for AI development are not governed, it not only leads to high costs and inability to manage the tools (as existing IT teams are aware), it also leads to not being able to create repeatability and traceability into AI models.

\textbf{Model Usage.} Once a sense of value is attached to the use case, then the next step would be to break down the use case to its sub-components which include, but are not limited to, identifying the consumer of the model, the model’s limitations, and potential bias that may exist within the model, along with its implications. Also, one would want to ensure inclusiveness of the target, public safety/user privacy, and identification of the model interface needed for their intended use case. 

\subsection{AI Development}

Development of a machine learning model, including data handling and analysis, modeling, generating explanations, bias detection, accuracy and efficacy analysis, security and robustness checks, model lineage, validation, and documentation.

\begin{enumerate}
\item \textbf{Data Handling, Analysis and Modeling}

\smallskip The first technical step to any AI project is the procurement and analysis of data, which is critical as it lays the foundation for all work going forward. Once data is analyzed, then one must decipher if modeling is needed for the use case at hand. If modeling is needed, then the application of AI can take place. Such an application is an iterative process spanned across many different types of people.

\item \textbf{Explanations and Bias}

\smallskip The goal of model explanations is to relate feature values to model predictions in a human-friendly manner \cite{molnar}. What one does with these explanations breaks down to 3 personas: modeler, intermediary user, and the end user. The modeler would use explanations for model debugging and gaining understanding of the model they just built. The intermediary user would use what the modeler made for actionable insights. And finally, the end user is the person the model affects directly. For these reasons, Explainable Artificial Intelligence (\textbf{XAI}) is a very active research topic~\cite{arrieta2020explainable, gunning2019xai}.

\smallskip Bias, whether intentional (disparate treatment) or unintentional (disparate impact), is a cause for concern in many applications of AI \cite{bias_nist, mehrabi2021survey}. Common things to investigate when it comes to preventing bias include the data source used for the modeling process, performance issues amongst different demographics, disparate impact, identifying known limitations \& potential adverse implications, and the models impact on public safety \cite{bias}. We refer the reader to the survey by Mehrabi \emph{et al.}~\cite{mehrabi2021survey} for more details about bias and fairness in AI.

\item \textbf{Accuracy, Efficacy, \& Robustness}

\smallskip Accuracy of a machine learning model is critical for any business application in which predictions drive potential actions. However, it is not the most important metric to optimize. One must also consider the efficacy of a model, \emph{i.e.} is the model making the intended business impact?

\smallskip When a model is serving predictions in a production setting, the data can be a little or significantly different from the data that the project team had access to. Although model drift and feature drift can capture this discrepancy, it is a lagging indicator, and by that time, the model has already made predictions. This is where \textbf{Robustness} comes in: project teams can proactively test for model robustness, using “out of scope” data, to understand whether the model perturbs. The out of scope data can be a combination of manual generation (toggle with feature values) and automatic generation (system toggles feature values).

\item \textbf{Security}
 
\smallskip ML systems today are subject to general attacks that can affect any public facing IT system \cite{security_of_ml}, \cite {papernot2018marauder}; specialized attacks that exploit insider access to data and ML code; external access to ML prediction APIs and endpoints \cite{model_stealing}, \cite{membership_inference}; and trojans that can hide in third-party ML artifacts. Such attacks must be accounted for and tested against before sending a machine learning model out into the real world.

\item \textbf{Documentation \& Validation}

\smallskip An overall lineage of the entire AI project life-cycle should be documented to ensure transparency and understanding \cite{Mitchell_2019}, \cite{data_cards}, which will be useful for the AI team working on the project and also future teams who must reference this project for their own application.

\smallskip Model validation \cite{sr117, landry1983model} is the set of processes and activities that are carried out by a third party, with the intent to verify that models are robust and performing as expected, in line with the business use case. 
It also identifies the impact of potential limitations and assumptions. From a technical standpoint, the following should be considered: (i) Sensitivity Analysis. (ii) In-sample vs. Out-of-sample performance. (iii) Replication of results from model development team. (iv) Stability analysis. Model ``validators'' should document all of their findings and share with relevant stakeholders.

\end{enumerate}

\subsection{AI Operationalization}

Deploying a machine learning model into production (\emph{i.e.} MLOps~\cite{alla2021mlops, treveil2020introducing}) is the first step to potentially receiving value out of it. The steps that go into the deployment process should include the following: 

\textbf{Review-Approval Flow:} Model building in an AI project will go through various stages: experimentation, model registration, deployment, and decommissioning. Moving from one stage to the next would require ``external'' reviewer(s) who will vet and provide feedback.

\textbf{Monitoring \& Alerts:} Once a model is deployed, it must be monitored for various metrics to ensure there is not any degradation in the model. The cause for a model degrading when deployed can include the following: feature and/or target drift, lack of data integrity, and outliers, amongst other things. In terms of monitoring, accuracy, fairness, and explanations of predictions are of interest~\cite{amodei2016concrete, leike2017ai}. 

\textbf{Decision Making:} The output of a machine learning model is a prediction, but that output must be turned into a decision. How to decide? Will it be autonomous? Will it involve a human in the loop? The answers to these questions vary across different applications, but the idea remains the same, ensuring decisions are made in the proper way to decrease risk for everyone involved.

\textbf{Incident Response and Escalation Process:} With AI models being used in production, there is always going to be a chance for issues to arise. Organizations should have an incident response plan and escalation process documented and known to all project teams.

Companies who successfully implement AI governance for AI applications will result in a highly impactful use of artificial intelligence. While those who fail to do so, risk catastrophic outcomes and an arduous road to recovery as shown in the following use-case. 

\section{AIG Use Case}

In this section we describe a recent use case where - we believe - AI Governance could have avoided a terrible outcome. 

In 2021, the online real estate technology giant, Zillow, shut down its \textbf{AI-powered} house-flipping business.
At it's core, this line of business relied heavily on forecasting from their machine learning models.
Zillow found itself overpaying for homes due to overestimating the price of a property. 
Such overestimation of property values led to a loss of \$569 million, and 28\% of their valuation\cite{bloomberg, bloomberg2}. 
The monetary loss and laying off of 2,000 employees also came with a reputational cost.

This \textbf{AI failure} begs the question ``What mistakes did Zillow make?'' and ``Could this have been prevented?''. The answer to the second question is a firm ``yes''. However, the answer to the first question has many facets, but we try to break down the key mistakes below:

\begin{enumerate}
\item \textbf{Removing ``Human in the Loop''}

\smallskip In its early days, this company hired local real estate agents and property experts to verify the output of their home price prediction ML algorithm; however, as the business scaled, the human verification process around the algorithms was minimized\cite{bloomberg} and the offer-making process was automated, which helped to cut expenses and increase acquisitions.
However, removing human verification in such a volatile domain led to predictions taken at face value without any verification, which played a big role in overpricing of properties.

\smallskip Keeping the human in the loop can help companies from relying on overestimated and biased predictions.
Considering a ML ecosystem for decision-making in organizations, having a human in the loop is a safety harness and should be used in any high stake decision making.

\item \textbf{Not Accounting for Concept Drift and Lack of Model Monitoring}

\smallskip Taking a look back at the timeline of events, it appears that the ML algorithms were not adjusted accordingly to the real market status\cite{bloomberg}. The algorithms continued to assume that the market was still ``hot'' and overestimated home prices.

\smallskip To avoid problems of drift, specifically concept drift, companies should leverage tools for monitoring and maintaining the quality of AI models. Ideally, in this example, they should have set up an infrastructure that automatically alerts data science teams when there is drift or performance degradation, support root cause analysis, and inform model updates with humans-in-the-loop.

\item \textbf{Lack of Model Validation}

\smallskip Before organizations deploy any of their algorithms, they should have enact a set of processes and activities intended to verify that their models are performing as expected and that they are in line with the business use case. Effective validation ensures that models are robust. It also identifies potential limitations and assumptions, and it assesses their possible impact. 
A lack of model validation played a crucial role in this case, and led to the overestimation of home prices. It should be noted that -ideally- model validation needs to be carried out by a third party that did not take part in the model building process.

\item \textbf {Lack of Incident Response}

\smallskip It is not clear when this company started to realize that their model's were degrading and producing erroneous results. 
The lack of having such an incident response plan can cost businesses an exuberant amount of money, time, and human resources. 
In retrospect, if proper AI Governance would had been implemented, the company could have started an incident response \emph{i.e.} incident identification and documentation, human review, and redirection of model traffic. 
These three steps alone could have prevented a lot of damage to their business. 

\end{enumerate}

This industry use case is an example of what can happen without proper AI governance. The mistakes that were made could have been avoided if proper AI governance principles were taken into account from the inception of their use case. Specifically, if this company continued to rely on their subject matter experts, accounted for various types of drift, added model validation, and implemented concrete model monitoring with proper incident response planning, then this whole situation could have been avoided or the damage could have been at a much lower magnitude. 


\section{Conclusion}
AI systems are used today to make life-altering decisions about employment, bail, parole, and lending, and the scope of decisions delegated by AI systems seems likely to expand in the future. The pervasiveness of AI across many fields is something that will not slowdown anytime soon and organizations will want to keep up with such applications. However, they must be cognisant of the risks that come with AI and have guidelines around how they approach applications of AI to avoid such risks. By establishing a framework for AI Governance, organizations will be able to harness AI for their use cases while at the same time avoiding risks and having plans in place for risk mitigation, which is paramount.

\paragraph{Social Impact}

As we discuss in this paper, governance and certain control over AI applications in organizations should be mandatory. \emph{AI Governance} aims to enable and facilitate connections between various aspects of trustworthy and socially responsible machine learning systems, and therefore it accounts for security, robustness, privacy, fairness, ethics, and transparency. We believe the implementation of these ideas should have a positive impact in the society. 

\paragraph{Acknowledgements}
We thank the Trustworthy and Socially Responsible Machine Learning (TSRML) Workshop at NeurIPS 2022. This work was supported by H2O.ai.

{\small
\bibliographystyle{plainnat}
\bibliography{references}
}


\end{document}