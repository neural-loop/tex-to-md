\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}

\newcommand{\versionnumber}{1.5}
\newcommand{\versiondate}{October 14, 2023}
\newcommand{\embargotime}{October 18, 2023 at 9:00AM ET / 6:00AM PT}
\newcommand{\materialsUrl}{{\url{https://www.github.com/stanford-crfm/fmti}}}
\newcommand{\indexUrl}{{\url{https://crfm.stanford.edu/fmti}}}
\newcommand\rb[1]{\textcolor{magenta}{[RB: #1]}}
\newcommand\pl[1]{\textcolor{purple}{[PL: #1]}}
\newcommand\kk[1]{\textcolor{cyan}{[KK: #1]}}
\newcommand\srl[1]{\textcolor{green}{[SL: #1]}}
\newcommand\nm[1]{\textcolor{blue}{[NM: #1]}}
\newcommand\ska[1]{\textcolor{orange}{[SK: #1]}}
\newcommand\bxi[1]{\textcolor{purple}{[BX: #1]}}


\newcommand\projectname{{Foundation Model Transparency Index}\xspace}
\newcommand\projectversionedname{{2023 Foundation Model Transparency Index}\xspace}
\newcommand\projectabbreviation{FMTI\xspace}
\newcommand\informationfreezedate{September 15, 2023}

\newcommand\numcompanies{10\xspace}
\newcommand\numindicators{100\xspace}
\newcommand\numcells{1000\xspace}
\newcommand\numsubdomains{23\xspace}
\newcommand\numupstreamindicators{32\xspace}
\newcommand\numupstreamsubdomains{6\xspace}
\newcommand\nummodelindicators{33\xspace}
\newcommand\nummodelsubdomains{8\xspace}
\newcommand\numdownstreamindicators{35\xspace}
\newcommand\numdownstreamsubdomains{9\xspace}
\newcommand\nummajorsubdomains{13\xspace}
\newcommand\majorsubdomainthreshold{3\xspace}
\newcommand\numdomains{3\xspace}
\newcommand\numgraders{2\xspace}
\newcommand\numbigfindings{10\xspace}
\newcommand\numtotalfindings{35\xspace}
\newcommand\numfeasible{{82}\xspace}
\newcommand\numfeasiblemultiple{{71}\xspace}

\newcommand\scorerange{{42}\xspace}
\newcommand\minscore{{12}\xspace}
\newcommand\maxscore{{54}\xspace}
\newcommand\meanscore{{37}\xspace}
\newcommand\stdev{{14.2}}


\newcommand\aitwentyone{AI21 Labs\xspace}
\newcommand\amazon{Amazon\xspace}
\newcommand\anthropic{Anthropic\xspace}
\newcommand\cohere{Cohere\xspace}
\newcommand\google{Google\xspace}
\newcommand\huggingface{Hugging Face\xspace}
\newcommand\bigscience{BigScience\xspace}
\newcommand\inflection{Inflection\xspace}
\newcommand\meta{Meta\xspace}
\newcommand\openai{OpenAI\xspace}
\newcommand\stability{Stability AI\xspace}

\newcommand\jurassic{Jurassic-2\xspace}
\newcommand\titan{Titan Text\xspace}
\newcommand\claude{Claude 2\xspace}
\newcommand\command{Command\xspace}
\newcommand\palm{PaLM 2\xspace}
\newcommand\bloomz{BLOOMZ\xspace}
\newcommand\inflectionone{Inflection-1\xspace}
\newcommand\llama{Llama 2\xspace}
\newcommand\gptfour{GPT-4\xspace}
\newcommand\stablediffusion{Stable Diffusion 2\xspace}

\newcommand\data{Data\xspace}
\newcommand\labor{Data Labor\xspace}
\newcommand\dataaccess{Data Access\xspace}
\newcommand\compute{Compute\xspace}
\newcommand\methods{Methods\xspace}
\newcommand\datamitigations{Data Mitigations\xspace}
\newcommand\modelbasics{Model Basics\xspace}
\newcommand\modelaccess{Model Access\xspace}
\newcommand\capabilities{Capabilities\xspace}
\newcommand\limitations{Limitations\xspace}
\newcommand\risks{Risks\xspace}
\newcommand\modelmitigations{Model Mitigations\xspace}
\newcommand\trustworthiness{Trustworthiness\xspace}
\newcommand\inference{Inference\xspace}
\newcommand\distribution{Distribution\xspace}
\newcommand\usagepolicy{Usage Policy\xspace}
\newcommand\modelbehaviorpolicy{Model Behavior Policy\xspace}
\newcommand\interface{User Interface\xspace}
\newcommand\dataprotection{User Data Protection\xspace}
\newcommand\updates{Model Updates\xspace}
\newcommand\feedback{Feedback\xspace}
\newcommand\impact{Impact\xspace}
\newcommand\documentation{Downstream Documentation\xspace}
\newcommand\tldr[1]{\textcolor{gray}{\textit{[#1]} \\}}
\newcommand\tldrDone[1]{}
\newcommand\todo[1]{\textcolor{red}{[TODO: #1]}}
\newcommand\todocite[1]{\textcolor{green}{[Add cite: #1]}}
\newcommand\eg{e.g.\xspace}
\newcommand\ie{i.e.\xspace}
\newcommand\cf{cf.\xspace}
\newcommand\dash{\xspace---\xspace}


\documentclass[screen, authorversion, acmsmall]{acmart}

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{natbib}
\usepackage{xcolor}
\usepackage{ifthen}
\usepackage[normalem]{ulem}
\usepackage{svg}
\usepackage{subfiles}
\usepackage{xspace}
\usepackage{cleveref}
\usepackage{wrapfig}
\usepackage{dialogue}
\usepackage[most]{tcolorbox}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{glossaries}
\usepackage{listings}
\usepackage{duckuments}
\usepackage{calc}

\usepackage{enumitem}


\input std-macros
\input macros
\usepackage{authblk}
\renewcommand\Authsep{\hskip1em\relax}
\renewcommand\Authands{\hskip1em\relax} 


\makeatletter 
\renewcommand\maketitle{
{\raggedright 
\centering
\vspace{-2.2in}
{\Huge \bfseries \sffamily \@title }

\vskip2ex

{\@author}

\vskip2ex
}}
\makeatother

\renewenvironment{abstract}{
\newline
\itshape
}
{}

\begin{document}
\title{The Foundation Model Transparency Index}
\input 
\author[1]{Rishi Bommasani*}
\author[1]{Kevin Klyman*}
\author[2]{\\Shayne Longpre}
\author[3]{Sayash Kapoor}
\author[1]{Nestor Maslej}
\author[1]{Betty Xiong}
\author[1]{Daniel Zhang}
\author[1]{\\Percy Liang}
\affil[1]{Stanford University}
\affil[2]{Massachusetts Institute of Technology}
\affil[3]{Princeton University\authorcr
\textcolor{white}{}
\authorcr
Stanford Center for Research on Foundation Models (CRFM)
\authorcr
Stanford Institute for Human-Centered Artificial Intelligence (HAI)
}

\renewcommand{\shortauthors}{
Bommasani et al.
}

\maketitle
\noindent 
\vspace{-0.2in}
\medskip
\begin{abstract}
Foundation models have rapidly permeated society, catalyzing a wave of generative AI applications spanning enterprise and consumer-facing contexts. 
While the societal impact of foundation models is growing, transparency is on the decline, mirroring the opacity that has plagued past digital technologies (\eg social media). 
Reversing this trend is essential: transparency is a vital precondition for public accountability, scientific innovation, and effective governance. 
To assess the transparency of the foundation model ecosystem and help improve transparency over time, we introduce the \textbf{\projectname}.
The \projectversionedname specifies \numindicators fine-grained indicators that comprehensively codify transparency for foundation models, spanning the \textit{upstream} resources used to build a foundation model (\eg data, labor, compute), details about the \textit{model} itself (\eg size, capabilities, risks), and the \textit{downstream} use (\eg distribution channels, usage policies, affected geographies).
We score \numcompanies major foundation model developers (\eg \openai, \google, \meta) against the \numindicators indicators to assess their transparency.
To facilitate and standardize assessment, we score developers in relation to their practices for their flagship foundation model (\eg \gptfour for \openai, \palm for \google, \llama for \meta). 
We present \numbigfindings top-level findings about the foundation model ecosystem: for example, no developer currently discloses significant information about the downstream impact of its flagship model, such as the number of users, affected market sectors, or how users can seek redress for harm.
Overall, the \projectname establishes the level of transparency today to drive progress on foundation model governance via industry standards and regulatory intervention.
\end{abstract}

<div id="fig-major-subdomain-scores" class="flex items-center justify-center">\{\{< image src="images-preprocessed/overall_scores.png" caption="" zoomable="true" >\}\}</div>



\newgeometry{top=2cm, head=10pt, left=2cm, right=2cm,bottom=2cm}
\clearpage
\renewcommand{\baselinestretch}{0.9685}\normalsize 
\addtocontents{toc}{\protect\setcounter{tocdepth}{-1}}
\tableofcontents
\addtocontents{toc}{\protect\setcounter{tocdepth}{2}}
\renewcommand{\baselinestretch}{1.0}\normalsize 
\clearpage


\setlength{\parindent}{0cm}
\setlength{\parskip}{4pt}
\sloppy
\hypertarget{introduction}{\section{Introduction}}
<div id="sec-introduction"></div>

Foundation models (FMs) like LLaMA and DALL-E 3 are an emerging class of digital technology that has transformed artificial intelligence \citep{bommasani2021opportunities}. 
These resource-intensive models are often built by processing trillions of bytes of data, with some of the most capable systems, like \openai's \gptfour, costing hundreds of millions of dollars to build.\footnote{\url{https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/}} 
Foundation models power some of the fastest-growing consumer technologies in history,\footnote{\url{https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/}}
including myriad generative AI applications,\footnote{\url{https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-economic-potential-of-generative-ai-the-next-productivity-frontier}} bringing immense commercial investment and public awareness to AI. 
Simultaneously, these models have captured the interest of policymakers around the world: the United States,\footnote{\url{https://www.whitehouse.gov/wp-content/uploads/2023/07/Ensuring-Safe-Secure-and-Trustworthy-AI.pdf} } China,\footnote{\url{http://www.cac.gov.cn/2023-07/13/c_1690898327029107.htm}} Canada,\footnote{\url{https://ised-isde.canada.ca/site/ised/en/voluntary-code-conduct-responsible-development-and-management-advanced-generative-ai-systems}} the European Union,\footnote{\url{https://www.europarl.europa.eu/news/en/press-room/20230609IPR96212/meps-ready-to-negotiate-first-ever-rules-for-safe-and-transparent-ai}} the United Kingdom,\footnote{\url{https://www.gov.uk/cma-cases/ai-foundation-models-initial-review}} India,\footnote{\url{https://indiaai.s3.ap-south-1.amazonaws.com/docs/generative-ai-report.pdf}} Japan,\footnote{\url{https://english.kyodonews.net/news/2023/10/3b83adf1e28d-japans-ai-draft-guidelines-ask-for-measures-to-address-overreliance.html}} the G7,\footnote{\url{https://www.politico.eu/wp-content/uploads/2023/09/07/3e39b82d-464d-403a-b6cb-dc0e1bdec642-230906_Ministerial-clean-Draft-Hiroshima-Ministers-Statement68.pdf}} and a wide range of other governments have already taken action on foundation models and generative AI.
Foundation models are positioned to be the defining digital technology of the decade ahead.

Transparency is an essential precondition for public accountability, scientific innovation, and effective governance of digital technologies.
Without adequate transparency, stakeholders cannot understand foundation models, who they affect, and the impact they have on society.
Historically, digital technologies often follow a familiar pattern: a new technology provides opportunities and benefits, but companies are not transparent in how they develop and deploy the technology, and this opacity eventually leads to harm. 
In the case of social media, companies have not been transparent about the ways in which they moderate content and share user data, contributing to massacres like the Rohingya genocide in Myanmar\footnote{\url{/https://about.fb.com/wp-content/uploads/2018/11/bsr-facebook-myanmar-hria_final.pdf}} and gross violations of privacy like the Cambridge Analytica scandal.\footnote{\url{https://www.nytimes.com/2018/04/04/us/politics/cambridge-analytica-scandal-fallout.html}}
Consequently, a chorus of academics, civil society organizations, firms, and governments have called for foundation model developers to improve transparency.\footnote{See \refapp{transparency} for further discussion.} 
Groups such as the Partnership on AI, Mozilla, and Freedom House have noted that increased transparency is a crucial intervention.\footnote{\url{http://partnershiponai.org/wp-content/uploads/2021/08/PAI-Responsible-Sourcing-of-Data-Enrichment-Services.pdf}}
UN Secretary-General António Guterres has proposed that the international community should “make transparency, fairness and accountability the core of AI governance ... [and] Consider the adoption of a declaration on data rights that enshrines transparency.”\footnote{\url{https://indonesia.un.org/sites/default/files/2023-07/our-common-agenda-policy-brief-gobal-digi-compact-en.pdf}} 

Foundation models appear to be on track to replicate the opacity of social media. 
Consider \openai's \gptfour, one of the most influential foundation models today. 
\openai states plainly its intention to be nontransparent in the \gptfour technical report, which “contains no further details about the architecture (including model size), hardware, training compute, dataset construction, training method, or similar” \citep{openai2023gpt4}. 
Companies often claim that such information is proprietary or that sharing it would undermine their market position and pose a danger to society as a whole, but this does not negate the enormous risks stemming from foundation models these same companies openly acknowledge, as well as the value of greater transparency. 

While the downsides of opacity are clear, transparency in the foundation model ecosystem today remains minimal.
Little to no evidence exists about which foundation model developers are transparent about which matters, and where there are blind spots in the industry.
How best to improve transparency remains an open question despite rising concerns.


To determine the status quo and track how it evolves over time, we introduce the \projectname (\projectabbreviation).\footnote{See \indexUrl.}
A composite \textit{index} measures a complex construct (\eg transparency) as the basis for scoring/ranking entities (\eg foundation model developers) by aggregatingmany low-level quantifiable \textit{indicators} of transparency.
Indexes are not common in AI\footnote{We note that the AI Index from the Stanford Institute for Human-Centered AI \citep{zhang2022ai, maslej2023ai} is a related effort, but the AI Index tracks broader trends in AI, rather than scoring specific entities or aggregating to a single value.} but are a standard methodology in the social sciences: iconic examples include the United Nations Development Programme's Human Development Index \citep{undp2022hdr}, which ranks countries, and Ranking Digital Rights' Corporate Accountability Index, which ranks companies \citep{rdr2020index}.
We score the transparency of foundation model developers in an effort to promote responsible business practices and greater public accountability. 
We deconstruct the concept of transparency into \numdomains high-level domains: the \textit{upstream} (\eg the data, labor, and compute resources used to build a foundation model), \textit{model}-level (\eg the capabilities, risks, and evaluations of the foundation model), and \textit{downstream} (\eg the distribution channels, usage policies, and affected geographies) practices of the foundation model developer.

\paragraph{The \projectversionedname.}
For the 2023 index, each domain is further broken down into 32--35 indicators: these are concrete, specific, and decidable aspects of transparency (\eg does the foundation model developer disclose the size of its model?). 
Ultimately, the index consists of \numindicators indicators (see \refapp{indicators}) that comprehensively codify what it means for a foundation model developer to be transparent, building upon formative works on transparency for AI and other digital technologies \citep{gebru2021datasheets, bender2018data, mitchell2018modelcards, raji2019actionable, gray2019ghost, crawford2021atlas, cdt2021, keller2022platform}.\footnote{See \url{https://transparency.dsa.ec.europa.eu/} and \url{https://www.tspa.org/curriculum/ts-fundamentals/transparency-report/}} 

We score \numcompanies major foundation model developers on each of the \numindicators indicators to determine how transparent each company is in the development and deployment of its models. 
In particular, we score developers based on their practices in relation to their flagship foundation models:
we assess \openai (\gptfour), \anthropic (\claude), \google (\palm), \meta (\llama), \inflection (\inflectionone), \amazon (\titan), \cohere (\command), \aitwentyone (\jurassic), \huggingface (\bloomz; as host of BigScience),\footnote{Our objective is to assess \huggingface as a company that can be tracked over time.
\bloomz however was not built unilaterally by \huggingface, but instead through the BigScience open collaboration. 
As a result, we refer to \huggingface in the prose but include the BigScience logo in visuals; we provide further discussion in \refsec{model-selection}.} and \stability (\stablediffusion). 
In addition, for downstream indicators we consider the flagship or in-house distribution channel: \openai (OpenAI API), \anthropic (Claude API), \google (PaLM API), \meta (Microsoft Azure),\footnote{\meta announced Microsoft as the "preferred partner" for \llama via Azure: \url{https://about.fb.com/news/2023/07/llama-2/}} \inflection (Pi), \amazon (Bedrock), \cohere (Cohere API), \aitwentyone (AI21 Studio), \huggingface (Hugging Face Model Hub), and \stability (Stability API).
We assess developers on the basis of publicly-available information to make our findings reproducible and encourage transparency vis-à-vis the public on the whole.
To ensure our scoring is consistent, we identify information using a rigorous search protocol (see \refapp{search-protocol}).
To ensure our scoring is accurate, we notified developers and provided them the opportunity to contest any scores prior to the release of this work (all 10 responded and 8 of the 10 explicitly contested some scores).
We summarize our core findings, recommendations, and contributions below and make all core materials (\eg indicators, scores, justifications, visuals) publicly available.\footnote{\materialsUrl}
\hypertarget{findings}{\subsection{Findings}}
<div id="sec-findings"></div>


On the basis of conducting the index, we extensively catalogue \numtotalfindings empirical findings in \refsec{results} spanning overarching trends, domain-level analyses, breakdowns for open vs. closed developers, and similarities in developer practices.
We summarize the \numbigfindings most critical findings.

\paragraph{Significant but obtainable headroom in overall transparency scores (\reffig{overall-scores}).}
Given that the highest overall score is \maxscore out of \numindicators and the mean overall score is \meanscore, all developers have significant room for improvement.
In many cases, such improvement is already feasible: \numfeasible of the indicators are achieved by some developer, and \numfeasiblemultiple are achieved by multiple developers.


\paragraph{Significant unevenness in overall scores with three major clusters (\reffig{overall-scores}).}
Overall scores vary significantly given a range of \scorerange between the highest-scoring developer, \meta, at \maxscore and the lowest-scoring developer, \amazon, at \minscore. 
Relative to the mean overall score of \meanscore, organizations group into three clusters: four well-above the mean (\meta, \huggingface, \openai, \stability), three around the mean (\google, \anthropic, \cohere), and three well-below the mean (\aitwentyone, \inflection, \amazon).

\paragraph{Upstream resource transparency scores the worst (\reffig{domain-scores}).}
Breaking down the trends by domain, scores are consistently the worst for the upstream domain, particularly the \data, \labor, and \compute subdomains.
Several developers (\aitwentyone, \inflection, \amazon) receive 0 points across the entire set of \numupstreamindicators indicators for the upstream domain.

\paragraph{Several upstream matters surrounding data creation are fully opaque (\reffig{upstream-scores}).}
Within the upstream domain, no company scores points for indicators about data creators, the copyright and license status of data, and mitigations related to copyright.
The industry-wide lack of transparency on these issues relates directly to pressing societal concerns related to copyright and intellectual property, which are the subject of ongoing litigation.

\paragraph{Transparency is highest, but still imperfect, for very basic model information and downstream distribution (\reffig{major-subdomain-scores}).}
Breaking down the trends by major dimensions of transparency, the highest-scoring dimensions are \methods, \modelbasics, \capabilities, and \distribution.
However, even when considering indicators for these high-scoring dimensions of transparency, most companies do not reveal basic information like model size nor do they explain how or why they made certain release decisions. 

\paragraph{Developer transparency on \capabilities does not translate to transparency on \limitations, \risks, and \modelmitigations (\reffig{model-scores}).}
Within the model domain, we consider \capabilities, \limitations, \risks, and \modelmitigations as four tightly-related subdomains that characterize a model's potential societal impact.
While many developers score well on \capabilities by describing, demonstrating, and evaluating capabilities, which reflect their models' strengths, the same cannot be said for the other three subdomains.
Instead, transparency is significantly worse: just two developers demonstrate limitations, none evaluate multiple intentional harms their models could facilitate, and none provide either externally reproducible or third-party assessments of mitigation efficacy.

\paragraph{There is virtually no transparency about the downstream impact of foundation models(\reffig{downstream-scores}).}
Within the downstream domain, no developer provides any transparency into the affected market sectors, affected individuals, affected geographies, or any form of usage reporting.
Overall, the average score on the \impact subdomain is the worst in the entire index at 11\%; only three developers provide even the most minimal characterization of the number of downstream applications, and no developer provides a mechanism for users to seek redress.

\paragraph{Open developers are consistently more transparent than closed developers (\reffig{open-closed}).}
Breaking down trends by how developers release their models, open developers (\ie those that release model weights and, potentially, data) show a clear edge in transparency over closed counterparts (\eg API providers). 
Two of the three open developers (\meta and \huggingface) score better than all other developers, while the third (\stability) scores one point below the highest-performing closed developer (\openai). Open developers have higher average scores on 17 of the \numsubdomains subdomains. 

\paragraph{Open developers are much more transparent on upstream resources and comparably transparent on downstream use when compared to closed developers (\reffig{open-closed}).}
The average score for open developers on upstream indicators is 53\% compared to a paltry 9\% of closed developers.
However, while closed developers have greater control over the downstream use of their foundation models, this does not translate to greater downstream transparency as the average for open developers on downstream indicators is 49\% compared to 43\% from closed developers.

\paragraph{Some companies have highly correlated scores (\reffig{overall-correlations}).}
Considering pairs of companies, we analyze the extent to which they agree on the indicators where they do and do not score points.
In particular, the three members of the Frontier Model Forum (\anthropic, \google, \openai) exhibit high indicator-level similarity, as do the two companies that release both model weights and data (\huggingface, \stability) and the four lowest-scoring companies (\cohere, \aitwentyone, \inflection, \amazon).
This leaves \meta as the sole outlier in terms of developer-developer indicator-level similarity.
\clearpage










\hypertarget{recommendations}{\subsection{Recommendations}}
<div id="sec-recommendations"></div>


On the basis of our findings, we make specific recommendations aimed at foundation model developers, foundation model deployers, and policymakers in \refsec{recommendations}. 
We highlight our top-most recommendation for each stakeholder group.

\paragraph{Foundation model developers should improve transparency by drawing on the practices of their competitors.}
By assessing developers directly, we clarify for each developer the indicators where they lack transparency.
In itself, this provides a clear diagnostic on where they stand relative to their competitors and, given our justifications for why transparency on these matters is valuable, why improving transparency would be beneficial for society.
Given that \numfeasible indicators are all satisfied by some developer, developers can directly consult the practices of their competitors to provide a clear example of how they might improve their transparency.
There is a tremendous gap between the \numfeasible already-feasible indicators and the current top score of \maxscore and the mean score of \meanscore, meaning there are many areas of low-hanging fruit where developers can readily improve transparency today. 

\paragraph{Foundation model deployers should push for greater transparency from developers.}
Foundation models intermediate a growing supply chain: deployers of foundation models (\eg cloud service providers and companies that license developers' models) as well as other downstream actors are influenced by, and can influence, the transparency of foundation model developers.
In particular, deployers should push developers for greater transparency when making the decision, and potentially negotiating the contract, to deploy a developer's model.
Deployers and other downstream actors wield leverage collectively: it is their downstream use that generates users and revenue for foundation model developers, meaning they should use this leverage to acquire the necessary transparency from foundation model developers.

\paragraph{Policymakers should prioritize transparency with sufficient precision.}
Given the importance of transparency, policymakers should make transparency a top priority in legislative proposals and regulatory enforcement related to foundation models. 
While transparency is already broadly recognized in most regulatory frameworks for AI, policymakers should be more precise about what they mean by transparency and the areas in which they hope to reduce opacity via transparency requirements or other measures.
In particular, policymakers should understand the status quo for transparency (\eg via the scores we provide) and use this evidence to inform interventions in the areas where transparency is most urgently needed (\eg on \labor and \impact, given these are lowest-scoring dimensions of transparency across the entire supply chain). 
\clearpage
\hypertarget{contributions}{\subsection{Contributions}}
<div id="sec-contributions"></div>

To summarize, our contributions are:
\begin{enumerate}
\item \textbf{Taxonomy.}
We taxonomize the vast conceptual space of transparency in the context of foundation models, following on widespread calls for transparency (see <a href='#app-transparency'>app-transparency</a>).
In particular, we structure the space hierarchically into \numdomains domains (\ie upstream, model, downstream), \numsubdomains subdomains (\eg data, compute, capabilities, risks, distribution, feedback), and \numindicators decidable and actionable indicators.
\item \textbf{Scoring of major foundation model developers.}
We score \numcompanies major foundation model developers and their flagship foundation models with a standardized protocol.
These developers vary in their company status (\eg startups, Big Tech), release strategy (\eg open weights, restricted API), modalities (\eg text-to-text, text-to-image), and involvement in global policy efforts (\eg White House voluntary commitments, Frontier Model Forum).
We allow developers to directly contest scores: all 10 developers engaged in correspondence and 8 contested specific scores.
\item \textbf{Empirical findings.}
Our extensive evaluation yields \numtotalfindings findings, which ground existing discourse and sharpen our understanding of the lack of transparency in the foundation model ecosystem.
In many cases, these findings directly bear on critical global AI policy efforts (\eg the EU AI Act) and provide the basis for clear recommendations on how developers may improve their practices (\eg by creating centralized documentation artifacts).
Our scores offer ample opportunities for further analysis. 
\item \textbf{Legibility and reproducibility.}
We provide a public website that presents our findings and recommendations broadly legible to the general audience.\footnote{\indexUrl}
To facilitate further research, and reproduce our scoring and analyses, we make all core materials (\eg indicators, scores, justifications, visuals) publicly available.\footnote{\materialsUrl}
\item \textbf{Theory of change and future versions.}
Our objective is to simultaneously articulate the status quo and increase transparency over time.
To this end, we make very explicit our theory of change: we view our work as compiling the transparency practices across es across companies as an instrument for driving change (see \refsec{change}) and the limitations/risks of our work (see \refsec{limitations}).
Critically, we will conduct additional iterations of the index to track progress over time to work towards a more transparent foundation model ecosystem.
\end{enumerate}
\clearpage
\hypertarget{background}{\section{Background}}
<div id="sec-background"></div>

To begin, we provide a brief primer on the three core concepts underlying this work: foundation models, transparency, and indexes. 

\hypertarget{fms}{\subsection{Foundation models}}
<div id="sec-background-fms"></div>

Foundation models are the defining paradigm of modern AI, 
reflecting a broad shift in the field from bespoke models for individual tasks to more general models that can be adapted for a wide range of use cases \cite{bommasani2021opportunities}.
In this sense, foundation models belong to the broader class of general-purpose technologies that have restructured society such as electricity, the Internet, and smartphones \citep{bresnahan1995gpt, brynjolfsson2021jcurve, bommasani2021opportunities, eloundou2023gpts}. Building foundation models requires significant resources: immense volumes of data are processed using immense amounts of computation to yield the foundation model.
Using foundation models often requires substantially fewer resources in comparison: models can be adapted, often in lightweight fashion (\eg through a simple textual interface), for an increasingly wide range of use cases.
The disparity in resource requirements between development and deployment has yielded a market where a small set of companies build the most prominent foundation models that are then adopted by thousands of companies and millions of consumers \citep{bommasani2023ecosystem, vipra2023concentration, widder2023open}. 

<div id="fig-supply-chain" class="flex items-center justify-center">\{\{< image src="images-preprocessed/supply_chain.png" caption="\textbf{Foundation Model Supply Chain.}  A conceptual depiction of the foundation model supply chain, beginning with the primary \textit{upstream} resources (\ie data, compute) and transitioning to the foundation model, subsequent hosts (or \textit{distribution channels}), and ending with \textit{downstream} applications. Image taken with permission from \citet{jones2023foundationmodels}." zoomable="true" >\}\}</div>



The structure of the foundation model paradigm implicates a broader ecosystem and supply chain \cite{bommasani2023ecosystem, cen2023supplychain, jones2023foundationmodels}.
We depict a conceptualized view of this supply chain in \reffig{supply-chain}.
The supply chain begins with the \textit{upstream} resources that are used to build a foundation model: data, computational hardware, energy, labor, and code. 
For each of these resources, a further supply chain exists: for example, data to build foundation models is often sourced from the Internet, but this data can only come to be on the Internet as a result of human data-generating process (\eg publishing news article, authoring personal blogs, uploading videos to YouTube, creating music) along with Internet infrastructure (\eg networking protocols).
Alongside these upstream resources and supply chains, foundation models are then used as the foundation for supply chains that derive from the model.
In particular, foundation models are made available for downstream use through \textit{distribution channels} (\eg an API to access the model or a host that facilitates inference using the model).
By way of these distribution channels, foundation models power \textit{downstream} applications (\eg commercial products and services) across a range of market sectors and geographies.
For instance, \openai's \gptfour powers applications in education (\eg Khan Academy's Khanmigo tutor), finance (\eg Stripe's fraud detection tool), banking (\eg Morgan Stanley's internal chatbot), and government (\eg Iceland's language preservation system).\footnote{See \url{https://openai.com/gpt-4} for a list of several applications built upon \openai's \gptfour.}
Overall, a comprehensive account of the societal impact of foundation models, and their transparency in particular, requires consideration of the different parts of the foundation model ecosystem \citep[][\S1.2]{bommasani2021opportunities}.

Foundation models have fueled the recent wave of generative AI technologies: these models can be used to generate fluent text, useful code, photorealistic images, and compelling audio.
New research efforts built foundation models in an even broader array of domains: biology \citep{lin2023evolutionary}, climate change \citep{lacoste2023geobench}, weather,\footnote{\url{https://www.earthdata.nasa.gov/news/weather-ai-fm-workshop}} astronomy \citep{nguyen2023astrollama}, radiology \citep{chambon2022roentgen}, and robotics \citep{openx2023openx}.
Nevertheless, much of the present public and commercial interest centers on language models (\eg \anthropic's \claude, \meta's \llama) and multimodal models with language interfaces (\eg \stability's \stablediffusion, \openai's \gptfour). 
Alongside their significant capabilities, researchers have highlighted a large number of potential risks posed by these foundation models spanning malicious uses like generating disinformation to unintended harms like generating text that reinforces societal biases \citep{bender2021dangers, bommasani2021opportunities, abid2021persistent, weidinger2022taxonomy}.
There have also been recent demonstrations of many concrete harms from language models.\footnote{Partnership on AI's AI Incident database (\url{https://incidentdatabase.ai/}) and the AI, Algorithmic, and Automation Incidents and Controversies database (\url{https://www.aiaaic.org/aiaaic-repository}) collect incidents of harm caused by AI. For a concrete example, see \url{https://www.404media.co/inside-the-ai-porn-marketplace-where-everything-and-everyone-is-for-sale/}.} 

\hypertarget{transparency}{\subsection{Transparency}}
<div id="sec-background-transparency"></div>

Transparency is broadly understood as the property of being visible and easily understood \citep{aristotle350deanima, kalderon2015transparency}, and is often a fundamental prerequisite of social responsibility and accountability \citep{florini2007right,robinson2012nations}.

Transparency is desirable from a variety of standpoints.
For example, transparently disclosing information makes that information available, shareable, legible, and verifiable. 
Transparency when conducting a complex process can make clear the processes' scope, stakes, and pitfalls \citep{lathrop2010open}. 
Similarly, transparency in decision-making can help those who are not involved in the decision assess the motivations behind the decision, the evidence used to justify it, as well as its costs and benefits.
Various philosophers, political theorists, scientists, and journalists have emphasized the importance of transparency across these and other domains \citep{johnston2006good,florini2007right, benkler2013practical, schudson2015rise}. 
Civil society, grassroots organizations, and consumers also regularly call for transparency as a mechanism for fact finding, accountability, and holding organizations responsible for harm \citep{heikkila2023high,diresta2022openblackbox}.\footnote{See \refapp{transparency} for additional details on calls for transparency.}
For our purposes, we consider transparency as it relates to the development and use of digital technologies, with a specific focus on the transparency of the practices of foundation model developers as measured by the information they share regarding their models.\footnote{Note that the term "transparency" is at times also used to describe efforts to make AI more explainable or interpretable at the level of specific AI-based predictions or decisions \citep{liao2023transparency, zou2023representation}. 
Such transparency is not the subject of our work.} 

\paragraph{Why transparent matters for digital technologies.}
Transparency in digital technologies is particularly relevant for three reasons.
First, new digital technologies, such as AI, are not well understood by society, often appearing as a black box \citep{castelvecchi2016openblackbox}. 
Second, digital technologies are easily rendered invisible, meaning it is difficult for nonexperts to understand when processes like algorithmic decision-making are taking place \citep{ng_conceptualizing_2021}.
Third, these technologies can have a profound influence on billions of users across society.
And yet these technologies are built by a small cadre of industry actors who do not represent society as a whole.
Under these conditions, transparency functions as a prerequisite for public accountability and responsible innovation \citep{klyman2023open}.
Shared visibility engenders public trust and facilitates interventions in the public interest \citep{hardin2002trust}. 
Without sufficient understanding of industry practices, researchers cannot characterize the societal impact of digital technologies, let alone propose concrete actions to improve business practices \citep{pasquale2015black}.
While the effects of transparency are often difficult to measure as they are diffuse and indirect, transparency helps to expose malpractice and enables the public to respond to such malpractice.

\paragraph{Limitations of transparency.} 
Transparency is far from sufficient on its own and it may not always bring about the desired change \citep{corbett2023interrogating}. 
Salient critiques of transparency include:
- sep0em
-  Transparency does not equate to responsibility. Without broad based grassroots movements to exert public pressure or concerted government scrutiny, organizations often do not change bad practices \citep{boyd2016algorithmic,ananny2016limits}.
-  Transparency-washing provides the illusion of progress.
- Some organizations may misappropriate transparency as a means for subverting further scrutiny.
- For instance, major technology companies that vocally support transparency have been accused of \emph{transparency-washing}, whereby "a focus on transparency acts as an obfuscation and redirection from more substantive and fundamental questions about the concentration of power, substantial policies and actions of technology behemoths" \citep{zalnieriute2021transparency}.
-  Transparency can be gamified. Digital platforms have been accused of performative transparency, offering less insightful information in the place of useful and actionable visibility ~\citep{doi:10.1177/20539517231164119, Mittelstadt2019}.
- As with other metrics, improving transparency can be turned into a game, the object of which is not necessarily to share valuable information.\footnote{According to Goodhart's Law, "when a measure becomes a target, it ceases to be a good measure" \citep{goodhart1984problems}.}
-  Transparency can inhibit privacy and promote surveillance.
- Transparency is not an apolitical concept and is often instrumentalized to increase surveillance and diminish privacy \citep{han2015transparency,Mohamed2020,birchall2021radical}.
- For foundation models, this critique underscores a potential tension between adequate transparency with respect to the data used to build foundation models and robust data privacy.
-  Transparency may compromise competitive advantage or intellectual property rights.
- Protections of competitive advantage plays a central role in providing companies to the incentives to innovate, thereby yielding competition in the marketplace that benefits consumers.
- Consequently, work in economics and management studies have studied the interplay and potential trade-off between competitive advantage and transparency \citep{bloomfield1999market, granados2013transparency, liu2023competitive}, especially in the discourse on corporate social responsibility \citep{}.

Transparency is not a panacea. 
In isolation, more information about foundation models will not necessarily produce a more just or equitable digital world. 
But if transparency is implemented through engagement with third-party experts, independent auditors, and communities who are directly affected by digital technologies, it can help ensure that foundation models benefit society.

\paragraph{Transparency in practice for prior digital technologies}
Digital technologies are marked by a long track record of poor transparency.
While each major new technology has dramatically restructured society, the powerful corporations that build these technologies have wielded outsized influence and maintained opacity to advance their commercial interests.
Consider the following examples of digital technologies that suffer from a lack of transparency as well as associated interventions/studies to reduce opacity:
the fight for net neutrality for internet service providers like Comcast \citep{crs2021netneutrality}, web cookies for online advertising like Google Ads \citep{englehardt2015cookies,englehardt2016online,narayanan2017princeton}, labor practices for crowd-sourcing platforms like Amazon Mechnical Turk \citep{gray2019ghost, crawford2021atlas}, wage schemes for ride sharing platforms like Uber \citep{rosenblat2016algorithmic}, and dark patterns for game companies like Epic Games \citep{ftc2023epicgames}.

Stepping through these examples, efforts like the Princeton Web Transparency Project \citep{englehardt2015cookies,englehardt2016online,narayanan2017princeton} have unveiled the ecosystem of online third-party tracking using cookies, which ``led to greater public awareness, the cessation of some privacy-infringing
practices, and the creation of new consumer privacy tools.''
Similarly, \citet{rosenblat2016algorithmic} empirically demonstrated that Uber drivers were the subject of a severely asymmetric power dynamic given the control exerted by Uber over their drivers, to the detriment of the ride sharing market.
In the context of crowd-sourcing, \citet{gray2019ghost} and \citet{crawford2021atlas} demonstrated exploitation of the ``ghost" workers powering AI, such as on Amazon Mechanical Turk, that was made invisible on these platforms.
More recently, these efforts have prompted the scrutiny of lawmakers as to improve transparency and, thereby, labor conditions.
As a final example, dark patterns have a pervasive practice for myriad technologies, leading to mismanaged consumer expectations and overall opacity. 
To this end, the FTC's recent inquiry into Epic Games for dark patterns used to deceive gamers, and particularly children, amounted to a \$245M fine on Epic Games \citep{ftc2023epicgames}.

Building on these prior examples, we consider social media more specifically.
Social media platforms provide a vivid example of transparency challenges in recent years, and the increasing level of acknowledgement among some technology companies that a baseline level of transparency is a necessity. 
Given the profound impact of social media in mediating how humans form relationships, communicate with each other, buy goods and services, and access information, a broad body of work argues for greater transparency \citep[see][]{keller2022platform}. 
Social media platforms have slowly begun to adopt transparency reporting practices.
For example, Facebook now hosts its own Ad Library\footnote{\url{https://www.facebook.com/ads/library/}}, Content Library\footnote{\url{https://transparency.fb.com/researchtools/meta-content-library}}, and a transparency center\footnote{\url{https://transparency.fb.com/}} that reports on content enforcement, widely viewed content, regulatory transparency, government data requests, and intellectual property, among other pieces of mostly voluntary transparency.
In parallel, transparency requirements have been enshrined in laws like the EU Digital Services Act \citep{dsa2022} and legislative proposals like the U.S. Platform Accountability and Transparency Act \citep{pata2021}. 

\paragraph{Transparency for AI.}
With the rise of AI in the past 10 years, its societal impact has received much greater attention \citep{barocas2016, abebe2020roles, hutchinson2021towards, bender2021dangers}.
Transparency is often referenced as a core ethical principle undergirding responsible AI \citep{fjeld2020principled, Hagendorff2020}.\footnote{See UNESCO's Recommendation on the Ethics of Artificial Intelligence, which was adopted by its 193 member states and constitutes the first global normative instrument on AI ethics. Our conceptualization of transparency covers several of UNESCO's 10 principles, namely Transparency and Explainability. See \url{https://www.unesco.org/en/artificial-intelligence/recommendation-ethics}} 
\citet{jobin2019global} find that transparency is the most frequently cited principle in AI ethics guidelines, appearing in 85\% of the assessed 84 guidelines. 

Given that the standard machine learning pipeline is divided into several stages, transparency efforts often target different stages.\footnote{As mentioned previously, the term "transparency" is also sometimes used in AI to refer to explainability/interpretability, referring to understanding how a specific model makes predictions \citep{zou2023representation}.
In part, the emphasis on this topic is due to the inscrutability of the deep neural networks that have powered AI's rise.
However, we focus on structural forms of transparency, taking a more macroscopic perspective.}
Documentation efforts are most common at the level of data \citep{gebru2021datasheets, bender2018data, pushkarna2022data} and models \citep{mitchell2018modelcards, crisan2022interactive}, with evaluations providing further insight into models \citep{deng2009imagenet, ribeiro2020beyond, perez2022red, liang2022helm, bommasani2023transparency}.
More recently, several efforts have studied the broader ecosystem-wide transparency of AI and its supply chains \citep{bommasani2023ecosystem, cen2023supplychain}, though transparency on the downstream impacts of AI is comparatively understudied \citep{narayanan2023transparencyreports}.
The \projectname advances this view, assessing transparency of foundation models with a comprehensive ecosystem-level approach that spans the data and broader upstream resources, the foundation models themselves, and the downstream use and impact.

\hypertarget{index}{\subsection{Indexes}}
<div id="sec-background-index"></div>

A (composite) index is a standard methodology \citep{oecd2008handbook, greco2019methodological} for assessing entities (\eg companies, countries) in relation to a specific construct (\eg transparency, responsibility).
Methodologically, the score on an index for a specific entity is the aggregate of multiple low-level indicators that can be more directly quantified. 
Composite indexes as a methodology has seen broad adoption across the social sciences, including to directly address major political, economic, and societal concerns such as public corruption \citep[\eg Transparency International’s Corruption Perceptions Index;][]{ti2022corrupt}, environmental welfare \citep[\eg the World Economic Forum’s Environmental Sustainability Index;][]{whitford2009political} and living standards \citep[\eg the United Nations Development Programme’s Human Development Index;][]{hopkins1991human}. 
However, indexes have not played a major role in mainstream AI discourse.\footnote{We highlight the AI Index from the Stanford Institute for Human-Centered AI \citep{maslej2023ai, zhang2022ai}, which tracks global progress of AI across a variety of quantitative indicators. 
In contrast to the composite indexes here, the AI Index neither directly scores specific entities nor does it aggregate individual indicators into a singular aggregate.
We also highlight the Generative AI Accountability Scorecard from Ranking Digital Rights as a forthcoming effort that targets the generative AI services downstream of foundation models: \url{https://rankingdigitalrights.org/mini-report/introducing-rdrs-preliminary-standards-for-generative-ai/}.
}

Indexes are designed to further several objectives and have certain characteristic strengths \citep{joint2008handbook, saisana2002state}. 
Most fundamentally, indexes can transform complex and amorphous constructs into straightforward and concrete scores.
Indexes and the aggregate quantitative metrics they provide can therefore allow for broad engagement on certain topics, furthering public understanding as well as providing a strong basis for various forms of decision-makingsuch as regulatory intervention. 
In addition, when indexes are maintained over time, they encourage a long-term focus and can be vital in fostering improvement over time.
In this way, while operating at a different level of abstraction and involving a different set of design decisions, indexes are analogous to model benchmarks that are commonplace in AI \citep{deng2009imagenet, wang2019superglue, liang2023holistic} and appeal to a similar theory of change \citep{donoho2017fifty, ethayarajh2020utility, raji2021benchmark, bommasani2022evaluation}.
Indexes also have shortcomings: namely, they can be reductive and overly subjective \citep{saisana2002state, oecd2008handbook, greco2019methodological}.
To design and score an index, researchers must make simplifying decisions about which indicators to include, how to weigh those indicators, and how to grade indicators.
Beyond these methodological issues, indexes are subject to a broader conceptual critique that they may oversimplify concepts that are intrinsically complex, discarding valuable nuances.\footnote{The literature and theory on composite indexes is much too extensive to be easily summarized in this brief primer.
We recommend the Handbook on Constructing Composite Indicators: Methodology and User Guide \citep{oecd2008handbook} as a proper introduction to the subject: \url{https://doi.org/10.1787/9789264043466-en}.} 
Indexes may also be subject to gaming, which we discuss more extensively in \refsec{limitations}.
\clearpage

\hypertarget{fmti}{\section{The Foundation Model Transparency Index}}
<div id="sec-fmti"></div>

The \projectname scores foundation model developers for their comprehensive transparency. 
We discuss specifics on the developers, indicators, and scoring in subsequent sections.
Strategically, our aim is for the index to clarify discourse on foundation models and AI that is muddled and lacks grounding in empirical data. 
We aim to improve the overall transparency of the AI ecosystem by encouraging foundation model developers to share more information about the development and deployment of their models. 
We also provide a clear taxonomization of the key issues related to transparency and demonstrate where greater transparency would be especially valuable. 
Therefore, the \projectname provides a frame of reference for assessing whether the ecosystem as a whole---and which developers in particular---become more or less transparency over time.
Simultaneously, given the limitations of indexes, we are fully transparent about our methodology, including the core decisions on indicator inclusion, indicator weighting, and indicator scoring.
We also discuss methodological shortcomings relating to each of these decisions in \refsec{limitations}.
To guard against unnecessary simplification, we provide discussion and analysis at several levels of abstraction in \refsec{results}. 

Overall, the \projectname captures the key dimensions of transparency that are relevant to foundation models at present.
As the foundation model ecosystem and AI policy evolves over time, the central questions regarding the transparency of foundation models will evolve as well.
Consequently, we will conduct future versions of the index that adjust the indicators to reflect these changes.
We more expansively discuss our intended impact (including our theory of change and associated limitations and risks) in \refsec{impact}.
\clearpage
\hypertarget{indicators}{\section{Indicators}}
<div id="sec-indicators"></div>

We define \numindicators indicators that comprehensively characterize transparency for foundation model developers.
To select these indicators, we compiled relevant concepts raised across past scientific literature as well as concerns animated by public discourse on foundation models and other digital technologies. In \refapp{indicators} we provide specific references for each indicator, and these references advocate for increased transparency and information sharing related to the indicator in question.
We derived a concrete set of indicators from this literature, engaging external researchers to converge on the final list of \numindicators (see \reffig{indicators}).
These indicators cover each dimension of the foundation model supply chain, from the data, compute, and labor required to build foundation models to model evaluations and developers' policies to restrict their use. 
We divide our indicators into three broad domains as described in \reffig{supply-chain}: indicators that are \textit{upstream} of the model, indicators that relate to the \textit{model} itself, and indicators that are \textit{downstream} of the model.

\hypertarget{upstream-indicators}{\subsection{Upstream indicators}}
<div id="sec-upstream-indicators"></div>

The upstream indicators identify the \emph{ingredients and processes} involved in building a foundation model. 
There are \numupstreamindicators upstream indicators, which we further taxonomize into the following \numupstreamsubdomains subdomains:

-  \textbf{\data (10 indicators).}
- Assesses transparency regarding the size and composition of the data used to build the model; the creators whose content is present in the data; and any steps to curate or augment the data.
- These indicators also address transparency regarding the inclusion of personal, copyrighted, or licensed data.
-  \textbf{\labor (7 indicators).}
- Assesses transparency regarding the use of human labor in producing the data used to build the model, including the wages, labor protections, employer, and geographic distribution of workers who contributed to data annotation and curation.
- These indicators also address transparency regarding the third parties that foundation model developers partnered with to construct their models.
-  \textbf{\dataaccess (2 indicators).}
- Assesses the scope of data access given to external parties.
-  \textbf{\compute (7 indicators).}
- Assesses transparency regarding the hardware and computation used to build the model, as well as the resulting energy use and environmental impacts.
-  \textbf{\methods (4 indicators).}
- Assesses basic technical specifications for the model's training stages and objectives, as well as the software frameworks and dependencies used.
-  \textbf{\datamitigations (2 indicators).}
- Assesses transparency regarding steps taken to mitigate data privacy and copyright concerns.

<div id="fig-indicators" class="flex items-center justify-center">\{\{< image src="images-preprocessed/indicators_list.png" caption="\textbf{Indicators.} The \numindicators indicators of the \projectname spanning the \numdomains domains: upstream, model, and downstream." zoomable="true" >\}\}</div>



<div id="fig-upstream-indicators" class="flex items-center justify-center">\{\{< image src="images-preprocessed/indicators_list_upstream.png" caption="\textbf{Upstream Indicators.} The \numupstreamindicators upstream indicators that span \data, \labor, \dataaccess, \compute, \methods, and \datamitigations." zoomable="true" >\}\}</div>



\noindent We depict the upstream indicators in \reffig{upstream-indicators}.
Researchers have widely advocated for greater transparency in relation to \data and \dataaccess \citep{bender-friedman-2018-data, gebru2018datasheets, hutchinson2021towards, dodge2021c4, bandy2021addressing} as a means for contextualizing model capabilities ~\citep{sambasivan2021everyone, longpre2023pretrainer} and risks related to privacy, bias, and copyright ~\citep{buolamwini2018gender,bender2021dangers, kandpal2022deduplicating,sobel2017artificial}.
\labor indicators uplift concerns related to labor practices, include irresponsible or exploitative use of human labor ~\citep{gray2019ghost, crawford2021atlas, hao2023cleaning, kittur2013future, dzieza2023ai, west2019data}.
\compute indicators relate to concerns around the high computational cost and energy expenditure associated with building foundation models, which can result in environmental harm \citep{lacoste2019quantifying,strubell2019energy,schwartz2020green,patterson2021carbon,bender2021dangers,henderson2020towards,luccioni2023counting,vipra2023comments}.
\datamitigations indicators also relate to the growing legal and sociotechnical concerns over data privacy, copyright, and licensing \citep{henderson2023foundation, brown2022does, lee2023talkin,genlaw2023,tremblay2023openai}.

\hypertarget{model-indicators}{\subsection{Model indicators}}
<div id="sec-model-indicators"></div>

The model indicators identify the \emph{properties and function} of the foundation model. 
There are \nummodelindicators model indicators, which we further taxonomize into the following \nummodelsubdomains subdomains:
-  \textbf{\modelbasics (6 indicators).}
- Assesses transparency regarding fundamental information about the model such as modalities, size, and architecture as well as the presence of centralized model documentation.
-  \textbf{\modelaccess (3 indicators).}
- Assesses the scope of model access given to external entities.
-  \textbf{\capabilities (5 indicators).}
- Assesses transparency regarding the capabilities of the model, including evaluations.
-  \textbf{\limitations (3 indicators).}
- Assesses transparency regarding the limitations of the model, including evaluations.
-  \textbf{\risks (7 indicators).}
- Assesses transparency regarding the risks of the model, including evaluations, with specific focus on both unintentional harm (\eg bias) and intentional harm (\eg fraud).
-  \textbf{\modelmitigations (5 indicators).}
- Assesses transparency regarding model-level mitigations, including evaluations of their efficacy.
-  \textbf{\trustworthiness (2 indicators).}
- Assesses transparency regarding the trustworthiness of the model, including evaluations.
-  \textbf{\inference (2 indicators).}
- Assesses transparency regarding standardized inference with the model.

<div id="fig-model-indicators" class="flex items-center justify-center">\{\{< image src="images-preprocessed/indicators_list_model-1.png" caption="\textbf{Model Indicators.}  The \nummodelindicators model indicators that span \modelbasics, \modelaccess, \capabilities, \limitations, \risks, \modelmitigations, \trustworthiness, and \inference." zoomable="true" >\}\}</div>



\noindent 
We depict the model indicators in \reffig{model-indicators}.
\modelbasics indicators refer to fundamental information that is expected by model documentation standards \citep{mitchell2019model, crisan2022interactive, bommasani2023ecosystem} and, historically, have been reliably reported in the release of machine learning models. 
\modelaccess indicators reflect literature tied to the spectrum of model release and the associated differences in external access \citep{solaiman2019release, sastry2021release, shevlane2022structured, liang2022norms, solaiman2023gradient}. 
The indicators on \capabilities, \limitations, \risks and \modelmitigations are motivated by a common understanding that these factors jointly influence the societal impact of machine learning models and AI systems \citep{tabassi2023airmf, weidinger2023sociotechnical}. 
For these subdomains, the description and demonstration indicators gauge whether there is some non-technical articulation and legibility of these concepts, primed by concerns surrounding public understanding of foundation models.\footnote{See \url{https://www.gov.uk/government/publications/public-perceptions-towards-the-use-of-foundation-models-in-the-public-sector}.} 
To make these assessments more rigorous, the evaluation indicators build on the extensive tradition of evaluation in AI spanning iconic benchmarks like ImageNet \citep{deng2009imagenet}, broader benchmarks like SuperGLUE \cite{wang2019superglue}, and extensive meta-benchmarks like LM-Harness, BIG-bench, HELM and BEHAVIOR \citep{gao2021harness, srivastava2022bigbench, liang2023holistic, srivastava2021behavior}. 
Indicators assessing evaluations also highlight the importance of reproducibility \citep{lipton2019troubling, kapoor2023reforms, kapoor2023leakage}\footnote{See the ML Reproducibility challenge: \url{https://paperswithcode.com/rc2022}, CodaLab worksheets for reproducible ML: \url{https://worksheets.codalab.org/}, and Joelle Pineau's reproducibility checklist: \url{https://www.cs.mcgill.ca/~jpineau/ReproducibilityChecklist.pdf}.} and independent assessment \citep{sandvig2014auditing, raji2019actionable, metaxa2021audit, costanzachock2022audit, raji2022audit, raji2022mozilla, lam2022user, weidinger2023sociotechnical}, which enable open science and external verification of developers' claims about their models.
In the case of risks, finer distinctions between unintentional harms (\eg biases, toxicity) and intentional harms (\eg disinformation, fraud) build on harm taxonomies \citep{bender2021dangers, bommasani2021opportunities, weidinger2021ethical, nist2023airmf, weidinger2023sociotechnical}.
Indicators on trustworthiness and inference are especially motivated by the Trustworthy ML Initiative\footnote{\url{https://www.trustworthyml.org/}} and MLPerf \citep{reddi2020mlperf} respectively, among other works \citep{brundage2020toward, cammarota2020trustworthy, kumar2020trustworthy, liu2022trustworthy, shneiderman2020bridging, patterson2021carbon, narayanan2023cheaply}.

\hypertarget{downstream-indicators}{\subsection{Downstream indicators}}
<div id="sec-downstream-indicators"></div>

The downstream indicators identify the \emph{use} of the foundation model, including details about its \textit{release}.
There are \numdownstreamindicators downstream indicators, which we further taxonomize into the following \numdownstreamsubdomains subdomains:

-  \textbf{\distribution (7 indicators).}
- Assesses transparency regarding the release process, the distribution channels for the model, and the products and services that arise through internal use.
- Additionally, this subdomain assesses the presence of model licenses, terms of service, and mechanisms for detecting model-generated content.
-  \textbf{\usagepolicy (5 indicators).}
- Assesses transparency regarding the developer's acceptable use policy such as restrictions on specific uses or users, as well as transparency regarding how it enforces such policies.
-  \textbf{\modelbehaviorpolicy (3 indicators).}
- Assesses transparency regarding the developer's policy on acceptable and unacceptable model behavior as well as transparency regarding enforcement of this policy and expectations in the event of usage policy violations.
-  \textbf{\interface (2 indicators).}
- Assesses transparency in the user interface for the developer's flagship distribution channel, if the channel includes a user interface.
-  \textbf{\dataprotection (3 indicators).}
- Assesses transparency regarding the developer's policies with respect to user data protection, such as how data is stored, shared, and accessed.
-  \textbf{\updates(3 indicators).}
- Assesses transparency regarding the developer's versioning protocol, change log, and deprecation policy.
-  \textbf{\feedback (3 indicators).}
- Assesses transparency regarding mechanisms for reporting feedback on the model, summaries of feedback received, and related government inquiries.
-  \textbf{\impact (7 indicators).}
- Assesses transparency regarding the downstream impact of the model on society, such as affected market sectors, individuals, and geographies.
- Additionally, this subdomain assesses transparency regarding downstream applications, usage statistics, and mechanisms for monitoring usage as well as providing redress in the event of harm to users.
-  \textbf{\documentation (2 indicators).}
- Assesses the presence of centralized documentation for downstream use and documentation for responsible downstream use.

<div id="fig-downstream-indicators" class="flex items-center justify-center">\{\{< image src="images-preprocessed/indicators_list_downstream.png" caption="\textbf{Downstream Indicators.}  The \numdownstreamindicators downstream indicators that span \distribution, \usagepolicy, \modelbehaviorpolicy, \interface, \dataprotection, \updates, \feedback, \impact, and \documentation." zoomable="true" >\}\}</div>



\noindent We depict the downstream indicators in \reffig{downstream-indicators}.
Given that foundation models are the basis for a downstream supply chain \citep{bommasani2021opportunities}, the distribution indicators are informed by the literature on AI supply chains \citep{bommasani2023ecosystem, vipra2023concentration, cen2023supplychain, cobbe2023supply, widder2023thinking, brown2023allocating} and release practices \citep{liang2022thetime, solaiman2023gradient, henderson2023foundation, pmlr-v202-kirchenbauer23a, Kuditipudi2023RobustDW, liesenfeld2023opening}. 
Usage policy indicators draw from company publications on responsible model deployment \cite{cohere2022} as well precedents from social media. 
Model behavior policy indicators are rooted in literature that discusses AI behavior and trustworthiness, risks, mitigation and refusal \cite{kumar2022language, weidinger2021ethical, brundage2020toward, cammarota2020trustworthy, kumar2020trustworthy, liu2022trustworthy, reuter2023im}. 
User interface indicators are derived from research on safety by design and human-centered user interfaces \cite{qiaosi2023ux, nakao2022responsible}. 
User data protection indicators are inspired by policy recommendations on user data minimization, privacy, preservation, protection and contextual integrity \cite{eu2016, brown2022does,vipra2023, winograd2023privacy, nissenbaum2004contextual, king2020privacy, mulligan2017privacy}.
Model updates indicators stem from work focused on adequately updating systems and version control of AI systems \citep{sathyavageesran2022privacy, hashesh2023version, chen2023chatgpts}. 
For feedback, impact and downstream documentation, the indicators were motivated by the literature on algorithmic auditing \cite{liang2022thetime, solaiman2023gradient, raji2022audit} as well as transparency reporting practices for social media.\footnote{See \url{https://www.tspa.org/curriculum/ts-fundamentals/transparency-report/}, \url{https://transparencyreport.google.com/} and \url{https://transparency.fb.com/reports/}.}

\paragraph{Note on assessment of indicators.}
We assess each indicator based on the information that developers share publicly about their flagship foundation models and their practices that apply to these models. 
Our standard for awarding points on an indicator is that the developer must explicitly state the information related to the indicator in its documentation, or it must explicitly point to the information in its documentation.
This implies that if developers are overly vague or do not link to a key external document for a particular indicator then they do not receive a point.
In addition, if developers explicitly state in their documentation that they \emph{do not} carry out a specific action related to an indicator (\eg they do not have a mechanism for users to provide feedback) then we generally award a point for that indicator.
We note that this is exceedingly rare and that, in general, developers share little information about the actions they do or do not take in the process of developing and deploying foundation models.

\paragraph{Note on inclusion of deployment.}
Our view of transparency is expansive, considering the broader supply chain beyond just foundation models.
As we discuss in \refsec{background-transparency}, existing conceptualizations of transparency in AI often consider upstream resources (especially data) in addition to machine learning models.
But these works and broader public discourse usually do not foreground the downstream use and impact of AI, even though this is the most direct way in which AI affects society.
To this end, we include the entire downstream domain to bring greater attention to this vital topic.

In particular, while we are assessing foundation model developers, we assess them in relation to distribution channels and other factors that determine their downstream impact.
At present, we recognize that characterizing the downstream impact of foundation models may be challenging, especially for open model developers.
By releasing a model openly, developers may cede the ability to easily monitor the model's downstream use and impact.
Open model developers can be fully transparent by being clear about the ways in which they do or do not monitor downstream use and impact.
In addition, we believe in the potential for greater coordination between foundation model developers and distribution channels to increase transparency; for example, distribution channels could supply information about how the model is used to the foundation model developer.
Partnerships with distribution channels that promote transparency provide a promising means for all foundation model developers to share more information about the impact their models have on society.
\clearpage
\hypertarget{developers}{\section{Foundation model developers}}
<div id="sec-developers"></div>



\input{tables/developers.table}

\noindent Transparency initiatives in AI (\eg datasheets and model cards) often introduce frameworks that support machine learning developers in achieving greater transparency in their own work.
In contrast, we proactively assess foundation model developers for their transparency using the \numindicators indicators we specify.
By conducting the assessment ourselves, we sidestep concerns of uneven uptake that have arisen with past transparency initiatives \citep[\eg][]{gebru2018datasheets, mitchell2018modelcards} and provide greater consistency in the scoring of each indicator across developers.
Most importantly, scoring many developers allows for the comparison of their scores, which provides a rich context for how to improve transparency in the foundation model ecosystem.
Efforts like Ecosystem Graphs \citep{bommasani2023ecosystem} and the UK Competition and Markets Authority (CMA) report on the foundation model market\footnote{\url{https://www.gov.uk/government/publications/ai-foundation-models-initial-report}} track the organizations that develop foundation models.
At the time of writing in September 2023, the CMA report documented 160 foundation models (based on data drawn from Ecosystem Graphs) built by more than 50 organizations.\footnote{\url{https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/1185508/Full_report\_.pdf\#page=22}} 
However, as the CMA report states, a small number of developers control the majority of the market at present \citep{vipra2023concentration}.
Due to this intense level of market concentration, we decided to assess \numcompanies major foundation model developers.\clearpage

\hypertarget{developer-selection}{\subsection{Selecting developers}}
<div id="sec-developer-selection"></div>

We considered a variety of selection criteria in choosing the \numcompanies developers to assess, arriving at the following three principles:
\begin{enumerate}
\item \textbf{Impact.} We selected developers that have built the most influential foundation models.
\item \textbf{Diversity.} We selected developers that, when considered collectively, represent many axes of variation in the foundation model ecosystem. For example, developers that release models along different points on the release gradient \cite[\eg open vs. closed,][]{solaiman2023gradient}, build models with different modalities (\eg text-to-text vs. text-to-image), and occupy different positions in the market (\eg startups vs. Big Tech). 
\item \textbf{Companies.} 
We selected developers that are established companies as enduring targets for longitudinal improvement. 
This to some extent parallels current regulatory initiatives that explicitly focus on companies as the target of policy for foundation models.\footnote{See \url{https://www.blumenthal.senate.gov/imo/media/doc/09072023bipartisanaiframework.pdf}.} 
\end{enumerate}
On this basis, we chose 10 companies that all are influential foundation model developers: 
\aitwentyone, \amazon, \anthropic, \cohere, \google, \huggingface, \inflection, \meta, \openai, and \stability.
These \numcompanies provide significant diversity in terms of release strategy (\eg \anthropic, \meta, and \huggingface all release flagship models with different levels of openness; see <a href='#fig-release-gradient'>fig-release-gradient</a>), modality (\eg \cohere, \openai, and \stability all provide different input-output modalities), and market position (\eg \google, \inflection, and \openai occupy different market positions).

Additionally, in parallel to our research, the White House made three announcements involving companies that develop foundation models: a red-teaming exercise announced in May 2023,\footnote{\url{https://www.whitehouse.gov/briefing-room/statements-releases/2023/05/04/fact-sheet-biden-harris-administration-announces-new-actions-to-promote-responsible-ai-innovation-that-protects-americans-rights-and-safety/}} a set of voluntary commitments announced in July 2023,\footnote{\url{https://www.whitehouse.gov/briefing-room/statements-releases/2023/07/21/fact-sheet-biden-harris-administration-secures-voluntary-commitments-from-leading-artificial-intelligence-companies-to-manage-the-risks-posed-by-ai/}} and another set of voluntary commitments announced in September 2023.\footnote{\url{https://www.whitehouse.gov/briefing-room/statements-releases/2023/09/12/fact-sheet-biden-harris-administration-secures-voluntary-commitments-from-eight-additional-artificial-intelligence-companies-to-manage-the-risks-posed-by-ai/}}
Separately, three of the companies we assess jointly announced the formation of the Frontier Model Forum in July 2023.\footnote{\url{https://blogs.microsoft.com/on-the-issues/2023/07/26/anthropic-google-microsoft-openai-launch-frontier-model-forum/}} 
When taken together, these announcements name 16 companies: Adobe, Amazon, Anthropic, Cohere, Google, Hugging Face, IBM, Inflection, Meta, Microsoft, NVIDIA, OpenAI, Palantir, Salesforce, Scale AI, and Stability AI.
We note that 9 of the 10 companies we selected are within this set of 16 (all but \aitwentyone).


<div id="fig-release-gradient" class="flex items-center justify-center">\{\{< image src="images-preprocessed/gradient.png" caption="\textbf{The gradient of release of foundation models.}  Foundation models can be fully closed (\eg only used internally within the company, without public release),  released gradually as their risks and benefits are better understood (\eg via a staged rollout involving initial testers),  released via a web or app interface (\eg users need to visit a website or join a Discord server to access the model's outputs),  released via a programmatic API (\eg users can query the model and receive outputs programmatically),  released via downloadable model weights (\eg users can access and adapt the model),  or released with the training data alongside downloadable model weights (\ie ostensibly maximal openness).  For the ten models we consider, one falls under the fully closed category at the time of writing (Inflection-1), though \inflection plans to make it available via an API; six are available via an API (\gptfour, \claude, \palm, \jurassic, \command, \titan); one is downloadable (\llama), and two are released with their model weights as well as underlying training data downloadable (\stablediffusion and \bloomz).  For simplicity, we at times binarize these distinctions into models with downloadable weights ('open') and models without downloadable weights ('closed'). Image taken with permission from \citet{solaiman2023gradient}." zoomable="true" >\}\}</div>



\paragraph{The gradient of release strategies.}
The strategies for releasing foundation models differ widely (see \reffig{release-gradient}). Some developers release the weights of the model as well as the data used, which allows independent researchers and developers to use the models on their own and investigate the data. For example, EleutherAI released the weights of its Neo-X model \citep{black2022neox} along with The Pile, which Neo-X was trained on \citep{gao2021thepile}.
\meta released the weights to its OPT model \citep{zhang2022opt}, but did not release the associated training data.
For our purposes, we will often refer to any release where model weights are made broadly available as "open," which includes the flagship models of \huggingface, \meta, and \stability.

In contrast, other developers do not release the weights of their flagship model, retaining greater control over who has access to the model and the extent to which it may be used externally (if at all).
The majority of the developers we assess provide a programmatic API to query their flagship model as a black box.
Other developers in the ecosystem do not provide a programmatic API but do allow for some forms of black box access, as Midjourney does for its text-to-image models that it makes available via a Discord server.\footnote{See \url{https://docs.midjourney.com/docs/midjourney-discord}.}
Still other developers provide no external access to their models as is the case for Google's Chinchilla model \citep{hoffmann2022chinchilla} and Meta's Make-A-Video model \citep{singer2022makeavideo}.
For our purposes, we will often refer to any release where model weights are not made externally available as "closed," which includes the flagship models of \aitwentyone, \amazon, \anthropic, \cohere, \google, \inflection, and \openai.

The overall approach to release is informed by a developer's business strategy and perspective on its model's utility and risks. 
In particular, many organizations may adopt different release approaches for different foundation models.
For example, when releasing \gptfour, \openai did not disclose many details about the modeling architecture and training data, citing competition and safety as the two main reasons.\footnote{Interview with \openai's chief scientist and co-founder: \url{https://www.theverge.com/2023/3/15/23640180/openai-gpt-4-launch-closed-research-ilya-sutskever-interview}} 
On the other hand, when releasing the text-to-speech Whisper model \cite{radford2022whisper}, \openai disclosed many details and released the model weights openly.
For other developers, the release decision may directly relate to their purpose for building a foundation model in the first place.
For example, the BigScience collaboration led by \huggingface that led to the BLOOM model \citep{scao2022bloom} was explicitly designed to democratize access to multilingual large language models with capabilities in traditionally underrepresented languages.
As a result, the initiative released model weights and data. \clearpage

\hypertarget{model-selection}{\subsection{Selecting flagship models}}
<div id="sec-model-selection"></div>

Almost all major foundation model developers release multiple foundation models over time and, even at the time of writing, many have multiple salient foundation models (often across different modalities).
For example, OpenAI has developed GPT, GPT-2, GPT-3, GPT-4, InstructGPT, WebGPT, Codex, CLIP, DALL-E, DALL-E 2, DALL-E 3, Jukebox, and Whisper among other models.
Given that developers are not guaranteed to provide uniform transparency for each foundation model (\eg OpenAI releases the weights openly for some of these models but not others), we decide to assess developers in relation to their \textit{flagship} foundation model.
By flagship foundation model, we mean the foundation model that is most salient and/or capable from the developer based on our judgment, which is directly informed by the company's public description of the model.
We provide basic information about each of the developers and their flagship model in \reftab{developer-info}.\footnote{For OpenAI, we evaluate GPT-4, which was released in March 2023, not GPT-4V, a model OpenAI released in September 2023 after we completed our analysis. With respect to input and output modality, \citet{openai2023gpt4} states that GPT-4 is "a large multimodal model capable of processing image and text inputs and producing text outputs."}

\paragraph{Note on \huggingface.}
In the case of \huggingface, we are assessing the company in general as an enduring target over time.
However, for this version of the index, we assess \bloomz \citep{muennighoff2022crosslingual}, which was collaboratively developed through the year-long BigScience initiative that was initiated and led by \huggingface from May 2021 to May 2022. 
As a result, we refer to \huggingface throughout the prose, but include the BigScience logo in visuals (which may also be distributed absent the context we provide in this paper) to highlight this nuance.
\clearpage
\hypertarget{scoring}{\section{Scoring}}
<div id="sec-scoring"></div>

By selecting the indicators and companies, we abstractly specify the form of the index.
By defining each indicator and designating the flagship foundation model to be assessed for each developer, we move to a more precise operationalization. 
To make the index fully precise, we describe how we sourced the information that was used to assess each developer on each indicator, resulting in the final scores.

\paragraph{Search protocol.}
To source information that we use to score developers, we exclusively use publicly available information provided by developers themselves.
We recognize that this information may be incomplete (\eg clients or governments may have greater access to information from the developer), but given that our focus includes public accountability, and we are academic researchers, we choose to consider only publicly available information.
Given that public information may change, we use information available as of \informationfreezedate.

For each developer, we initially compile a basic set of resources disclosed by the developer about their model development practices and their flagship foundation model.
To gather information for a specific indicator, we perform a structured search to identify all relevant information that is public.
The exact details of how we execute this search are provided in \refapp{search-protocol}.

\paragraph{Initial scoring.}
Having identified the information basis for scoring an indicator, \numgraders researchers on the team independently scored the developer on the indicator.
This entails specifying a \textit{score} (\ie 0 or 1), \textit{source} used in arriving at that score (\eg one or more webpages), and a textual \textit{justification} for how the evidence from sources is weighed against the criteria for the indicator in determining the score. 
Given these initial score assignments, the researchers reviewed their scores to identify any errors. 

Binary scoring provided several advantages. First, it simplified the scoring process by allowing researchers to focus on the sharp distinction between 0 and 1 point for each indicator. 
Second, a narrow criterion for making a binary scoring decision for each indicator reduced subjectivity in the initial scoring. 
Third, by reducing the level of complexity of each indicator we were able to reduce overlap between indicators, ensuring that we assess distinct dimensions of transparency.
At the same time, binary scoring limits the level of complexity of each indicator, potentially leaving out valuable information that can be captured by more complex scoring schemes \citep[\cf][]{bommasani2023eu-ai-act}.\footnote{See \refsec{limitations} for further discussion.}

In some instances, the researchers responsible for the same (indicator, developer) pair arrived at different scores, indicating disagreement. 
Given the systematic information gathering process, the iterative refinement of indicator definitions, and the binary scoring scheme, we found that disagreements were fairly infrequent.
Disagreements generally related to relevant information being erroneously neglected by one researcher or differences in the fine-grained interpretation of how to score an indicator.
Overall, across all \numindicators $\times$ \numcompanies (indicator, developer) pairs, the agreement rate was 85.2\% \citep[Cohen's $\kappa = 0.67$, indicating substantial agreement;][]{landis1977agreement}. 
To resolve disagreements, the researchers discussed and jointly came to a resolution.
Following the disagreement resolution, the scores were finalized and sources and justifications were merged to yield an initial set of \numcells (score, source, justification) triples for all \numcells (indicator, developer) pairs. \clearpage

\paragraph{Company feedback.}
Given that these scores constitute a direct assessment of specific companies, we engaged these companies to provide them with the opportunity to review, respond, and potentially rebut or contest the scores we assigned. 
Concretely, we contacted leaders at each of the companies with (i) a description of the \projectname, 
(ii) the \numindicators indicators and their definitions, and (iii) their \numindicators (score, source, justification) triples. 
We encouraged each company to review our scores, provide any general feedback and, especially, to directly contest any scores the company viewed as incorrect (by referencing public information available as of \informationfreezedate).
Companies were provided two business weeks to respond with clear assurance that all correspondence would be strictly private. 

Of the \numcompanies companies, all 10 responded.
Of these, 8 companies (\amazon, \anthropic, \cohere, \huggingface, \inflection, \meta, \openai, \stability) provided rebuttals for specific scores, which we extensively reviewed. 
In most cases, we did not change scores, though some rebuttals led to improvements in the scores (an average increase of 1.25 points across the 8 developers that contested on average 8.75 scores).
Rather than improving developers' scores, these rebuttals often revealed misunderstandings regarding definitions of indicators or our justifications for scores, leading to more robust definitions and justifications.
Beyond the scores, several companies scheduled calls with us or provided broader forms of feedback, which provided insight regarding how they conceptualize best practices for transparency and responsible AI.
Following company feedback, we again verified all scores, sources, and justifications that constitute the finalized materials used throughout this paper and made publicly available. 

We also notified the companies prior to the release of this paper, responding to their feedback. 
In addition, we encouraged companies to provide a public written response regarding their perspective on this initiative, their specific scores, and their broader approach as an organization to transparency and responsible AI as it relates to foundation models.
Moving forward, we hope these organizations implement more transparent practices and we provide specific recommendations to that effect in \refsec{recommendations-developers}.
\clearpage
\clearpage
\hypertarget{results}{\section{Analysis}}
<div id="sec-results"></div>


The finalized results of the \projectname are the scores for each of the \numindicators indicators across all \numcompanies companies.
These result are accessible at \materialsUrl, to facilitate subsequent analyses.
Here, we specifically consider overarching trends in the results, along with more specific trends based on the structure of the index.
Namely, we analyze along the rows/indicators (\eg domains), the columns/companies (\eg release strategy), as well as data-driven trends (\eg correlations).

\hypertarget{overarching-results}{\subsection{Overarching results}}
<div id="sec-overarching-results"></div>


<div id="fig-overall-scores" class="flex items-center justify-center">\{\{< image src="images-preprocessed/f7.png" caption="\textbf{Overall Scores.} The overall \projectname score and ranking across all \numindicators indicators." zoomable="true" >\}\}</div>



<div id="fig-domain-scores" class="flex items-center justify-center">\{\{< image src="images-preprocessed/f8.png" caption="\textbf{Scores by Domain.} The aggregate score of each developer broken down by the three domains: upstream, model, and downstream." zoomable="true" >\}\}</div>



<div id="fig-major-subdomain-scores" class="flex items-center justify-center">\{\{< image src="images-preprocessed/heatmap_selected_subdomains.png" caption="\textbf{Scores by Major Dimensions of Transparency.}  The fraction of achieved indicators in each of the \nummajorsubdomains major dimension of transparency.  Major dimension of transparency are large subdomains within the \numsubdomains subdomains." zoomable="true" >\}\}</div>



We begin our analysis by first establishing the broad trends when viewing the index as a whole.
We consider results aggregated at the level of a single overall score per company (\reffig{overall-scores}) as well as the scores broken down into the \numdomains domains (upstream, model, downstream; \reffig{domain-scores}).
We supplement our findings on these overarching trends with a more granular consideration of the \textit{major dimensions of transparency} in the index in \reffig{major-subdomain-scores}.\footnote{
The major dimensions of transparency we highlight are \nummajorsubdomains large subdomains among the \numsubdomains subdomains. }


\paragraph{All developers have significant room for improvement. But most transparency indicators are very obtainable, having been implemented by at least one developer.}
Based on \reffig{overall-scores}, the highest-scoring developer scores points for \maxscore of the \numindicators indicators, and the average score across all developers is \meanscore.
This establishes a pervasive lack of transparency across major foundation model developers.
With that said, for \numfeasible of the \numindicators indicators, there exists some developer that scores points, and of these there are \numfeasiblemultiple where multiple developers score points. 
Consequently, there is clear reason to believe that across all developers, the necessary change to become more transparent is feasible.
That companies' competitors are more transparent in certain issue areas suggests that such transparency, even if not fully costless, is unlikely to cause serious damage to their business.
Companies can emulate the higher level of transparency their competitors exhibit on certain indicators, providing a precedent and a starting point for improving transparency in the foundation model ecosystem. 

\paragraph{Developers show significant variance in overall transparency scores.}
While all developers have significant room for improvement, the current transparency of developers is strikingly uneven. 
Namely, the range in overall scores is \scorerange between the highest-scoring \meta at \maxscore and the lowest-scoring \amazon at \minscore.
Even excluding \amazon's score as especially low, we still see an effective range of 30 points between \meta and the next lowest \inflection.
Overall, with respect to the mean of \meanscore, the standard deviation is \stdev, which is quite substantial.
The four top-scoring developers (\meta, \huggingface, \openai, \stability) all cluster well above the mean, the next three are very close to the mean (\google, \anthropic, \cohere), and the three lowest-scoring developers (\aitwentyone, \inflection, \amazon) are well below the mean.
In many cases, the lowest-scoring developers have clear opportunities for improvement through straightforward changes related to some of the least challenging indicators. 
Examples include improved documentation(\eg change logs, versioning protocols, model cards, centralized documentation for downstream use), clearer language in corporate policies(\eg usage policies, model behavior policies, deprecation policies), and disclosing additional information that is unlikely to have implications for business competitiveness or safety (\eg basic details on methods, dependencies, feedback).

\paragraph{The Upstream domain sees the worst transparency scores.}
To gain additional insight beyond developers' basic overall scores, we consider scores broken down by the \numdomains top-level domains in \reffig{domain-scores}.
On this basis, we see clear evidence that developers are, on average, least transparent with respect to the upstream resources required to build their models, such as data, labor, and compute. 
Concretely, the mean score on upstream indicators is 7.2 out of 32 (22.5\%), compared to 14.1 out of 33 (42.7\%) for model indicators and 15.7 out of 35 (44.9\%) for downstream indicators.
To confirm this is not overly biased by outliers, we note that the medians show the same trend: the median score on upstream indicators is 3.5, compared to 12.5 for model indicators and 16 for downstream indicators.
We specifically highlight that the four lowest-scoring developers overall (\reffig{overall-scores}) also fare the worst on the upstream domain (\reffig{domain-scores}), with \cohere receiving 3 points and all of \aitwentyone, \inflection, and \amazon receiving 0 points.
In contrast, for both the model and downstream domains, all \numcompanies companies receive at least 6 points. 

\paragraph{Domain-level discrepancies explain some of the differences between companies with similar overall scores.}
We partition the \numcompanies companies into three groups based on whether their overall score (\reffig{overall-scores}) is well-above (\meta, \huggingface, \openai, \stability), around (\google, \anthropic, \cohere), or well-below (\aitwentyone, \inflection, \amazon) the mean. 
Within these groups, while companies receive somewhat similar scores, we find that their domain-level scores clarify discrepancies between them. 
Among the highest scorers, \openai is considerably less transparent on upstream matters (7) as compared to the other three high-scoring companies (\meta with 14, \huggingface with 21, \stability with 16).
In particular, \openai and \stability receive the nearly the same overall score, with \openai making up the deficit to \stability on upstream transparency mostly through better model-level transparency (and, specifically, many of the indicators on evaluations and risks).
For the middle category of \google, \anthropic, and \cohere, the discrepancies are less stark, but we do see that \cohere is at 3 in the upstream category compared to \google with 6 and \anthropic with 5.
Given the broadly similar scores for these three developers across all of the domains, we revisit the extent to which they are correlated at a finer-grained level in \refsec{correlations}.
Among the three lowest-scoring developers, we see that \aitwentyone and \inflection are differentiated by the model domain, with both scoring a zero on the upstream domain and similarly on the downstream domain.

\paragraph{\data, \labor, and \compute are pervasive blind spots across developers.} 
While the overall and domain-level results provide a basic lay of the land, we find that the major dimensions of transparency provide the Goldilocks region for clear and incisive analysis as shown in \reffig{major-subdomain-scores}.
In particular, these dimensions of transparency are subdomains with several indicators (so the subdomain scores are more reliable) that are tied to broadly-understandable concepts like labor and capabilities. 
We hone in on the following major dimensions of transparency: \data, \labor, \compute, \methods, \modelbasics, \modelaccess, \capabilities, \risks, \modelmitigations, \distribution, \usagepolicy, \modelbehaviorpolicy, \updates, \dataprotection, \feedback, and \impact. 
Analysis at this level reveals actionable insight into what types of transparency or opacity lead to many of our top findings. 
For example, we find that the poor upstream transparency stems from low performance on the \data, \labor, and \compute subdomains; developers average just 20\%, 17\%, and 17\% for \data, \labor, and \compute respectively. 
In terms of smaller subdomains, developers on average score 25\% of the available points on \datamitigations.

\paragraph{\modelbasics, \capabilities, \limitations, and \dataprotection are the most transparent subdomains at present, but still short of the ideal.}
Developers score the highest proportion of points on indicators related to the following subdomains: \interface (85\%), \documentation (70\%), \dataprotection (67\%), \modelbasics (63\%), and \updates (63\%).
This reflects some baseline level of transparency across developers with respect to notifying users they are interacting with AI systems, providing centralized documentation for downstream use, publishing data protection policies, and disclosing the modalities associated with their model. 
Still, there are gaps in even for these subdomains. 
No developer provides a protocol for accessing usage data.
Most developers (8 of 10) do not disclose the size of their model.
And only half of the developers provide any form of deprecation policy.

\hypertarget{upstream-results}{\subsection{Upstream results}}
<div id="sec-upstream-results"></div>


Upstream indicators assess transparency regarding the ingredients that go into the foundation model including data, labor, compute, methods, and code. 
These ingredients are important predictors of the capabilities and risks of the foundation model they produce, as well as externalities of the model development process (\eg impacts on human laborers and the environment). 
As we show in \reffig{domain-scores}, the upstream indicators are the most sparsely awarded (22.5\% coverage on average).
Here, we analyze at the level of subdomains and indicators based on \reffig{upstream-scores}. 

\paragraph{The Upstream domain shows the greatest spread.}
Building on the fact that developers score worst on the upstream domain--with several developers scoring exactly or nearly 0 points--we find the range in scores is the greatest for this domain. Namely, only one developer (\huggingface) scores more than half of the indicators (21 of the available \numupstreamindicators indicators; 65.6\%), yielding a range of 21 when compared to the lowest-scoring developers: \aitwentyone, \inflection, and \amazon (0 of the available \numupstreamindicators indicators; 0\%).
We emphasize this striking disparity given that many of the fundamental societal issues in connection with foundation models relate to upstream resources: bias, copyright, and privacy in relation to data, worker protections and fair compensation in relation to labor, environmental impact and energy expenditure in relation to compute, reproducibility in relation to methods, and cybersecurity in relation to code. 

\paragraph{The \methods subdomain is the most transparent in aggregate, while \labor is the least transparent.} 
Among the upstream subdomains, only \methods shows some degree of coverage, with six of the developers giving some description of training stages, training objectives, and dependencies.
On the other end of the spectrum, \labor sees little to no coverage with the exception of \bloomz, which involved volunteers providing data.
Developers generally share no information about the use of human labor in their data pipeline, the employer, wages, and geographical distribution of these workers, instructions they give to data annotators, or any labor protections they implement.
This industry norm of being nontransparent with respect to data labor is in tension with the fact that such information is critical to reinforcement learning with human feedback \citep{ziegler2019finetuning, ouyang2022instructions, casper2023open}.
That data labor is one of the two least transparent subdomains is consistent with prior work documenting widespread ethical challenges with data labor \citep{gray2019ghost, crawford2021atlas, hao2023cleaning}. 

\paragraph{The \compute subdomain shows major discrepancies among developers.} 
\meta and \stability document some aspects of compute, energy, and hardware usage, as well as the carbon footprint of model development, whereas many developers do not. 
Given the significant compute expenditure required to build many foundation models, the practice of documenting energy use and environmental impact is well-established along with associated tooling to measure these quantities \citep{lacoste2019quantifying,strubell2019energy,schwartz2020green,luccioni2023counting}. 
In spite of this, most developers do not disclose minimal, or sometimes any, details related to compute usage, particularly with respect to energy usage, carbon footprint, and environmental impact.

The broader environmental impact of building foundation models is also essential to consider; although there has been significant public attention concerning energy expenditure, other matters such as water usage may be of similar consequence environmentally \citep{luccioni2023counting}.
\citet{luccioni2022estimating} provides an excellent example, documenting the embodied emissions, dynamic consumption, and idle consumption associated with BLOOM \citep{scao2022bloom}.
Given that \bloomz is derived from BLOOM, we note the potential for \textit{documentation transience}, where prior documentation is not updated to reflect substantial changes and, therefore, does not correctly persist to the new asset. 
In particular, the additional broader environmental impact of deriving \bloomz from BLOOM is not disclosed.

\paragraph{Widespread lack of upstream transparency on data creators, data license, copyrighted data and associated mitigations, and broader environmental impact.} 
Of the \numupstreamindicators indicators, no company scores points on six of them.
These are the indicators for data creators, data license status, copyrighted data, copyright mitigations, compute usage and broader environmental impact.
For data creators, in part we believe this reflects the nascent status of methods for providing web-scale understanding of who created the data (\eg text, images) scraped from the Internet. 
However, we recognize that \huggingface in particular has taken important steps to characterize aspects of who created the data, along with associated metadata for copyright, license, and personal information, for the ROOTS corpus used to build BLOOM (though not the additional data involved in building \bloomz). 
With respect to the copyrighted data and data license status indicators, we emphasize that information related to these indicatorsis at issue in ongoing litigation. 
In particular, \stability has explicitly argued that training foundation models on copyrighted data is protected by fair use doctrine in the U.S.\footnote{See \url{https://www.judiciary.senate.gov/imo/media/doc/2023-07-12_pm_-_testimony_-_brooks.pdf} and \url{https://www.documentcloud.org/documents/23589439-openai-motion-to-dismiss} as well as \citet{lemley2020fair}.} 
Closed developers may also view information related to their data as a key competitive advantage, or be disincentivized to share this information due to a perception of legal risk.
Additionally, we note that we are surprised no developer directly discloses the compute usage in FLOPs to sufficient precision, though several disclose information that could be used to compute an estimate or upper bound. 

\edef\originalwidth{\the\pdfpagewidth}
\edef\originalheight{\the\pdfpageheight}




\eject
\pdfpageheight=\originalwidth
\newlength{\mylength}
\setlength{\mylength}{\originalheight-3.4cm}
\pdfpagewidth=\mylength
\newgeometry{margin=1.7cm}

<div id="fig-upstream-scores" class="flex items-center justify-center">\{\{< image src="images-preprocessed/f10.png" caption="\textbf{Upstream Scores by Indicator.} The scores for each of the \numupstreamindicators upstream indicators." zoomable="true" >\}\}</div>



\clearpage
\eject
\pdfpagewidth=\originalwidth \pdfpageheight=\originalheight
\newgeometry{top=2cm, head=10pt, left=2cm, right=2cm,bottom=2cm}



\eject
\pdfpageheight=\originalwidth
\setlength{\mylength}{\originalheight-3.4cm}
\pdfpagewidth=\mylength
\newgeometry{margin=1.7cm}

<div id="fig-model-scores" class="flex items-center justify-center">\{\{< image src="images-preprocessed/heatmap_allindicators_model-1.png" caption="\textbf{Model Scores by Indicator.} The scores for each of the \nummodelindicators model indicators." zoomable="true" >\}\}</div>



\clearpage
\eject
\pdfpagewidth=\originalwidth \pdfpageheight=\originalheight
\newgeometry{top=2cm, head=10pt, left=2cm, right=2cm,bottom=2cm}


\paragraph{No upstream indicators are satisfied by all developers.}
At the indicator level, there is no upstream indicator for which every developer receives points.
Of course, this is guaranteed by the presence of (multiple) developers that score 0 points on the entire upstream domain.
Even putting these 3 developers aside, there is no indicator that is satisfied by all of the remaining 7.
The indicators where the greatest number of developers score points are data curation (all but \anthropic) and model stages (all but \cohere), which both suggest that developers are generally willing to describe the basics of the overall pipeline of model development.
With that said, we take the absence of any upstream indicator where all companies score points, and the fact that 5 or more developers score no points on 30 of \numupstreamindicators upstream indicators, as strong evidence that upstream transparency is the domain with the broadest room for improvement.

\hypertarget{model-results}{\subsection{Model results}}
<div id="sec-model-results"></div>


Model indicators assess transparency regarding the function of foundation models, spanning model access, capabilities, risks, limitations, mitigations, trustworthiness and inference efficiency, as well as basic information about the model. 
The indicators in this domain comprehensively characterize the foundation model as a standalone artifact: what tasks the model can and cannot perform, what is the model's basic structure, who has access to the model, and more. 
Here, we analyze developers at the level of subdomains and indicators based on \reffig{model-scores}.


\paragraph{Model subdomains are some of the highest-scoring across the index.}
Overall, the mean score on model indicators is 14.1 out of 33 (42.7\%) and the median developer receives 12.5 points (37.9\%). 
With this in mind, several of the highest-scoring subdomains belong to the model domain.
Developers score best on \modelbasics (63\%), \capabilities (62\%), \limitations (60\%), and \modelaccess (57\%) within the domain.
These scores arise partially because of very generous indicators within these subdomains (\eg input modality, output modality, description of capabilities, description of limitations).

\paragraph{Transparency on capabilities does not translate to transparency on limitations, risks, or mitigations.}
Of the 33 model indicators, 20 are in the \capabilities, \limitations, \risks, and \modelmitigations subdomains.
Within these subdomains, \capabilities is clearly the most transparent subdomain: nearly all developers provide descriptions (9 of 10) and demonstrations (8 of 10) of multiple model capabilities, with the majority reporting evaluations (6 of 10), half reporting reproducible evaluations (5 of 10), and few providing third party evaluations (3 of 10).
In general, we see a decline in the number of developers who score the point from the most rudimentary (\ie description) to the most substantive (\ie third party evaluations) across these four subdomains.
With respect to \capabilities, while we assume most or all developers conduct internal evaluations, they may not score points on evaluations indicators because (i) they do not disclose sufficient details about internal evaluations for these evaluations to be externally reproducible, (ii) they do not assess multiple capabilities, or (iii) they do not report the results of the evaluations, perhaps due to a concern that a model may underperform competitors' models. 

With this in mind, developers consistently score worse on \limitations, \risks, and \modelmitigations indicators than on \capabilities.
For example, only \cohere receive points for demonstrating limitations, while 8 developers score points for demonstrating capabilities. 
These asymmetries where companies are more willing to share information about capabilities than limitations, risks, and mitigations are concerning, as they may lead to an inflated sense of trust in companies' foundation models. 
In fact, these asymmetries are especially pronounced for \risks (average score of 24\%) and \modelmitigations (average score of 26\%), given that these scores are considerably worse than the average scores for \capabilities (62\%) and \limitations (60\%). 

\paragraph{Developers score poorly on \trustworthiness, largely in line with \risks and \modelmitigations.}
With respect to the \trustworthiness subdomain, only \openai, \cohere, and \aitwentyone provide information about rigorous evaluations of their flagship model related to robustness, reliability, hallucinations, calibration, or explainability.
Of those developers, only \cohere and \aitwentyone provide sufficient detail for their evaluations to be deemed externally reproducible due to their use of the HELM benchmark \citep{liang2023holistic}, compared to \openai's unclear description of their evaluations of model calibration.
Given the previous asymmetry we establish around greater disclosure of capabilities as compared to limitations, risks, and mitigations, the absence of trustworthiness evaluations exacerbates these concerns.
Put together, the lack of sufficient public information on limitations, risks, mitigations, and trustworthiness makes it more likely that consumers will not have well-calibrated expectations.
In turn, this could lead to undesirable overreliance on foundation models because not enough is done to calibrate consumers on the appropriate levels of trust \citep{parasuraman2010complacency}.\footnote{See \url{https://www.theverge.com/2023/5/30/23741996/openai-chatgpt-false-information-misinformation-responsibility} as an example.}
With this said, we do acknowledge that developers may take other routes towards improving trustworthiness including methods like reinforcement learning from human feedback \citep[RLHF;]{ziegler2019finetuning, ouyang2022instructions} and constitutional AI \citep{bai2022constitutional}, though transparency is lacking on these approaches \citep{casper2023open}.

\paragraph{\modelaccess reveals slight differences beyond just release strategy.}
In aggregate, companies score 17 of the 30 points (57\%) in the \modelaccess subdomain across the 3 indicators and \numcompanies companies.
On the external model access protocol indicator, \meta, \huggingface, \openai, and \stability are the only developers to score points.
We find this particularly interesting given \meta, \huggingface and \stability release their models openly in terms of both model weights and data, whereas \openai is considerably more closed, providing only API access.
However, in particular, \openai has a clear researcher access program with a form to request access, criteria it discloses for granting access, and a period of 4--6 weeks disclosed as the expected turnaround for a decision.
This demonstrates that developers across the release spectrum \citep{solaiman2023gradient} may achieve transparency on some indicators while taking substantively different approaches.
In practice, we find that several closed developers have access forms that allow external entities greater access to the model, but these forms often lack key components of transparency that clarify the specific steps the developer will take to assess and grant applications (\eg in comparison to \openai's process).
With that said, the indicator for full external model access is exclusively achieved by the three open developers, though every developer other than \inflection provides black box access access to its model.


\paragraph{\modelmitigations are a weak point for most developers.} 
Developers on average scored just 26\% of the total available points on the five \modelmitigations indicators. 
\huggingface, \stability, and \aitwentyone score 0 points, while \cohere, \inflection, and \amazon score only the point on mitigations description, which is the most lightweight of these indicators. 
In general, we highlight an important mismatch between the many risks that are enumerated and the relatively few mitigations that are described, implemented, and/or evaluated.
Even when mitigations are described, in scoring we find the mapping between stated risks and stated mitigations is often vague or nonexistent. 
Moving forward, we hope developers will directly aim mitigations at addressing specific risks, with appropriate evaluations to confirm the efficacy of mitigations in achieving the stated goals. 


\eject
\pdfpageheight=\originalwidth
\setlength{\mylength}{\originalheight-3.4cm}
\pdfpagewidth=\mylength
\newgeometry{margin=1.7cm}

<div id="fig-downstream-scores" class="flex items-center justify-center">\{\{< image src="images-preprocessed/f12.png" caption="\textbf{Downstream Scores by Indicator.}  The scores for each of the \numdownstreamindicators downstream indicators." zoomable="true" >\}\}</div>



\clearpage
\eject
\pdfpagewidth=\originalwidth \pdfpageheight=\originalheight
\newgeometry{top=2cm, head=10pt, left=2cm, right=2cm,bottom=2cm}


\paragraph{Most model indicators are scored by some developer, though most developers score poorly on indicators related to evaluating intentional harms, mitigations, and inference efficiency.}
Of the 33 indicators in the model domain, at least one developer scores a point on 29 of them. 
Further, multiple developers score points on 27 model indicators.
The 4 indicators for which no developer scores points are (i) intentional harm evaluation, (ii) external reproducibility of mitigations evaluations, (iii) third party mitigations evaluations, and (iv) inference compute evaluation.
The 2 additional indicators for which only one developer scores points are limitations demonstration (\cohere) and external reproducibility of internal harm evaluation (\openai).
While many companies describe risks (including the risk of intentional harms), they do not share sufficient information related to evaluations of intentional harm or the reproducibility of evaluations of mitigations.
In the case of inference, we believe standards are needed akin to MLPerf \citep{reddi2020mlperf} to rigorously benchmark the inference of foundation models \citep{narayanan2023cheaply} given the key role of efficient inference and low latency in the usability of models \citep{lee2023evaluating}.
We see that \bloomz in particular provides a potential benchmark for language models by tracking the time spent for a fixed task (generating 100 tokens given a 7 token prefix) on fixed hardware (a NVIDIA A100-80GB GPU), though compute is not measured.\footnote{See \url{https://huggingface.co/blog/habana-gaudi-2-bloom}.}

\hypertarget{downstream-results}{\subsection{Downstream results}}
<div id="sec-downstream-results"></div>


Downstream indicators assess transparency regarding the use of foundation models, spanning subdomains related to distribution, policies constraining the use and behavior of the model, user interfaces, user data protection, model updates, feedback, impact, and documentation. 
Indicators in these subdomains characterize transparency related to how the foundation model is deployed and its downstream effects on the ecosystem and society. 
Our analysis is based on publicly available information about how the foundation model is distributed, how it can and cannot be used, how users can give feedback and seek redress, broader societal impacts, and the how the model affects actors downstream of the developer in the supply chain.
Here, we conduct a fine-grained analysis at the level of subdomains and indicators based on \reffig{downstream-scores}.

\paragraph{Downstream scores show less spread across developers.}
Total scores on downstream indicators are tightly clustered around the mean of 15.7 out of 35, which corresponds to 44.9\% of the \numdownstreamindicators downstream indicators. 
With the exception of \amazon (6 out of \numdownstreamindicators; 17.1\%), the other nine developers all score between 14 and 21 points.
The highest-scoring on the downstream domain is \openai at 21 points and the lowest-scoring (barring \amazon) are \aitwentyone and \inflection at 14 points.
In \refsec{correlations}, we clarify the extent to which these smaller margins in scoring discrepancies in the downstream domain are due to high agreement in indicator-level scores across companies.

\paragraph{\impact is the least transparent subdomain in the entire index.}
To clarify the downstream impact of a given foundation model, the \impact subdomain includes indicators on monitoring mechanisms, affected market sectors, affected individuals, usage reports, geographic statistics, and redress mechanisms.
Strikingly, the mean score across all developers on this subdomain is just 11\%, with 8 developers scoring points on just 1 of the possible 7 indicators and the remaining 2 scoring none of the indicators. 
No developer scores points on affected market sectors, affected individuals, usage reports, geographic statistics, or redress mechanism.
This means that there is essentially no information about how many people, sectors, and regions foundation models are impacting. 
\openai, \google, \cohere, \aitwentyone, and \inflection are the only developers to disclose a potential monitoring mechanism for tracking model use. 
And only open foundation model developers share limited information about downstream applications, whereas the rest provide no information.\footnote{We score the downstream applications indicator quite generously: all of the open developers score points because they discloses which Hugging Face "Spaces" are also using the model via Hugging Face's platform.
However, we emphasize that this is still a poor proxy for the number of applications dependent on the foundation model.} 

\paragraph{Developers are significantly more transparent about \distribution than other major dimensions of (downstream) transparency.}
Across the four major dimensions of transparency in the downstream domain (\distribution, \usagepolicy, \feedback, \impact), mean scores are on the higher end only for \distribution at 59\%, with the other three all below 50\%. 
Every developer shares information about distribution channels, or the pathways by which the model is made available to entities beyond the model developer organization.
Every developer provides terms of service that cover the distribution of its foundation model.\footnote{As with several downstream indicators, we assessed the terms of service of the primary distribution channel. For example, this meant that we assessed Microsoft Azure's terms of service for Meta.}
Most developers share information about their process for releasing their flagship model (8 of 10) as well as the developer's products and services that use the foundation model (6 of 10).
Half of developers share information about the license under which the model is distributed.

\paragraph{In spite of broad transparency on the \distribution subdomain, developers are highly opaque around release decisions.}
Within the \distribution subdomain, developers score poorly on the release decision-making protocol indicator; \huggingface is the only developer that shares information about its decision-making protocol for release.
Although there has been an extensive focus on release strategies in the literature on foundation models \citep{solaiman2019release, sastry2021release, shevlane2022structured, liang2022community-norms, liang2022condemning, solaiman2023gradient, widder2023open, seger2023open}, developers across the release spectrum share very little information about how and why they release their flagship models. 
In particular, we highlight that many of companies we assess have written about the broader topic of release, but not in a way that is precise to their specific decisions for their flagship models.\footnote{We note that following \informationfreezedate, \anthropic released information about its approach to responsible scaling: \url{https://www.anthropic.com/index/anthropics-responsible-scaling-policy}.}

\paragraph{\usagepolicy and \modelbehaviorpolicy subdomain scores are uneven across developers.}
Scores on the \usagepolicy subdomain are uneven, with all developers scoring points on the indicator for permitted, restricted, and prohibited uses, but only two (\openai and \inflection) scoring points on the usage policy violation appeals indicator.
This reflects the lack of industry standards regarding precisely how foundation model developers should restrict the use of their models. 
We found that different developers provide this information in different types of documentation, ranging from standalone Acceptable Use Policies to Content Policies to terms in the model license, and that many developers share some of this information in several different documents. 

While developers did provide some transparency on usage policies related to a user's obligations, they did not provide a similar level of transparency on the restrictions they place on their model's behavior. 
Scores on indicators in the \modelbehaviorpolicy subdomain were relatively weaker, with a mean across the 3 indicators of 23\% compared to 44\% for the 5 usage policy indicators.
\openai, \anthropic, and \inflection are the only developers who provide information about permitted, restricted, and prohibited model behaviors, while only \inflection and \stability provide information about how they might enforce such restrictions.
\openai and \anthropic are the only developers who make clear how their models are expected to behave in the event that a user violates the usage policy. 
In part, we believe the norms and standards around model behavior are rather immature, meaning that developers do not provide a clear conceptualization of if/how they impose a model behavior policy.
For example, the role of modeling decisions (\eg the use of reinforcement learning from human feedback or constitutional AI) on behaviors (\eg model refusals to specific requests) are not made clear.

\paragraph{Identical scores on the \dataprotection subdomain across all developers.}
For the \dataprotection subdomain, scores are uniform across developers, with every developer scoring points on user data protection policy, as well as permitted and prohibited uses of user data.
However, no developer scores points on usage data access protocol. 
This may reflect that few, if any, companies actually share usage data externally, meaning companies may perceive that the need to develop protocols for sharing such data is limited. 
However, developers' data protection policies include many provisions that would allow them to share such usage data, and specific protocols for how and when they do so are not transparent.

\paragraph{Developers lack transparency on the \feedback subdomain.} 
Developers score relatively poorly on \feedback indicators, scoring only 30\% of the available points. 
While every developer but \amazon has a public mechanism for collecting feedback on its model, none provide information such as a feedback summary or details on government inquiries, such as requests for user data (which social media companies disclose).
This is likely a function of how nascent the foundation model ecosystem is: companies have only been collecting feedback for a few years, and it took social media companies several years to respond to public calls for transparency around the feedback they receive from users and governments.
Moving forward, more robust transparency reporting practices that provide the public with more information regarding these forms of feedback will likely be necessary.\footnote{For example, consider the EU's DSA Transparency Database, implemented on the basis of the Digital Services Act to provide transparency on content moderation decisions: \url{https://transparency.dsa.ec.europa.eu/}.}

\paragraph{Developers are fairly transparent on the \updates subdomain.}
5 of 10 developers provide clear information about their versioning protocol, change log, and deprecation policy.
\inflection and \amazon, however, score zero points on these indicators, which may be due in part due to the face that \inflectionone and \titan are at an earlier stage of release than some other flagship models. 
While there is a wide variation in the type, specificity, and quality of documentation provided related to \updates, as with other indicators, we assess these metrics generously and allocate points on the basis of transparency alone. 

\paragraph{Developers score well on the \interface subdomain, though this may change due to deployments on mobile phones.}
Developers scored highly on \interface indicators (average score of 85\%), with more than half of developers scoring points on both indicators, which assess if users are told they are interacting with an AI system and if users are provided appropriate disclaimers. 
Developers frequently disclose to users that they are interacting with a specific foundation model by including the name of the foundation model somewhere in the user interface, while they give usage disclaimers upon sign-up for the user interface via a link to the terms of service or usage policy. 
Unlike all other indicators, we generally had to make use of step 7 in the and directly interact with developers' models via a user interface to assess these indicators. 
However, \amazon did not have a publicly available user interface in advance of \informationfreezedate, meaning that it could not receive these points. 
We initially assessed transparency of deployments on mobile devices in some cases, though we ultimately did not consider these deployments for scoring.
With that said, we highlight that the same standard for transparency of user interfaces does not currently appear to be met by mobile deployments from \openai and \inflection.
Overall, we believe in the importance of providing transparency through user interfaces as it can help foundation models avoid the formation of the "dark patterns" we have seen develop with other digital technologies \citep{10.1145/3359183}.
For example, we highlight that \anthropic does not make clear that a user is interacting with an AI system, except for the textual description "Message Claude."

\hypertarget{release-results}{\subsection{Results for open and closed developers}} <div id="sec-release-results"></div>

<div id="fig-open-closed" class="flex items-center justify-center">\{\{< image src="images-preprocessed/f13.png" caption="\textbf{Open vs. Closed by Subdomains.}  The mean score for the 3 open developers (\meta, \huggingface, \stability) and the 7 closed developers (\openai, \anthropic, \google, \cohere, \aitwentyone, \inflection, \amazon) across each of the 23 subdomains. Note: the number of indicators per subdomain varies widely." zoomable="true" >\}\}</div>




Foundation models are released by different developers using a variety of release strategies \citep{liang2022community-norms, solaiman2023gradient}.
In particular, we deliberately chose several developers that are more \textit{open} (\eg release the weights of their model, perhaps along with the data used to build the model)and others that are more \textit{closed} (\eg only provide access via an API).
The topic of release and the (reductive) dichotomy of open vs. closed has emerged as a primary topic of technical and policy research on foundation models \citep{solaiman2019release, sastry2021release, shevlane2022structured, liang2022community-norms, liang2022condemning, solaiman2023gradient, widder2023open, seger2023open}.
To clarify how transparency differs between the open developers we assess (\ie \meta, \huggingface, \stability) and the closed developers (\ie \openai, \google, \anthropic, \cohere, \aitwentyone, \inflection, \amazon), we emphasize the distinction in \reffig{open-closed}. 

\paragraph{Open developers score higher in aggregate and on every domain.}
We establish a clear trend that the open developers score higher overall, with all three being among the four highest-scoring developers (see \reffig{overall-scores}).
In particular, every open developer is nearly at least as transparent in terms of aggregate score as the highest-scoring closed developer (\openai): \meta and \huggingface are at least 5 points higher, and \stability is within a point of \openai.
Further, this trend is established more strongly through domain-level analysis, where open developers score higher on average than closed developers across all domains (\ie upstream, model, downstream).
The mean score of open developers on upstream indicators is 53\% compared to 9\% for closed developers, 51\% for open developers on model indicators compared to 39\% for closed developers, and 49\% on downstream indicators compared to 43\% for closed developers.
To ensure these trends are robust to outliers, we highlight that the trends hold even when considering medians instead of means (upstream: 50\% to 9\%, model: 48\% to 45\%, downstream: 51\% to 43\%).

We emphasize that our findings confirm common hypotheses that open developers will in general be more transparent with respect to the upstream resources required to build their models (which also aligns with some making the data they use publicly available), but our findings dispute hypotheses that open developers will be less transparent on downstream matters due to their weaker control over downstream use.
While we believe that closed developers providing APIs are better positioned to collect information on the downstream use of their models, in practice these developers do not disclose this information to provide greater public transparency.

\paragraph{Open developers score higher on most subdomains.}
Open developers score higher than closed developers on 15 of the \numsubdomains subdomains, which account for 68 of the 100 indicators. 
The mean score of closed developers is higher than that of open developers on indicators in the subdomains of \capabilities, \risks, \modelmitigations, \trustworthiness, \usagepolicy, \modelbehaviorpolicy, and \documentation.
We highlight that these seven subdomains point to two broader themes: closed developers in some cases may be higher-resourced or face stronger incentives to proactively address certain matters around responsible AI (\eg \risks, \modelmitigations, \trustworthiness).
In addition, closed developers often have a closer coupling between the foundation model we assessed and downstream services, meaning that certain user-related aspects of transparency are potentially of higher priority (namely the \usagepolicy). 
For example, many closed developers provide products built on top of their flagship foundation model, providing users of their platforms and clients who license their proprietary foundation models with an opportunity to push for transparency.

The mean score of open developers is higher than closed developers on every upstream subdomain, with major score differentials especially for the \data, \compute, and \methods subdomains.
Looking at the difference in average scores by release strategy, we see large disparities in favor of open models in each domain, with the largest gaps for \dataaccess (67\% to 0\%), \methods (92\% to 29\%), and \datamitigations(67\% to 7\%).
We also observe similar large differentials (40\%+) for \modelbasics, \modelaccess, and \updates.
While less stark, we highlight the superior transparency on average for the \distribution subdomain as especially surprising given that closed developers maintain greater control over distribution by virtue of being closed.

\paragraph{Indicator-level analysis further demonstrates the disparity between open and closed developers.}
At the indicator level, the median open developer outscores the median closed developer on 28 indicators (18 upstream, 7 model, 3 downstream), while the median closed developer scores higher on just 6 indicators (0 upstream, 2 model, 4 downstream). 
The median open developer and the median closed developer both score points on 22 indicators and neither scores points on 44 indicators. 

\paragraph{The open developers we assessed provide greater transparency than their closed counterparts.}
Overall, each level of analysis points in the same direction: open developers are reliably more transparent.
In particular, we highlight that the release of assets (\eg model weights, data, code) may be significantly underweighted in terms of its broader transparency effects.
Our findings dispel the belief that closed developers are more likely to be transparent about downstream matters due to their greater control over deployment, while emphasizing that both open and closed developers continue to be extremely opaque in terms of the downstream impact of their foundation models.
With this in mind, we caution that our assessment is necessarily based on the practices of some of the highest-resourced open and closed developers, so these trends should not be taken as sufficient evidence to claim that all open developers are more transparent than closed developers.
And we believe there is ample opportunity for closed developers to address these gaps in transparency as we discuss in \refsec{recommendations-developers}.

\hypertarget{correlations}{\subsection{Correlations between companies}}
<div id="sec-correlations"></div>

<div id="fig-overall-correlations" class="flex items-center justify-center">\{\{< image src="images-preprocessed/all_company_correlations.png" caption="\textbf{Correlations between Companies.} The correlation between the scores for pairs of companies across all indicators. Correlation is measured using the simple matching coefficient (\ie agreement rate), which is the fraction of all indicators for which both companies receive the same score (\ie both receive the point or both do not receive the point)." zoomable="true" >\}\}</div>


\paragraph{Measuring correlations.}
The \numindicators $\times$ \numcompanies scores introduces data-driven structure.
In particular, it clarifies relationships that arise in practice between different regions of the index.
Here, we consider the \textit{correlations}, in scores, focusing on company-to-company similarity for simplicity.
For example, if two companies receive similar aggregate scores, is this because they satisfy all the same indicators or do they score points on two very different sets of indicators?

In \reffig{overall-correlations}, we plot the correlation between every pair of companies.
To measure correlation, we report the simple matching coefficient (SMC) or the agreement rate.
The SMC is the fraction of the \numindicators indicators for which both companies receive the same score (\ie both receive a zero or both receive a 1). 
As a result, a SMC of 0 indicates there is no indicator such that both companies receive the same score and a SMC of 1 indicates that for all indicators both companies receive the same score. 
For this reason, the correlation matrix is symmetric and guaranteed to be 1 on the diagonal. 

To systematically analyze the results, we consider three patterns in the correlation matrix:
(i) individual cells with very small or very large values (\ie highly similar or highly dissimilar company pairs),
(ii) individual rows with consistently small, consistently large, or highly varied values (\ie unusual companies),
and
(iii) structural patterns across the correlation matrix.

\paragraph{Strongly correlated company practices.}
In terms of the most correlated company pairs, we identify a few regions of the correlation matrix.
First, we identify the three most correlated pairs: 
(\cohere, \aitwentyone; SMC = 0.87),
(\aitwentyone, \amazon; SMC = 0.85),
and
(\inflection, \amazon; SMC = 0.85).
These pairs are all among the four lowest-scoring companies, though we note the inclusion of \cohere is interesting given \cohere's overall score (34) is closer to the average (37) and the middle-scoring group of companies (\ie including \google and \anthropic).
In addition to these pairs, if we consider the other highly-correlated pairs (SMC $\geq$ 0.8), we identify:
(\huggingface, \stability; SMC = 0.80),
(\openai, \anthropic; SMC = 0.80),
and
(\google, \anthropic; SMC = 0.80).
In particular, we observe that the company correlations identify clear structure: \huggingface and \stability are the only two developers to release both data and models openly, and the trio of \openai, \google, and \anthropic are the three members of the Frontier Model Forum that we assess.

\paragraph{Weakly correlated company practices.}
In contrast, we see that the least correlated pairs (SMC < 0.6) are pairs involving \meta and the three lowest-scoring developers as well as pairs involving \huggingface and five of the seven closed developers (\openai, \cohere, \aitwentyone, \inflection, \amazon).
These are all pairings between an open and a closed developer.
More broadly, we highlight that \meta is the sole developer that is not correlated with SMC at least 0.80 with any other developer, with the most similar other developer being Google at 0.78 (see below for further analysis). 
This means \meta is rather unique in terms of the indicators where it scores points; it is the sole developer that is not strongly correlated with any other company, even including the two other open developers.
Nevertheless, the least correlated pair of companies still agrees in over half the indicators (SMC = 0.54), which is not surprising given that all the companies are opaque (\eg if all the companies all scored 0 on every indicator, they would necessarily be perfectly correlated with SMC = 1).

<div id="fig-upstream-correlations" class="flex items-center justify-center">\{\{< image src="images-preprocessed/upstream_company_correlations.png" caption="\textbf{Correlations between Companies (Upstream Indicators).} The correlation between the scores for pairs of companies across all indicators when only considering upstream indicators. Correlation is measured using the simple matching coefficient (\ie agreement rate), which is the fraction of all indicators for which both companies receive the same score (\ie both receive the point or both do not receive the point)." zoomable="true" >\}\}</div>



\paragraph{Upstream correlations.}
In \reffig{upstream-correlations}, we plot the correlation between every pair of companies when considering only indicators from the upstream domain.
Since the four lowest-scoring companies overall also score zero (or near-zero in the case of \cohere) points on the upstream indicators, they are necessarily extremely correlated.
For the same reason, the extent to which the remaining six companies are correlated with the three lowest-scoring companies is precisely proportional to their own opacity on the upstream domain.
Looking at the three companies that score in the middle overall (\google, \anthropic, \cohere), we see their indicator-level transparency is reasonably correlated.
We also see a similar trend where \openai, \google, and \anthropic are correlated, though in this case \openai and \cohere are even more correlated with an SMC of 0.81.
Interestingly, while the three open developers score much higher overall than any of the seven closed developers for the upstream domain, the correlations between them are somewhat different than in the other domains: there is a weaker correlation between \huggingface and \stability, and \meta's correlation with \openai and \stability is stronger than its correlation with \huggingface.
Despite the fact that \meta and \huggingface are the two highest-scoring companies on upstream, they are not especially correlated (SMC = 0.53) in that domain.
These discrepancies coincide with the indicators where \huggingface scores points and the other two open developers (\meta, \stability) do not, namely those relating to data sources and data labor. 
Given the large spread in scores across developers in the upstream domain, we see the related effect that the correlations can be quite variable with some at or near 1 and others well below 0.5 (minimum upstream SMC = 0.34).

<div id="fig-model-correlations" class="flex items-center justify-center">\{\{< image src="images-preprocessed/model_company_correlations.png" caption="\textbf{Correlations between Companies (Model Indicators).} The correlation between the scores for pairs of companies across all indicators when only considering model indicators. Correlation is measured using the simple matching coefficient (\ie agreement rate), which is the fraction of all indicators for which both companies receive the same score (\ie both receive the point or both do not receive the point)." zoomable="true" >\}\}</div>



\paragraph{Model correlations.}
In \reffig{model-correlations}, we plot the correlation between every pair of companies when considering only indicators from the model domain.
In contrast to the upstream correlations, we see a much more varied picture.
First, much like the overall correlations, we see strong correlations for (\cohere, \aitwentyone; SMC = 0.85) and (\inflection, \amazon; SMC = 0.85) but not necessarily for the other pairs between these four companies. 
Among the three Frontier Model Forum companies, we see a very strong correlation of 0.88 between \google and \anthropic, a fairly high correlation of 0.79 between \openai and \anthropic, but a considerably lower correlation for the third pair of \openai and \google at 0.67.
These trends, where \anthropic is highly correlated with both, but \openai and \google are not necessarily correlated, mirror what we observe for the overall correlations.
Similar to what we observed for the overall correlations, \huggingface and \stability are quite correlated as well with a correlation of 0.85, and \meta is not particularly correlated with any company (the highest is \huggingface at 0.76).


<div id="fig-downstream-correlations" class="flex items-center justify-center">\{\{< image src="images-preprocessed/downstream_company_correlations.png" caption="\textbf{Correlations between Companies (Downstream Indicators).} The correlation between the scores for pairs of companies across all indicators when considering only downstream indicators. Correlation is measured using the simple matching coefficient (\ie agreement rate), which is the fraction of all indicators for which both companies receive the same score (\ie both receive the point or both do not receive the point)." zoomable="true" >\}\}</div>



\paragraph{Downstream correlations.}
In \reffig{downstream-correlations}, we plot the correlation between every pair of companies when considering only indicators from the downstream domain.
The downstream correlations surface considerably different trends from the overall correlations or those for the other two domains.
In particular, we first highlight that \meta is strongly correlated with \google in their scores on downstream indicators.
Given that several of the downstream indicators related to broader corporate practices, the similarities between these companies may contribute to this result, though both companies are not strongly correlated with \amazon, the other Big Tech company we assess.
Relatedly, we see fairly strong correlations between \openai and \anthropic, which again may relate to their fairly similar business practices mapping onto specific downstream indicators (\eg indicators in the \modelbehaviorpolicy subdomain).
On the other hand, akin to the upstream subdomain, we see that \inflection is especially dissimilar from all of the open model developers (\meta, \huggingface, \stability). 
And, unlike the other correlation matrices, \openai and \amazon are more dissimilar than usual. 
Overall, while we do not observe it as clearly in the other correlation analyses, here we see all three pairs of open developers are highly correlated:
(\meta, \huggingface; SMC = 0.89),
(\meta, \stability; SMC = 0.83),
(\huggingface, \stability; SMC = 0.89).
This may reflect that all open developers have shared transparency challenges on specific indicators within the downstream domain (\eg monitoring mechanism and model behavior policy enforcement), perhaps stemming from the weaker control they have over downstream use. 
Overall, we find the complex structure and heterogeneity in the correlation for the downstream domain especially intriguing, given the aggregate scores for this domain are the most tightly clustered (see \refsec{downstream-results}).
That is to say, disaggregated indicator-level analysis is especially revealing for this domain compared to domain-level analysis.

\clearpage
\newlist{myitemize}{itemize}{1}
\setlist[myitemize]{label=$\bullet$, itemsep=5pt}

\hypertarget{recommendations}{\section{Recommendations}}
<div id="sec-recommendations"></div>

The design of our indicators, execution of our assessment, and analysis of our results provide a rich supply of ideas for how to improve transparency. 
We center our attention on foundation model developers and deployers, along with policymakers.
For each group of stakeholders, we provide concrete recommendations on the basis of this research.
Additionally, we encourage researchers to scrutinize our overall approach in order to clarify how transparency for foundation models can be better understood in the future. 

\hypertarget{recommendations-developers}{\subsection{Recommendations for foundation model developers}}
<div id="sec-recommendations-developers"></div>

By directly scoring foundation model developers, we provide explicit feedback on where developers are and are not transparent.
In itself, this provides immediate guidance for these \numcompanies companies in the context of their current flagship foundation models.
Moreover, the \projectname provides valuable insights for these companies to consider related to their other models and future releases; it also has bearing on how foundation model developers that we did not assess can promote transparency. We provide 10 specific recommendations for foundation model developers.

\paragraph{1.\phantom{X}Increase transparency for existing foundation models.} 
\begin{myitemize}
\item As our results show, the development and use of major foundation model developers' current flagship models is opaque. 
Developers should remedy this situation by releasing more information about the systems that are central to today's foundation model ecosystem. 
Increasing the transparency of future releases will not resolve this issue as there are hundreds of products, services, and other models that are built on top of today's flagship models.\footnote{Developers that signed on to the White House's first round of voluntary commitments (including Amazon, Anthropic, Google, Inflection, Meta, and OpenAI)have pledged only to improve transparency "for all new significant model public releases within scope," where the scope is defined as "generative models that are overall more powerful than the current industry frontier (e.g. models that are overall more powerful than any currently released models, including GPT-4, Claude 2, PaLM 2, Titan and, in the case of image generation, DALL-E 2)." Developers that signed on to the White House's second round of voluntary commitments (including Cohere and Stability AI) have pledged only to improve transparency for "generative models that are overall more powerful than the current most advanced model produced by the company making the commitment." See \url{https://www.whitehouse.gov/wp-content/uploads/2023/07/Ensuring-Safe-Secure-and-Trustworthy-AI.pdf} and \url{https://www.whitehouse.gov/wp-content/uploads/2023/09/Voluntary-AI-Commitments-September-2023.pdf}}
\item Developers should begin by focusing on low-hanging fruit, such as clarifying ambiguous language in their documentation, centralizing existing information sources, and sharing information about models that poses minimal concerns related to market competitiveness or legal risk.
\footnote{For example, Anthropic released significantly more information about Claude 2 than its previous flagship model, Claude, including in the form of a model card.} 
Developers should also be clear about why they will not release certain information about their foundation models; developers should explicitly state the subdomains where they do not release information and explain why they do not do so. 
\end{myitemize}
\paragraph{2.\phantom{X}Increase transparency for future foundation model releases.}
\begin{myitemize}
\item Developers should substantially increase the transparency of future foundation model releases. Wherever possible, they should publicly disclose information related to the 100 indicators we outline as well as additional information they feel is important to share with the industry, the public, and governments. This might look like taking a transparency-first approach in which the developer prioritizes transparency throughout the model development process and includes transparency as an important performance metric for research teams.\footnote{One relevant analogy is to the development of open foundation models. Much as some developers begin the process of building a foundation model with the intention of making all model assets openly available, then subsequently decide if the risks of making a model asset openly available outweigh the potential benefits, developers could begin the development process with the assumption of maximum transparency and remove only some items along the way \citep{klyman2023open}.}
\item Profit-oriented developers commonly argue that certain forms of transparency can endanger their competitive advantage.
Nevertheless, developers have a basic responsibility to weigh this concern against the the risks posed by their technology to society and the benefits of increasing societal understanding of this technology via transparency.
These risks should determined by not only the developer but also the assessment of third party experts. 
Voluntary access for \emph{independent}, third party audits (\ie auditors not selected by the developer itself), can achieve a greater degree of transparency, and safeguard competition concerns with non-disclosure agreements.
We would also argue audits are not always a good substitute for public transparency, and developers' arguments around competitive advantage should be carefully assessed for each indicator of transparency. 
These arguments are a common refrain to avoid meaningful community discussion about widespread practices that do not in actuality endanger competitive advantages.
\end{myitemize} \clearpage
\paragraph{3.\phantom{X}Follow industry best practices with respect to transparency.}
\begin{myitemize}
\item Our findings suggest that every developer could significantly improve transparency by drawing on different approaches to transparency from across the industry. 
At least one developer scores points on \numfeasible of our 100 indicators: where developers are struggling to increase transparency in a specific issue area, they should look to developers that have already done so.
\item While the foundation model ecosystem is nascent, some developers have outlined best practices for responsible development that relate to transparency. For example, in their "Joint Recommendations for Language Model Development," OpenAI, Cohere, and AI21 Labs state that developers should "publish usage guidelines and terms of use ... document known weaknesses and vulnerabilities ... [and] model and use-case-specific safety best practices." (See \refapp{transparency} for additional examples of calls from developers for transparency.)
\end{myitemize}
\paragraph{4.\phantom{X}Work with deployers to increase transparency.} 
-  In cases where a developer is not the sole deployer of a foundation model, the developer should partner with deployers to increase transparency.
- For example, developers should attempt to require that deployers disclose usage statistics and provide usage disclaimers. Developers might do so through legal agreements that they sign with deployers that grant deployers the right to offer the foundation model.
- If a developer has little leverage over larger deployers it should consider partnering with similarly situated developers to increase their collective bargaining power.
- Without such efforts, it may be difficult for a developer to assess the downstream impact of its foundation models.
\paragraph{5.\phantom{X}Work with downstream developers to increase transparency.} 
-  Foundation model developers should make it easy for downstream developers to be transparent in their release of fine-tuned models. In addition to increasing transparency for their own models, foundation model developers should release documentation to help downstream developers be more transparent and actively encourage them to do so.
\paragraph{6.\phantom{X}Work with regulators to increase transparency.} 
-  While we believe that the public is entitled to information about each of the indicators of transparency that we examine, we recognize that it is unlikely that every foundation model developers will publicly release all of this information.
- In some cases, foundation model developers may argue the risks of disclosing such information are too great to justify public release.
- In many such cases, developers should still share this information with regulators such that governments have sufficient information to adequately scrutinize developers in the public interest.
\paragraph{7.\phantom{X}Use transparency to improve trust, safety and reliability.} 
-  Sharing internal practices, documentation, and details about risks can lead to short term criticism and negative media coverage, but in the long term it can foster greater community trust than is possible with a more opaque approach.
- Investigative journalists will eventually expose practices that lead to systemic harms, and these harms are often exacerbated the longer they remain hidden, as illustrated by the Facebook Files \cite{wsj2021fb}.
- Foundation models are technologies that could cause widespread harm, and the evidence suggests that safety and reliability will require dedicated and strong forms of transparency from foundation model developers. \clearpage
\paragraph{8.\phantom{X}Dedicate resources to continue improving transparency over time.} 
-  As technologies and risks rapidly evolve, the varieties of and baselines for meaningful transparency will also change.
- Well-resourced developers should dedicate personnel to adapting their documentation and releases to take account of this shifting landscape, rather than adhering to static benchmarks.
- Low-resourced developers should seek out funding in order to similarly improve transparency.
\paragraph{9.\phantom{X}Work to improve transparency in the foundation model ecosystem.} 
\begin{myitemize}
\item There are many areas where transparency is sorely needed, ranging from the downstream impact of foundation model releases to the use of human labor in producing the data used to build foundation models. 
One cross-cutting issue is the fact that developers do not exist in a vacuum: the foundation models a developer releases depend on and significantly affect other parts of the ecosystem.
Taking this into account, developers should increase transparency as a means of improving the health of the overall ecosystem.
\item Developers should use semantic versioning for their models (as is the norm in software engineering) such that there is no ambiguity as to the version of the model that is being distributed.
Developers should also give as much notice as is practicable (\eg 3 months notice) in advance of deprecating models in order to give any downstream dependencies adequate time to migrate to a new version.
\item Developers should release an artifact alongside their foundation models that includes information about models' upstream and downstream dependencies \citep{bommasani2023ecosystem}.
Information about the datasets, software frameworks, and applications the model depends upon, as well as products, services, and other models that depend upon the model, are essential for effective supply chain monitoring. 
\end{myitemize}
\paragraph{10.\phantom{X}Use the Foundation Model Transparency Index to increase transparency.} 
\begin{myitemize}
\item The Foundation Model Transparency Index provides an extensive taxonomy of the key elements of transparency in the field. 
We encourage developers to score their non-flagship models on the index and see where they have room for improvement. 
\item Each indicator contains significant detail that developers can utilize to increase transparency in specific issue areas. For quantitative metrics, indicators include information regarding the appropriate unit of measurement and level of precision. For qualitative metrics, indicators often provide de facto instructions for how to clearly share information about a specific subdomain with the public.
\end{myitemize}
\clearpage

\hypertarget{recommendations-deployers}{\subsection{Recommendations for foundation model deployers}}
<div id="sec-recommendations-deployers"></div>

Foundation model developers are not the only actors with a responsibility to promote transparency: deployers of foundation models such as cloud services providers and companies that license foundation models from developers also have a significant role to play. 
Although deployers cannot unilaterally increase transparency as they are not the party responsible for building a foundation model, there are still some tools at their disposal for doing so and they should think seriously about the implications of relying on systems for which there is little publicly available information. 

\paragraph{1.\phantom{X}Assess the risks of deploying a foundation model without adequate transparency.} 
-  Deployers that make use of a developer's foundation model in their products and services should conduct pre-deployment risk assessments that include specific assessments of risks stemming from a lack of transparency. These risks may include increased legal liability for difficult-to-explain model behaviors, reduced trust from users due to the product's opacity, and lower product performance without adequate information about the data used to build the model.
 \paragraph{2.\phantom{X}Require sufficient transparency in working with foundation model developers} 
\begin{myitemize}
\item Foundation model deployers should work with developers to increase the level of transparency regarding their models. 
It is not only in a deployers' interest for developers to share information bilaterally, but also for developers to be transparent with the public about the risks and limitations of their models. 
Deployers themselves can help developers increase transparency by sharing usage statistics.
\item Deployers should go beyond information sharing requests to improve transparency. 
For example, deployers should aim to negotiate contracts with developers that require developers to publicly share information that is relevant to the developers' customers as well as the broader public, such as information regarding \updates, changes in \usagepolicy, and \impact. 
In cases where deployers have little leverage over larger developers they should consider partnering with similarly situated deployers to increase their collective bargaining power. 
\end{myitemize}
 \paragraph{3.\phantom{X}Do not put undue trust in opaque foundation models.} 
-  Some deployers may take a foundation model from a reputable company at face value, assuming that all of the relevant information about that system is available to deployers and regulators.
- This could be a serious misjudgment: as our findings show, developers are overwhelmingly not transparent about the development and use of their foundation models.
- Assuming that a model complies with regulatory requirements regarding information sharing could come with substantial legal risk; for example, if new regulations primarily place information sharing requirements on deployers, they may face legal exposure related to their deployment of opaque foundation models.
- While developers are presumably more transparent in their relationships with deployers than in their public facing documentation, this is no guarantee that relevant information is shared across the 23 subdomains we identify.
\clearpage
\hypertarget{recommendations-policy}{\subsection{Recommendations for policymakers}}
<div id="sec-recommendations-policy"></div>


Policymakers across the United States, China, Canada, the European Union, the United Kingdom, India, Japan, the G7, and many other governments have already taken specific actions on foundation models and generative AI (see \refapp{transparency}).
Evidence-driven policy that is grounded in a rich and sophisticated understanding of the current foundation model market is likely to achieve the best outcomes.
As a result, our extensive characterization of transparency provides three core insights: (i) what aspects of transparency are present in status quo absent regulatory intervention, (ii) if mandated and enforced, what aspects of transparency would change relative to the status quo, and (iii) what substantive requirements beyond transparency would be most appropriate given the newfound transparency?
We hope that lawmakers will draw on the information we aggregate in the \projectname to better inform policy initiatives.
To be clear, our intent is to not to make a claim about whether specific governments should or should not regulate foundation models at this time, though some policy intervention is likely needed. 
Nor is our intent to recommend broad disclosure requirements, which could cause substantial harm if they are implemented without regard for differences in developers' business models and their level of financial resources, or without adequate government support for regulatory compliance.
Our view is that a better understanding of the status quo will lead to smarter policy, which leads to the following recommendations.

\paragraph{1.\phantom{X}Transparency should be a top priority for AI legislation.} 
\begin{myitemize}
\item Mechanisms to promote transparency should be among the suite of policy tools that lawmakers use to encourage responsible development of foundation models \citep{engler2023casc, hacker2023gpt}.
Unlike many other policy tools, transparency can be relatively low cost---where developers already possess the relevant information, sharing it does not require data collection. 
Another advantage of pro-transparency policies are that they can help solve collective action problems with respect to sharing information with the public. 
If one developer shares much more information about its foundation model with the public, then it could theoretically be penalized by investors or scrutinized by regulations for having more information about the model's risks and limitations publicly available than its competitors. 
As a result, a developer may be hesitant to be a first mover on transparency if its competitors are steadfast in maintaining opacity. 
By contrast, if that developer's peers must also share information about the risks and limitations of their foundation models, there is much less potential for transparency to represent a competitive disadvantage. 
\item Transparency is a fundamental prerequisite for accountability, robust science, continuous innovation, and effective regulation. 
With additional information about companies' business practices, the impact of their foundation models, the resources used to build models, and the AI supply chain, governments would be much better positioned to enact comprehensive AI regulations. 
\item Policymakers have a responsibility to ensure that the public has adequate information about extremely powerful AI systems that hundreds of millions of people use. 
\end{myitemize}
\paragraph{2.\phantom{X}Regulators should enforce existing regulation to promote transparency for foundation model developers.} 
-  Governments already have substantial authority to require companies to share information about their business practices \cite{ho2012fudging, hess2019ttrap, irion2022algoff}.
- For example, in recent years data protection authorities have increased their efforts to regulate the development and use of AI \citep{zanfir-f2023fpf}; they should consider using these authorities to solicit additional information from foundation model developers regarding the data they use to build foundation models and the labor that goes into producing that data.
- Similarly, sectoral regulators should consider scrutinizing the deployment of foundation models within their purview and require transparency where appropriate.
 \paragraph{3.\phantom{X}Policymakers should be realistic about the limits of transparency.}
\begin{myitemize}
\item Transparency is not an end in itself. 
While having more information about companies' business practices and the foundation model ecosystem will undoubtedly be helpful, the most significant benefits from transparency will stem from the ways in which it elicits changes in business practices and promotes responsible development and use of foundation models.
\item Transparency is not a viable alternative to substantive change.
Some interest groups and policymakers have nonetheless pushed for transparency requirements as a form of "light-touch" AI regulation.
Rather than mandating that companies change their policies and practices, this approach would merely require some level of information sharing with the government.
But transparency is only useful insofar as the information it yields is actionable.
Increased transparency can help policymakers have sufficient information about the state of the industry that many governments seek to regulate.
\item While transparency requirements may appear more feasible and even-handed than other policy interventions in the near term, policymakers should recognize that they are likely insufficient to reduce harm in many areas. 
Even if companies share more information about the impacts of their models on workers and the environment, that may not lead them to improve working conditions or reduce emissions. 
Policymakers should consider measures beyond transparency requirements in a wide variety of areas while balancing other important equities related to competition and algorithmic justice. 
\end{myitemize}
\paragraph{4.\phantom{X}Governments should craft a policy architecture that enables responsible development of open foundation models, which will in turn promote transparency.}
\begin{myitemize}
\item Open foundation models are more transparent than closed foundation models, often by a significant margin. 
This means that policymakers with an interest in transparency should be hesitant to impose regulations on foundation model developers or deployers that make it considerably more difficult to build open foundation models. 
Measures that substantially increase the legal risk of developing open foundation models by holding foundation model developers liable for model outputs or by requiring comprehensive monitoring of downstream use may ultimately undermine transparency.
\item Pro-competitive policies such as those that encourage a variety of different business models in the foundation model ecosystem can promote transparency. 
If there are only a few major technology companies that develop flagship foundation models, it will be easier for those companies to circumvent transparency rules by coordinating their activities. 
For instance, a handful of major closed developers could agree that a certain level of transparency is sufficient to satisfy their goals and to meet regulatory requirements, leading them to obfuscate their business practices in similar ways.
If the foundation model ecosystem is dominated by a few incumbents, it will also be easier for those incumbents to jointly engage in regulatory capture as there will be no countervailing narrative from other developers in the ecosystem.
By contrast, policies that result in a diverse array of open and closed foundation model developers could create a positive feedback loop for transparency.
The higher level of transparency of open developers can help draw attention to the lack of information available about the resources required to build closed foundation models.
Some closed developers in this environment may see it as in their interest to share more information about their models in order to engender more trust in their products and services, which can in turn push less transparent closed developers to alter their business practices.
\end{myitemize}
\clearpage
\hypertarget{impact}{\section{Impact}}
<div id="sec-impact"></div>

The \projectname characterizes the transparency of foundation model developers at present.
While this descriptive work already yields significant insights and value, our ambition for this work is to drive change. 
In short, our objective is to \textit{improve} transparency in the foundation model ecosystem: we believe improved transparency will result in better science, more innovation, greater accountability, and ultimately give society greater collective confidence that this promising technology can truly advance the public interest.
To achieve these lofty goals requires changing the conduct of powerful organizations.
As with many similar efforts to drive change, we have conceptualized a specific theory of change and have considered specific limitations and risks of our work.
Consistent with the work's spirit of transparency, we describe both matters plainly below. 

\hypertarget{change}{\subsection{Theory of change}}
<div id="sec-change"></div>

Assessment of any kind naturally characterizes the status quo.
However, our intent is for our assessment to drive change, especially given that our most fundamental finding is that there is insufficient transparency in the foundation model ecosystem.
\citet{bommasani2022evaluation} argues that evaluation and assessment can drive change if there is sufficient uptake: we specifically articulate how assessment can motivate improvement through different forms of uptake.

\paragraph{Assessment motivates improvement.}
By quantifying transparency and simultaneously scoring many developers, we hope that these organizations will improve their transparency scores over time.
We aim to provide a characterization of the status quo that is broadly legible across the ecosystem by directly comparing organizations' transparency scores. 
Furthermore, specific areas where there is pervasive opacity, or where specific companies are less transparent than their counterparts, are prime targets for public pressure and scrutiny. 

With respect to AI companies, we believe that certain teams and employees will play an outsized role in shaping transparency practices.
Namely, responsible AI teams along with other teams that address ethics and safety are likely to shape many company-wide practices on transparency, including for their flagship foundation models.
For this key group of individuals, we hope the \projectname provides a concrete list of indicators to proactively consider in making decisions.
At present, we believe that some companies are not transparent about certain issue areas not because of specific countervailing concerns (\eg profits, privacy, safety) but because they have not explicitly considered whether they should be transparent on this issue. 
In these cases, we believe the index provides a structured, well-argued resource that responsible AI teams can directly consider in making decisions around transparency.
The index also provides an extensive account of why these specific indicators are valuable, which could help responsible AI teams advocate for greater transparency within their organizations.
To be concrete, linking outward-facing transparency reporting with internal-facing company tracking could be a natural outcome where our index could bring about desired change while adding minimal overhead for these companies.

Indexes draw power from their subsequent iterations, allowing for improvements to be clearly measured and acknowledged over time.
In the fast-moving foundation model ecosystem, subsequent versions could be motivated by (i) changes in the indicators, (ii) changes in the key companies to assess, (iii) changes in the flagship foundation models of those companies, and (iv) changes in the underlying materials for a specific company. 
As a result, we believe the maintenance and subsequent versions of the \projectname will be necessary for its sustained impact.
We have not yet determined an exact cadence and strategy, though we will conduct future versions of the index.\footnote{See \indexUrl~for the latest details.} 

\paragraph{Assessment guides standards and mandates.}
Fundamentally, the \projectname assesses companies on metrics of transparency that are selected and evaluated based on our judgments as experts in this domain.
With this in mind, the indicators selected as well as the results could directly inform more formal processes.
For instance, policymakers around the world are considering a spectrum of voluntary commitments, industry standards, and mandatory requirements for foundation model developers.
Many current policy efforts, across the varying levels of voluntary and mandatory requirements, explicitly name transparency as a top-level priority and directly identify specific indicators and subdomains covered by our index (see \refapp{transparency}).

Our index is likely to be more comprehensive, more fine-grained, and more empirically grounded than most ongoing policy initiatives.
As a result, our index provides direct value to policymakers.
In selecting requirements, policymakers can use the index to explore the broader universe of potential transparency and disclosure requirements.
In defining requirements, policymakers can use the index to explore specific definitions as well as edge cases that may complicate a requirement. 
And, in ultimately deciding which organizations to regulate and how to enforce regulations, policymakers can look at the current status quo to efficiently allocate resources.

As a brief example, consider the European Parliament's position for the EU AI Act,\footnote{\url{https://www.europarl.europa.eu/doceo/document/TA-9-2023-0236_EN.pdf}} which was adopted by the Parliament on June 14, 2023 by a vote of 499 in favour, 28 against and 93 abstentions.\footnote{\url{https://www.europarl.europa.eu/news/en/press-room/20230609IPR96212/meps-ready-to-negotiate-first-ever-rules-for-safe-and-transparent-ai}}
\citet{bommasani2023eu-ai-act} provide an initial analysis of potential compliance with the the Act as proposed in the context of foundation model developers.
Given this legislative proposal, European lawmakers might recognize that topics related to upstream labor and downstream impact, which are covered in the \projectname, are not adequately addressed in the draft AI Act.
Policymakers might also acknowledge that requirements to disclose a summary of any copyrighted training data are too vague and a more specific definition, such as the definition we provide in \refapp{indicators}, may be desirable to improve compliance.
And, finally, policymakers might view the results of how open and closed developers fare in deciding which requirements are best targeted at which developers along the release spectrum.
Overall, much as transparency is instrumental for key societal objectives like public trust, we believe the \projectname can be similarly instrumental for key societal processes like sound policy-making.

\hypertarget{limitations}{\subsection{Limitations and risks}}
<div id="sec-limitations"></div>


\paragraph{Equating transparency and responsibility.} 
Because we foreground transparency in our assessment of developers and their flagship models, it is likely that some will misinterpret the \projectname as a measure of the responsibility of companies. 
This is not the case for a number of reasons; most importantly, we award points on the basis of whether a developer is transparent about each indicator, not whether it has responsible business practices tied to that indicator.
Concretely: if a developer discloses that it pays data laborers just one cent per hour, it would score points on the wages indicator under our methodology, while a developer that pays data laborers \$20 an hour but does not make that information publicly available would score no points. 

This means that one risk of our approach is that it could incentivize developers to be transparent in performative ways that merely increase the amount of information available about their flagship models but do not reflect an effort on the part of the developer to substantively improve its business practices. 
Nevertheless, we believe that additional information about each of these indicators is an invaluable first step towards understanding how developers build and use foundation models. 
This will in turn allow many other evaluations of responsible business practices, in which the level of transparency should be but one factor.

\paragraph{Transparency-washing.} 
There is no guarantee that improved transparency of foundation models will result in more responsible development.
As critics of transparency-washing have persuasively argued \citep{zalnieriute2021transparency}, major technology companies have used transparency to create the illusion that they are responsible players with the public's best interest at heart.
In this way, transparency can be a shield against further scrutiny, helping to convince the public that foundation models are safe and trustworthy when they may not be.

Similarly, companies may use transparency as a shield against comprehensive regulation. 
Companies could face substantial costs if they were required to increase pay for data laborers or forego certain risky use cases for their foundation models, leading some to argue that governments should simply require transparency in these verticals.
However, transparency alone will not change a business' fundamental incentives and, if used to water down regulation, can perpetuate harm.
Notwithstanding this risk, transparency may be a more appropriate regulatory option for many of the indicators we consider given the early stage of the foundation model ecosystem and the risk that substantive requirements will disproportionately damage small and open developers.

\paragraph{Gaming the index.}
Moving forward, developers might attempt to game the \projectname without actually improving transparency. 
They could do this by clarifying that they do not share information about certain practices and giving a justification for doing so. 
Developers might also exploit the fact that indicators are relatively generous, meaning that they could share minor additional information on indicators that are comparatively easy to satisfy without meaningfully improving transparency.
Since scores could theoretically be gamed in this way, it is important to consider the Foundation Model Transparency Index in conjunction with other metrics of companies' business practices. 

\paragraph{Binary scoring.} 
The fact that each indicator is binary limits the amount of information that each score can reflect when compared with more expressive scoring schemes.
For the same indicator, it is often the case that several developers share much less information than others but they all score one point nonetheless as they cross the threshold for receiving points.
Conversely, in certain instances developers disclose some information related to a particular indicator but it is insufficient to receive points, yet they are grouped alongside developers who disclose no information whatsoever about an indicator. 
We attempt to address this limitation by breaking complex indicators into discrete chunks, meaning that each indicator assesses one key dimension of transparency and can more easily be made binary.

\paragraph{The models we assessed are predominantly language models.} 
For the developers we assess, their associated flagship models are predominantly text-to-text language models (8 of the 10).
Of the remaining two, only one includes images as an input (\gptfour) and only one outputs images (\stablediffusion).
None of the flagship models we considered include modalities beyond text and images, though these modalities may become more common in the coming years. 
With this in mind, in principle the indicators are chosen and defined in a largely modality-agnostic fashion to facilitate future assessment as the flagship models in the ecosystem diversify in terms of modalities.

\paragraph{Most companies we assessed are headquartered in the U.S.}
Of the 10 developers we assess, 7 are headquartered in the United States. 
Although this reflects the disproportionate global reach of U.S. technology companies, there are salient foundation model developers in other parts of the world that we did not assess in this work. 
For instance, the index excludes foundation model developers in East Asia that we believe are sufficiently important to evaluate, but they often did not share enough information publicly to even attempt evaluation.
We also did not consider Falcon-180B from the Technology Innovation Institute in Abu Dhabi,\footnote{\url{https://falconllm.tii.ae}} as we had already finalized our evaluations when the model was released in September. 
We hope that researchers will use \projectname and our fully transparent methodology to assess the transparency of these developers as well as others around the world.

\paragraph{Low bar for awarding points.} 
We were generally quite generous in the scoring process. 
When we determined that a developer scored some version of a half-point, we usually rounded up. 
Since we assess transparency, we award developers points if they explicitly disclose that they do not share information about a particular indicator.
We also read developers' documents with deference where possible, meaning that we often awarded points where there are grey areas.
This means that developers' scores may actually be higher than their documentation warrants in certain cases as we had a low bar for awarding points on many indicators.
\clearpage
\hypertarget{conclusion}{\section{Conclusion}}
<div id="sec-conclusion"></div>

Our research establishes the extent to which foundation model developers are transparent, set against the backdrop of decreasing transparency.
Our findings show that the status quo is characterized by a widespread lack of transparency across developers, with significant unevenness in how individual developers fare and where they have room for improvement.
We take this as a serious indictment of the overall ecosystem.
Transparency is a broadly-necessary condition for other more substantive societal progress, and without improvement opaque foundation models are likely to contribute to harm.
Foundation models are being developed, deployed, and adopted at a frenetic pace: for this technology to advance the public interest, real change must be made to rectify the fundamental lack of transparency in the ecosystem. 
\paragraph{Acknowledgements.}
We thank Alex Engler, Anna Lee Nabors, Anna-Sophie Harling, Arvind Narayanan, Ashwin Ramaswami, Aspen Hopkins, Aviv Ovadya, Benedict Dellot, Christie Lawrence, Connor Dunlop, Conor Griffin, Dan Ho, Dan Jurafsky, Deb Raji, Dilara Soylu, Divyansh Kaushik, Gerard de Graaf, Iason Gabriel, Irene Solaiman, John Hewitt, Joslyn Barnhart, Judy Shen, Madhu Srikumar, Marietje Schaake, Markus Anderljung, Mehran Sahami, Neel Guha, Peter Cihon, Peter Henderson, Rebecca Finlay, Rob Reich, Rohan Taori, Rumman Chowdhury, Russell Wald, Seliem El-Sayed, Seth Lazar, Stella Biderman, Steven Cao, Tatsu Hashimoto, Toby Shevlane, Vanessa Parli, Yann Dubois, Yo Shavit, and Zak Rogoff for discussions on the topics of foundation models, transparency, and/or indexes that informed the \projectname. 
We especially thank Loredana Fattorini for her extensive work on the visuals for this project, as well as Shana Lynch for her work in publicizing this effort.

\paragraph{Foundation Model Developers.}
We thank the following individuals at their respective organizations for their engagement with our effort, including involvement in responding to our initial scores on behalf of their organizations. 
We emphasize that \textbf{this acknowledgement should not be understood as an endorsement of the \projectname by these individuals}, but simply that they were involved in our engagement with their organizations.
-  \aitwentyone. Yoav Shoham
-  \amazon. Bratin Saha, Vasi Philomin, Atul Deo, Swami Sivasubramanian, Peter Hallinan
-  \anthropic. Jack Clark, Deep Ganguli, Thomas Liao
-  \cohere. Aidan Gomez, Danielle Smalls, Seraphina Goldfarb-Tarrant, Nick Jakobi, Saurabh Baji
-  \google. Slav Petrov, James Manyika, Kremena Goranova, Sarah Portik, Alexandra Belias
-  \huggingface. Clement Delangue, Meg Mitchell, Yacine Jernite
-  \inflection. Mustafa Suleyman, Tim Hwang
-  \meta. Joelle Pineau, Melanie Kambadur, Joe Spisak, Eric Smith, Louis Martin
-  \openai. Miles Brundage, Lama Ahmad
-  \stability. Emad Mostaque, Ben Brooks

\paragraph{Funding.}
This work was supported in part by the 2022 Hoffman-Yee program at the Stanford Institute for Human-Centered Artificial Intelligence (HAI).\footnote{\url{https://hai.stanford.edu/2022-hoffman-yee-grant-recipients}}
This work was supported in part by the AI2050 program at Schmidt Futures (Grant G-22-63429).

\paragraph{Conflict of Interest.}
Given the nature of this work (\eg potential to significantly impact particular companies and shape public opinion), we proactively bring attention to any potential conflicts of interest, deliberately taking a more expansive view of conflict of interest to be especially forthcoming.

-  Betty Xiong is not, and has not, been affiliated with any of the companies evaluated in this effort or any other private sector entities.
-  Daniel Zhang is not, and has not, been affiliated with any of the companies evaluated in this effort or any other private sector entities.
-  Kevin Klyman is not, and has not, been affiliated with any of the companies evaluated in this effort or any other private sector entities.
-  Nestor Maslej is not, and has not, been affiliated with any of the companies evaluated in this effort or any other private sector entities.
-  Percy Liang was a post-doc at Google (September 2011--August 2012), a consultant at Microsoft (May 2018--May 2023), and a co-founder of Together AI (July 2022--present).
- He is not involved in any other companies.
-  Rishi Bommasani is not, and has not, been affiliated with any of the companies evaluated in this effort.
- Rishi is an author of \citet{jernite2022governance}, as part of the BigScience initiative, that guided the data governance practices for developing BLOOM.
- As a result, he is also an author on the 350+ author BLOOM paper \citep{scao2022bloom} that is often cited in the scoring of \bloomz.
-  Sayash Kapoor worked at Meta until December 2020. He has not since worked for the company.
-  Shayne Longpre has three connections to the assessed developers.
- He has worked as a Student Researcher at Google Brain in 2022, and is an on-going contributor to Cohere For AI, Cohere's non-profit volunteer research organization.
- Lastly, he was part of the BigScience initiative, where he contributed to BLOOM \citep{scao2022bloom}.

\newpage
\bibliography{refdb/all, main}
\bibliographystyle{acl_natbib}

\clearpage
\appendix

\hypertarget{author-contributions}{\section{Author contributions}}
<div id="app-author-contributions"></div>

This project was a team effort, built on countless contributions from everyone involved. 
Contributions span the conceptualization, design, execution, analysis, and presentation phases, along with the overarching vision, organization, coordination, and leadership.
Below, we describe each team member's contribution.


\noindent 
\textbf{Betty Xiong}: Betty contributed to the design, execution, analysis, and presentation.
\\
\textbf{Daniel Zhang}: Daniel contributed to the design, execution, and presentation.
\\
\textbf{Kevin Klyman}: Kevin co-led the project and coordinated nearly all aspects of the initiative. Kevin contributed to the design, execution, analysis, and presentation. 
\\
\textbf{Nestor Maslej}: Nestor contributed to the design, execution, analysis, and presentation.
\\
\textbf{Percy Liang}: Percy advised the project, shaping the initial vision. Percy contributed to the conceptualization, design, analysis, and presentation along with providing overarching guidance on all stages. 
\\
\textbf{Rishi Bommasani}: Rishi led the project, bringing together the team, providing the initial vision, and coordinating all aspects of the initiative. Rishi contributed to the conceptualization, design, execution, analysis, and presentation. 
\\
\textbf{Sayash Kapoor}: Sayash contributed to the design, execution, analysis, and presentation.
\\
\textbf{Shayne Longpre}: Shayne contributed to the design, execution, analysis, and presentation.
\\
\pagebreak
\hypertarget{indicators}{\section{Indicators}}
<div id="app-indicators"></div>


\input{appendices/indicator_output_tex}
\pagebreak
\hypertarget{search-protocol}{\section{Search protocol}}
<div id="app-search-protocol"></div>


In this section, we outline the search process we used to look for evidence that a foundation model developer satisfies our requirements for a given indicator. 

\subsection{General search process}

\subsubsection{Keyword Definitions}
Each item under review has associated search keywords in our GitHub repository: \url{https://github.com/stanford-crfm/fmti/}

\subsubsection{Model-Item Pair Searches}
For every model-item pair, we conduct a search using the defined keywords within the centralized resources associated with the respective models listed below.

\subsubsection{Search Methodology}
We employ the following format for every model-item-keyword tuple while using Google search, and read through the first 10 search results.

\begin{lstlisting}[breaklines=true]
site:[Refer to developer's website list below] [Refer to model name list below] [Enter keyword]
\end{lstlisting}


\noindent
For example, for GPT-4's energy efficiency item, the searches would be:

\begin{lstlisting}
site:openai.com gpt-4 energy
site:openai.com gpt-4 efficien 
\end{lstlisting}

\subsubsection{Justification}
We note the source (e.g., website, company blog post, paper) for each piece of evidence that helped confirm an item is present, alongside the justification. We link to an archive.org URL that contains the justification (instead of linking to developers’ pages directly), to maintain records.

\subsubsection{Avoid Search Personalization}
To minimize the influence of personalized search results, we perform all searches in a private or incognito browser tab.

\subsubsection{Determination Criteria}
If we find one piece of evidence that fully justifies 1 point - or, in rarer cases, 0 points - for an item, we don't perform other searches.

\subsubsection{Distribution Channels}
In certain limited cases where the above steps fail to generate any information for indicators related to distribution channels, we interact with the developer’s intended distribution channel (if disclosed), such as its API or its preferred deployment partner’s API, or the documentation related to this API. We search for the required information via this distribution channel to the extent possible. We also use proxies, such as model playgrounds, if enterprise access is otherwise required.

\subsection{Developer website}

-  AI21 Labs (Jurassic-2): \url{ai21.com}
-  Amazon (Titan Text): \url{aws.amazon.com/bedrock/titan/}
-  Anthropic (Claude): \url{anthropic.com}
-  Cohere (Command): \url{cohere.com}
-  Google (PaLM 2): \url{ai.google}
-  Hugging Face (BLOOMZ): \url{bigscience.huggingface.co}
-  Inflection (Inflection-1): \url{inflection.ai}
-  Meta (Llama 2): \url{ai.meta.com}
-  OpenAI (GPT-4): \url{openai.com}
-  StabilityAI (Stable Diffusion 2): \url{stability.ai}

\subsection{Centralized resources for all models}

\subsubsection{AI21 Labs (Jurassic-2)}
-  \url{https://docs.ai21.com/docs/jurassic-2-models}
-  \url{https://docs.ai21.com/docs/responsible-use}
-  \url{https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf}
-  \url{https://www.ai21.com/blog/introducing-j2}
-  \url{https://docs.ai21.com/docs/responsible-use#usage-guidelines}
-  \url{https://studio.ai21.com/terms-of-use}
-  \url{https://studio.ai21.com/privacy-policy}
-  \url{https://docs.ai21.com/changelog}

\subsubsection{Amazon (Titan Text)}
-  \url{https://aws.amazon.com/bedrock/titan/}
-  \url{https://docs.aws.amazon.com/pdfs/bedrock/latest/APIReference/bedrock-api.pdf#API_ListFoundationModels}
-  \url{https://aws.amazon.com/aup/}

\subsubsection{Anthropic (Claude 2)}
-  \url{https://legal.anthropic.com/#aup}
-  \url{https://vault.pactsafe.io/s/9f502c93-cb5c-4571-b205-1e479da61794/legal.html#aup}
-  \url{https://console.anthropic.com/docs/api/supported-regions}
-  \url{https://legal.anthropic.com/#terms}
-  \url{https://legal.anthropic.com/#privacy}
-  \url{https://docs.anthropic.com/claude/docs}
-  \url{https://www.anthropic.com/index/claude-2}
-  \url{https://www.anthropic.com/earlyaccess}
-  \url{https://www-files.anthropic.com/production/images/Model-Card-Claude-2.pdf}
-  \url{https://www.anthropic.com/index/frontier-threats-red-teaming-for-ai-safety}

\clearpage

\subsubsection{Cohere (Command)}
-  \url{https://docs.cohere.com/docs/}
-  \url{https://cohere.com/security}
-  \url{https://dashboard.cohere.ai/playground/generate}
-  \url{https://cohere.com/terms-of-use}
-  \url{https://cloud.google.com/blog/products/ai-machine-learning/accelerating-language-model-training-with-cohere-and-google-cloud-tpus}
-  \url{https://cohere.com/data-usage-policy}
-  \url{https://cohere.com/privacy}
-  \url{https://cohere-inc.secureframetrust.com/}

\subsubsection{Google (PaLM 2)}
-  \url{https://ai.google/static/documents/palm2techreport.pdf}
-  \url{https://developers.generativeai.google/models/language}
-  \url{https://policies.google.com/terms/generative-ai/use-policy}
-  \url{https://developers.generativeai.google/guide/safety_guidance}
-  \url{https://developers.generativeai.google/products/palm}
-  \url{https://developers.generativeai.google/available_regions}
-  \url{https://developers.generativeai.google/terms#content_license_and_data_use}

\subsubsection{Hugging Face (BLOOMZ)}
-  \url{https://arxiv.org/abs/2211.01786}
-  \url{https://huggingface.co/docs/transformers/model_doc/bloom}
-  \url{https://huggingface.co/bigscience/bloom}
-  \url{https://arxiv.org/abs/2303.03915}
-  \url{https://arxiv.org/abs/2211.05100}
-  \url{https://proceedings.neurips.cc/paper_files/paper/2022/file/ce9e92e3de2372a4b93353eb7f3dc0bd-Paper-Datasets_and_Benchmarks.pdf}

\subsubsection{Inflection (Inflection-1)}
-  \url{https://inflection.ai/assets/Inflection-1.pdf}
-  \url{https://inflection.ai/inflection-1}
-  \url{https://inflection.ai/assets/MMLU-Examples.pdf}
-  \url{https://heypi.com/policy#privacy}
-  \url{https://inflection.ai/safety}

\subsubsection{Meta (Llama 2)}
-  \url{https://arxiv.org/pdf/2307.09288.pdf}
-  \url{https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md}
-  \url{https://ai.meta.com/static-resource/responsible-use-guide/}

\clearpage

\subsubsection{OpenAI (GPT-4)}
-  \url{https://openai.com/research/gpt-4}
-  \url{https://openai.com/policies/usage-policies}
-  \url{https://openai.com/form/chat-model-feedback}
-  \url{https://platform.openai.com/docs}
-  \url{https://openai.com/customer-stories}
-  \url{https://status.openai.com/}
-  \url{https://openai.com/policies/terms-of-use}
-  \url{https://cdn.openai.com/policies/employee-data-privacy-notice.pdf}
-  \url{https://cdn.openai.com/papers/gpt-4-system-card.pdf}
-  \url{https://arxiv.org/pdf/2303.08774.pdf}
-  \url{https://openai.com/research/triton}
-  \url{https://openai.com/pricing}
-  \url{https://platform.openai.com/docs/deprecations}
-  \url{https://openai.com/waitlist/gpt-4-api}
-  \url{https://openai.com/our-structure}
-  \url{https://openai.com/api-data-privacy}

\subsubsection{StabilityAI (Stable Diffusion 2)}
-  \url{https://huggingface.co/stabilityai/stable-diffusion-2}
-  \url{https://openreview.net/forum?id=M3Y74vmsMcY}
-  \url{https://huggingface.co/terms-of-service}
-  \url{https://huggingface.co/stabilityai/stable-diffusion-2/blob/main/LICENSE-MODEL}
-  \url{https://platform.stability.ai/legal/terms-of-service}
-  \url{https://stability.ai/use-policy}
\clearpage
\hypertarget{transparency-app}{\section{Calls for transparency}}
<div id="app-transparency"></div>


In recent years, transparency has been a rallying cry for activists, a boon to researchers, and a tangible first step for governments interested in regulating foundation models. Here we outline some of the salient calls for transparency to illustrate the different stakeholders with an interest in a more transparent foundation model ecosystem.

\textbf{Calls for transparency from governments.}

A wide variety of governments have made transparency in the development of foundation models a top priority in their wider agenda for AI regulation. 
In the U.S., the White House has secured voluntary commitments from 16 companies that include a commitment "to publicly reporting their AI systems’ capabilities, limitations, and areas of appropriate and inappropriate use" in the form of "transparency reports."\footnote{See \url{https://www.whitehouse.gov/briefing-room/statements-releases/2023/07/21/fact-sheet-biden-harris-administration-secures-voluntary-commitments-from-leading-artificial-intelligence-companies-to-manage-the-risks-posed-by-ai/} and \url{https://www.whitehouse.gov/wp-content/uploads/2023/07/Ensuring-Safe-Secure-and-Trustworthy-AI.pdf} and \url{https://www.whitehouse.gov/briefing-room/statements-releases/2023/09/12/fact-sheet-biden-harris-administration-secures-voluntary-commitments-from-eight-additional-artificial-intelligence-companies-to-manage-the-risks-posed-by-ai/} and \url{https://www.whitehouse.gov/wp-content/uploads/2023/09/Voluntary-AI-Commitments-September-2023.pdf}} 
The AI Risk Management Framework from the U.S. National Institute for Standards and Technology outlines the U.S. federal government’s current approach to transparency for foundation models and other AI systems.\footnote{\url{https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf}}
The AI Risk Management Framework states "Trustworthy AI depends upon accountability. Accountability presupposes transparency. Transparency reflects the extent to which information about an AI system and its outputs is available to individuals interacting with such a system ... Meaningful transparency provides access to appropriate levels of information based on the stage of the AI lifecycle and tailored to the role or knowledge of AI actors or individuals interacting with or using the AI system." 

The SAFE framework for regulating AI proposed by Senate Majority Leader Schumer aims to ensure that "AI is developed and deployed in a responsible and transparent manner" and to "support US-led innovation in AI technologies–-including innovation in security, transparency and accountability."\footnote{\url{https://www.democrats.senate.gov/imo/media/doc/schumer_ai_framework.pdf}}
Transparency is also one of the five pillars of the bipartisan framework for a U.S. AI Act proposed by Senators Hawley and Blumenthal; their framework specifically suggests "requiring transparency from the companies developing and deploying A.I. systems" as it relates to training data, limitations, accuracy, safety, and user interaction with an AI system.\footnote{\url{https://www.blumenthal.senate.gov/imo/media/doc/09072023bipartisanaiframework.pdf}}
A variety of other draft legislation in the U.S. would require a higher level of transparency for foundation model developers, such as the Algorithmic Accountability Act\footnote{See \url{https://www.congress.gov/bill/118th-congress/house-bill/5628/all-info?s=2&r=1} and \url{https://docs.google.com/document/d/1A1bJ1mkIfE3eZuSbDmz3HGVtOvQDegHl53q3ArO7m44/}} at the federal level and California's Safety in Artificial Intelligence Act.\footnote{\url{https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=202320240SB294}} 

In the EU,transparency and information sharing have become a central focus of the draft EU AI Act. 
For instance, Article 52 of the Act imposes "transparency obligations" for some types of AI systems.
The European Parliament's draft of the AI Act included specific obligations for foundation model developers: "foundation models should have information obligations and prepare all necessary technicaldocumentation for potential downstream providers to be able to comply with their obligations under this Regulation. Generative foundation models should ensure transparency about the fact the content is generated by an AI system, not by humans."\footnote{\url{https://www.europarl.europa.eu/doceo/document/TA-9-2023-0236_EN.pdf}}
Developers of high-risk AI systems may also be required to provide additional transparency about their systems such that deployers have adequate information about risks and how to mitigate them.

China has gone a step further, with the central government adopting regulations that impose transparency requirements on foundation model deployers. 
China's "Interim Measures for the Management of Generative Artificial Intelligence Services" state that organizations deploying foundation models, including via an API, must "employ effective measures to increase transparency in generative AI services."\footnote{\url{http://www.cac.gov.cn/2023-07/13/c_1690898327029107.htm}} 
The law further specifies that "providers shall formulate clear, specific, and feasible tagging rules" for data and that "providers shall establish and complete mechanisms for making complaints and reports, setting up easy complaint and reporting portals, disclosing the process for handling them and the time limits for giving responses."

Many other governments have also highlighted the importance of transparency in the development and use of foundation models. 
Canada has released a "Voluntary Code of Conduct on the Responsible Development and Management of Advanced Generative AI Systems," which has been signed by Cohere, the Montreal Institute for Learning Algorithms, and the Vector Institute among other organizations.\footnote{\url{https://ised-isde.canada.ca/site/ised/en/voluntary-code-conduct-responsible-development-and-management-advanced-generative-ai-systems}} 
Canada's Voluntary Code of Conduct states that signatories commit to achieve transparency such that "sufficient information is published to allow consumers to make informed decisions and for experts to evaluate whether risks have been adequately addressed."
It further specifies that "developers of advanced generative systems available for public use" are required to "Publish information on capabilities and limitations of the system... Develop and implement a reliable and freely available method to detect content generated by the system, with a near-term focus on audio-visual content (e.g., watermarking). ... Publish a description of the types of training data used to develop the system, as well as measures taken to identify and mitigate risks." Japan is reportedly in the process of adopting its own code of conduct, which may go beyond voluntary commitments.\footnote{\url{https://english.kyodonews.net/news/2023/10/3b83adf1e28d-japans-ai-draft-guidelines-ask-for-measures-to-address-overreliance.html}}

India's report on "Impact, Opportunity, and Challenges of Generative AI," coauthored by India's Ministry of Electronics and Information Technology, states that transparency should be a central feature of India's regulatory framework for ensuring responsible use of generative AI.\footnote{\url{https://indiaai.s3.ap-south-1.amazonaws.com/docs/generative-ai-report.pdf}} The United Arab Emirates' generative AI guide, published by the Office of the Minister for Artificial Intelligence, Digital Economy, and Remote Work Applications,highlights the importance of transparency for generative AI in terms of data protection: "Transparency is crucial to data privacy because it enables individuals to know how their data is collected, processed, and used by organizations. By being transparent, organizations can provide clear and concise information about their data privacy practices, policies, and procedures."\footnote{\url{https://ai.gov.ae/wp-content/uploads/2023/04/406.-Generative-AI-Guide_ver1-EN.pdf}} Data protection authorities around the world are "de facto regulating generative AI" by using their existing authorities, including those related to information sharing; for example, data protection authorities in Brazil, Japan, and South Korea launched investigations into OpenAI's ChatGPT in 2023.\footnote{\url{https://fpf.org/blog/how-data-protection-authorities-are-de-facto-regulating-generative-ai/}}

Some governments have highlighted the fact that existing transparency requirements already apply to foundation model developers and ought to be enforced as such.
The UK Competition and Markets Authority notes that transparency requirements are already in place under consumer protection law, and that foundation model developers must comply with the transparency provisions of the UK Consumer Rights Act.\footnote{\url{https://www.gov.uk/government/publications/ai-foundation-models-initial-report}} The U.S. Federal Trade Commission has stated that "we take note–and can take action–if companies aren’t upfront about what consumers are buying, who made it, how it was made, or what rights people have in their own creations. ... When offering a generative AI product, [companies] may need to tell customers whether and the extent to which the training data includes copyrighted or otherwise protected material."\footnote{https://www.ftc.gov/business-guidance/blog/2023/08/cant-lose-what-you-never-had-claims-about-digital-ownership-creation-age-generative-ai}

It is also worth noting that many governments have emphasized the importance of transparency in the development and use of AI systems outside of the context of foundation models. The national AI strategies of Colombia,\footnote{\url{https://colaboracion.dnp.gov.co/CDT/Conpes/Económicos/3975.pdf}}, Egypt,\footnote{\url{https://mcit.gov.eg/Upcont/Documents/Publications_672021000_Egypt-National-AI-Strategy-English.pdf}} Indonesia,\footnote{\url{https://ai-innovation.id/images/gallery/ebook/stranas-ka.pdf}}, and India\footnote{\url{https://www.niti.gov.in/sites/default/files/2019-01/NationalStrategy-for-AI-Discussion-Paper.pdf}} highlight the importance of transparency as do the national AI strategies of other countries.\footnote{\url{https://oecd.ai/en/dashboards/overview}} 


\paragraph{\textbf{Calls for transparency from international organizations.}}
The UN High Commissioner for Human Rights, Volker Türk, has argued that existing rules for businesses squarely apply to foundation model developers. 
In a speech in July 2023, Türk stated that generative AI "companies must live up to their responsibilities to respect human rights in line with the Guiding Principles on Business and Human Rights."\footnote{\url{https://www.ohchr.org/en/statements/2023/07/artificial-intelligence-must-be-grounded-human-rights-says-high-commissioner}}
In addition to requiring human rights due diligence, the UN Guiding Principles on Business and Human Rights explicitly refer to transparency as it relates to a company's obligation to (i) transparently communicate the human rights impact of its products and (ii) be transparent in administering grievance processes.\footnote{For instance, the UN Guiding Principles on Business and Human Rights state, "The responsibility to respect human rights requires that business enterprises have in place policies and processes through which they can both know and show that they respect human rights in practice. Showing involves communication, providing a measure of transparency and accountability to individuals or groups who may be impacted and to other relevant stakeholders, including investors." See \url{https://www.ohchr.org/sites/default/files/documents/publications/guidingprinciplesbusinesshr_en.pdf}}

Türk further argued that without adequate guarantees of transparency, generative AI and other types of AI systems should be banned or suspended.
He said "regulations need to require assessment of the human rights risks and impacts of AI systems before, during, and after their use. Transparency guarantees, independent oversight, and access to effective remedies are needed, particularlywhen the State itself is using AI technologies. AI technologies that cannot be operated in compliance with international human rights law must bebanned or suspended until such adequate safeguards are in place."

UN Secretary-General António Guterres has foregrounded transparency as well. 
The UN's digital agenda, summarized in Guterres' Global Digital Compact, makes three key proposals related to transparency: (i) the international community should "make transparency, fairness and accountability the core of AI governance," (ii) governments should "consider the adoption of a declaration on data rights that enshrines transparency," and (iii) researchers and companies should be responsible for transparently communicating the risks of AI systems.\footnote{\url{https://indonesia.un.org/sites/default/files/2023-07/our-common-agenda-policy-brief-gobal-digi-compact-en.pdf}}

The G7 Hiroshima AI Process, which was launched in May 2023 and focuses on generative AI, makes "promotion of transparency" one of its core aims.\footnote{\url{https://www.whitehouse.gov/briefing-room/statements-releases/2023/05/20/g7-hiroshima-leaders-communique/}} A September 2023 joint statement on the Hiroshima AI Process by G7 Digital and Technology Ministers committed the G7 to "develop guiding principles for organizations developing, deploying, and using advanced AI systems, in particular foundation models and generative AI," and stated that one such guiding principle could be "publicly report models’ capabilities, limitations and domains of appropriate and inappropriate use, ensuring sufficient transparency."\footnote{\url{https://www.politico.eu/wp-content/uploads/2023/09/07/3e39b82d-464d-403a-b6cb-dc0e1bdec642-230906_Ministerial-clean-Draft-Hiroshima-Ministers-Statement68.pdf}}

More broadly, international organizations have long noted that transparency is essential for responsible development of AI systems.
The OECD AI Principles, adopted in 2019, include transparency as one of five principles for trustworthy AI. 
The principle on “transparency and explainability” reads: “AI Actors should commit to transparency and responsible disclosure regarding AI systems. To this end, they should provide meaningful information, appropriate to the context, and consistent with the state of art: (i) to foster a general understanding of AI systems; (ii) to make stakeholders aware of their interactions with AI systems, including in the workplace; (iii) to enable those affected by an AI system to understand the outcome; and, (iv.) to enable those adversely affected by an AI system to challenge its outcome based on plain and easy-to-understand information on the factors, and the logic that served as the basis for the prediction, recommendation or decision.”\footnote{\url{ https://legalinstruments.oecd.org/en/instruments/OECD-LEGAL-0449}} 
The G20 AI Principles, also adopted in 2019, include this OECD principle on transparency verbatim. \footnote{\url{https://wp.oecd.ai/app/uploads/2021/06/G20-AI-Principles.pdf}} 
A number of other countries have committed to the OECD AI Principles, including Argentina, Brazil, Egypt, and Singapore.\footnote{\url{https://oecd.ai/en/ai-principles}} 


\paragraph{\textbf{Calls for transparency from foundation model developers.}}

Foundation model developers have also called for greater transparency and touted the benefits of transparency in their own business practices. 
For example, in June 2022 AI21 Labs, Cohere, and OpenAI published "Joint Recommendation for Language Model Deployment" that advocated for increased transparency \citep{cohere2022}. 
Their recommendations stated that developers should “Publish usage guidelines and terms of use of LLMs ... Document known weaknesses and vulnerabilities, such as bias or ability to produce insecure code ... Documentation should also include model and use-case-specific safety best practices."

Individual developers have highlighted the importance of transparency as well. 
Anthropic ties the importance of transparency to interpretability in its paper on Constitutional AI and in describing the company's "Core Views on AI Safety" \citep{bai2022constitutional}.\footnote{As the blog post summarizing the paper states, "Constitutional AI is also helpful for transparency: we can easily specify, inspect, and understand the principles the AI system is following." See \url{https://www.anthropic.com/index/claudes-constitution and https://www.anthropic.com/index/core-views-on-ai-safety}} Inflection prioritizes transparency in its decision-making about the choices it makes with regard to safety. Inflection's Safety Policy states "Safety at its heart is a question of values. Companies choose what risks to prioritize, and how to address them. We believe the best principle is to be deliberate about these choices, and transparent with our users about the specific values we build into our AIs. We may prioritize values that you disagree with. That’s OK. We think that there is room for many perspectives ... We commit to sharing publicly what positions we aim to take in our AIs."\footnote{\url{https://inflection.ai/safety}}

OpenAI has argued that transparency can help companies work together to mitigate safety concerns regarding foundation models.\footnote{\url{https://openai.com/research/cooperation-on-safety}}
\citet{askell2019role} argue "information that companies provide about their intentions and actions—how transparent they are—can play an important role in whether other companies will cooperate with them."
OpenAI also requires transparency from its suppliers: OpenAI's Supplier Code of Conduct states that "OpenAI expects all Suppliers to adhere to the highest standards of integrity, transparency, honesty, and ethical conduct in all their business dealings."\footnote{\url{https://openai.com/policies/supplier-code}} 

Cohere states that transparency is important for its responsible development of large language models, noting that it has "invested in technical and non-technical measures to mitigate potential harm and make our development processes transparent."\footnote{\url{https://cohere.com/responsibility}} 
Cohere's Usage Guidelines prohibit users from using Cohere's platform for applications with "no transparency," meaning those that"do not disclose that the content is generated through automated means."\footnote{\url{https://docs.cohere.com/docs/usage-guidelines}} 

Stability AI has called for transparency in connection with its advocacy for open foundation models. In a May 2023 report submitted to the U.S. Senate Judiciary Subcommittee on Privacy, Technology, and the Law, Stability AI wrote "Models like Stable Diffusion and StableLM demonstrate our commitment to AI technology that is transparent, accessible, and human-centric: ... We develop open models for transparency. Researchers can `look under the hood' to verify performance, identify potential risks, and help develop safeguards. Organizations across the public and private sector can customize these models for their own needs without exposing sensitive data or ceding control of their AI capabilities."\footnote{\url{https://stability.ai/blog/stability-ai-letter-us-senate-ai-oversight}} The report further argues "These principles can help to advance important policy objectives. Transparent models promote safety and security. ... open models enable the transparent identification, assessment, and management of risks consistent with the National Institute of Standards and Technology AI Risk Management Framework."

Hugging Face has also called for transparency as part of its push for open foundation models. In written testimony before the U.S. House Committee on Science, Space, and Technology, Hugging Face CEO Clement Delangue stated "Rigorous documentation practices for AI systems, with transparent reporting that follows well-defined protocols, serves three main goals: incentivizing responsible development; ensuring researchers and developers consider values and priorities that may otherwise be overlooked; and creating a paper trail for review. ... transparency from entities about how and where they deploy AI systems to understand what evaluations are most urgently needed."\footnote{\url{https://republicans-science.house.gov/_cache/files/5/5/551f066b-4483-4efd-b960-b36bc02d4b66/B82DBAFFA56F31799E058FB2755C2348.2023-06-22-mr.-delangue-testimony.pdf}} Hugging Face has, along with various partners, released a number of artifacts that advance transparency such as tools for exploring datasets \citep{piktus2023roots}. 

In articulating Meta's position with respect to Llama 2, \citet{touvron2023llama} state that "It is important to understand what is in the pretraining data both to increase transparency and to shed light on root causes of potential downstream issues, such as potential biases. ... open releases promote transparency and allow more people to access AI tools, democratizing the technology and decentralizing AI expertise." Meta's Responsible Use Guide for Llama 2 encourages downstream developers to "build transparency and reporting mechanisms in user interactions ... consider ways to provide transparency to end users regarding potential risks and limitations of the system prior to or at the time of user interaction."
\footnote{\url{https://ai.meta.com/static-resource/responsible-use-guide/}} 

Amazon makes clear that transparency is important with respect to the way in which it communicates its policies to users.
Amazon Web Services' Data Privacy Center states that "Our contracts are written in plain, straightforward language to be transparent and help you understand the data privacy protections that we offer. We also provide ongoing data transparency reporting."\footnote{\url{https://aws.amazon.com/compliance/data-privacy/Privacy_at_AWS_}} 

Google highlights transparency in its AI principles, writing "For datasets and models, the consistent outcome is to create and publish detailed documentation of datasets and models in the form of structured transparency artifacts known as data and model cards (see the following section for details), which function like nutrition labels, providing information such as the provenance of the data (if a data card) and model performance when tested for fairness (if a model card)."\footnote{\url{https://ai.google/static/documents/ai-principles-2022-progress-update.pdf}} 
Google's AI principles also detail the "Transparency Artifacts" that Google researchers have built, such as Healthsheets and a Data Cards Playbook.

Microsoft has also produced such artifacts, namely in the form of "Transparency Notes," which "are intended to help you understand how our AI technology works, the choices system owners can make that influence system performance and behavior, and the importance of thinking about the whole system, including the technology, the people, and the environment."\footnote{\url{https://learn.microsoft.com/en-us/legal/cognitive-services/language-service/transparency-note}} 

A large number of developers and deployers that we do not assess have also expressed the importance of transparency \citep{jobin2019global,fjeld2020principled,wef2023presidio}. Notable among them is EleutherAI, a non-profit research group that is a leading developer of open foundation models \citep{skowron2023euaiact}. 
\citet{phang2022eleutherai} write that "EleutherAI’s approach to research goes beyond transparency: by doing research entirely in public, anyone in the world can observe and contribute at every stage," adding that such public-facing research fosters a highly collaborative, diverse, and innovative research community. 

\paragraph{\textbf{Calls for transparency from researchers, civil society, and labor.}}

While governments and companies have consistently underscored the value of transparency, less powerful actors have banded together to push public and private entities to meaningfully improve transparency along with the business practices that transparency uncovers. 

Researchers have driven much of the improvement in transparency for foundation model developers, with innovations like model cards, datasheets, and data statements leading to substantial gains \citep{mitchell2018modelcards,gebru2018datasheets,bender-friedman-2018-data}.
Some have sought to solidify these improvements in transparency by strengthening the field of algorithmic auditing \citep{costanzachock2022audit}. 
Mozilla's Open Source Audit Tooling project calls for better infrastructure to evaluate and audit AI systems ~\citep{raji2022mozilla}.
Another proposal to bolster the auditing ecosystem is for governments to conduct third-party audits of AI systems under their existing authority to protect consumers and data subjects ~\citep{miller2021radical}.

Recently, coalitions of researchers led by organizations like LAION have come together to call for greater transparency in the foundation model ecosystem \citep{laion2023transparentai}.
In recent congressional hearings, expert testimony has expressed "The Need for Transparency in Artificial Intelligence" ~\citep{gregory2023testimony}.
\citet{belli2023igf} detail the central importance of transparent foundation models from the perspective of experts across Asia, Africa, and Latin America. 
Other researchers still have argued that transparency, while necessary, is far from sufficient to regulate AI ~\citep{hartzog2023oversight}.

Data workers employed as contractors by foundation model developers have also mobilized for increased transparency \citep{gray_ghost_2019}.\footnote{Some policymakers have focused on the importance of transparency with respect to data labor. For example, in a letter to the CEOs of major foundation model developers, eight members of the U.S. Congress wrote "Tech companies also must be more transparent about the role data workers play in their AI, so that consumers can make informed choices about the products they use. Unfortunately, many companies have sidestepped these duties, and that must change. ... Please share any plans your company has to be more transparent about the role its data workers play and their working conditions." See \url{https://www.markey.senate.gov/imo/media/doc/letter_to_artificial_intelligence_companies_on_data_worker_labor_conditions_-_091323pdf1.pdf}}
For example, in July 2023 members of the African Content Moderators Union filed a petition with Kenya's parliament requesting an investigation into OpenAI, Meta, Google, and other multinational technology companies that employ content moderators in Kenya.\footnote{The African Content Moderators Union has also sued Meta, alleging that it unlawfully fired workers for their union organizing. See \url{https://techcrunch.com/2023/08/23/meta-and-moderators-agree-to-mediation/}} 
The petition states that OpenAI used a vendor, Sama, to hire the petitioners as contractors who "trained the ChatGPT algorithm," and alleges that "the contracts did not sufficiently describe the nature of the job ... we were not properly informed of the nature of the work we would be undertaking." The petition further alleges that although this data labor included "reading and viewing material that depicted sexual and graphic violence and categorizing it accordingly so that ChatGPT's artificial intelligence could learn it for the purposes of its future interactions with people ... throughout the contract of training ChatGPT we were not afforded psychosocial support."\footnote{\url{https://x.com/mercymutemi/status/1678984336996028416?s=46}}

The Partnership on AI has advocated for transparency with respect to the employment of data enrichment workers, writing "While shifting how the broader field approaches data enrichment is not a trivial task, increasing transparency regarding current practices and developing more practical guidance can move the field towards improved conditions for data enrichment workers. Greater transparency can help emphasize the central role of data enrichment workers, create the basis for a rich public dialogue of how to improve conditions for workers, and increase confidence in AI models themselves."\footnote{In addition to conducting a case study in partnership with Google DeepMind exploring how to increase transparency regarding data labor, the Partnership on AI has separately published a white paper recommending that developers increase transparency in wages and pay structure for data enrichment workers. See \url{https://partnershiponai.org/wp-content/uploads/2022/11/case-study_deepmind.pdf} and \url{http://partnershiponai.org/wp-content/uploads/2021/08/PAI-Responsible-Sourcing-of-Data-Enrichment-Services.pdf}}


Civil society groups with a range of different focus areas agree that transparency is a pressing priority for policymakers and foundation model developers.
For instance, 123 civil society organizations, including AccessNow, Algorithm Watch, and the European Center for Not-for-Profit Law, released a statement advocating for the prioritization of more serious transparency requirements in the EU AI Act.\footnote{\url{https://www.fairtrials.org/app/uploads/2022/05/Civil-society-reacts-to-EP-AI-Act-draft-report_FINAL.pdf}}
The statement advocates the inclusion of a "mandatory impact assessments are a
crucial measure to ensure foresight and accountability for potential AI-related harms," and that "information on all uses of AI systems by public authorities, regardless of
the systems’ risk level, should be made public in the EU database." 
Additionally, they call for "an obligation for providers and/or users to include
information regarding the environmental impact of AI systems," which is not a provision in the EU AI Act.
Freedom House has also warned that "AI has allowed governments to refine their online censorship" and threatens to exacerbate the decline in global internet freedom.
AI has allowed governments to enhance and refine their online censorship, and foundation models may exacerbate this trend.\footnote{\url{https://freedomhouse.org/report/freedom-net/2023/repressive-power-artificial-intelligence}}
Freedom House points to transparency requirements as a mechanism to identify and combat evolving and subtle censorship pressures.

In October 2023, the U.S. Federal Trade Commission convened a workshop on the "Creative Economy and Generative AI," where creators from across different industries demanded increased transparency. In the words of one participant, "The creative economy only works when the basic tenants of consent, credit, compensation, and transparency are followed. ... Without transparency, we can't even know the extent of how much of these companies have taken. They took our work and data to train for-profit technologies that then directly compete against us in our own markets using generative media that is meant to mimic us."\footnote{\url{https://www.ftc.gov/system/files/ftc_gov/pdf/creative-economy-and-generative-ai-transcript-october-4-2023.pdf}} 

Despite its limits, transparency is a necessary and broadly popular first step towards accountability for harm caused by AI systems \citep{kaminski_2020, bates2023socially}. In the context of the rapid rollout of extremely powerful AI systems such as foundation models, transparency is all the more urgent. Companies developing and deploying foundation models should heed the call.
\end{document}