\section{The Role of Technologists}

The continuously evolving regulatory landscape of AI, combined with the limitations of existing regulation in providing clarity on how transparency should be implemented into AI systems, has left open questions concerning responsibilities for AI design and implementation. We argue that (1) practitioners should bear the bulk of the responsibility for designing and implementing compliant, transparent AI systems (2) it is in the best interest of practitioners to bear this responsibility. Researchers have also shown that there may be risks of only partially complying with AI regulations, and that fusll compliance is the best way forward~\cite{dai2021fair}. {\bf Technologists} include AI practitioners, researchers, designers, programmers, and developers.

{\bf Practitioners have the right technical expertise.} Transparency has been a central topic of AI research for the past decade, and is motivated beyond just regulatory compliance by ideas like making systems more efficient, debugging systems, and giving decision making agency to the data subjects (i.e., those affected by AI-assisted decisions) or to the users of AI systems (i.e., those making decisions with the help of AI). New technologies in transparent AI are being created at a fast pace, and there is no indication that the rapid innovation of explainable AI will slow any time soon~\cite{DBLP:conf/nips/LundbergL17, ribeiro2016should, datta2016algorithmic, DBLP:journals/corr/abs-2004-00668, DBLP:journals/corr/abs-2004-00668}, meaning that of all the stakeholders involved in the socio-technical environment of AI systems, technologists are the most likely to be aware of available tools for creating transparent AI systems. Furthermore, there are currently no objective measures for the quality of transparency in AI systems~\cite{gunning2019xai, abdul2020cogam, yang2019study, holzinger2020measuring, lu2019good}, and so technologists are necessary to discern the difference between a ``good explanation'' and a ``bad explanation'' about a system.

{\bf Practitioners are the least-cost avoiders.} This idea is based on the principle of the least-cost avoider, which states that obligations and liabilities should be allocated entirely to the party with the lowest cost of care ~\cite{stoyanovich2016revealing}. AI practitioners are the least-cost avoiders because they are already equipped with the technical know-how for building and implementing transparency tools into AI systems, especially when compared to policymakers and the individuals affected by the outcome of the system. Notably, given the wide range of existing transparency tools, implementing the ``bare minimum'' is trivially easy for most technologists.

One argument practitioners give against building transparent systems is that they may be less accurate than highly complex, black-box systems \cite{huysmans2006using}. However, there has been a growing amount of evidence suggesting that building transparent systems actually results into little to no trade-off in the accuracy of AI systems~\cite{rudin2019stop, bell2019proactive, stiglic2015comprehensible, de2018predicting}. In other words: building transparent systems is not a Pareto-reducing constraint for practitioners.

{\bf Practitioners already bear the responsibility for implementing transparency into AI systems.} A study interviewing AI practitioners found that using AI responsibly in their work is viewed as the practitionerâ€™s burden, not the institutions for which they work. Practitioners noted that existing structures within institutions are often antithetical to the goals of responsible AI, and that it is up to them to push for structural change within that institution ~\cite{rakova2020responsible}. Section <a href='#sec-laws'>sec-laws</a> shows that AI regulation is converging on requiring transparent AI systems that offer meaningful explanations to stakeholders. Therefore, it is in the best interest of practitioners to continue the bottom-up approach of building transparent AI systems in the face of looming regulations.