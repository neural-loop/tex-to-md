\hypertarget{impact}{\section{Impact}}
\label{sec:impact}
The \projectname characterizes the transparency of foundation model developers at present.
While this descriptive work already yields significant insights and value, our ambition for this work is to drive change. 
In short, our objective is to \textit{improve} transparency in the foundation model ecosystem: we believe improved transparency will result in better science, more innovation, greater accountability, and ultimately give society greater collective confidence that this promising technology can truly advance the public interest.
To achieve these lofty goals requires changing the conduct of powerful organizations.
As with many similar efforts to drive change, we have conceptualized a specific theory of change and have considered specific limitations and risks of our work.
Consistent with the work's spirit of transparency, we describe both matters plainly below. 

\hypertarget{change}{\subsection{Theory of change}}
\label{sec:change}
Assessment of any kind naturally characterizes the status quo.
However, our intent is for our assessment to drive change, especially given that our most fundamental finding is that there is insufficient transparency in the foundation model ecosystem.
\citet{bommasani2022evaluation} argues that evaluation and assessment can drive change if there is sufficient uptake: we specifically articulate how assessment can motivate improvement through different forms of uptake.

\paragraph{Assessment motivates improvement.}
By quantifying transparency and simultaneously scoring many developers, we hope that these organizations will improve their transparency scores over time.
We aim to provide a characterization of the status quo that is broadly legible across the ecosystem by directly comparing organizations' transparency scores. 
Furthermore, specific areas where there is pervasive opacity, or where specific companies are less transparent than their counterparts, are prime targets for public pressure and scrutiny. 

With respect to AI companies, we believe that certain teams and employees will play an outsized role in shaping transparency practices.
Namely, responsible AI teams along with other teams that address ethics and safety are likely to shape many company-wide practices on transparency, including for their flagship foundation models.
For this key group of individuals, we hope the \projectname provides a concrete list of indicators to proactively consider in making decisions.
At present, we believe that some companies are not transparent about certain issue areas not because of specific countervailing concerns (\eg profits, privacy, safety) but because they have not explicitly considered whether they should be transparent on this issue. 
In these cases, we believe the index provides a structured, well-argued resource that responsible AI teams can directly consider in making decisions around transparency.
The index also provides an extensive account of why these specific indicators are valuable, which could help responsible AI teams advocate for greater transparency within their organizations.
To be concrete, linking outward-facing transparency reporting with internal-facing company tracking could be a natural outcome where our index could bring about desired change while adding minimal overhead for these companies.

Indexes draw power from their subsequent iterations, allowing for improvements to be clearly measured and acknowledged over time.
In the fast-moving foundation model ecosystem, subsequent versions could be motivated by (i) changes in the indicators, (ii) changes in the key companies to assess, (iii) changes in the flagship foundation models of those companies, and (iv) changes in the underlying materials for a specific company. 
As a result, we believe the maintenance and subsequent versions of the \projectname will be necessary for its sustained impact.
We have not yet determined an exact cadence and strategy, though we will conduct future versions of the index.\footnote{See \indexUrl~for the latest details.} 

\paragraph{Assessment guides standards and mandates.}
Fundamentally, the \projectname assesses companies on metrics of transparency that are selected and evaluated based on our judgments as experts in this domain.
With this in mind, the indicators selected as well as the results could directly inform more formal processes.
For instance, policymakers around the world are considering a spectrum of voluntary commitments, industry standards, and mandatory requirements for foundation model developers.
Many current policy efforts, across the varying levels of voluntary and mandatory requirements, explicitly name transparency as a top-level priority and directly identify specific indicators and subdomains covered by our index (see \refapp{transparency}).

Our index is likely to be more comprehensive, more fine-grained, and more empirically grounded than most ongoing policy initiatives.
As a result, our index provides direct value to policymakers.
In selecting requirements, policymakers can use the index to explore the broader universe of potential transparency and disclosure requirements.
In defining requirements, policymakers can use the index to explore specific definitions as well as edge cases that may complicate a requirement. 
And, in ultimately deciding which organizations to regulate and how to enforce regulations, policymakers can look at the current status quo to efficiently allocate resources.

As a brief example, consider the European Parliament's position for the EU AI Act,\footnote{\url{https://www.europarl.europa.eu/doceo/document/TA-9-2023-0236_EN.pdf}} which was adopted by the Parliament on June 14, 2023 by a vote of 499 in favour, 28 against and 93 abstentions.\footnote{\url{https://www.europarl.europa.eu/news/en/press-room/20230609IPR96212/meps-ready-to-negotiate-first-ever-rules-for-safe-and-transparent-ai}}
\citet{bommasani2023eu-ai-act} provide an initial analysis of potential compliance with the the Act as proposed in the context of foundation model developers.
Given this legislative proposal, European lawmakers might recognize that topics related to upstream labor and downstream impact, which are covered in the \projectname, are not adequately addressed in the draft AI Act.
Policymakers might also acknowledge that requirements to disclose a summary of any copyrighted training data are too vague and a more specific definition, such as the definition we provide in \refapp{indicators}, may be desirable to improve compliance.
And, finally, policymakers might view the results of how open and closed developers fare in deciding which requirements are best targeted at which developers along the release spectrum.
Overall, much as transparency is instrumental for key societal objectives like public trust, we believe the \projectname can be similarly instrumental for key societal processes like sound policy-making.

\hypertarget{limitations}{\subsection{Limitations and risks}}
\label{sec:limitations}

\paragraph{Equating transparency and responsibility.} 
Because we foreground transparency in our assessment of developers and their flagship models, it is likely that some will misinterpret the \projectname as a measure of the responsibility of companies. 
This is not the case for a number of reasons; most importantly, we award points on the basis of whether a developer is transparent about each indicator, not whether it has responsible business practices tied to that indicator.
Concretely: if a developer discloses that it pays data laborers just one cent per hour, it would score points on the wages indicator under our methodology, while a developer that pays data laborers \$20 an hour but does not make that information publicly available would score no points. 

This means that one risk of our approach is that it could incentivize developers to be transparent in performative ways that merely increase the amount of information available about their flagship models but do not reflect an effort on the part of the developer to substantively improve its business practices. 
Nevertheless, we believe that additional information about each of these indicators is an invaluable first step towards understanding how developers build and use foundation models. 
This will in turn allow many other evaluations of responsible business practices, in which the level of transparency should be but one factor.

\paragraph{Transparency-washing.} 
There is no guarantee that improved transparency of foundation models will result in more responsible development.
As critics of transparency-washing have persuasively argued \citep{zalnieriute2021transparency}, major technology companies have used transparency to create the illusion that they are responsible players with the public's best interest at heart.
In this way, transparency can be a shield against further scrutiny, helping to convince the public that foundation models are safe and trustworthy when they may not be.

Similarly, companies may use transparency as a shield against comprehensive regulation. 
Companies could face substantial costs if they were required to increase pay for data laborers or forego certain risky use cases for their foundation models, leading some to argue that governments should simply require transparency in these verticals.
However, transparency alone will not change a business' fundamental incentives and, if used to water down regulation, can perpetuate harm.
Notwithstanding this risk, transparency may be a more appropriate regulatory option for many of the indicators we consider given the early stage of the foundation model ecosystem and the risk that substantive requirements will disproportionately damage small and open developers.

\paragraph{Gaming the index.}
Moving forward, developers might attempt to game the \projectname without actually improving transparency. 
They could do this by clarifying that they do not share information about certain practices and giving a justification for doing so. 
Developers might also exploit the fact that indicators are relatively generous, meaning that they could share minor additional information on indicators that are comparatively easy to satisfy without meaningfully improving transparency.
Since scores could theoretically be gamed in this way, it is important to consider the Foundation Model Transparency Index in conjunction with other metrics of companies' business practices. 

\paragraph{Binary scoring.} 
The fact that each indicator is binary limits the amount of information that each score can reflect when compared with more expressive scoring schemes.
For the same indicator, it is often the case that several developers share much less information than others but they all score one point nonetheless as they cross the threshold for receiving points.
Conversely, in certain instances developers disclose some information related to a particular indicator but it is insufficient to receive points, yet they are grouped alongside developers who disclose no information whatsoever about an indicator. 
We attempt to address this limitation by breaking complex indicators into discrete chunks, meaning that each indicator assesses one key dimension of transparency and can more easily be made binary.

\paragraph{The models we assessed are predominantly language models.} 
For the developers we assess, their associated flagship models are predominantly text-to-text language models (8 of the 10).
Of the remaining two, only one includes images as an input (\gptfour) and only one outputs images (\stablediffusion).
None of the flagship models we considered include modalities beyond text and images, though these modalities may become more common in the coming years. 
With this in mind, in principle the indicators are chosen and defined in a largely modality-agnostic fashion to facilitate future assessment as the flagship models in the ecosystem diversify in terms of modalities.

\paragraph{Most companies we assessed are headquartered in the U.S.}
Of the 10 developers we assess, 7 are headquartered in the United States. 
Although this reflects the disproportionate global reach of U.S. technology companies, there are salient foundation model developers in other parts of the world that we did not assess in this work. 
For instance, the index excludes foundation model developers in East Asia that we believe are sufficiently important to evaluate, but they often did not share enough information publicly to even attempt evaluation.
We also did not consider Falcon-180B from the Technology Innovation Institute in Abu Dhabi,\footnote{\url{https://falconllm.tii.ae}} as we had already finalized our evaluations when the model was released in September. 
We hope that researchers will use \projectname and our fully transparent methodology to assess the transparency of these developers as well as others around the world.

\paragraph{Low bar for awarding points.} 
We were generally quite generous in the scoring process. 
When we determined that a developer scored some version of a half-point, we usually rounded up. 
Since we assess transparency, we award developers points if they explicitly disclose that they do not share information about a particular indicator.
We also read developers' documents with deference where possible, meaning that we often awarded points where there are grey areas.
This means that developers' scores may actually be higher than their documentation warrants in certain cases as we had a low bar for awarding points on many indicators.