% Try to use refdb instead of adding to this file!
@article{lee2022interaction,
  title={On the Ability of Language Models to Interact with Humans},
  author={Mina Lee and Megha Srivastava and Amelia Hardy and Esin Durmus and Ashwin Paranjape and John Thickstun and Ines Gerard-Ursin and Faisal Ladhak and Frieda Rong and Rose E. Wang and Xiang Lisa Li and Minae Kwon and Joon Sung Park and Hancheng Cao and Tony Lee and Rishi Bommasani and Michael Bernstein and Percy Liang},
  year={Forthcoming}
}

@article{narayanan2022evaluating,
  title={{Evaluating Efficiency-Capability Tradeoffs for Black-Box Autoregressive Transformer APIs}},
  author={Deepak Narayanan and Keshav Santhanam and Peter Henderson and Rishi Bommasani and Tony Lee and Percy Liang},
  year={Forthcoming}
}

@article{jobin2019global,
  author       = {Anna Jobin and Marcello Ienca and Effy Vayena},
  title        = {The Global Landscape of AI Ethics Guidelines},
  journal      = {Nature Machine Intelligence},
  volume       = {1},
  number       = {9},
  pages        = {389--399},
  month        = {September},
  year         = {2019},
  doi          = {10.1038/s42256-019-0088-2},
  url          = {https://doi.org/10.1038/s42256-019-0088-2}
}

@article{undp2022hdr,
  author = {UNDP},
  title = {Human Development Report 2021-22},
  year = {2022},
  location = {New York},
  publisher = {United Nations Development Programme},
  URL = {http://report.hdr.undp.org}
}

@article{10.1145/3359183,
author = {Mathur, Arunesh and Acar, Gunes and Friedman, Michael J. and Lucherini, Eli and Mayer, Jonathan and Chetty, Marshini and Narayanan, Arvind},
title = {Dark Patterns at Scale: Findings from a Crawl of 11K Shopping Websites},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {CSCW},
url = {https://doi.org/10.1145/3359183},
doi = {10.1145/3359183},
abstract = {Dark patterns are user interface design choices that benefit an online service by coercing, steering, or deceiving users into making unintended and potentially harmful decisions. We present automated techniques that enable experts to identify dark patterns on a large set of websites. Using these techniques, we study shopping websites, which often use dark patterns to influence users into making more purchases or disclosing more information than they would otherwise. Analyzing ~53K product pages from ~11K shopping websites, we discover 1,818 dark pattern instances, together representing 15 types and 7 broader categories. We examine these dark patterns for deceptive practices, and find 183 websites that engage in such practices. We also uncover 22 third-party entities that offer dark patterns as a turnkey solution. Finally, we develop a taxonomy of dark pattern characteristics that describes the underlying influence of the dark patterns and their potential harm on user decision-making. Based on our findings, we make recommendations for stakeholders including researchers and regulators to study, mitigate, and minimize the use of these patterns.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {nov},
articleno = {81},
numpages = {32},
keywords = {consumer protection, deceptive content, manipulation, dark patterns, nudging}
} 



@misc{rdr2020index,
  author = {RDR},
  title = {2020 Ranking Digital Rights Corporate Accountability Index},
  year = {2020},
  publisher = {Ranking Digital Rights},
  URL = {https://rankingdigitalrights.org/index2020/}
}

@article{Hagendorff2020,
  author       = {Thilo Hagendorff},
  title        = {The Ethics of AI Ethics: An Evaluation of Guidelines},
  journal      = {Minds and Machines},
  volume       = {30},
  number       = {1},
  pages        = {99--120},
  year         = {2020},
  month        = {March},
  doi          = {10.1007/s11023-020-09517-8},
  url          = {https://doi.org/10.1007/s11023-020-09517-8},
  abstract     = {Current advances in research, development and application of artificial intelligence (AI) systems have yielded a far-reaching discourse on AI ethics. In consequence, a number of ethics guidelines have been released in recent years. These guidelines comprise normative principles and recommendations aimed to harness the “disruptive” potentials of new AI technologies. Designed as a semi-systematic evaluation, this paper analyzes and compares 22 guidelines, highlighting overlaps but also omissions. As a result, I give a detailed overview of the field of AI ethics. Finally, I also examine to what extent the respective ethical principles and values are implemented in the practice of research, development and application of AI systems—and how the effectiveness in the demands of AI ethics can be improved.},
  issn         = {1572-8641},
}
    @misc{ti2022corrupt,
 author = {{Transparency International}},
 title = {Corruption Perception Index 2022},
 url = {http://www.transparency.org/cpi},
 year = {2023}
}


    
@article{shannon1948mathematical,
  title={A mathematical theory of communication},
  author={Shannon, Claude Elwood},
  journal={The Bell system technical journal},
  volume={27},
  number={3},
  pages={379--423},
  year={1948},
  publisher={Nokia Bell Labs}
}

@article{benkler2013practical,
  title={Practical anarchism: Peer mutualism, market power, and the fallible state},
  author={Benkler, Yochai},
  journal={Politics \& Society},
  volume={41},
  number={2},
  pages={213--251},
  year={2013},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}

@misc{bai2022constitutional,
      title={Constitutional AI: Harmlessness from AI Feedback}, 
      author={Yuntao Bai and Saurav Kadavath and Sandipan Kundu and Amanda Askell and Jackson Kernion and Andy Jones and Anna Chen and Anna Goldie and Azalia Mirhoseini and Cameron McKinnon and Carol Chen and Catherine Olsson and Christopher Olah and Danny Hernandez and Dawn Drain and Deep Ganguli and Dustin Li and Eli Tran-Johnson and Ethan Perez and Jamie Kerr and Jared Mueller and Jeffrey Ladish and Joshua Landau and Kamal Ndousse and Kamile Lukosuite and Liane Lovitt and Michael Sellitto and Nelson Elhage and Nicholas Schiefer and Noemi Mercado and Nova DasSarma and Robert Lasenby and Robin Larson and Sam Ringer and Scott Johnston and Shauna Kravec and Sheer El Showk and Stanislav Fort and Tamera Lanham and Timothy Telleen-Lawton and Tom Conerly and Tom Henighan and Tristan Hume and Samuel R. Bowman and Zac Hatfield-Dodds and Ben Mann and Dario Amodei and Nicholas Joseph and Sam McCandlish and Tom Brown and Jared Kaplan},
      year={2022},
      eprint={2212.08073},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{casper2023open,
      title={Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback}, 
      author={Stephen Casper and Xander Davies and Claudia Shi and Thomas Krendl Gilbert and Jérémy Scheurer and Javier Rando and Rachel Freedman and Tomasz Korbak and David Lindner and Pedro Freire and Tony Wang and Samuel Marks and Charbel-Raphaël Segerie and Micah Carroll and Andi Peng and Phillip Christoffersen and Mehul Damani and Stewart Slocum and Usman Anwar and Anand Siththaranjan and Max Nadeau and Eric J. Michaud and Jacob Pfau and Dmitrii Krasheninnikov and Xin Chen and Lauro Langosco and Peter Hase and Erdem Bıyık and Anca Dragan and David Krueger and Dorsa Sadigh and Dylan Hadfield-Menell},
      year={2023},
      eprint={2307.15217},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{askell2019role,
      title={The Role of Cooperation in Responsible AI Development}, 
      author={Amanda Askell and Miles Brundage and Gillian Hadfield},
      year={2019},
      eprint={1907.04534},
      archivePrefix={arXiv},
      primaryClass={cs.CY}
}

@article{lounsbury1954transitional,
  title={ Transitional probability, linguistic structure and systems of habitfamily hierarchies.},
  author={Floyd G. Lounsburg},
  journal={Psycholinguistics: a survey of
theory and research},
  year={1954},
  publisher={Indiana University Press}
}

@misc{klyman2023open,
  url = {https://hai.stanford.edu/news/how-promote-responsible-open-foundation-models}, 
  author = {Kevin Klyman},
  title = {How to Promote Responsible Open Foundation Models},
  publisher = {Stanford Institute for Human-Centered AI}, 
  year = {2023}
}

@misc{engler2023casc,
    url = {https://www.brookings.edu/articles/a-comprehensive-and-distributed-approach-to-ai-regulation/},
    author = {Alex Engler},
    title = {A comprehensive and distributed approach to AI regulation},
    publisher = {Brookings}, 
    year = {2023}
}

@misc{zanfir-f2023fpf,
    url = {https://fpf.org/blog/how-data-protection-authorities-are-de-facto-regulating-generative-ai/},
    author = {Gabriela Zanfir-Fortuna},
    title = {How Data Protection Authorities are De Facto Regulating Generative AI},
    publisher = {Future of Privacy Forum}, 
    year = {2023}
}

@inproceedings{hacker2023gpt,
author = {Hacker, Philipp and Engel, Andreas and Mauer, Marco},
title = {Regulating ChatGPT and Other Large Generative AI Models},
year = {2023},
isbn = {9798400701924},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3593013.3594067},
doi = {10.1145/3593013.3594067},
abstract = {Large generative AI models (LGAIMs), such as ChatGPT, GPT-4 or Stable Diffusion, are rapidly transforming the way we communicate, illustrate, and create. However, AI regulation, in the EU and beyond, has primarily focused on conventional AI models, not LGAIMs. This paper will situate these new generative models in the current debate on trustworthy AI regulation, and ask how the law can be tailored to their capabilities. After laying technical foundations, the legal part of the paper proceeds in four steps, covering (1) direct regulation, (2) data protection, (3) content moderation, and (4) policy proposals. It suggests a novel terminology to capture the AI value chain in LGAIM settings by differentiating between LGAIM developers, deployers, professional and non-professional users, as well as recipients of LGAIM output. We tailor regulatory duties to these different actors along the value chain and suggest strategies to ensure that LGAIMs are trustworthy and deployed for the benefit of society at large. Rules in the AI Act and other direct regulation must match the specificities of pre-trained models. The paper argues for three layers of obligations concerning LGAIMs (minimum standards for all LGAIMs; high-risk obligations for high-risk use cases; collaborations along the AI value chain). In general, regulation should focus on concrete high-risk applications, and not the pre-trained model itself, and should include (i) obligations regarding transparency and (ii) risk management. Non-discrimination provisions (iii) may, however, apply to LGAIM developers. Lastly, (iv) the core of the DSA's content moderation rules should be expanded to cover LGAIMs. This includes notice and action mechanisms, and trusted flaggers.},
booktitle = {Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1112–1123},
numpages = {12},
location = {Chicago, IL, USA},
series = {FAccT '23}
}


@article{ho2012fudging,
  author       = {Daniel E. Ho},
  title        = {Fudging the Nudge: Information Disclosure and Restaurant Grading},
  journal      = {Yale Law Journal},
  volume       = {122},
  pages        = {574--583},
  year         = {2012},
}


@article{hess2019ttrap,
	abstract = {This article examines the potential for transparency programs to improve corporations' human rights performance. The primary focus is on ``general'' transparency programs such as the inclusion of human rights issues in sustainability reports. Regulators increasingly rely on such programs, one of which is the EU Directive on the Disclosure of Non-financial Information, which many commentators view as a model for legislation in other countries and for a business and human rights treaty. This article identifies several problems with this approach. The human rights metrics used in current sustainability reporting standards often lack validity or are based upon data that is most easily collected, rather than most important. Moreover, the empirical evidence on sustainability reporting shows continued problems of selective disclosure, impression management, incomparable disclosures, and the use of disclosure as an end in itself (as opposed to a process that leads to organizational change). To move forward, regulators should shift focus to a model grounded in regulatory pluralism. Under this approach, regulators would combine a selection of targeted transparency mechanisms to create a more complete regulatory system that corrects for one disclosure mechanism's weaknesses by including others that have complementary strengths.},
	author = {Hess, David},
	doi = {https://doi.org/10.1111/ablj.12134},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/ablj.12134},
	journal = {American Business Law Journal},
	number = {1},
	pages = {5-53},
	title = {The Transparency Trap: Non-Financial Disclosure and the Responsibility of Business to Respect Human Rights},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/ablj.12134},
	volume = {56},
	year = {2019},
	bdsk-url-1 = {https://onlinelibrary.wiley.com/doi/abs/10.1111/ablj.12134},
	bdsk-url-2 = {https://doi.org/10.1111/ablj.12134}}

@inproceedings{irion2022algoff,
author = {Irion, Kristina},
title = {Algorithms Off-Limits? If Digital Trade Law Restricts Access to Source Code of Software Then Accountability Will Suffer},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533212},
doi = {10.1145/3531146.3533212},
abstract = {Free trade agreements are increasingly used to construct an additional layer of protection for source code of software. This comes in the shape of a new prohibition for governments to require access to, or transfer of, source code of software, subject to certain exceptions. A clause on software source code is also part and parcel of an ambitious set of new rules on trade-related aspects of electronic commerce currently negotiated by 86 members of the World Trade Organization. Our understanding to date of how such a commitment inside trade law impacts on governments right to regulate digital technologies and the policy space that is allowed under trade law is limited. Access to software source code is for example necessary to meet regulatory and judicial needs in order to ensure that digital technologies are in conformity with individuals’ human rights and societal values. This article will unpack and analyze the implications of such a source code clause for current and future digital policies by governments that aim to ensure transparency, fairness and accountability of computer and machine learning algorithms.},
booktitle = {Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1561–1570},
numpages = {10},
keywords = {Application Programming Interface, Source code, Computer algorithms, Transparency, Accountability, International trade law, Software, Fairness, Digital policy},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}

@misc{wsj2021fb,
  author       = {Keach Hagey and Jeff Horwitz},
  title        = {Facebook Tried to Make Its Platform a Healthier Place. It Got Angrier Instead.},
  year         = {2021},
  url          = {https://www.wsj.com/articles/facebook-algorithm-change-zuckerberg-11631654215},
  publisher    = {The Wall Street Journal}
}

@article{sobel2017artificial,
  title={Artificial Intelligence's Fair Use Crisis},
  author={Sobel, Benjamin LW},
  journal={Colum. JL \& Arts},
  volume={41},
  pages={45},
  year={2017},
  publisher={HeinOnline}
}

@article{franceschelli2022copyright,
  title={Copyright in generative deep learning},
  author={Franceschelli, Giorgio and Musolesi, Mirco},
  journal={Data \& Policy},
  volume={4},
  year={2022},
  publisher={Cambridge University Press}
}

@inproceedings{taylor2022galactica,
    title={GALACTICA: A Large Language Model for Science},
    author={Ross Taylor and Marcin Kardas and Guillem Cucurull and Thomas Scialom and Anthony Hartshorn and Elvis Saravia and Andrew Poulton and Viktor Kerkez and Robert Stojnic},
    year={2022},
    url={https://galactica.org/static/paper.pdf}
}

@article{khashabi2022unified,
  title={UnifiedQA-v2: Stronger Generalization via Broader Cross-Format Training},
  author={Daniel Khashabi and Yeganeh Kordi and Hannaneh Hajishirzi},
  journal={ArXiv},
  year={2022},
  volume={abs/2202.12359}
}

@article{bhatt2022recontextualizing,
  title={Re-contextualizing Fairness in NLP: The Case of India},
  author={Shaily Bhatt and Sunipa Dev and Partha P. Talukdar and Shachi Dave and Vinodkumar Prabhakaran},
  journal={ArXiv},
  year={2022},
  volume={abs/2209.12226}
}

@article{zhou2022large,

      title={Large Language Models Are Human-Level Prompt Engineers}, 

      author={Yongchao Zhou and Andrei Ioan Muresanu and Ziwen Han and Keiran Paster and Silviu Pitis and Harris Chan and Jimmy Ba},

      year={2022},

      eprint={2211.01910},

      archivePrefix={arXiv},

      primaryClass={cs.LG}

}

@article{zhao2022inherent,
  author  = {Han Zhao and Geoffrey J. Gordon},
  title   = {Inherent Tradeoffs in Learning Fair Representations},
  journal = {Journal of Machine Learning Research},
  year    = {2022},
  volume  = {23},
  number  = {57},
  pages   = {1--26},
  url     = {http://jmlr.org/papers/v23/21-1427.html}
}

@article{liu2022prompt,
author = {Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
title = {Pre-Train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {0360-0300},
url = {https://doi.org/10.1145/3560815},
doi = {10.1145/3560815},
abstract = {This paper surveys and organizes research works in a new paradigm in natural language processing, which we dub “prompt-based learning”. Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P(y|x), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x′ that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string (hat{bm {x}} ) , from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: it allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this paper we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g.&nbsp;the choice of pre-trained language models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts, but also release other resources, e.g., a website NLPedia–Pretrain including constantly-updated survey, and paperlist.},
note = {Just Accepted},
journal = {ACM Comput. Surv.},
month = {sep},
keywords = {pre-trained language models, prompting}
}

@article{burk2019algorithmic,
  title={Algorithmic fair use},
  author={Burk, Dan L},
  journal={U. Chi. L. Rev.},
  volume={86},
  pages={283},
  year={2019},
  publisher={HeinOnline}
}

@inbook{HarmanRationality,
author = {Harman, Gilbert},
publisher = {John Wiley \& Sons, Ltd},
title = {Rationality},
booktitle = {International Encyclopedia of Ethics},
year = {2013}
}

@inproceedings{wang2013domain,
    title = "Domain-Independent Abstract Generation for Focused Meeting Summarization",
    author = "Wang, Lu  and
      Cardie, Claire",
    booktitle = "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2013",
    address = "Sofia, Bulgaria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P13-1137",
    pages = "1395--1405",
}

@book{MercierSperber,
    author = {Hugo Mercier and Dan Sperber},
    title = {The Enigma of Reason},
    year = {2017},
    publisher = {Harvard University Press}
}

@book{Dehaene,
    author = {Stanislas Dehaene},
    title = {How We Learn: Why Brains are Better than Any Machine \dots for now},
    year = {2020},
    publisher = {Penguin Random House}
}

@inproceedings{nguer2020sencorpus,
  title={SENCORPUS: A French-Wolof Parallel Corpus},
  author={Nguer, Elhadji Mamadou and Lo, Alla and Dione, Cheikh M Bamba and Ba, Sileye O and Lo, Moussa},
  booktitle={Proceedings of the 12th Language Resources and Evaluation Conference},
  pages={2803--2811},
  year={2020}
}

@inproceedings{dutta2020tradeoff,
author = {Dutta, Sanghamitra and Wei, Dennis and Yueksel, Hazar and Chen, Pin-Yu and Liu, Sijia and Varshney, Kush R.},
title = {Is There a Trade-off between Fairness and Accuracy? A Perspective Using Mismatched Hypothesis Testing},
year = {2020},
publisher = {JMLR.org},
abstract = {A trade-off between accuracy and fairness is almost taken as a given in the existing literature on fairness in machine learning. Yet, it is not preordained that accuracy should decrease with increased fairness. Novel to this work, we examine fair classification through the lens of mismatched hypothesis testing: trying to find a classifier that distinguishes between two ideal distributions when given two mismatched distributions that are biased. Using Chernoff information, a tool in information theory, we theoretically demonstrate that, contrary to popular belief, there always exist ideal distributions such that optimal fairness and accuracy (with respect to the ideal distributions) are achieved simultaneously: there is no trade-off. Moreover, the same classifier yields the lack of a trade-off with respect to ideal distributions while yielding a trade-off when accuracy is measured with respect to the given (possibly biased) dataset. To complement our main result, we formulate an optimization to find ideal distributions and derive fundamental limits to explain why a trade-off exists on the given biased dataset. We also derive conditions under which active data collection can alleviate the fairness-accuracy trade-off in the real world. Our results lead us to contend that it is problematic to measure accuracy with respect to data that reflects bias, and instead, we should be considering accuracy with respect to ideal, unbiased data.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {263},
numpages = {11},
series = {ICML'20}
}

@article{wang2021understanding,
  title={Understanding and Improving Fairness-Accuracy Trade-offs in Multi-Task Learning},
  author={Yuyan Wang and Xuezhi Wang and Alex Beutel and Flavien Prost and Jilin Chen and Ed H. Chi},
  journal={Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \& Data Mining},
  year={2021}
}

@inproceedings{
he2021deberta,
title={{\{}DEBERTA{\}}: {\{}DECODING{\}}-{\{}ENHANCED{\}} {\{}BERT{\}} {\{}WITH{\}} {\{}DISENTANGLED{\}} {\{}ATTENTION{\}}},
author={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=XPZIaotutsD}
}

@inproceedings{sambasivan2021reimagining,
author = {Sambasivan, Nithya and Arnesen, Erin and Hutchinson, Ben and Doshi, Tulsee and Prabhakaran, Vinodkumar},
title = {Re-Imagining Algorithmic Fairness in India and Beyond},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445896},
doi = {10.1145/3442188.3445896},
abstract = {Conventional algorithmic fairness is West-centric, as seen in its subgroups, values, and methods. In this paper, we de-center algorithmic fairness and analyse AI power in India. Based on 36 qualitative interviews and a discourse analysis of algorithmic deployments in India, we find that several assumptions of algorithmic fairness are challenged. We find that in India, data is not always reliable due to socio-economic factors, ML makers appear to follow double standards, and AI evokes unquestioning aspiration. We contend that localising model fairness alone can be window dressing in India, where the distance between models and oppressed communities is large. Instead, we re-imagine algorithmic fairness in India and provide a roadmap to re-contextualise data and models, empower oppressed communities, and enable Fair-ML ecosystems.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {315–328},
numpages = {14},
keywords = {India, decoloniality, ability, algorithmic fairness, gender, anti-caste politics, religion, caste, feminism, class, critical algorithmic studies},
location = {Virtual Event, Canada},
series = {FAccT '21}
}

@article{vspetko2021dgx,
  title={DGX-A100 Face to Face DGX-2—Performance, Power and Thermal Behavior Evaluation},
  author={{\v{S}}pet'ko, Matej and Vysock{\`y}, Ond{\v{r}}ej and Jans{\'\i}k, Branislav and {\v{R}}{\'\i}ha, Lubom{\'\i}r},
  journal={Energies},
  volume={14},
  number={2},
  pages={376},
  year={2021},
  publisher={MDPI}
}

@article{fjeld2020principled,
  author      = {Jessica Fjeld and Nele Achten and Hannah Hilligoss and Adam Nagy and Madhulika Srikumar},
  title       = {Principled Artificial Intelligence: Mapping Consensus in Ethical and Rights-Based Approaches to Principles for AI},
  year        = {2020},
  month       = {January 15},
  journal     = {Berkman Klein Center Research Publication},
  number      = {2020-1},
  url         = {https://ssrn.com/abstract=3518482},
  doi         = {10.2139/ssrn.3518482},
}

@inproceedings{kiela2021dynabench,
    title = "Dynabench: Rethinking Benchmarking in {NLP}",
    author = "Kiela, Douwe  and
      Bartolo, Max  and
      Nie, Yixin  and
      Kaushik, Divyansh  and
      Geiger, Atticus  and
      Wu, Zhengxuan  and
      Vidgen, Bertie  and
      Prasad, Grusha  and
      Singh, Amanpreet  and
      Ringshia, Pratik  and
      Ma, Zhiyi  and
      Thrush, Tristan  and
      Riedel, Sebastian  and
      Waseem, Zeerak  and
      Stenetorp, Pontus  and
      Jia, Robin  and
      Bansal, Mohit  and
      Potts, Christopher  and
      Williams, Adina",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.324",
    doi = "10.18653/v1/2021.naacl-main.324",
    pages = "4110--4124",
    abstract = "We introduce Dynabench, an open-source platform for dynamic dataset creation and model benchmarking. Dynabench runs in a web browser and supports human-and-model-in-the-loop dataset creation: annotators seek to create examples that a target model will misclassify, but that another person will not. In this paper, we argue that Dynabench addresses a critical need in our community: contemporary models quickly achieve outstanding performance on benchmark tasks but nonetheless fail on simple challenge examples and falter in real-world scenarios. With Dynabench, dataset creation, model development, and model assessment can directly inform each other, leading to more robust and informative benchmarks. We report on four initial NLP tasks, illustrating these concepts and highlighting the promise of the platform, and address potential objections to dynamic benchmarking as a new standard for the field.",
}

@article{loevinger1957objective,
author = {Jane Loevinger},
title ={Objective Tests as Instruments of Psychological Theory},
journal = {Psychological Reports},
volume = {3},
number = {3},
pages = {635-694},
year = {1957},
doi = {10.2466/pr0.1957.3.3.635},
URL = {https://doi.org/10.2466/pr0.1957.3.3.635},
eprint = {https://doi.org/10.2466/pr0.1957.3.3.635}
}

@article{goldman1958speech,
author = {Frieda Goldman-Eisler},
title ={Speech Production and the Predictability of Words in Context},
journal = {Quarterly Journal of Experimental Psychology},
volume = {10},
number = {2},
pages = {96-106},
year = {1958},
doi = {10.1080/17470215808416261},

URL = { 
        https://doi.org/10.1080/17470215808416261
    
},
eprint = { 
        https://doi.org/10.1080/17470215808416261
    
}
,
    abstract = { The purpose of the experiments was to examine the function of hesitation pauses in speech. Pauses were conceived of as anticipating increase of information in subsequent speech and as involving acts of choice.This hypothesis was tested by relating the incidence of pauses within sentences to the transition probabilities of the words constituting them. Estimates of these probabilities were obtained experimentally by an adaptation of Shannon's guessing technique and were based on reverse as well as forward guessing. The hypothesis was borne out by the facts; hesitancy in speech was shown to be closely related to uncertainty of prediction (entropy) and fluency of utterance to redundancy. These results are shown to be in line with the facts of language statistics. Their theoretical implications for the concept of information and for understanding the processes of speech organization is discussed. }
}

@ARTICLE{baker1975dragon,
    author = {James K. Baker},
    title = {The dragon system – an overview},
    journal = {IEEE Transactions on Acoustic Speech Signal Processing},
    year = {1975}
}

@inbook{baker1975stochastic,
author = {Baker, James K.},
title = {Stochastic Modeling for Automatic Speech Understanding},
year = {1975},
isbn = {1558601244},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
booktitle = {Readings in Speech Recognition},
pages = {297–307},
numpages = {11}
}

@article{jelinek1976continuous,
  title={Continuous speech recognition by statistical methods},
  author={Frederick Jelinek},
  journal={Proceedings of the IEEE},
  year={1976},
  volume={64},
  pages={532-556}
}

@inbook{jelinek1990self,
author = {Jelinek, Frederick},
title = {Self-Organized Language Modeling for Speech Recognition},
year = {1990},
isbn = {1558601244},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
booktitle = {Readings in Speech Recognition},
pages = {450–506},
numpages = {57}
}

@article{hendrycksmath2021,
  title={Measuring Mathematical Problem Solving With the MATH Dataset},
  author={Dan Hendrycks and Collin Burns and Saurav Kadavath and Akul Arora and Steven Basart and Eric Tang and Dawn Song and Jacob Steinhardt},
  journal={NeurIPS},
  year={2021}
}

@inproceedings{
liao2021are,
title={Are We Learning Yet? A Meta Review of Evaluation Failures Across Machine Learning},
author={Thomas Liao and Rohan Taori and Inioluwa Deborah Raji and Ludwig Schmidt},
booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)},
year={2021},
url={https://openreview.net/forum?id=mPducS1MsEK}
}

@inproceedings{Clark2020TransformersAS,
  title={Transformers as Soft Reasoners over Language},
  author={Peter Clark and Oyvind Tafjord and Kyle Richardson},
  booktitle={IJCAI},
  year={2020}
}

@article{chomsky1957syntactic,
  title={Syntactic structures},
  author={Chomsky, Noam},
  journal={The Hague, The Netherlands},
  year={1957}
}

@article{anthony2020carbontracker,
  title={Carbontracker: Tracking and predicting the carbon footprint of training deep learning models},
  author={Anthony, Lasse F Wolff and Kanding, Benjamin and Selvan, Raghavendra},
  journal={arXiv preprint arXiv:2007.03051},
  year={2020}
}


@article{cao2020towards,
  title={Towards accurate and reliable energy measurement of NLP models},
  author={Cao, Qingqing and Balasubramanian, Aruna and Balasubramanian, Niranjan},
  journal={arXiv preprint arXiv:2010.05248},
  year={2020}
}


@book{salton1971smart,
  title={The SMART retrieval system—experiments in automatic document processing},
  author={Salton, Gerard},
  year={1971},
  publisher={Prentice-Hall, Inc.}
}

@article{jones1972statistical,
  title={A statistical interpretation of term specificity and its application in retrieval},
  author={Sp\"arck Jones, Karen},
  journal={Journal of documentation},
  year={1972},
  publisher={MCB UP Ltd}
}

@book{salton1983introduction,
  title={Introduction to modern information retrieval},
  author={Salton, Gerard and McGill, Michael J},
  year={1983},
  publisher={mcgraw-hill}
}

@incollection{goodhart1984problems,
  title={Problems of monetary management: the UK experience},
  author={Goodhart, Charles AE},
  booktitle={Monetary theory and practice},
  pages={91--121},
  year={1984},
  publisher={Springer}
}

@article{messick1987validity,
  title={Validity},
  author={Messick, Samuel},
  journal={ETS Research Report Series},
  volume={1987},
  number={2},
  pages={i--208},
  year={1987},
  publisher={Wiley Online Library},
  url={https://onlinelibrary.wiley.com/doi/abs/10.1002/j.2330-8516.1987.tb00244.x}
}

@article{messick1988assessing,
  title={The once and future issues of validity: Assessing the meaning and consequences of measurement.},
  author={Messick, Samuel},
  year={1988},
  publisher={Lawrence Erlbaum Associates, Inc},
  url={https://onlinelibrary.wiley.com/doi/abs/10.1002/j.2330-8516.1986.tb00185.x},
  journal = {ETS Research Report Series}
}

@article{crenshaw1989intersectional,
  title={Demarginalizing the intersection of race and sex: A black feminist critique of antidiscrimination doctrine, feminist theory and antiracist politics},
  author={Crenshaw, Kimberl{\'e}},
  journal={University of Chicago Legal Forum},
  volume={Vol.1989, Article 8},
  year={1989},
  url={https://chicagounbound.uchicago.edu/cgi/viewcontent.cgi?article=1052&context=uclf}
}

@article{friedman1996bias,
author = {Friedman, Batya and Nissenbaum, Helen},
title = {Bias in Computer Systems},
year = {1996},
issue_date = {July 1996},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/230538.230561},
doi = {10.1145/230538.230561},
abstract = {From an analysis of actual cases, three categories of bias in computer systems have been developed: preexisting, technical, and emergent. Preexisting bias has its roots in social institutions, practices, and attitudes. Technical bias arises from technical constraints of considerations. Emergent bias arises in a context of use. Although others have pointed to bias inparticular computer systems and have noted the general problem, we know of no comparable work that examines this phenomenon comprehensively and which offers a framework for understanding and remedying it. We conclude by suggesting that freedom from bias should by counted amoung the select set of criteria—including reliability, accuracy, and efficiency—according to which the quality of systems in use in society should be judged.},
journal = {ACM Transactions on Information Systems},
month = jul,
pages = {330–347},
numpages = {18},
keywords = {computers and society, standards, human values, universal design, ethics, computer ethics, social impact, design methods, system design, bias, social computing, values}
}

@article{strathern1997improving,
  title={{‘Improving ratings’: audit in the British University system}},
  author={Strathern, Marilyn},
  journal={European Review},
  volume={5},
  number={3},
  pages={305--321},
  year={1997},
  publisher={Cambridge University Press},
  url={https://www.cambridge.org/core/journals/european-review/article/abs/improving-ratings-audit-in-the-british-university-system/FC2EE640C0C44E3DB87C29FB666E9AAB}
}

@inproceedings{yang1997comparative,
  title={A Comparative Study on Feature Selection in Text Categorization},
  author={Yang, Yiming and Pedersen, Jan O.},
  booktitle={Proceedings of the Fourteenth International Conference on Machine Learning},
  pages={412--420},
  year={1997}
}

@inproceedings{joachims1998svm,
  title={Text categorization with support vector machines: Learning with many relevant features},
  author={Joachims, Thorsten},
  booktitle={European conference on machine learning},
  pages={137--142},
  year={1998},
  organization={Springer}
}

@article{yang1999evaluation,
  title={An evaluation of statistical approaches to text categorization},
  author={Yang, Yiming},
  journal={Information retrieval},
  volume={1},
  number={1-2},
  pages={69--90},
  year={1999},
  publisher={Springer}
}

@book{mani1999advances,
 author = {Mani, Inderjeet},
 editor = {Maybury, Mark T.},
 title = {Advances in Automatic Text Summarization},
 year = {1999},
 isbn = {0262133598},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 

@article{sparck1999automatic,
  title={Automatic summarizing: factors and directions},
  author={Sp\"arck Jones, Karen},
  journal={Advances in automatic text summarization},
  pages={1--12},
  year={1999},
  publisher={Cambridge, MA: MIT Press}
}

@inproceedings{hale2001probabilistic,
    title = "A Probabilistic {E}arley Parser as a Psycholinguistic Model",
    author = "Hale, John",
    booktitle = "Second Meeting of the North {A}merican Chapter of the Association for Computational Linguistics",
    year = "2001",
    url = "https://aclanthology.org/N01-1021",
}

@inproceedings{turney2002thumbs,
    title = "Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews",
    author = "Turney, Peter",
    booktitle = "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2002",
    address = "Philadelphia, Pennsylvania, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P02-1053",
    doi = "10.3115/1073083.1073153",
    pages = "417--424",
}

@inproceedings{pang2002thumbs,
    title = "Thumbs up? Sentiment Classification using Machine Learning Techniques",
    author = "Pang, Bo  and
      Lee, Lillian  and
      Vaithyanathan, Shivakumar",
    booktitle = "Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing ({EMNLP} 2002)",
    month = jul,
    year = "2002",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W02-1011",
    doi = "10.3115/1118693.1118704",
    pages = "79--86",
}

@article{wiebe2005annotating,
  title={Annotating Expressions of Opinions and Emotions in Language},
  author={Janyce Wiebe and Theresa Wilson and Claire Cardie},
  journal={Language Resources and Evaluation},
  year={2005},
  volume={39},
  pages={165-210}
}

@article{jones2005acl,
    title = "{ACL} Lifetime Achievement Award: Some Points in a Time",
    author = {Sp{\"a}rck Jones, Karen},
    journal = "Computational Linguistics",
    volume = "31",
    number = "1",
    year = "2005",
    url = "https://aclanthology.org/J05-1001",
    doi = "10.1162/0891201053630237",
    pages = "1--14",
}

@article{pang2008opinion,
author = {Pang, Bo and Lee, Lillian},
title = {Opinion Mining and Sentiment Analysis},
year = {2008},
issue_date = {January 2008},
publisher = {Now Publishers Inc.},
address = {Hanover, MA, USA},
volume = {2},
number = {1–2},
issn = {1554-0669},
url = {https://doi.org/10.1561/1500000011},
doi = {10.1561/1500000011},
abstract = {An important part of our information-gathering behavior has always been to find out what other people think. With the growing availability and popularity of opinion-rich resources such as online review sites and personal blogs, new opportunities and challenges arise as people now can, and do, actively use information technologies to seek out and understand the opinions of others. The sudden eruption of activity in the area of opinion mining and sentiment analysis, which deals with the computational treatment of opinion, sentiment, and subjectivity in text, has thus occurred at least in part as a direct response to the surge of interest in new systems that deal directly with opinions as a first-class object.This survey covers techniques and approaches that promise to directly enable opinion-oriented information-seeking systems. Our focus is on methods that seek to address the new challenges raised by sentiment-aware applications, as compared to those that are already present in more traditional fact-based analysis. We include material on summarization of evaluative text and on broader issues regarding privacy, manipulation, and economic impact that the development of opinion-oriented information-access services gives rise to. To facilitate future work, a discussion of available resources, benchmark datasets, and evaluation campaigns is also provided.},
journal = {Found. Trends Inf. Retr.},
month = {jan},
pages = {1–135},
numpages = {135}
}

@book{jackman2008measurement,
  title={Measurement},
  author={Jackman, Simon},
  series={The Oxford Handbook of Political Methodology},
  url={https://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780199286546.001.0001/oxfordhb-9780199286546-e-6},
  year={2008},
  publisher={Oxford Handbooks}
}

@article{levy2008expectation,
title = {Expectation-based syntactic comprehension},
journal = {Cognition},
volume = {106},
number = {3},
pages = {1126-1177},
year = {2008},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2007.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S0010027707001436},
author = {Roger Levy},
keywords = {Parsing, Frequency, Sentence processing, Information theory, Prediction, Syntax, Word order, Syntactic complexity},
abstract = {This paper investigates the role of resource allocation as a source of processing difficulty in human sentence comprehension. The paper proposes a simple information-theoretic characterization of processing difficulty as the work incurred by resource reallocation during parallel, incremental, probabilistic disambiguation in sentence comprehension, and demonstrates its equivalence to the theory of Hale [Hale, J. (2001). A probabilistic Earley parser as a psycholinguistic model. In Proceedings of NAACL (Vol. 2, pp. 159–166)], in which the difficulty of a word is proportional to its surprisal (its negative log-probability) in the context within which it appears. This proposal subsumes and clarifies findings that high-constraint contexts can facilitate lexical processing, and connects these findings to well-known models of parallel constraint-based comprehension. In addition, the theory leads to a number of specific predictions about the role of expectation in syntactic comprehension, including the reversal of locality-based difficulty patterns in syntactically constrained contexts, and conditions under which increased ambiguity facilitates processing. The paper examines a range of established results bearing on these predictions, and shows that they are largely consistent with the surprisal theory.}
}

@incollection{aggarwal2012survey,
  title={A survey of text classification algorithms},
  author={Aggarwal, Charu C. and Zhai, ChengXiang},
  booktitle={Mining text data},
  pages={163--222},
  year={2012},
  publisher={Springer}
}

@incollection{nenkova2012survey,
  title={A survey of text summarization techniques},
  author={Nenkova, Ani and McKeown, Kathleen},
  booktitle={Mining text data},
  pages={43--76},
  year={2012},
  publisher={Springer}
}

@article{mcauley2012learning,
  title={Learning Attitudes and Attributes from Multi-aspect Reviews},
  author={Julian McAuley and Jure Leskovec and Dan Jurafsky},
  journal={2012 IEEE 12th International Conference on Data Mining},
  year={2012},
  pages={1020-1025}
}

@article{sweeney2013discrimination,
 author = {Sweeney, Latanya},
 title = {Discrimination in Online Ad Delivery},
 journal = {Queue},
 issue_date = {March 2013},
 volume = {11},
 number = {3},
 month = mar,
 year = {2013},
 issn = {1542-7730},
 pages = {10:10--10:29},
 articleno = {10},
 numpages = {20},
 url = {http://doi.acm.org/10.1145/2460276.2460278},
 doi = {10.1145/2460276.2460278},
 acmid = {2460278},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@inproceedings{nakov2016semeval,
    title = "{S}em{E}val-2016 Task 4: Sentiment Analysis in {T}witter",
    author = "Nakov, Preslav  and
      Ritter, Alan  and
      Rosenthal, Sara  and
      Sebastiani, Fabrizio  and
      Stoyanov, Veselin",
    booktitle = "Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016)",
    month = jun,
    year = "2016",
    address = "San Diego, California",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/S16-1001",
    doi = "10.18653/v1/S16-1001",
    pages = "1--18",
}

@article{linzen2016lstm,
    author = {Linzen, Tal and Dupoux, Emmanuel and Goldberg, Yoav},
    title = "{Assessing the Ability of LSTMs to Learn Syntax-Sensitive
                    Dependencies}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {4},
    pages = {521-535},
    year = {2016},
    month = {12},
    abstract = "{The success of long short-term memory (LSTM) neural networks in language
                    processing is typically attributed to their ability to capture long-distance
                    statistical regularities. Linguistic regularities are often sensitive to
                    syntactic structure; can such dependencies be captured by LSTMs, which do not
                    have explicit structural representations? We begin addressing this question
                    using number agreement in English subject-verb dependencies. We probe the
                    architecture’s grammatical competence both using training objectives with an
                    explicit grammatical target (number prediction, grammaticality judgments) and
                    using language models. In the strongly supervised settings, the LSTM achieved
                    very high overall accuracy (less than 1\\% errors), but errors increased when
                    sequential and structural information conflicted. The frequency of such errors
                    rose sharply in the language-modeling setting. We conclude that LSTMs can
                    capture a non-trivial amount of grammatical structure given targeted
                    supervision, but stronger architectures may be required to further reduce
                    errors; furthermore, the language modeling signal is insufficient for capturing
                    syntax-sensitive dependencies, and should be supplemented with more direct
                    supervision if such dependencies need to be captured.}",
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00115},
    url = {https://doi.org/10.1162/tacl\_a\_00115},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00115/1567418/tacl\_a\_00115.pdf},
}

@inproceedings{schmidt2017survey,
    title = "A Survey on Hate Speech Detection using Natural Language Processing",
    author = "Schmidt, Anna  and
      Wiegand, Michael",
    booktitle = "Proceedings of the Fifth International Workshop on Natural Language Processing for Social Media",
    month = apr,
    year = "2017",
    address = "Valencia, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W17-1101",
    doi = "10.18653/v1/W17-1101",
    pages = "1--10",
    abstract = "This paper presents a survey on hate speech detection. Given the steadily growing body of social media content, the amount of online hate speech is also increasing. Due to the massive scale of the web, methods that automatically detect hate speech are required. Our survey describes key areas that have been explored to automatically recognize these types of utterances using natural language processing. We also discuss limits of those approaches.",
}

@article{merity2018analysis,
  title={An Analysis of Neural Language Modeling at Multiple Scales},
  author={Stephen Merity and Nitish Shirish Keskar and Richard Socher},
  journal={ArXiv},
  year={2018},
  volume={abs/1803.08240}
}

@inproceedings{grusky2018newsroom,
    title = "{N}ewsroom: A Dataset of 1.3 Million Summaries with Diverse Extractive Strategies",
    author = "Grusky, Max  and
      Naaman, Mor  and
      Artzi, Yoav",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N18-1065",
    doi = "10.18653/v1/N18-1065",
    pages = "708--719",
    abstract = "We present NEWSROOM, a summarization dataset of 1.3 million articles and summaries written by authors and editors in newsrooms of 38 major news publications. Extracted from search and social media metadata between 1998 and 2017, these high-quality summaries demonstrate high diversity of summarization styles. In particular, the summaries combine abstractive and extractive strategies, borrowing words and phrases from articles at varying rates. We analyze the extraction strategies used in NEWSROOM summaries against other datasets to quantify the diversity and difficulty of our new data, and train existing methods on the data to evaluate its utility and challenges. The dataset is available online at summari.es.",
}

@misc{gokaslan2019openwebtext,  
	title={OpenWebText Corpus},
	author={Aaron Gokaslan and Vanya Cohen},
	howpublished={\url{http://Skylion007.github.io/OpenWebTextCorpus}}, 
	year={2019}
}

@inproceedings{peyrard2019simple,
    title = "A Simple Theoretical Model of Importance for Summarization",
    author = "Peyrard, Maxime",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1101",
    pages = "1059--1073",
    abstract = "Research on summarization has mainly been driven by empirical approaches, crafting systems to perform well on standard datasets with the notion of information Importance remaining latent. We argue that establishing theoretical models of Importance will advance our understanding of the task and help to further improve summarization systems. To this end, we propose simple but rigorous definitions of several concepts that were previously used only intuitively in summarization: Redundancy, Relevance, and Informativeness. Importance arises as a single quantity naturally unifying these concepts. Additionally, we provide intuitions to interpret the proposed quantities and experiments to demonstrate the potential of the framework to inform and guide subsequent works.",
}

@book{benjamin2019race,
  title={Race after Technology},
  author={Benjamin, Ruha},
  publisher={Polity Press},
  year= {2019},
}

@inproceedings{bordia2019bias,
    title = "Identifying and Reducing Gender Bias in Word-Level Language Models",
    author = "Bordia, Shikha  and
      Bowman, Samuel R.",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Student Research Workshop",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-3002",
    doi = "10.18653/v1/N19-3002",
    pages = "7--15",
    abstract = "Many text corpora exhibit socially problematic biases, which can be propagated or amplified in the models trained on such data. For example, doctor cooccurs more frequently with male pronouns than female pronouns. In this study we (i) propose a metric to measure gender bias; (ii) measure bias in a text corpus and the text generated from a recurrent neural network language model trained on the text corpus; (iii) propose a regularization loss term for the language model that minimizes the projection of encoder-trained embeddings onto an embedding subspace that encodes gender; (iv) finally, evaluate efficacy of our proposed method on reducing gender bias. We find this regularization method to be effective in reducing gender bias up to an optimal weight assigned to the loss term, beyond which the model becomes unstable as the perplexity increases. We replicate this study on three training corpora{---}Penn Treebank, WikiText-2, and CNN/Daily Mail{---}resulting in similar conclusions.",
}

@inproceedings{ethayarajh2019undesirable,
    title = "Understanding Undesirable Word Embedding Associations",
    author = "Ethayarajh, Kawin  and
      Duvenaud, David  and
      Hirst, Graeme",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1166",
    doi = "10.18653/v1/P19-1166",
    pages = "1696--1705",
    abstract = "Word embeddings are often criticized for capturing undesirable word associations such as gender stereotypes. However, methods for measuring and removing such biases remain poorly understood. We show that for any embedding model that implicitly does matrix factorization, debiasing vectors post hoc using subspace projection (Bolukbasi et al., 2016) is, under certain conditions, equivalent to training on an unbiased corpus. We also prove that WEAT, the most common association test for word embeddings, systematically overestimates bias. Given that the subspace projection method is provably effective, we use it to derive a new measure of association called the relational inner product association (RIPA). Experiments with RIPA reveal that, on average, skipgram with negative sampling (SGNS) does not make most words any more gendered than they are in the training corpus. However, for gender-stereotyped words, SGNS actually amplifies the gender association in the corpus.",
}

@inproceedings{bommasani2020intrinsic,
    title = "Intrinsic Evaluation of Summarization Datasets",
    author = "Bommasani, Rishi  and
      Cardie, Claire",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.649",
    doi = "10.18653/v1/2020.emnlp-main.649",
    pages = "8075--8096",
    abstract = "High quality data forms the bedrock for building meaningful statistical models in NLP. Consequently, data quality must be evaluated either during dataset construction or *post hoc*. Almost all popular summarization datasets are drawn from natural sources and do not come with inherent quality assurance guarantees. In spite of this, data quality has gone largely unquestioned for many of these recent datasets. We perform the first large-scale evaluation of summarization datasets by introducing 5 intrinsic metrics and applying them to 10 popular datasets. We find that data usage in recent summarization research is sometimes inconsistent with the underlying properties of the data. Further, we discover that our metrics can serve the additional purpose of being inexpensive heuristics for detecting generically low quality examples.",
}

@inproceedings{linzen2020accelerate,
    title = "How Can We Accelerate Progress Towards Human-like Linguistic Generalization?",
    author = "Linzen, Tal",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.465",
    doi = "10.18653/v1/2020.acl-main.465",
    pages = "5210--5217",
    abstract = "This position paper describes and critiques the Pretraining-Agnostic Identically Distributed (PAID) evaluation paradigm, which has become a central tool for measuring progress in natural language understanding. This paradigm consists of three stages: (1) pre-training of a word prediction model on a corpus of arbitrary size; (2) fine-tuning (transfer learning) on a training set representing a classification task; (3) evaluation on a test set drawn from the same distribution as that training set. This paradigm favors simple, low-bias architectures, which, first, can be scaled to process vast amounts of data, and second, can capture the fine-grained statistical properties of a particular data set, regardless of whether those properties are likely to generalize to examples of the task outside the data set. This contrasts with humans, who learn language from several orders of magnitude less data than the systems favored by this evaluation paradigm, and generalize to new tasks in a consistent way. We advocate for supplementing or replacing PAID with paradigms that reward architectures that generalize as quickly and robustly as humans.",
}


@inproceedings{abebe2020roles,
  title={Roles for computing in social change},
  author={Abebe, Rediet and Barocas, Solon and Kleinberg, Jon and Levy, Karen and Raghavan, Manish and Robinson, David G},
  booktitle={Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
  pages={252--260},
  year={2020}
}

@inproceedings{gauthier2020syntaxgym,
    title = "{S}yntax{G}ym: An Online Platform for Targeted Evaluation of Language Models",
    author = "Gauthier, Jon  and
      Hu, Jennifer  and
      Wilcox, Ethan  and
      Qian, Peng  and
      Levy, Roger",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-demos.10",
    doi = "10.18653/v1/2020.acl-demos.10",
    pages = "70--76",
    abstract = "Targeted syntactic evaluations have yielded insights into the generalizations learned by neural network language models. However, this line of research requires an uncommon confluence of skills: both the theoretical knowledge needed to design controlled psycholinguistic experiments, and the technical proficiency needed to train and deploy large-scale language models. We present SyntaxGym, an online platform designed to make targeted evaluations accessible to both experts in NLP and linguistics, reproducible across computing environments, and standardized following the norms of psycholinguistic experimental design. This paper releases two tools of independent value for the computational linguistics community: 1. A website, syntaxgym.org, which centralizes the process of targeted syntactic evaluation and provides easy tools for analysis and visualization; 2. Two command-line tools, {`}syntaxgym{`} and {`}lm-zoo{`}, which allow any user to reproduce targeted syntactic evaluations and general language model inference on their own machine.",
}

@inproceedings{zhang2020pegasus,
author = {Zhang, Jingqing and Zhao, Yao and Saleh, Mohammad and Liu, Peter J.},
title = {PEGASUS: Pre-Training with Extracted Gap-Sentences for Abstractive Summarization},
year = {2020},
publisher = {JMLR.org},
abstract = {Recent work pre-training Transformers with self-supervised objectives on large text corpora has shown great success when fine-tuned on downstream NLP tasks including text summarization. However, pre-training objectives tailored for abstractive text summarization have not been explored. Furthermore there is a lack of systematic evaluation across diverse domains. In this work, we propose pretraining large Transformer-based encoder-decoder models on massive text corpora with a new self-supervised objective. In PEGASUS, important sentences are removed/masked from an input document and are generated together as one output sequence from the remaining sentences, similar to an extractive summary. We evaluated our best PEGASUS model on 12 downstream summarization tasks spanning news, science, stories, instructions, emails, patents, and legislative bills. Experiments demonstrate it achieves state-of-the-art performance on all 12 downstream datasets measured by ROUGE scores. Our model also shows surprising performance on low-resource summarization, surpassing previous state-of-the-art results on 6 datasets with only 1000 examples. Finally we validated our results using human evaluation and show that our model summaries achieve human performance on multiple datasets.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {1051},
numpages = {12},
series = {ICML'20}
}

@inproceedings{blodgett2020critical,
    title = "Language (Technology) is Power: A Critical Survey of {``}Bias{''} in {NLP}",
    author = "Blodgett, Su Lin  and
      Barocas, Solon  and
      Daum{\'e} III, Hal  and
      Wallach, Hanna",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.485",
    doi = "10.18653/v1/2020.acl-main.485",
    pages = "5454--5476",
    abstract = "We survey 146 papers analyzing {``}bias{''} in NLP systems, finding that their motivations are often vague, inconsistent, and lacking in normative reasoning, despite the fact that analyzing {``}bias{''} is an inherently normative process. We further find that these papers{'} proposed quantitative techniques for measuring or mitigating {``}bias{''} are poorly matched to their motivations and do not engage with the relevant literature outside of NLP. Based on these findings, we describe the beginnings of a path forward by proposing three recommendations that should guide work analyzing {``}bias{''} in NLP systems. These recommendations rest on a greater recognition of the relationships between language and social hierarchies, encouraging researchers and practitioners to articulate their conceptualizations of {``}bias{''}---i.e., what kinds of system behaviors are harmful, in what ways, to whom, and why, as well as the normative reasoning underlying these statements{---}and to center work around the lived experiences of members of communities affected by NLP systems, while interrogating and reimagining the power relations between technologists and such communities.",
}

@inproceedings{jo2020archives,
author = {Jo, Eun Seo and Gebru, Timnit},
title = {Lessons from Archives: Strategies for Collecting Sociocultural Data in Machine Learning},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372829},
doi = {10.1145/3351095.3372829},
abstract = {A growing body of work shows that many problems in fairness, accountability, transparency, and ethics in machine learning systems are rooted in decisions surrounding the data collection and annotation process. In spite of its fundamental nature however, data collection remains an overlooked part of the machine learning (ML) pipeline. In this paper, we argue that a new specialization should be formed within ML that is focused on methodologies for data collection and annotation: efforts that require institutional frameworks and procedures. Specifically for sociocultural data, parallels can be drawn from archives and libraries. Archives are the longest standing communal effort to gather human information and archive scholars have already developed the language and procedures to address and discuss many challenges pertaining to data collection such as consent, power, inclusivity, transparency, and ethics &amp; privacy. We discuss these five key approaches in document collection practices in archives that can inform data collection in sociocultural ML. By showing data collection practices from another field, we encourage ML research to be more cognizant and systematic in data collection and draw from interdisciplinary expertise.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {306–316},
numpages = {11},
keywords = {datasets, machine learning, ML fairness, data collection, sociocultural data, archives},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{shin2020autoprompt,
    title = "{A}uto{P}rompt: {E}liciting {K}nowledge from {L}anguage {M}odels with {A}utomatically {G}enerated {P}rompts",
    author = "Shin, Taylor  and
      Razeghi, Yasaman  and
      Logan IV, Robert L.  and
      Wallace, Eric  and
      Singh, Sameer",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.346",
    doi = "10.18653/v1/2020.emnlp-main.346",
    pages = "4222--4235",
    abstract = "The remarkable success of pretrained language models has motivated the study of what kinds of knowledge these models learn during pretraining. Reformulating tasks as fill-in-the-blanks problems (e.g., cloze tests) is a natural approach for gauging such knowledge, however, its usage is limited by the manual effort and guesswork required to write suitable prompts. To address this, we develop AutoPrompt, an automated method to create prompts for a diverse set of tasks, based on a gradient-guided search. Using AutoPrompt, we show that masked language models (MLMs) have an inherent capability to perform sentiment analysis and natural language inference without additional parameters or finetuning, sometimes achieving performance on par with recent state-of-the-art supervised models. We also show that our prompts elicit more accurate factual knowledge from MLMs than the manually created prompts on the LAMA benchmark, and that MLMs can be used as relation extractors more effectively than supervised relation extraction models. These results demonstrate that automatically generated prompts are a viable parameter-free alternative to existing probing methods, and as pretrained LMs become more sophisticated and capable, potentially a replacement for finetuning.",
}

@inproceedings{hovy2021importance,
    title = "The Importance of Modeling Social Factors of Language: Theory and Practice",
    author = "Hovy, Dirk  and
      Yang, Diyi",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.49",
    doi = "10.18653/v1/2021.naacl-main.49",
    pages = "588--602",
    abstract = "Natural language processing (NLP) applications are now more powerful and ubiquitous than ever before. With rapidly developing (neural) models and ever-more available data, current NLP models have access to more information than any human speaker during their life. Still, it would be hard to argue that NLP models have reached human-level capacity. In this position paper, we argue that the reason for the current limitations is a focus on information content while ignoring language{'}s social factors. We show that current NLP systems systematically break down when faced with interpreting the social factors of language. This limits applications to a subset of information-related tasks and prevents NLP from reaching human-level performance. At the same time, systems that incorporate even a minimum of social factors already show remarkable improvements. We formalize a taxonomy of seven social factors based on linguistic theory and exemplify current failures and emerging successes for each of them. We suggest that the NLP community address social factors to get closer to the goal of human-like language understanding.",
}

@inproceedings{bowman2021fix,
    title = "What Will it Take to Fix Benchmarking in Natural Language Understanding?",
    author = "Bowman, Samuel R.  and
      Dahl, George",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2021.naacl-main.385",
    doi = "10.18653/v1/2021.naacl-main.385",
    pages = "4843--4855",
    abstract = "Evaluation for many natural language understanding (NLU) tasks is broken: Unreliable and biased systems score so highly on standard benchmarks that there is little room for researchers who develop better systems to demonstrate their improvements. The recent trend to abandon IID benchmarks in favor of adversarially-constructed, out-of-distribution test sets ensures that current models will perform poorly, but ultimately only obscures the abilities that we want our benchmarks to measure. In this position paper, we lay out four criteria that we argue NLU benchmarks should meet. We argue most current benchmarks fail at these criteria, and that adversarial data collection does not meaningfully address the causes of these failures. Instead, restoring a healthy evaluation ecosystem will require significant progress in the design of benchmark datasets, the reliability with which they are annotated, their size, and the ways they handle social bias.",
}

@inproceedings{alex2021raft,
 author = {Alex, Neel and Lifland, Eli and Tunstall, Lewis and Thakur, Abhishek and Maham, Pegah and Riedel, C. and Hine, Emmie and Ashurst, Carolyn and Sedille, Paul and Carlier, Alexis and Noetel, Michael and Stuhlm\"{u}ller, Andreas},
 booktitle = {Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks},
 editor = {J. Vanschoren and S. Yeung},
 pages = {},
 title = {RAFT: A Real-World Few-Shot Text Classification Benchmark},
 url = {https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/file/ca46c1b9512a7a8315fa3c5a946e8265-Paper-round2.pdf},
 volume = {1},
 year = {2021}
}

@inproceedings{potts2021dynasent,
    title = "{D}yna{S}ent: A Dynamic Benchmark for Sentiment Analysis",
    author = "Potts, Christopher  and
      Wu, Zhengxuan  and
      Geiger, Atticus  and
      Kiela, Douwe",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.186",
    doi = "10.18653/v1/2021.acl-long.186",
    pages = "2388--2404",
    abstract = "We introduce DynaSent ({`}Dynamic Sentiment{'}), a new English-language benchmark task for ternary (positive/negative/neutral) sentiment analysis. DynaSent combines naturally occurring sentences with sentences created using the open-source Dynabench Platform, which facilities human-and-model-in-the-loop dataset creation. DynaSent has a total of 121,634 sentences, each validated by five crowdworkers, and its development and test splits are designed to produce chance performance for even the best models we have been able to develop; when future models solve this task, we will use them to create DynaSent version 2, continuing the dynamic evolution of this benchmark. Here, we report on the dataset creation effort, focusing on the steps we took to increase quality and reduce artifacts. We also present evidence that DynaSent{'}s Neutral category is more coherent than the comparable category in other benchmarks, and we motivate training models from scratch for each round over successive fine-tuning.",
}

@article{paullada2021data,
title = {Data and its (dis)contents: A survey of dataset development and use in machine learning research},
journal = {Patterns},
volume = {2},
number = {11},
pages = {100336},
year = {2021},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2021.100336},
url = {https://www.sciencedirect.com/science/article/pii/S2666389921001847},
author = {Amandalynne Paullada and Inioluwa Deborah Raji and Emily M. Bender and Emily Denton and Alex Hanna},
keywords = {datasets machine learning},
abstract = {Summary
In this work, we survey a breadth of literature that has revealed the limitations of predominant practices for dataset collection and use in the field of machine learning. We cover studies that critically review the design and development of datasets with a focus on negative societal impacts and poor outcomes for system performance. We also cover approaches to filtering and augmenting data and modeling techniques aimed at mitigating the impact of bias in datasets. Finally, we discuss works that have studied data practices, cultures, and disciplinary norms and discuss implications for the legal, ethical, and functional challenges the field continues to face. Based on these findings, we advocate for the use of both qualitative and quantitative approaches to more carefully document and analyze datasets during the creation and usage phases.}
}

@inproceedings{lescao2021prompt,
    title = "How many data points is a prompt worth?",
    author = "Le Scao, Teven  and
      Rush, Alexander",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.208",
    doi = "10.18653/v1/2021.naacl-main.208",
    pages = "2627--2636",
    abstract = "When fine-tuning pretrained models for classification, researchers either use a generic model head or a task-specific prompt for prediction. Proponents of prompting have argued that prompts provide a method for injecting task-specific guidance, which is beneficial in low-data regimes. We aim to quantify this benefit through rigorous testing of prompts in a fair setting: comparing prompted and head-based fine-tuning in equal conditions across many tasks and data sizes. By controlling for many sources of advantage, we find that prompting does indeed provide a benefit, and that this benefit can be quantified per task. Results show that prompting is often worth 100s of data points on average across classification tasks.",
}

@inproceedings{antoniak2021seeds,
    title = "Bad Seeds: Evaluating Lexical Methods for Bias Measurement",
    author = "Antoniak, Maria  and
      Mimno, David",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.148",
    doi = "10.18653/v1/2021.acl-long.148",
    pages = "1889--1904",
    abstract = "A common factor in bias measurement methods is the use of hand-curated seed lexicons, but there remains little guidance for their selection. We gather seeds used in prior work, documenting their common sources and rationales, and in case studies of three English-language corpora, we enumerate the different types of social biases and linguistic features that, once encoded in the seeds, can affect subsequent bias measurements. Seeds developed in one context are often re-used in other contexts, but documentation and evaluation remain necessary precursors to relying on seeds for sensitive measurements.",
}

@inproceedings{blodgett2021norwegian,
    title = "Stereotyping {N}orwegian Salmon: An Inventory of Pitfalls in Fairness Benchmark Datasets",
    author = "Blodgett, Su Lin  and
      Lopez, Gilsinia  and
      Olteanu, Alexandra  and
      Sim, Robert  and
      Wallach, Hanna",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.81",
    doi = "10.18653/v1/2021.acl-long.81",
    pages = "1004--1015",
    abstract = "Auditing NLP systems for computational harms like surfacing stereotypes is an elusive goal. Several recent efforts have focused on benchmark datasets consisting of pairs of contrastive sentences, which are often accompanied by metrics that aggregate an NLP system{'}s behavior on these pairs into measurements of harms. We examine four such benchmarks constructed for two NLP tasks: language modeling and coreference resolution. We apply a measurement modeling lens{---}originating from the social sciences{---}to inventory a range of pitfalls that threaten these benchmarks{'} validity as measurement models for stereotyping. We find that these benchmarks frequently lack clear articulations of what is being measured, and we highlight a range of ambiguities and unstated assumptions that affect how these benchmarks conceptualize and operationalize stereotyping.",
}

@inproceedings{goldfarb2021intrinsic,
    title = "Intrinsic Bias Metrics Do Not Correlate with Application Bias",
    author = "Goldfarb-Tarrant, Seraphina  and
      Marchant, Rebecca  and
      Mu{\~n}oz S{\'a}nchez, Ricardo  and
      Pandya, Mugdha  and
      Lopez, Adam",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.150",
    doi = "10.18653/v1/2021.acl-long.150",
    pages = "1926--1940",
    abstract = "Natural Language Processing (NLP) systems learn harmful societal biases that cause them to amplify inequality as they are deployed in more and more situations. To guide efforts at debiasing these systems, the NLP community relies on a variety of metrics that quantify bias in models. Some of these metrics are intrinsic, measuring bias in word embedding spaces, and some are extrinsic, measuring bias in downstream tasks that the word embeddings enable. Do these intrinsic and extrinsic metrics correlate with each other? We compare intrinsic and extrinsic metrics across hundreds of trained models covering different tasks and experimental conditions. Our results show no reliable correlation between these metrics that holds in all scenarios across tasks and languages. We urge researchers working on debiasing to focus on extrinsic measures of bias, and to make using these measures more feasible via creation of new challenge sets and annotated test data. To aid this effort, we release code, a new intrinsic metric, and an annotated test set focused on gender bias in hate speech.",
}

@article{askell2021general,
  title={A General Language Assistant as a Laboratory for Alignment},
  author={Amanda Askell and Yushi Bai and Anna Chen and Dawn Drain and Deep Ganguli and T. J. Henighan and Andy Jones and Nicholas Joseph and Benjamin Mann and Nova DasSarma and Nelson Elhage and Zac Hatfield-Dodds and Danny Hernandez and John Kernion and Kamal Ndousse and Catherine Olsson and Dario Amodei and Tom B. Brown and Jack Clark and Sam McCandlish and Christopher Olah and Jared Kaplan},
  journal={ArXiv},
  year={2021},
  volume={abs/2112.00861}
}

@inproceedings{rogers2021changing,
    title = "Changing the World by Changing the Data",
    author = "Rogers, Anna",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.170",
    doi = "10.18653/v1/2021.acl-long.170",
    pages = "2182--2194",
    abstract = "NLP community is currently investing a lot more research and resources into development of deep learning models than training data. While we have made a lot of progress, it is now clear that our models learn all kinds of spurious patterns, social biases, and annotation artifacts. Algorithmic solutions have so far had limited success. An alternative that is being actively discussed is more careful design of datasets so as to deliver specific signals. This position paper maps out the arguments for and against data curation, and argues that fundamentally the point is moot: curation already is and will be happening, and it is changing the world. The question is only how much thought we want to invest into that process.",
}

@article{chen2021codex,
  title={Evaluating Large Language Models Trained on Code},
  author={Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde and Jared Kaplan and Harrison Edwards and Yura Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and David W. Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William H. Guss and Alex Nichol and Igor Babuschkin and S. Arun Balaji and Shantanu Jain and Andrew Carr and Jan Leike and Joshua Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew M. Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
  journal={ArXiv},
  year={2021},
  volume={abs/2107.03374}
}

@inproceedings{srivastava2021behavior,
      title={BEHAVIOR: Benchmark for Everyday Household Activities in Virtual, Interactive, and Ecological Environments}, 
      author={Sanjana Srivastava and Chengshu Li and Michael Lingelbach and Roberto Mart\'in-Mart\'in and Fei Xia and Kent Vainio and Zheng Lian and Cem Gokmen and Shyamal Buch and Karen Liu and Silvio Savarese and Hyowon Gweon and Jiajun Wu and Li Fei-Fei},
      booktitle={Conference in Robot Learning (CoRL)},
      year={2021},
      pages={accepted}
}

@inproceedings{dodge2021c4,
    title = "Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus",
    author = "Dodge, Jesse  and
      Sap, Maarten  and
      Marasovi{\'c}, Ana  and
      Agnew, William  and
      Ilharco, Gabriel  and
      Groeneveld, Dirk  and
      Mitchell, Margaret  and
      Gardner, Matt",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.98",
    doi = "10.18653/v1/2021.emnlp-main.98",
    pages = "1286--1305",
    abstract = "Large language models have led to remarkable progress on many NLP tasks, and researchers are turning to ever-larger text corpora to train them. Some of the largest corpora available are made by scraping significant portions of the internet, and are frequently introduced with only minimal documentation. In this work we provide some of the first documentation for the Colossal Clean Crawled Corpus (C4; Raffel et al., 2020), a dataset created by applying a set of filters to a single snapshot of Common Crawl. We begin by investigating where the data came from, and find a significant amount of text from unexpected sources like patents and US military websites. Then we explore the content of the text itself, and find machine-generated text (e.g., from machine translation systems) and evaluation examples from other benchmark NLP datasets. To understand the impact of the filters applied to create this dataset, we evaluate the text that was removed, and show that blocklist filtering disproportionately removes text from and about minority individuals. Finally, we conclude with some recommendations for how to created and document web-scale datasets from a scrape of the internet.",
}

@article{lieber2021jurassic,
  title={Jurassic-1: Technical details and evaluation},
  author={Opher Lieber and Or Sharir and Barak Lenz and Yoav Shoham},
  journal={White Paper, AI21 Labs},
  year={2021},
  url={https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf}
}

@inproceedings{
koch2021reduced,
title={Reduced, Reused and Recycled: The Life of a  Dataset in Machine Learning Research},
author={Bernard Koch and Emily Denton and Alex Hanna and Jacob Gates Foster},
booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)},
year={2021},
url={https://openreview.net/forum?id=zNQBIBKJRkd}
}

@article{tay2022unifying,
  title={Unifying Language Learning Paradigms},
  author={Yi Tay and Mostafa Dehghani and Vinh Q. Tran and Xavier Garc{\'i}a and Dara Bahri and Tal Schuster and Huaixiu Zheng and Neil Houlsby and Donald Metzler},
  journal={ArXiv},
  year={2022},
  volume={abs/2205.05131}
}

@inproceedings{
wei2022flan,
title={Finetuned Language Models are Zero-Shot Learners},
author={Jason Wei and Maarten Bosma and Vincent Zhao and Kelvin Guu and Adams Wei Yu and Brian Lester and Nan Du and Andrew M. Dai and Quoc V Le},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=gEZrGCozdqR}
}

@article{soltan2022alexa,
  title={AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model},
  author={Saleh Soltan and Shankar Ananthakrishnan and Jack G. M. FitzGerald and Rahul Gupta and Wael Hamza and Haidar Khan and Charith S. Peris and Stephen Rawls and Andrew Rosenbaum and Anna Rumshisky and Chandan Prakash and Mukund Sridhar and Fabian Triefenbach and Apurv Verma and Gokhan Tur and Premkumar Natarajan},
  journal={ArXiv},
  year={2022},
  volume={abs/2208.01448}
}

@article{chung2022scaling,
  title={Scaling Instruction-Finetuned Language Models},
  author={Hyung Won Chung and Le Hou and S. Longpre and Barret Zoph and Yi Tay and William Fedus and Eric Li and Xuezhi Wang and Mostafa Dehghani and Siddhartha Brahma and Albert Webson and Shixiang Shane Gu and Zhuyun Dai and Mirac Suzgun and Xinyun Chen and Aakanksha Chowdhery and Sharan Narang and Gaurav Mishra and Adams Wei Yu and Vincent Zhao and Yanping Huang and Andrew M. Dai and Hongkun Yu and Slav Petrov and Ed Chi and Jeff Dean and Jacob Devlin and Adam Roberts and Denny Zhou and Quoc Le and Jason Wei},
  journal={ArXiv},
  year={2022},
  volume={abs/2210.11416}
}

@article{muennighoff2022crosslingual,
  title={Crosslingual Generalization through Multitask Finetuning},
  author={Niklas Muennighoff and Thomas Wang and Lintang Sutawika and Adam Roberts and Stella Rose Biderman and Teven Le Scao and M Saiful Bari and Sheng Shen and Zheng Xin Yong and Hailey Schoelkopf and Xiangru Tang and Dragomir Radev and Alham Fikri Aji and Khalid Almubarak and Samuel Albanie and Zaid Alyafeai and Albert Webson and Edward Raff and Colin Raffel},
  journal={ArXiv},
  year={2022},
  volume={abs/2211.01786}
}

@inproceedings{wang2022supernatural,
  title={Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks},
  author={Yizhong Wang and Swaroop Mishra and Pegah Alipoormolabashi and Yeganeh Kordi and Amirreza Mirzaei and A. Arunkumar and Arjun Ashok and Arut Selvan Dhanasekaran and Atharva Naik and David Stap and Eshaan Pathak and Giannis Karamanolakis and Haizhi Gary Lai and Ishan Purohit and Ishani Mondal and Jacob Anderson and Kirby Kuznia and Krima Doshi and Maitreya Patel and Kuntal Kumar Pal and M. Moradshahi and Mihir Parmar and Mirali Purohit and Neeraj Varshney and Phani Rohitha Kaza and Pulkit Verma and Ravsehaj Singh Puri and Rushang Karia and Shailaja Keyur Sampat and Savan Doshi and Siddharth Deepak Mishra and Sujan Reddy and Sumanta Patro and Tanay Dixit and Xudong Shen and Chitta Baral and Yejin Choi and Noah A. Smith and Hanna Hajishirzi and Daniel Khashabi},
  year={2022}
}

@article{efrat2022lmentry,
  doi = {10.48550/ARXIV.2211.02069},
  author = {Efrat, Avia and Honovich, Or and Levy, Omer},
    keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {LMentry: A Language Model Benchmark of Elementary Language Tasks},
    publisher = {arXiv},
    year = {2022},
  url = {https://arxiv.org/abs/2211.02069},
  copyright = {Creative Commons Attribution 4.0 International}
}

@article{tay2022transcending,
  title={Transcending Scaling Laws with 0.1\% Extra Compute},
  author={Yi Tay and Jason Wei and Hyung Won Chung and Vinh Q. Tran and David R. So and Siamak Shakeri and Xavier Garc{\'i}a and Huaixiu Zheng and Jinfeng Rao and Aakanksha Chowdhery and Denny Zhou and Donald Metzler and Slav Petrov and Neil Houlsby and Quoc V. Le and Mostafa Dehghani},
  journal={ArXiv},
  year={2022},
  volume={abs/2210.11399}
}

@misc{shevlane2022structured,
  doi = {10.48550/ARXIV.2201.05159},
  url = {https://arxiv.org/abs/2201.05159}, 
  author = {Shevlane, Toby},
  keywords = {Artificial Intelligence (cs.AI), Human-Computer Interaction (cs.HC), Software Engineering (cs.SE), FOS: Computer and information sciences, FOS: Computer and information sciences, 68T99},
  title = {Structured access: an emerging paradigm for safe AI deployment},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{qian2022perturbation,
  title={Perturbation Augmentation for Fairer NLP},
  author={Rebecca Qian and Candace Ross and Jude Fernandes and Eric Michael Smith and Douwe Kiela and Adina Williams},
  journal={ArXiv},
  year={2022},
  volume={abs/2205.12586}
}

@article{goyal2022news,
  title={News Summarization and Evaluation in the Era of GPT-3},
  author={Tanya Goyal and Junyi Jessy Li and Greg Durrett},
  journal={ArXiv},
  year={2022},
  volume={abs/2209.12356}
}

@inproceedings{wu2022aichains,
author = {Wu, Tongshuang and Terry, Michael and Cai, Carrie Jun},
title = {AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3517582},
doi = {10.1145/3491102.3517582},
abstract = {Although large language models (LLMs) have demonstrated impressive potential on simple tasks, their breadth of scope, lack of transparency, and insufficient controllability can make them less effective when assisting humans on more complex tasks. In response, we introduce the concept of Chaining LLM steps together, where the output of one step becomes the input for the next, thus aggregating the gains per step. We first define a set of LLM primitive operations useful for Chain construction, then present an interactive system where users can modify these Chains, along with their intermediate results, in a modular way. In a 20-person user study, we found that Chaining not only improved the quality of task outcomes, but also significantly enhanced system transparency, controllability, and sense of collaboration. Additionally, we saw that users developed new ways of interacting with LLMs through Chains: they leveraged sub-tasks to calibrate model expectations, compared and contrasted alternative strategies by observing parallel downstream effects, and debugged unexpected model outputs by “unit-testing” sub-components of a Chain. In two case studies, we further explore how LLM Chains may be used in future applications.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {385},
numpages = {22},
keywords = {Large Language Models, Human-AI Interaction, Natural Language Processing},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{
he2022towards,
title={Towards a Unified View of Parameter-Efficient Transfer Learning},
author={Junxian He and Chunting Zhou and Xuezhe Ma and Taylor Berg-Kirkpatrick and Graham Neubig},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=0RDcd5Axok}
}

@article{press2022selfask,
  title={Measuring and Narrowing the Compositionality Gap in Language Models},
  author={Ofir Press and Muru Zhang and Sewon Min and Ludwig Schmidt and Noah A. Smith and Mike Lewis},
  journal={ArXiv},
  year={2022},
  volume={abs/2210.03350}
}

@article{arora2022ama,
  title={Ask Me Anything: A simple strategy for prompting language models},
  author={Simran Arora and Avanika Narayan and Mayee F. Chen and Laurel J. Orr and Neel Guha and Kush S Bhatia and Ines Chami and Frederic Sala and Christopher R'e},
  journal={ArXiv},
  year={2022},
  volume={abs/2210.02441}
}

@inproceedings{
chen2022hapi,
title={{HAPI}: A Large-scale Longitudinal Dataset of Commercial {ML} {API} Predictions},
author={Lingjiao Chen and Zhihua Jin and Sabri Eyuboglu and Christopher Re and Matei Zaharia and James Y. Zou},
booktitle={Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
year={2022},
url={https://openreview.net/forum?id=CZeIOfCjMf}
}

@article{gehrmann2022repairing,
  title={Repairing the Cracked Foundation: A Survey of Obstacles in Evaluation Practices for Generated Text},
  author={Sebastian Gehrmann and Elizabeth Clark and Thibault Sellam},
  journal={ArXiv},
  year={2022},
  volume={abs/2202.06935}
}

@article{reiter2022summarization,
  title={Summarisation datasets should contain summaries!},
  author={Ehud Reiter},
  url={https://ehudreiter.com/2022/10/13/summarisation-datasets/},
  year={2022},
}


@article{suzgun2022challenging,
  doi = {10.48550/ARXIV.2210.09261},
  url = {https://arxiv.org/abs/2210.09261},
  author = {Suzgun, Mirac and Scales, Nathan and Schärli, Nathanael and Gehrmann, Sebastian and Tay, Yi and Chung, Hyung Won and Chowdhery, Aakanksha and Le, Quoc V. and Chi, Ed H. and Zhou, Denny and Wei, Jason},
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}

@article{liu2022fewshot,
  title={Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning},
  author={Haokun Liu and Derek Tam and Mohammed Muqeeth and Jay Mohta and Tenghao Huang and Mohit Bansal and Colin Raffel},
  journal={ArXiv},
  year={2022},
  volume={abs/2205.05638}
}

@inproceedings{raji2022fallacy,
author = {Raji, Inioluwa Deborah and Kumar, I. Elizabeth and Horowitz, Aaron and Selbst, Andrew},
title = {The Fallacy of AI Functionality},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533158},
doi = {10.1145/3531146.3533158},
abstract = {Deployed AI systems often do not work. They can be constructed haphazardly, deployed indiscriminately, and promoted deceptively. However, despite this reality, scholars, the press, and policymakers pay too little attention to functionality. This leads to technical and policy solutions focused on “ethical” or value-aligned deployments, often skipping over the prior question of whether a given system functions, or provides any benefits at all. To describe the harms of various types of functionality failures, we analyze a set of case studies to create a taxonomy of known AI functionality issues. We then point to policy and organizational responses that are often overlooked and become more readily available once functionality is drawn into focus. We argue that functionality is a meaningful AI policy challenge, operating as a necessary first step towards protecting affected communities from algorithmic harm.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {959–972},
numpages = {14},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}

@inproceedings{bommasani2022homogenization,
  title={Picking on the Same Person: Does Algorithmic Monoculture lead to Outcome Homogenization?},
  author={Rishi Bommasani and Kathleen A. Creel and Ananya Kumar and Dan Jurafsky and Percy Liang},
    booktitle = {Advances in Neural Information Processing Systems},
  year={2022}
}

@article{thoppilan2022lamda,
  title={LaMDA: Language Models for Dialog Applications},
  author={Romal Thoppilan and Daniel De Freitas and Jamie Hall and Noam M. Shazeer and Apoorv Kulshreshtha and Heng-Tze Cheng and Alicia Jin and Taylor Bos and Leslie Baker and Yu Du and Yaguang Li and Hongrae Lee and Huaixiu Zheng and Amin Ghafouri and Marcelo Menegali and Yanping Huang and Maxim Krikun and Dmitry Lepikhin and James Qin and Dehao Chen and Yuanzhong Xu and Zhifeng Chen and Adam Roberts and Maarten Bosma and Yanqi Zhou and Chung-Ching Chang and I. A. Krivokon and Willard James Rusch and Marc Pickett and Kathleen S. Meier-Hellstern and Meredith Ringel Morris and Tulsee Doshi and Renelito Delos Santos and Toju Duke and Johnny Hartz S{\o}raker and Ben Zevenbergen and Vinodkumar Prabhakaran and Mark D{\'i}az and Ben Hutchinson and Kristen Olson and Alejandra Molina and Erin Hoffman-John and Josh Lee and Lora Aroyo and Ravindran Rajakumar and Alena Butryna and Matthew Lamm and V. O. Kuzmina and Joseph Fenton and Aaron Cohen and Rachel Bernstein and Ray Kurzweil and Blaise Aguera-Arcas and Claire Cui and Marian Croak and Ed Chi and Quoc Le},
  journal={ArXiv},
  year={2022},
  volume={abs/2201.08239}
}

@InProceedings{borgeaud2022retro,
  title = 	 {Improving Language Models by Retrieving from Trillions of Tokens},
  author =       {Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie and Van Den Driessche, George Bm and Lespiau, Jean-Baptiste and Damoc, Bogdan and Clark, Aidan and De Las Casas, Diego and Guy, Aurelia and Menick, Jacob and Ring, Roman and Hennigan, Tom and Huang, Saffron and Maggiore, Loren and Jones, Chris and Cassirer, Albin and Brock, Andy and Paganini, Michela and Irving, Geoffrey and Vinyals, Oriol and Osindero, Simon and Simonyan, Karen and Rae, Jack and Elsen, Erich and Sifre, Laurent},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {2206--2240},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/borgeaud22a/borgeaud22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/borgeaud22a.html},
  abstract = 	 {We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a 2 trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25{\texttimes} fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale.}
}


@inproceedings{weidinger2022taxonomy,
author = {Weidinger, Laura and Uesato, Jonathan and Rauh, Maribeth and Griffin, Conor and Huang, Po-Sen and Mellor, John and Glaese, Amelia and Cheng, Myra and Balle, Borja and Kasirzadeh, Atoosa and Biles, Courtney and Brown, Sasha and Kenton, Zac and Hawkins, Will and Stepleton, Tom and Birhane, Abeba and Hendricks, Lisa Anne and Rimell, Laura and Isaac, William and Haas, Julia and Legassick, Sean and Irving, Geoffrey and Gabriel, Iason},
title = {Taxonomy of Risks Posed by Language Models},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533088},
doi = {10.1145/3531146.3533088},
abstract = {Responsible innovation on large-scale Language Models (LMs) requires foresight into and in-depth understanding of the risks these models may pose. This paper develops a comprehensive taxonomy of ethical and social risks associated with LMs. We identify twenty-one risks, drawing on expertise and literature from computer science, linguistics, and the social sciences. We situate these risks in our taxonomy of six risk areas: I. Discrimination, Hate speech and Exclusion, II. Information Hazards, III. Misinformation Harms, IV. Malicious Uses, V. Human-Computer Interaction Harms, and VI. Environmental and Socioeconomic harms. For risks that have already been observed in LMs, the causal mechanism leading to harm, evidence of the risk, and approaches to risk mitigation are discussed. We further describe and analyse risks that have not yet been observed but are anticipated based on assessments of other language technologies, and situate these in the same taxonomy. We underscore that it is the responsibility of organizations to engage with the mitigations we discuss throughout the paper. We close by highlighting challenges and directions for further research on risk evaluation and mitigation with the goal of ensuring that language models are developed responsibly.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {214–229},
numpages = {16},
keywords = {responsible innovation, responsible AI, risk assessment, language models, technology risks},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}

@inproceedings{jernite2022governance,
author = {Jernite, Yacine and Nguyen, Huu and Biderman, Stella and Rogers, Anna and Masoud, Maraim and Danchev, Valentin and Tan, Samson and Luccioni, Alexandra Sasha and Subramani, Nishant and Johnson, Isaac and Dupont, Gerard and Dodge, Jesse and Lo, Kyle and Talat, Zeerak and Radev, Dragomir and Gokaslan, Aaron and Nikpoor, Somaieh and Henderson, Peter and Bommasani, Rishi and Mitchell, Margaret},
title = {Data Governance in the Age of Large-Scale Data-Driven Language Technology},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3534637},
doi = {10.1145/3531146.3534637},
abstract = {The recent emergence and adoption of Machine Learning technology, and specifically of Large Language Models, has drawn attention to the need for systematic and transparent management of language data. This work proposes an approach to global language data governance that attempts to organize data management amongst stakeholders, values, and rights. Our proposal is informed by prior work on distributed governance that accounts for human values and grounded by an international research collaboration that brings together researchers and practitioners from 60 countries. The framework we present is a multi-party international governance structure focused on language data, and incorporating technical and organizational tools needed to support its work.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {2206–2222},
numpages = {17},
keywords = {data rights, language data, technology governance, datasets},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}

@inproceedings{steed2022upstream,
    title = "{U}pstream {M}itigation {I}s \textit{ {N}ot} {A}ll {Y}ou {N}eed: {T}esting the {B}ias {T}ransfer {H}ypothesis in {P}re-{T}rained {L}anguage {M}odels",
    author = "Steed, Ryan  and
      Panda, Swetasudha  and
      Kobren, Ari  and
      Wick, Michael",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.247",
    doi = "10.18653/v1/2022.acl-long.247",
    pages = "3524--3542",
    abstract = "A few large, homogenous, pre-trained models undergird many machine learning systems {---} and often, these models contain harmful stereotypes learned from the internet. We investigate the \textit{bias transfer hypothesis}: the theory that social biases (such as stereotypes) internalized by large language models during pre-training transfer into harmful task-specific behavior after fine-tuning. For two classification tasks, we find that reducing intrinsic bias with controlled interventions \textit{before} fine-tuning does little to mitigate the classifier{'}s discriminatory behavior \textit{after} fine-tuning. Regression analysis suggests that downstream disparities are better explained by biases in the fine-tuning dataset. Still, pre-training plays a role: simple alterations to co-occurrence rates in the fine-tuning dataset are ineffective when the model has been pre-trained. Our results encourage practitioners to focus more on dataset quality and context-specific harms.",
}

@inproceedings{bach2022promptsource,
    title = "{P}rompt{S}ource: An Integrated Development Environment and Repository for Natural Language Prompts",
    author = "Bach, Stephen  and
      Sanh, Victor  and
      Yong, Zheng Xin  and
      Webson, Albert  and
      Raffel, Colin  and
      Nayak, Nihal V.  and
      Sharma, Abheesht  and
      Kim, Taewoon  and
      Bari, M Saiful  and
      Fevry, Thibault  and
      Alyafeai, Zaid  and
      Dey, Manan  and
      Santilli, Andrea  and
      Sun, Zhiqing  and
      Ben-david, Srulik  and
      Xu, Canwen  and
      Chhablani, Gunjan  and
      Wang, Han  and
      Fries, Jason  and
      Al-shaibani, Maged  and
      Sharma, Shanya  and
      Thakker, Urmish  and
      Almubarak, Khalid  and
      Tang, Xiangru  and
      Radev, Dragomir  and
      Jiang, Mike Tian-jian  and
      Rush, Alexander",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-demo.9",
    doi = "10.18653/v1/2022.acl-demo.9",
    pages = "93--104",
    abstract = "PromptSource is a system for creating, sharing, and using natural language prompts. Prompts are functions that map an example from a dataset to a natural language input and target output. Using prompts to train and query language models is an emerging area in NLP that requires new tools that let users develop and refine these prompts collaboratively. PromptSource addresses the emergent challenges in this new setting with (1) a templating language for defining data-linked prompts, (2) an interface that lets users quickly iterate on prompt development by observing outputs of their prompts on many examples, and (3) a community-driven set of guidelines for contributing new prompts to a common pool. Over 2,000 prompts for roughly 170 datasets are already available in PromptSource. PromptSource is available at https://github.com/bigscience-workshop/promptsource.",
}

@inproceedings{liu2022incontext,
    title = "What Makes Good In-Context Examples for {GPT}-3?",
    author = "Liu, Jiachang  and
      Shen, Dinghan  and
      Zhang, Yizhe  and
      Dolan, Bill  and
      Carin, Lawrence  and
      Chen, Weizhu",
    booktitle = "Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures",
    month = may,
    year = "2022",
    address = "Dublin, Ireland and Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.deelio-1.10",
    doi = "10.18653/v1/2022.deelio-1.10",
    pages = "100--114",
    abstract = "GPT-3 has attracted lots of attention due to its superior performance across a wide range of NLP tasks, especially with its in-context learning abilities. Despite its success, we found that the empirical results of GPT-3 depend heavily on the choice of in-context examples. In this work, we investigate whether there are more effective strategies for judiciously selecting in-context examples (relative to random sampling) that better leverage GPT-3{'}s in-context learning capabilities.Inspired by the recent success of leveraging a retrieval module to augment neural networks, we propose to retrieve examples that are semantically-similar to a test query sample to formulate its corresponding prompt. Intuitively, the examples selected with such a strategy may serve as more informative inputs to unleash GPT-3{'}s power of text generation. We evaluate the proposed approach on several natural language understanding and generation benchmarks, where the retrieval-based prompt selection approach consistently outperforms the random selection baseline. Moreover, it is observed that the sentence encoders fine-tuned on task-related datasets yield even more helpful retrieval results. Notably, significant gains are observed on tasks such as table-to-text generation (44.3{\%} on the ToTTo dataset) and open-domain question answering (45.5{\%} on the NQ dataset).",
}


@misc{liang2022community-norms,
    author = {Liang, Percy and Bommasani, Rishi and Creel, Kathleen A. and Reich, Rob},
    title  = {The Time Is Now to Develop Community Norms for the Release of Foundation Models},
    url    = {https://crfm.stanford.edu/2022/05/17/community-norms.html},
    year   = {2022}
}

@misc{kilcher2022gpt4chan,
    author = {Yannic Kilcher},
    title  = {GPT-4chan},
    url    = {https://github.com/yk/gpt-4chan-public},
    year   = {2022}
}

@misc{liang2022condemning,
    author = {Liang, Percy and Reich, Rob},
    title  = {Condemning the deployment of GPT-4chan},
    url    = {https://docs.google.com/forms/d/e/1FAIpQLSdh3Pgh0sGrYtRihBu-GPN7FSQoODBLvF7dVAFLZk2iuMgoLw/viewform},
    year   = {2022}
}

@inproceedings{xu2021detoxifying,
    title = "Detoxifying Language Models Risks Marginalizing Minority Voices",
    author = "Xu, Albert  and
      Pathak, Eshaan  and
      Wallace, Eric  and
      Gururangan, Suchin  and
      Sap, Maarten  and
      Klein, Dan",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.190",
    doi = "10.18653/v1/2021.naacl-main.190",
    pages = "2390--2397",
    abstract = "Language models (LMs) must be both safe and equitable to be responsibly deployed in practice. With safety in mind, numerous detoxification techniques (e.g., Dathathri et al. 2020; Krause et al. 2020) have been proposed to mitigate toxic LM generations. In this work, we show that these detoxification techniques hurt equity: they decrease the utility of LMs on language used by marginalized groups (e.g., African-American English and minority identity mentions). In particular, we perform automatic and human evaluations of text generation quality when LMs are conditioned on inputs with different dialects and group identifiers. We find that detoxification makes LMs more brittle to distribution shift, especially on language used by marginalized groups. We identify that these failures stem from detoxification methods exploiting spurious correlations in toxicity datasets. Overall, our results highlight the tension between the controllability and distributional robustness of LMs.",
}

@inproceedings{perez2021true,
 author = {Perez, Ethan and Kiela, Douwe and Cho, Kyunghyun},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {11054--11070},
 publisher = {Curran Associates, Inc.},
 title = {True Few-Shot Learning with Language Models},
 url = {https://proceedings.neurips.cc/paper/2021/file/5c04925674920eb58467fb52ce4ef728-Paper.pdf},
 volume = {34},
 year = {2021}
}

@inproceedings{roller2021recipes,
    title = "Recipes for Building an Open-Domain Chatbot",
    author = "Roller, Stephen  and
      Dinan, Emily  and
      Goyal, Naman  and
      Ju, Da  and
      Williamson, Mary  and
      Liu, Yinhan  and
      Xu, Jing  and
      Ott, Myle  and
      Smith, Eric Michael  and
      Boureau, Y-Lan  and
      Weston, Jason",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.24",
    doi = "10.18653/v1/2021.eacl-main.24",
    pages = "300--325",
    abstract = "Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we highlight other ingredients. Good conversation requires blended skills: providing engaging talking points, and displaying knowledge, empathy and personality appropriately, while maintaining a consistent persona. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter models, and make our models and code publicly available. Human evaluations show our best models outperform existing approaches in multi-turn dialogue on engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.",
}

@inproceedings{baumgartner2020pushshift,
  title={The Pushshift Reddit Dataset},
  author={Jason Baumgartner and Savvas Zannettou and Brian Keegan and Megan Squire and Jeremy Blackburn},
  booktitle={ICWSM},
  year={2020}
}


@article{patterson2021carbon,
  title={Carbon emissions and large neural network training},
  author={Patterson, David and Gonzalez, Joseph and Le, Quoc and Liang, Chen and Munguia, Lluis-Miquel and Rothchild, Daniel and So, David and Texier, Maud and Dean, Jeff},
  journal={arXiv preprint arXiv:2104.10350},
  year={2021}
}


@article{kaack2021aligning,
  title={Aligning artificial intelligence with climate change mitigation},
  author={Kaack, Lynn and Donti, Priya and Strubell, Emma and Kamiya, George and Creutzig, Felix and Rolnick, David},
  year={2021}
}

@article{Hendrycks2021MeasuringCC,
  title={Measuring Coding Challenge Competence With APPS},
  author={Dan Hendrycks and Steven Basart and Saurav Kadavath and Mantas Mazeika and Akul Arora and Ethan Guo and Collin Burns and Samir Puranik and Horace He and Dawn Xiaodong Song and Jacob Steinhardt},
  journal={ArXiv},
  year={2021},
  volume={abs/2105.09938}
}

@article{Chen2021EvaluatingLL,
  title={Evaluating Large Language Models Trained on Code},
  author={Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde and Jared Kaplan and Harrison Edwards and Yura Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and David W. Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William H. Guss and Alex Nichol and Igor Babuschkin and S. Arun Balaji and Shantanu Jain and Andrew Carr and Jan Leike and Joshua Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew M. Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
  journal={ArXiv},
  year={2021},
  volume={abs/2107.03374}
}

@inproceedings{bender2021dangers,
  title={On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
  author={Bender, Emily M and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
  booktitle={Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
  pages={610--623},
  year={2021}
}

@article{gardner2019qa,
  title={Question Answering is a Format; When is it Useful?},
  author={Matt Gardner and Jonathan Berant and Hannaneh Hajishirzi and Alon Talmor and Sewon Min},
  journal={ArXiv},
  year={2019},
  volume={abs/1909.11291}
}

@inproceedings{conneau2019xlm,
 author = {Conneau, Alexis and Lample, Guillaume},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Cross-lingual Language Model Pretraining},
 url = {https://proceedings.neurips.cc/paper/2019/file/c04c19c2c2474dbf5f7ac4372c5b9af1-Paper.pdf},
 volume = {32},
 year = {2019}
}

@article{beukeboom2019stereotypes,
  title={{How stereotypes are shared through language: a review and introduction of the aocial categories and stereotypes communication (SCSC) framework}},
  author={Beukeboom, Camiel J. and Burgers, Christian},
  journal={Review of Communication Research},
  volume={7},
  pages={1--37},
  year={2019},
  publisher={ESP},
  url={https://research.vu.nl/en/publications/how-stereotypes-are-shared-through-language-a-review-and-introduc}
}

@misc{https://doi.org/10.48550/arxiv.1912.08777,
  doi = {10.48550/ARXIV.1912.08777},
  
  url = {https://arxiv.org/abs/1912.08777},
  
  author = {Zhang, Jingqing and Zhao, Yao and Saleh, Mohammad and Liu, Peter J.},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{weidinger2023sociotechnical,
  title={Sociotechnical Safety Evaluation of Generative AI Systems},
  author={Laura Weidinger and Maribeth Rauh and Nahema Marchal and Arianna Manzini and Lisa Anne Hendricks and Juan Mateos-Garcia and Stevie Bergman and Jackie Kay and Conor Griffin and Ben Bariach and Iason Gabriel and Verena Rieser and William S. Isaac},
  year={2023},
  url={https://arxiv.org/abs/2310.11986}
}

@inproceedings{du2022glm,
    title = "{GLM}: General Language Model Pretraining with Autoregressive Blank Infilling",
    author = "Du, Zhengxiao  and
      Qian, Yujie  and
      Liu, Xiao  and
      Ding, Ming  and
      Qiu, Jiezhong  and
      Yang, Zhilin  and
      Tang, Jie",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.26",
    doi = "10.18653/v1/2022.acl-long.26",
    pages = "320--335",
    abstract = "There have been various types of pretraining architectures including autoencoding models (e.g., BERT), autoregressive models (e.g., GPT), and encoder-decoder models (e.g., T5). However, none of the pretraining frameworks performs the best for all tasks of three main categories including natural language understanding (NLU), unconditional generation, and conditional generation. We propose a General Language Model (GLM) based on autoregressive blank infilling to address this challenge. GLM improves blank filling pretraining by adding 2D positional encodings and allowing an arbitrary order to predict spans, which results in performance gains over BERT and T5 on NLU tasks. Meanwhile, GLM can be pretrained for different types of tasks by varying the number and lengths of blanks. On a wide range of tasks across NLU, conditional and unconditional generation, GLM outperforms BERT, T5, and GPT given the same model sizes and data, and achieves the best performance from a single pretrained model with 1.25{\mbox{$\times$}} parameters of BERT Large , demonstrating its generalizability to different downstream tasks.",
}

@inproceedings{wang2022deepstruct,
    title = "{D}eep{S}truct: Pretraining of Language Models for Structure Prediction",
    author = "Wang, Chenguang  and
      Liu, Xiao  and
      Chen, Zui  and
      Hong, Haoyun  and
      Tang, Jie  and
      Song, Dawn",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.67",
    doi = "10.18653/v1/2022.findings-acl.67",
    pages = "803--823",
    abstract = "We introduce a method for improving the structural understanding abilities of language models. Unlike previous approaches that finetune the models with task-specific augmentation, we pretrain language models to generate structures from the text on a collection of task-agnostic corpora. Our structure pretraining enables zero-shot transfer of the learned knowledge that models have about the structure tasks. We study the performance of this approach on 28 datasets, spanning 10 structure prediction tasks including open information extraction, joint entity and relation extraction, named entity recognition, relation classification, semantic role labeling, event extraction, coreference resolution, factual probe, intent detection, and dialogue state tracking. We further enhance the pretraining with the task-specific training sets. We show that a 10B parameter language model transfers non-trivially to most tasks and obtains state-of-the-art performance on 21 of 28 datasets that we evaluate. Our code and datasets will be made publicly available.",
}

@inproceedings{durmus-etal-2022-spurious,
    title = "Spurious Correlations in Reference-Free Evaluation of Text Generation",
    author = "Durmus, Esin  and
      Ladhak, Faisal  and
      Hashimoto, Tatsunori",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.102",
    doi = "10.18653/v1/2022.acl-long.102",
    pages = "1443--1454",
    abstract = "Model-based, reference-free evaluation metricshave been proposed as a fast and cost-effectiveapproach to evaluate Natural Language Generation(NLG) systems. Despite promising recentresults, we find evidence that reference-freeevaluation metrics of summarization and dialoggeneration may be relying on spuriouscorrelations with measures such as word overlap,perplexity, and length. We further observethat for text summarization, these metrics havehigh error rates when ranking current state-ofthe-art abstractive summarization systems. Wedemonstrate that these errors can be mitigatedby explicitly designing evaluation metrics toavoid spurious features in reference-free evaluation.",
}
@article{10.1162/tacl_a_00453,
    author = {Laban, Philippe and Schnabel, Tobias and Bennett, Paul N. and Hearst, Marti A.},
    title = "{SummaC: Re-Visiting NLI-based Models for Inconsistency Detection in Summarization}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {10},
    pages = {163-177},
    year = {2022},
    month = {02},
    abstract = "{In the summarization domain, a key requirement for summaries is to be factually consistent with the input document. Previous work has found that natural language inference (NLI) models do not perform competitively when applied to inconsistency detection. In this work, we revisit the use of NLI for inconsistency detection, finding that past work suffered from a mismatch in input granularity between NLI datasets (sentence-level), and inconsistency detection (document level). We provide a highly effective and light-weight method called SummaCConv that enables NLI models to be successfully used for this task by segmenting documents into sentence units and aggregating scores between pairs of sentences. We furthermore introduce a new benchmark called SummaC (Summary Consistency) which consists of six large inconsistency detection datasets. On this dataset, SummaCConv obtains state-of-the-art results with a balanced accuracy of 74.4\\%, a 5\\% improvement compared with prior work.}",
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00453},
    url = {https://doi.org/10.1162/tacl\_a\_00453},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00453/1987014/tacl\_a\_00453.pdf},
}

@inproceedings{birhane2022values,
author = {Birhane, Abeba and Kalluri, Pratyusha and Card, Dallas and Agnew, William and Dotan, Ravit and Bao, Michelle},
title = {The Values Encoded in Machine Learning Research},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533083},
doi = {10.1145/3531146.3533083},
abstract = {Machine learning currently exerts an outsized influence on the world, increasingly affecting institutional practices and impacted communities. It is therefore critical that we question vague conceptions of the field as value-neutral or universally beneficial, and investigate what specific values the field is advancing. In this paper, we first introduce a method and annotation scheme for studying the values encoded in documents such as research papers. Applying the scheme, we analyze 100 highly cited machine learning papers published at premier machine learning conferences, ICML and NeurIPS. We annotate key features of papers which reveal their values: their justification for their choice of project, which attributes of their project they uplift, their consideration of potential negative consequences, and their institutional affiliations and funding sources. We find that few of the papers justify how their project connects to a societal need (15) and far fewer discuss negative potential (1). Through line-by-line content analysis, we identify 59 values that are uplifted in ML research, and, of these, we find that the papers most frequently justify and assess themselves based on Performance, Generalization, Quantitative evidence, Efficiency, Building on past work, and Novelty. We present extensive textual evidence and identify key themes in the definitions and operationalization of these values. Notably, we find systematic textual evidence that these top values are being defined and applied with assumptions and implications generally supporting the centralization of power. Finally, we find increasingly close ties between these highly cited papers and tech companies and elite universities.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {173–184},
numpages = {12},
keywords = {Power asymmetries, ICML, NeurIPS, Corporate ties, Encoded values of ML},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}

@InProceedings{wu2021lime,
  title = 	 {LIME: Learning Inductive Bias for Primitives of Mathematical Reasoning},
  author =       {Wu, Yuhuai and Rabe, Markus N and Li, Wenda and Ba, Jimmy and Grosse, Roger B and Szegedy, Christian},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {11251--11262},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/wu21c/wu21c.pdf},
  url = 	 {https://proceedings.mlr.press/v139/wu21c.html},
  abstract = 	 {While designing inductive bias in neural architectures has been widely studied, we hypothesize that transformer networks are flexible enough to learn inductive bias from suitable generic tasks. Here, we replace architecture engineering by encoding inductive bias in the form of datasets. Inspired by Peirce’s view that deduction, induction, and abduction are the primitives of reasoning, we design three synthetic tasks that are intended to require the model to have these three abilities. We specifically design these tasks to be synthetic and devoid of mathematical knowledge to ensure that only the fundamental reasoning biases can be learned from these tasks. This defines a new pre-training methodology called "LIME" (Learning Inductive bias for Mathematical rEasoning). Models trained with LIME significantly outperform vanilla transformers on four very different large mathematical reasoning benchmarks. Unlike dominating the computation cost as traditional pre-training approaches, LIME requires only a small fraction of the computation cost of the typical downstream task. The code for generating LIME tasks is available at https://github.com/tonywu95/LIME.}
}


@article{gao2021sizes,
  title={On the sizes of OpenAI API models},
  author={Gao, Leo},
  journal={EleutherAI Blog},
  year={2021}
}

@inproceedings{gehrmann2021gem,
    title = "The {GEM} Benchmark: Natural Language Generation, its Evaluation and Metrics",
    author = "Gehrmann, Sebastian  and
      Adewumi, Tosin  and
      Aggarwal, Karmanya  and
      Ammanamanchi, Pawan Sasanka  and
      Aremu, Anuoluwapo  and
      Bosselut, Antoine  and
      Chandu, Khyathi Raghavi  and
      Clinciu, Miruna-Adriana  and
      Das, Dipanjan  and
      Dhole, Kaustubh  and
      Du, Wanyu  and
      Durmus, Esin  and
      Du{\v{s}}ek, Ond{\v{r}}ej  and
      Emezue, Chris Chinenye  and
      Gangal, Varun  and
      Garbacea, Cristina  and
      Hashimoto, Tatsunori  and
      Hou, Yufang  and
      Jernite, Yacine  and
      Jhamtani, Harsh  and
      Ji, Yangfeng  and
      Jolly, Shailza  and
      Kale, Mihir  and
      Kumar, Dhruv  and
      Ladhak, Faisal  and
      Madaan, Aman  and
      Maddela, Mounica  and
      Mahajan, Khyati  and
      Mahamood, Saad  and
      Majumder, Bodhisattwa Prasad  and
      Martins, Pedro Henrique  and
      McMillan-Major, Angelina  and
      Mille, Simon  and
      van Miltenburg, Emiel  and
      Nadeem, Moin  and
      Narayan, Shashi  and
      Nikolaev, Vitaly  and
      Niyongabo Rubungo, Andre  and
      Osei, Salomey  and
      Parikh, Ankur  and
      Perez-Beltrachini, Laura  and
      Rao, Niranjan Ramesh  and
      Raunak, Vikas  and
      Rodriguez, Juan Diego  and
      Santhanam, Sashank  and
      Sedoc, Jo{\~a}o  and
      Sellam, Thibault  and
      Shaikh, Samira  and
      Shimorina, Anastasia  and
      Sobrevilla Cabezudo, Marco Antonio  and
      Strobelt, Hendrik  and
      Subramani, Nishant  and
      Xu, Wei  and
      Yang, Diyi  and
      Yerukola, Akhila  and
      Zhou, Jiawei",
    booktitle = "Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.gem-1.10",
    doi = "10.18653/v1/2021.gem-1.10",
    pages = "96--120",
    abstract = "We introduce GEM, a living benchmark for natural language Generation (NLG), its Evaluation, and Metrics. Measuring progress in NLG relies on a constantly evolving ecosystem of automated metrics, datasets, and human evaluation standards. Due to this moving target, new models often still evaluate on divergent anglo-centric corpora with well-established, but flawed, metrics. This disconnect makes it challenging to identify the limitations of current models and opportunities for progress. Addressing this limitation, GEM provides an environment in which models can easily be applied to a wide set of tasks and in which evaluation strategies can be tested. Regular updates to the benchmark will help NLG research become more multilingual and evolve the challenge alongside models. This paper serves as the description of the data for the 2021 shared task at the associated GEM Workshop.",
}

@inproceedings{dhamala2021bold,
author = {Dhamala, Jwala and Sun, Tony and Kumar, Varun and Krishna, Satyapriya and Pruksachatkun, Yada and Chang, Kai-Wei and Gupta, Rahul},
title = {BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445924},
doi = {10.1145/3442188.3445924},
abstract = {Recent advances in deep learning techniques have enabled machines to generate cohesive
open-ended text when prompted with a sequence of words as context. While these models
now empower many downstream applications from conversation bots to automatic storytelling,
they have been shown to generate texts that exhibit social biases. To systematically
study and benchmark social biases in open-ended language generation, we introduce
the Bias in Open-Ended Language Generation Dataset (BOLD), a large-scale dataset that
consists of 23,679 English text generation prompts for bias benchmarking across five
domains: profession, gender, race, religion, and political ideology. We also propose
new automated metrics for toxicity, psycholinguistic norms, and text gender polarity
to measure social biases in open-ended text generation from multiple angles. An examination
of text generated from three popular language models reveals that the majority of
these models exhibit a larger social bias than human-written Wikipedia text across
all domains. With these results we highlight the need to benchmark biases in open-ended
language generation and caution users of language generation models on downstream
tasks to be cognizant of these embedded prejudices.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {862–872},
numpages = {11},
keywords = {Fairness, natural language generation},
location = {Virtual Event, Canada},
series = {FAccT '21}
}

@inproceedings{
raji2021benchmark,
title={{AI} and the Everything in the Whole Wide World Benchmark},
author={Inioluwa Deborah Raji and Emily Denton and Emily M. Bender and Alex Hanna and Amandalynne Paullada},
booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)},
year={2021},
url={https://openreview.net/forum?id=j6NxpQbREA1}
}

@inproceedings{ethayarajh2020utility,
    title = "Utility is in the Eye of the User: A Critique of {NLP} Leaderboards",
    author = "Ethayarajh, Kawin  and
      Jurafsky, Dan",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.393",
    doi = "10.18653/v1/2020.emnlp-main.393",
    pages = "4846--4853",
    abstract = "Benchmarks such as GLUE have helped drive advances in NLP by incentivizing the creation of more accurate models. While this leaderboard paradigm has been remarkably successful, a historical focus on performance-based evaluation has been at the expense of other qualities that the NLP community values in models, such as compactness, fairness, and energy efficiency. In this opinion paper, we study the divergence between what is incentivized by leaderboards and what is useful in practice through the lens of microeconomic theory. We frame both the leaderboard and NLP practitioners as consumers and the benefit they get from a model as its utility to them. With this framing, we formalize how leaderboards {--} in their current form {--} can be poor proxies for the NLP community at large. For example, a highly inefficient model would provide less utility to practitioners but not to a leaderboard, since it is a cost that only the former must bear. To allow practitioners to better estimate a model{'}s utility to them, we advocate for more transparency on leaderboards, such as the reporting of statistics that are of practical concern (e.g., model size, energy efficiency, and inference latency).",
}

@article{gao2021harness,
  title={A framework for few-shot language model evaluation},
  author={Gao, Leo and Tow, Jonathan and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and McDonell, Kyle and Muennighoff, Niklas and Phang, Jason and Reynolds, Laria and Tang, Eric and Thite, Anish and Wang, Ben and Wang, Kevin and Zou, Andy},
  journal={Version v0. 0.1. Sept},
  year={2021}
}

@article{rauh2022characteristics,
  title={Characteristics of Harmful Text: Towards Rigorous Benchmarking of Language Models},
  author={Maribeth Rauh and John F. J. Mellor and Jonathan Uesato and Po-Sen Huang and Johannes Welbl and Laura Weidinger and Sumanth Dathathri and Amelia Glaese and Geoffrey Irving and Iason Gabriel and William S. Isaac and Lisa Anne Hendricks},
  journal={ArXiv},
  year={2022},
  volume={abs/2206.08325}
}

@article{gehrmann2022gemv2,
  title={GEMv2: Multilingual NLG Benchmarking in a Single Line of Code},
  author={Sebastian Gehrmann and Abhik Bhattacharjee and Abinaya Mahendiran and Alex Wang and Alexandros Papangelis and Aman Madaan and Angelina McMillan-Major and Anna V. Shvets and Ashish Upadhyay and Bingsheng Yao and Bryan Wilie and Chandra Bhagavatula and Chaobin You and Craig Thomson and Cristina Garbacea and Dakuo Wang and Daniel Deutsch and Deyi Xiong and Di Jin and Dimitra Gkatzia and Dragomir Radev and Elizabeth Clark and Esin Durmus and Faisal Ladhak and Filip Ginter and Genta Indra Winata and Hendrik Strobelt and Hiroaki Hayashi and Jekaterina Novikova and Jenna Kanerva and Jenny Chim and Jiawei Zhou and Jordan Clive and Joshua Maynez and Jo{\~a}o Sedoc and Juraj Juraska and Kaustubh D. Dhole and Khyathi Raghavi Chandu and Leonardo F. R. Ribeiro and Lewis Tunstall and Li Zhang and Mahima Pushkarna and Mathias Creutz and Michael White and Mihir Kale and Moussa Kamal Eddine and Nico Daheim and Nishant Subramani and Ondrej Dusek and Paul Pu Liang and Pawan Sasanka Ammanamanchi and Qinqin Zhu and Ratish Puduppully and Reno Kriz and Rifat Shahriyar and Ronald Cardenas and Saad Mahamood and Salomey Osei and Samuel Cahyawijaya and Sanja vStajner and S{\'e}bastien Montella and Shailza and Shailza Jolly and Simon Mille and Tahmid Hasan and Tianhao Shen and Tosin P. Adewumi and Vikas Raunak and Vipul Raheja and Vitaly Nikolaev and Vivian Tsai and Yacine Jernite and Yi Xu and Yisi Sang and Yixin Liu and Yufang Hou},
  journal={ArXiv},
  year={2022},
  volume={abs/2206.11249}
}

@article{srivastava2022bigbench,
  title={Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models},
  author={Aarohi Srivastava and Abhinav Rastogi and Abhishek B Rao and Abu Awal Md Shoeb and Abubakar Abid and Adam Fisch and Adam R. Brown and Adam Santoro and Aditya Gupta and Adri{\`a} Garriga-Alonso and Agnieszka Kluska and Aitor Lewkowycz and Akshat Agarwal and Alethea Power and Alex Ray and Alex Warstadt and Alexander W. Kocurek and Ali Safaya and Ali Tazarv and Alice Xiang and Alicia Parrish and Allen Nie and Aman Hussain and Amanda Askell and Amanda Dsouza and Ameet Annasaheb Rahane and Anantharaman S. Iyer and Anders Johan Andreassen and Andrea Santilli and Andreas Stuhlmuller and Andrew M. Dai and Andrew D. La and Andrew Kyle Lampinen and Andy Zou and Angela Jiang and Angelica Chen and Anh Vuong and Animesh Gupta and Anna Gottardi and Antonio Norelli and Anu Venkatesh and Arash Gholamidavoodi and Arfa Tabassum and Arul Menezes and Arun Kirubarajan and Asher Mullokandov and Ashish Sabharwal and Austin Herrick and Avia Efrat and Aykut Erdem and Ayla Karakacs and Bridget R. Roberts and Bao Sheng Loe and Barret Zoph and Bartlomiej Bojanowski and Batuhan Ozyurt and Behnam Hedayatnia and Behnam Neyshabur and Benjamin Inden and Benno Stein and Berk Ekmekci and Bill Yuchen Lin and Blake Stephen Howald and Cameron Diao and Cameron Dour and Catherine Stinson and Cedrick Argueta and C'esar Ferri Ram'irez and Chandan Singh and Charles Rathkopf and Chenlin Meng and Chitta Baral and Chiyu Wu and Chris Callison-Burch and Chris Waites and Christian Voigt and Christopher D. Manning and Christopher Potts and Cindy Tatiana Ramirez and Clara Rivera and Clemencia Siro and Colin Raffel and Courtney Ashcraft and Cristina Garbacea and Damien Sileo and Daniel H Garrette and Dan Hendrycks and Dan Kilman and Dan Roth and Daniel Freeman and Daniel Khashabi and Daniel Levy and Daniel Gonz'alez and Danny Hernandez and Danqi Chen and Daphne Ippolito and Dar Gilboa and David Dohan and D. Drakard and David Jurgens and Debajyoti Datta and Deep Ganguli and Denis Emelin and Denis Kleyko and Deniz Yuret and Derek Chen and Derek Tam and Dieuwke Hupkes and Diganta Misra and Dilyar Buzan and Dimitri Coelho Mollo and Diyi Yang and Dong-Ho Lee and Ekaterina Shutova and Ekin Dogus Cubuk and Elad Segal and Eleanor Hagerman and Elizabeth Barnes and Elizabeth P. Donoway and Ellie Pavlick and Emanuele Rodol{\`a} and Emma FC Lam and Eric Chu and Eric Tang and Erkut Erdem and Ernie Chang and Ethan A. Chi and Ethan Dyer and Ethan Jerzak and Ethan Kim and Eunice Engefu Manyasi and Evgenii Zheltonozhskii and Fan Xia and Fatemeh Siar and Fernando Mart'inez-Plumed and Francesca Happ'e and François Chollet and Frieda Rong and Gaurav Mishra and Genta Indra Winata and Gerard de Melo and Germ{\'a}n Kruszewski and Giambattista Parascandolo and Giorgio Mariani and Gloria Wang and Gonzalo Jaimovitch-L'opez and Gregor Betz and Guy Gur-Ari and Hana Galijasevic and Han Sol Kim and Hannah Rashkin and Hanna Hajishirzi and Harsh Mehta and Hayden Bogar and Henry Shevlin and Hinrich Sch{\"u}tze and Hiromu Yakura and Hongming Zhang and Hubert Wong and Ian Aik-Soon Ng and Isaac Noble and Jaap Jumelet and Jack Geissinger and John Kernion and Jacob Hilton and Jaehoon Lee and Jaime Fern{\'a}ndez Fisac and J. Brooker Simon and James Koppel and James Zheng and James Zou and Jan Koco'n and Jana Thompson and Jared Kaplan and Jarema Radom and Jascha Narain Sohl-Dickstein and Jason Phang and Jason Wei and Jason Yosinski and Jekaterina Novikova and Jelle Bosscher and Jenni Marsh and Jeremy Kim and Jeroen Taal and Jesse Engel and Jesujoba Oluwadara Alabi and Jiacheng Xu and Jiaming Song and Jillian Tang and Jane W Waweru and John Burden and John Miller and John U. Balis and Jonathan Berant and Jorg Frohberg and Jos Rozen and Jos{\'e} Hern{\'a}ndez-Orallo and Joseph Boudeman and Joseph Jones and Joshua B. Tenenbaum and Joshua S. Rule and Joyce Chua and Kamil Kanclerz and Karen Livescu and Karl Krauth and Karthik Gopalakrishnan and Katerina Ignatyeva and Katja Markert and Kaustubh D. Dhole and Kevin Gimpel and Kevin Ochieng’ Omondi and Kory Wallace Mathewson and Kristen Chiafullo and Ksenia Shkaruta and Kumar Shridhar and Kyle McDonell and Kyle Richardson and Laria Reynolds and Leo Gao and Li Zhang and Liam Dugan and Lianhui Qin and Lidia Contreras-Ochando and Louis-Philippe Morency and Luca Moschella and Luca Lam and Lucy Noble and Ludwig Schmidt and Luheng He and Luis Oliveros Col'on and Luke Metz and Lutfi Kerem cSenel and Maarten Bosma and Maarten Sap and Maartje ter Hoeve and Madotto Andrea and Maheen Saleem Farooqi and Manaal Faruqui and Mantas Mazeika and Marco Baturan and Marco Marelli and Marco Maru and M Quintana and Marie Tolkiehn and Mario Giulianelli and Martha Lewis and Martin Potthast and Matthew Leavitt and Matthias Hagen and M'aty'as Schubert and Medina Baitemirova and Melissa Arnaud and Melvin Andrew McElrath and Michael A. Yee and Michael Cohen and Mi Gu and Michael I. Ivanitskiy and Michael Starritt and Michael Strube and Michal Swkedrowski and Michele Bevilacqua and Michihiro Yasunaga and Mihir Kale and Mike Cain and Mimee Xu and Mirac Suzgun and Monica Tiwari and Mohit Bansal and Moin Aminnaseri and Mor Geva and Mozhdeh Gheini and T MukundVarma and Nanyun Peng and Nathan Chi and Nayeon Lee and Neta Gur-Ari Krakover and Nicholas Cameron and Nicholas S. Roberts and Nicholas Doiron and Nikita Nangia and Niklas Deckers and Niklas Muennighoff and Nitish Shirish Keskar and Niveditha Iyer and Noah Constant and Noah Fiedel and Nuan Wen and Oliver Zhang and Omar Agha and Omar Elbaghdadi and Omer Levy and Owain Evans and Pablo Antonio Moreno Casares and Parth Doshi and Pascale Fung and Paul Pu Liang and Paul Vicol and Pegah Alipoormolabashi and Peiyuan Liao and Percy Liang and Peter W. Chang and Peter Eckersley and Phu Mon Htut and Pi-Bei Hwang and P. Milkowski and Piyush S. Patil and Pouya Pezeshkpour and Priti Oli and Qiaozhu Mei and QING LYU and Qinlang Chen and Rabin Banjade and Rachel Etta Rudolph and Raefer Gabriel and Rahel Habacker and Ram'on Risco Delgado and Rapha{\"e}l Milli{\`e}re and Rhythm Garg and Richard Barnes and Rif A. Saurous and Riku Arakawa and Robbe Raymaekers and Robert Frank and Rohan Sikand and Roman Novak and Roman Sitelew and Ronan Le Bras and Rosanne Liu and Rowan Jacobs and Rui Zhang and Ruslan Salakhutdinov and Ryan Chi and Ryan Lee and Ryan Stovall and Ryan Teehan and Rylan Yang and Sahib J. Singh and Saif M. Mohammad and Sajant Anand and Sam Dillavou and Sam Shleifer and Sam Wiseman and Samuel Gruetter and Sam Bowman and Samuel S. Schoenholz and Sanghyun Han and Sanjeev Kwatra and Sarah A. Rous and Sarik Ghazarian and Sayan Ghosh and Sean Casey and Sebastian Bischoff and Sebastian Gehrmann and Sebastian Schuster and Sepideh Sadeghi and Shadi Sameh Hamdan and Sharon Zhou and Shashank Srivastava and Sherry Shi and Shikhar Singh and Shima Asaadi and Shixiang Shane Gu and Shubh Pachchigar and Shubham Toshniwal and Shyam Upadhyay and Shyamolima Debnath and Siamak Shakeri and Simon Thormeyer and Simone Melzi and Siva Reddy and Sneha Priscilla Makini and Soo-hwan Lee and Spencer Bradley Torene and Sriharsha Hatwar and Stanislas Dehaene and Stefan Divic and Stefano Ermon and Stella Rose Biderman and Stephanie C. Lin and Stephen Prasad and Steven T. Piantadosi and Stuart M. Shieber and Summer Misherghi and Svetlana Kiritchenko and Swaroop Mishra and Tal Linzen and Tal Schuster and Tao Li and Tao Yu and Tariq A. Ali and Tatsuo Hashimoto and Te-Lin Wu and Theo Desbordes and Theodore Rothschild and Thomas Phan and Tianle Wang and Tiberius Nkinyili and Timo Schick and T. N. Kornev and Timothy Telleen-Lawton and Titus Tunduny and Tobias Gerstenberg and Trenton Chang and Trishala Neeraj and Tushar Khot and Tyler O. Shultz and Uri Shaham and Vedant Misra and Vera Demberg and Victoria Nyamai and Vikas Raunak and Vinay V. Ramasesh and Vinay Uday Prabhu and Vishakh Padmakumar and Vivek Srikumar and William Fedus and William Saunders and William Zhang and W Vossen and Xiang Ren and Xiaoyu F Tong and Xinyi Wu and Xudong Shen and Yadollah Yaghoobzadeh and Yair Lakretz and Yang Song and Yasaman Bahri and Ye Ji Choi and Yichi Yang and Yiding Hao and Yifu Chen and Yonatan Belinkov and Yu Hou and Yu Hou and Yushi Bai and Zachary Seid and Zhao Xinran and Zhuoye Zhao and Zi Fu Wang and Zijie J. Wang and Zirui Wang and Ziyi Wu},
  journal={ArXiv},
  year={2022},
  volume={abs/2206.04615}
}

@article{ji2022hallucination,
  title={Survey of Hallucination in Natural Language Generation},
  author={Ziwei Ji and Nayeon Lee and Rita Frieske and Tiezheng Yu and Dan Su and Yan Xu and Etsuko Ishii and Yejin Bang and Andrea Madotto and Pascale Fung},
  journal={ArXiv},
  year={2022},
  volume={abs/2202.03629}
}

@inproceedings{goyal-durrett-2021-annotating,
    title = "Annotating and Modeling Fine-grained Factuality in Summarization",
    author = "Goyal, Tanya  and
      Durrett, Greg",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.114",
    doi = "10.18653/v1/2021.naacl-main.114",
    pages = "1449--1462",
    abstract = "Recent pre-trained abstractive summarization systems have started to achieve credible performance, but a major barrier to their use in practice is their propensity to output summaries that are not faithful to the input and that contain factual errors. While a number of annotated datasets and statistical models for assessing factuality have been explored, there is no clear picture of what errors are most important to target or where current techniques are succeeding and failing. We explore both synthetic and human-labeled data sources for training models to identify factual errors in summarization, and study factuality at the word-, dependency-, and sentence-level. Our observations are threefold. First, exhibited factual errors differ significantly across datasets, and commonly-used training sets of simple synthetic errors do not reflect errors made on abstractive datasets like XSum. Second, human-labeled data with fine-grained annotations provides a more effective training signal than sentence-level annotations or synthetic data. Finally, we show that our best factuality detection model enables training of more factual XSum summarization models by allowing us to identify non-factual tokens in the training data.",
}

@inproceedings{lewis-etal-2020-bart,
    title = "{BART}: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
    author = "Lewis, Mike  and
      Liu, Yinhan  and
      Goyal, Naman  and
      Ghazvininejad, Marjan  and
      Mohamed, Abdelrahman  and
      Levy, Omer  and
      Stoyanov, Veselin  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.703",
    doi = "10.18653/v1/2020.acl-main.703",
    pages = "7871--7880",
    abstract = "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.",
}

@article{narayanan2018fairness,
title = "{21 Fairness Definitions and their Politics}",
author = "Arvind Narayanan",
year = "2018",
note = "{Tutorial presented at the Conference on Fairness, Accountability,
  and Transparency}",
  url={https://www.youtube.com/watch?v=jIXIuYdnyyk}
}

@book{noble2018algorithms,
  title={Algorithms of Oppression},
  author={Noble, Safiya Umoja},
  year={2018},
  publisher={New York University Press}
}


@inproceedings{narayan-etal-2018-dont,
    title = "Don{'}t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization",
    author = "Narayan, Shashi  and
      Cohen, Shay B.  and
      Lapata, Mirella",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1206",
    doi = "10.18653/v1/D18-1206",
    pages = "1797--1807",
    abstract = "We introduce {``}extreme summarization{''}, a new single-document summarization task which does not favor extractive strategies and calls for an abstractive modeling approach. The idea is to create a short, one-sentence news summary answering the question {``}What is the article about?{''}. We collect a real-world, large-scale dataset for this task by harvesting online articles from the British Broadcasting Corporation (BBC). We propose a novel abstractive model which is conditioned on the article{'}s topics and based entirely on convolutional neural networks. We demonstrate experimentally that this architecture captures long-range dependencies in a document and recognizes pertinent content, outperforming an oracle extractive system and state-of-the-art abstractive approaches when evaluated automatically and by humans.",
}

@inproceedings{10.5555/2969239.2969428,
author = {Hermann, Karl Moritz and Ko\v{c}isk\'{y}, Tom\'{a}\v{s} and Grefenstette, Edward and Espeholt, Lasse and Kay, Will and Suleyman, Mustafa and Blunsom, Phil},
title = {Teaching Machines to Read and Comprehend},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1693–1701},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}



@article{schwartz2020green,
  title={Green ai},
  author={Schwartz, Roy and Dodge, Jesse and Smith, Noah A and Etzioni, Oren},
  journal={Communications of the ACM},
  volume={63},
  number={12},
  pages={54--63},
  year={2020},
  publisher={ACM New York, NY, USA}
}

@article{Mohamed2020,
  author       = {Shakir Mohamed and Marie-Therese Png and William Isaac},
  title        = {Decolonial AI: Decolonial Theory as Sociotechnical Foresight in Artificial Intelligence},
  journal      = {Philosophy \& Technology},
  volume       = {33},
  number       = {4},
  pages        = {659--684},
  year         = {2020},
  month        = {December},
  doi          = {10.1007/s13347-020-00405-8},
  url          = {https://doi.org/10.1007/s13347-020-00405-8},
  abstract     = {This paper explores the important role of critical science, and in particular of post-colonial and decolonial theories, in understanding and shaping the ongoing advances in artificial intelligence. Artificial intelligence (AI) is viewed as amongst the technological advances that will reshape modern societies and their relations. While the design and deployment of systems that continually adapt holds the promise of far-reaching positive change, they simultaneously pose significant risks, especially to already vulnerable peoples. Values and power are central to this discussion. Decolonial theories use historical hindsight to explain patterns of power that shape our intellectual, political, economic, and social world. By embedding a decolonial critical approach within its technical practice, AI communities can develop foresight and tactics that can better align research and technology development with established ethical principles, centring vulnerable peoples who continue to bear the brunt of negative impacts of innovation and scientific progress. We highlight problematic applications that are instances of coloniality, and using a decolonial lens, submit three tactics that can form a decolonial field of artificial intelligence: creating a critical technical practice of AI, seeking reverse tutelage and reverse pedagogies, and the renewal of affective and political communities. The years ahead will usher in a wave of new scientific breakthroughs and technologies driven by AI research, making it incumbent upon AI communities to strengthen the social contract through ethical foresight and the multiplicity of intellectual perspectives available to us, ultimately supporting future technologies that enable greater well-being, with the goal of beneficence and justice for all.},
  issn         = {2210-5441},
}


@article{henderson2020towards,
  title={Towards the systematic reporting of the energy and carbon footprints of machine learning},
  author={Henderson, Peter and Hu, Jieru and Romoff, Joshua and Brunskill, Emma and Jurafsky, Dan and Pineau, Joelle},
  journal={Journal of Machine Learning Research},
  volume={21},
  number={248},
  pages={1--43},
  year={2020}
}

@article{lacoste2019quantifying,
  title={Quantifying the carbon emissions of machine learning},
  author={Lacoste, Alexandre and Luccioni, Alexandra and Schmidt, Victor and Dandres, Thomas},
  journal={arXiv preprint arXiv:1910.09700},
  year={2019}
}


@article{strubell2019energy,
  title={Energy and policy considerations for deep learning in {NLP}},
  author={Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
  journal={arXiv preprint arXiv:1906.02243},
  year={2019}
}

@article{gao2021thepile,
  author    = {Leo Gao and
               Stella Biderman and
               Sid Black and
               Laurence Golding and
               Travis Hoppe and
               Charles Foster and
               Jason Phang and
               Horace He and
               Anish Thite and
               Noa Nabeshima and
               Shawn Presser and
               Connor Leahy},
  title     = {{T}he {P}ile: {A}n 800{GB} {D}ataset of {D}iverse {T}ext for {L}anguage {M}odeling},
  journal   = {arXiv preprint arXiv:2101.00027},
  year      = {2021},
  url       = {https://arxiv.org/abs/2101.00027},
}

@inproceedings{bommasani2020interpreting,
    title = "{I}nterpreting {P}retrained {C}ontextualized {R}epresentations via {R}eductions to {S}tatic {E}mbeddings",
    author = "Bommasani, Rishi  and
      Davis, Kelly  and
      Cardie, Claire",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.431",
    doi = "10.18653/v1/2020.acl-main.431",
    pages = "4758--4781",
    abstract = "Contextualized representations (e.g. ELMo, BERT) have become the default pretrained representations for downstream NLP applications. In some settings, this transition has rendered their static embedding predecessors (e.g. Word2Vec, GloVe) obsolete. As a side-effect, we observe that older interpretability methods for static embeddings {---} while more diverse and mature than those available for their dynamic counterparts {---} are underutilized in studying newer contextualized representations. Consequently, we introduce simple and fully general methods for converting from contextualized representations to static lookup-table embeddings which we apply to 5 popular pretrained models and 9 sets of pretrained weights. Our analysis of the resulting static embeddings notably reveals that pooling over many contexts significantly improves representational quality under intrinsic evaluation. Complementary to analyzing representational quality, we consider social biases encoded in pretrained representations with respect to gender, race/ethnicity, and religion and find that bias is encoded disparately across pretrained models and internal layers even for models with the same training data. Concerningly, we find dramatic inconsistencies between social bias estimators for word embeddings.",
}


@inproceedings{nangia2020crows,
    title = "{C}row{S}-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models",
    author = "Nangia, Nikita  and
      Vania, Clara  and
      Bhalerao, Rasika  and
      Bowman, Samuel R.",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.154",
    doi = "10.18653/v1/2020.emnlp-main.154",
    pages = "1953--1967",
    abstract = "Pretrained language models, especially masked language models (MLMs) have seen success across many NLP tasks. However, there is ample evidence that they use the cultural biases that are undoubtedly present in the corpora they are trained on, implicitly creating harm with biased representations. To measure some forms of social bias in language models against protected demographic groups in the US, we introduce the Crowdsourced Stereotype Pairs benchmark (CrowS-Pairs). CrowS-Pairs has 1508 examples that cover stereotypes dealing with nine types of bias, like race, religion, and age. In CrowS-Pairs a model is presented with two sentences: one that is more stereotyping and another that is less stereotyping. The data focuses on stereotypes about historically disadvantaged groups and contrasts them with advantaged groups. We find that all three of the widely-used MLMs we evaluate substantially favor sentences that express stereotypes in every category in CrowS-Pairs. As work on building less biased models advances, this dataset can be used as a benchmark to evaluate progress.",
}


@article{nissim2020fair,
    title = "Fair Is Better than Sensational: Man Is to Doctor as Woman Is to Doctor",
    author = "Nissim, Malvina  and
      van Noord, Rik  and
      van der Goot, Rob",
    journal = "Computational Linguistics",
    volume = "46",
    number = "2",
    month = jun,
    year = "2020",
    url = "https://aclanthology.org/2020.cl-2.7",
    doi = "10.1162/coli_a_00379",
    pages = "487--497",
    abstract = "Analogies such as man is to king as woman is to X are often used to illustrate the amazing power of word embeddings. Concurrently, they have also been used to expose how strongly human biases are encoded in vector spaces trained on natural language, with examples like man is to computer programmer as woman is to homemaker. Recent work has shown that analogies are in fact not an accurate diagnostic for bias, but this does not mean that they are not used anymore, or that their legacy is fading. Instead of focusing on the intrinsic problems of the analogy task as a bias detection tool, we discuss a series of issues involving implementation as well as subjective choices that might have yielded a distorted picture of bias in word embeddings. We stand by the truth that human biases are present in word embeddings, and, of course, the need to address them. But analogies are not an accurate tool to do so, and the way they have been most often used has exacerbated some possibly non-existing biases and perhaps hidden others. Because they are still widely popular, and some of them have become classics within and outside the NLP community, we deem it important to provide a series of clarifications that should put well-known, and potentially new analogies, into the right perspective.",
}

@inproceedings{ethayarajh2019understanding,
    title = "Understanding Undesirable Word Embedding Associations",
    author = "Ethayarajh, Kawin  and
      Duvenaud, David  and
      Hirst, Graeme",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1166",
    doi = "10.18653/v1/P19-1166",
    pages = "1696--1705",
    abstract = "Word embeddings are often criticized for capturing undesirable word associations such as gender stereotypes. However, methods for measuring and removing such biases remain poorly understood. We show that for any embedding model that implicitly does matrix factorization, debiasing vectors post hoc using subspace projection (Bolukbasi et al., 2016) is, under certain conditions, equivalent to training on an unbiased corpus. We also prove that WEAT, the most common association test for word embeddings, systematically overestimates bias. Given that the subspace projection method is provably effective, we use it to derive a new measure of association called the relational inner product association (RIPA). Experiments with RIPA reveal that, on average, skipgram with negative sampling (SGNS) does not make most words any more gendered than they are in the training corpus. However, for gender-stereotyped words, SGNS actually amplifies the gender association in the corpus.",
}

@inproceedings{selbst2019fairness,
author = {Selbst, Andrew D. and Boyd, Danah and Friedler, Sorelle A. and Venkatasubramanian, Suresh and Vertesi, Janet},
title = {Fairness and Abstraction in Sociotechnical Systems},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287598},
doi = {10.1145/3287560.3287598},
abstract = {A key goal of the fair-ML community is to develop machine-learning based systems that, once introduced into a social context, can achieve social and legal outcomes such as fairness, justice, and due process. Bedrock concepts in computer science---such as abstraction and modular design---are used to define notions of fairness and discrimination, to produce fairness-aware learning algorithms, and to intervene at different stages of a decision-making pipeline to produce "fair" outcomes. In this paper, however, we contend that these concepts render technical interventions ineffective, inaccurate, and sometimes dangerously misguided when they enter the societal context that surrounds decision-making systems. We outline this mismatch with five "traps" that fair-ML work can fall into even as it attempts to be more context-aware in comparison to traditional data science. We draw on studies of sociotechnical systems in Science and Technology Studies to explain why such traps occur and how to avoid them. Finally, we suggest ways in which technical designers can mitigate the traps through a refocusing of design in terms of process rather than solutions, and by drawing abstraction boundaries to include social actors rather than purely technical ones.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {59–68},
numpages = {10},
keywords = {Sociotechnical Systems, Interdisciplinary, Fairness-aware Machine Learning},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}

@inproceedings{pavlopoulos2020toxicity,
    title = "Toxicity Detection: Does Context Really Matter?",
    author = "Pavlopoulos, John  and
      Sorensen, Jeffrey  and
      Dixon, Lucas  and
      Thain, Nithum  and
      Androutsopoulos, Ion",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.396",
    doi = "10.18653/v1/2020.acl-main.396",
    pages = "4296--4305",
    abstract = "Moderation is crucial to promoting healthy online discussions. Although several {`}toxicity{'} detection datasets and models have been published, most of them ignore the context of the posts, implicitly assuming that comments may be judged independently. We investigate this assumption by focusing on two questions: (a) does context affect the human judgement, and (b) does conditioning on context improve performance of toxicity detection systems? We experiment with Wikipedia conversations, limiting the notion of context to the previous post in the thread and the discussion title. We find that context can both amplify or mitigate the perceived toxicity of posts. Moreover, a small but significant subset of manually labeled posts (5{\%} in one of our experiments) end up having the opposite toxicity labels if the annotators are not provided with context. Surprisingly, we also find no evidence that context actually improves the performance of toxicity classifiers, having tried a range of classifiers and mechanisms to make them context aware. This points to the need for larger datasets of comments annotated in context. We make our code and data publicly available.",
}

@inproceedings{blodgett2021stereotyping,
    title = "Stereotyping {N}orwegian Salmon: An Inventory of Pitfalls in Fairness Benchmark Datasets",
    author = "Blodgett, Su Lin  and
      Lopez, Gilsinia  and
      Olteanu, Alexandra  and
      Sim, Robert  and
      Wallach, Hanna",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.81",
    doi = "10.18653/v1/2021.acl-long.81",
    pages = "1004--1015",
    abstract = "Auditing NLP systems for computational harms like surfacing stereotypes is an elusive goal. Several recent efforts have focused on benchmark datasets consisting of pairs of contrastive sentences, which are often accompanied by metrics that aggregate an NLP system{'}s behavior on these pairs into measurements of harms. We examine four such benchmarks constructed for two NLP tasks: language modeling and coreference resolution. We apply a measurement modeling lens{---}originating from the social sciences{---}to inventory a range of pitfalls that threaten these benchmarks{'} validity as measurement models for stereotyping. We find that these benchmarks frequently lack clear articulations of what is being measured, and we highlight a range of ambiguities and unstated assumptions that affect how these benchmarks conceptualize and operationalize stereotyping.",
}

@inproceedings{du2021assessing,
    title = "Assessing the Reliability of Word Embedding Gender Bias Measures",
    author = "Du, Yupei  and
      Fang, Qixiang  and
      Nguyen, Dong",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.785",
    doi = "10.18653/v1/2021.emnlp-main.785",
    pages = "10012--10034",
    abstract = "Various measures have been proposed to quantify human-like social biases in word embeddings. However, bias scores based on these measures can suffer from measurement error. One indication of measurement quality is reliability, concerning the extent to which a measure produces consistent results. In this paper, we assess three types of reliability of word embedding gender bias measures, namely test-retest reliability, inter-rater consistency and internal consistency. Specifically, we investigate the consistency of bias scores across different choices of random seeds, scoring rules and words. Furthermore, we analyse the effects of various factors on these measures{'} reliability scores. Our findings inform better design of word embedding gender bias measures. Moreover, we urge researchers to be more critical about the application of such measures",
}

@inproceedings{hede2021toxicity,
    title = "From Toxicity in Online Comments to Incivility in {A}merican News: Proceed with Caution",
    author = "Hede, Anushree  and
      Agarwal, Oshin  and
      Lu, Linda  and
      Mutz, Diana C.  and
      Nenkova, Ani",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.225",
    doi = "10.18653/v1/2021.eacl-main.225",
    pages = "2620--2630",
    abstract = "The ability to quantify incivility online, in news and in congressional debates is of great interest to political scientists. Computational tools for detecting online incivility for English are now fairly accessible and potentially could be applied more broadly. We test the Jigsaw Perspective API for its ability to detect the degree of incivility on a corpus that we developed, consisting of manual annotations of civility in American news. We demonstrate that toxicity models, as exemplified by Perspective, are inadequate for the analysis of incivility in news. We carry out error analysis that points to the need to develop methods to remove spurious correlations between words often mentioned in the news, especially identity descriptors and incivility. Without such improvements, applying Perspective or similar models on news is likely to lead to wrong conclusions, that are not aligned with the human perception of incivility.",
}


@misc{stephanielin2021trutfulqa,
  doi = {10.48550/ARXIV.2109.07958},
  url = {https://arxiv.org/abs/2109.07958},
  author = {Lin, Stephanie and Hilton, Jacob and Evans, Owain},
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Computers and Society (cs.CY), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {TruthfulQA: Measuring How Models Mimic Human Falsehoods},
  publisher = {arXiv},
  year = {2021},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{ziems2022value,
  doi = {10.48550/ARXIV.2204.03031},
  url = {https://arxiv.org/abs/2204.03031},
  author = {Ziems, Caleb and Chen, Jiaao and Harris, Camille and Anderson, Jessica and Yang, Diyi},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {VALUE: Understanding Dialect Disparity in NLU},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution 4.0 International}
}

@article{lees2022perspective,
  title={A New Generation of Perspective API: Efficient Multilingual Character-level Transformers},
  author={Alyssa Lees and Vinh Q. Tran and Yi Tay and Jeffrey Scott Sorensen and Jai Gupta and Donald Metzler and Lucy Vasserman},
  journal={Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  year={2022}
}

@inproceedings{gordon2022jury,
author = {Gordon, Mitchell L. and Lam, Michelle S. and Park, Joon Sung and Patel, Kayur and Hancock, Jeff and Hashimoto, Tatsunori and Bernstein, Michael S.},
title = {Jury Learning: Integrating Dissenting Voices into Machine Learning Models},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3502004},
doi = {10.1145/3491102.3502004},
abstract = {Whose labels should a machine learning (ML) algorithm learn to emulate? For ML tasks ranging from online comment toxicity to misinformation detection to medical diagnosis, different groups in society may have irreconcilable disagreements about ground truth labels. Supervised ML today resolves these label disagreements implicitly using majority vote, which overrides minority groups’ labels. We introduce jury learning, a supervised ML approach that resolves these disagreements explicitly through the metaphor of a jury: defining which people or groups, in what proportion, determine the classifier’s prediction. For example, a jury learning model for online toxicity might centrally feature women and Black jurors, who are commonly targets of online harassment. To enable jury learning, we contribute a deep learning architecture that models every annotator in a dataset, samples from annotators’ models to populate the jury, then runs inference to classify. Our architecture enables juries that dynamically adapt their composition, explore counterfactuals, and visualize dissent. A field evaluation finds that practitioners construct diverse juries that alter 14% of classification outcomes.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {115},
numpages = {19},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@misc{lauscher2022pronouns,
  doi = {10.48550/ARXIV.2202.11923},
  url = {https://arxiv.org/abs/2202.11923},
  author = {Lauscher, Anne and Crowley, Archie and Hovy, Dirk},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Welcome to the Modern World of Pronouns: Identity-Inclusive Natural Language Processing beyond Gender},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution 4.0 International}
}

@article{garg2018stereotypes,
  author = {Nikhil Garg  and Londa Schiebinger  and Dan Jurafsky  and James Zou},
  title = {Word embeddings quantify 100 years of gender and ethnic stereotypes},
  journal = {Proceedings of the National Academy of Sciences},
  number = {16},
  pages = {E3635-E3644},
  year = {2018},
  doi = {10.1073/pnas.1720347115},
  URL = {https://www.pnas.org/doi/abs/10.1073/pnas.1720347115},
  eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.1720347115},
}

@article{coleman2017dawnbench,
  title={Dawnbench: An end-to-end deep learning benchmark and competition},
  author={Coleman, Cody and Narayanan, Deepak and Kang, Daniel and Zhao, Tian and Zhang, Jian and Nardi, Luigi and Bailis, Peter and Olukotun, Kunle and R{\'e}, Chris and Zaharia, Matei},
  year=2017,
}

@inproceedings{zehlike2017fair,
author = {Zehlike, Meike and Bonchi, Francesco and Castillo, Carlos and Hajian, Sara and Megahed, Mohamed and Baeza-Yates, Ricardo},
title = {FA*IR: A Fair Top-k Ranking Algorithm},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132938},
doi = {10.1145/3132847.3132938},
abstract = {In this work, we define and solve the Fair Top-k Ranking problem, in which we want to determine a subset of k candidates from a large pool of n » k candidates, maximizing utility (i.e., select the "best" candidates) subject to group fairness criteria.Our ranked group fairness definition extends group fairness using the standard notion of protected groups and is based on ensuring that the proportion of protected candidates in every prefix of the top-k ranking remains statistically above or indistinguishable from a given minimum. Utility is operationalized in two ways: (i) every candidate included in the top-k should be more qualified than every candidate not included; and (ii) for every pair of candidates in the top-k, the more qualified candidate should be ranked above.An efficient algorithm is presented for producing the Fair Top-k Ranking, and tested experimentally on existing datasets as well as new datasets released with this paper, showing that our approach yields small distortions with respect to rankings that maximize utility without considering fairness criteria. To the best of our knowledge, this is the first algorithm grounded in statistical tests that can mitigate biases in the representation of an under-represented group along a ranked list.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1569–1578},
numpages = {10},
keywords = {algorithmic fairness, ranking, top-k selection, bias in computer systems},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@misc{chalabi2017dear,
  title={Dear Mona, what’s the most common name in America},
  author={Chalabi, Mona and Flowers, Andrew},
  year={2017}
}

@inproceedings{talat2017understanding,
    title = "Understanding Abuse: A Typology of Abusive Language Detection Subtasks",
    author = "Talat, Zeerak  and
      Davidson, Thomas  and
      Warmsley, Dana  and
      Weber, Ingmar",
    booktitle = "Proceedings of the First Workshop on Abusive Language Online",
    month = aug,
    year = "2017",
    address = "Vancouver, BC, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W17-3012",
    doi = "10.18653/v1/W17-3012",
    pages = "78--84",
    abstract = "As the body of research on abusive language detection and analysis grows, there is a need for critical consideration of the relationships between different subtasks that have been grouped under this label. Based on work on hate speech, cyberbullying, and online abuse we propose a typology that captures central similarities and differences between subtasks and discuss the implications of this for data annotation and feature construction. We emphasize the practical actions that can be taken by researchers to best approach their abusive language detection subtask of interest.",
}

@article{greenwald1998measuring,
  title={Measuring individual differences in implicit cognition: the implicit association test.},
  author={Greenwald, Anthony G and McGhee, Debbie E and Schwartz, Jordan LK},
  journal={Journal of personality and social psychology},
  volume={74},
  number={6},
  pages={1464},
  year={1998},
  publisher={American Psychological Association}
}


@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@inproceedings{halevy2021mitigating,
author = {Halevy, Matan and Harris, Camille and Bruckman, Amy and Yang, Diyi and Howard, Ayanna},
title = {Mitigating Racial Biases in Toxic Language Detection with an Equity-Based Ensemble Framework},
year = {2021},
isbn = {9781450385534},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465416.3483299},
doi = {10.1145/3465416.3483299},
abstract = {Recent research has demonstrated how racial biases against users who write African American English exists in popular toxic language datasets. While previous work has focused on a single fairness criteria, we propose to use additional descriptive fairness metrics to better understand the source of these biases. We demonstrate that different benchmark classifiers, as well as two in-process bias-remediation techniques, propagate racial biases even in a larger corpus. We then propose a novel ensemble-framework that uses a specialized classifier that is fine-tuned to the African American English dialect. We show that our proposed framework substantially reduces the racial biases that the model learns from these datasets. We demonstrate how the ensemble framework improves fairness metrics across all sample datasets with minimal impact on the classification performance, and provide empirical evidence for its ability to unlearn the annotation biases towards authors who use African American English. ** Please note that this work may contain examples of offensive words and phrases.},
booktitle = {Equity and Access in Algorithms, Mechanisms, and Optimization},
articleno = {7},
numpages = {11},
keywords = {moderation, bias mitigation, hate speech detection, AI fairness},
location = {--, NY, USA},
series = {EAAMO '21}
}

@misc{zhong2021arlsat,
      title={AR-LSAT: Investigating Analytical Reasoning of Text}, 
      author={Wanjun Zhong and Siyuan Wang and Duyu Tang and Zenan Xu and Daya Guo and Jiahai Wang and Jian Yin and Ming Zhou and Nan Duan},
      year={2021},
      eprint={2104.06598},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{hendrycks2021measuringcoding,
  title={Measuring coding challenge competence with apps},
  author={Hendrycks, Dan and Basart, Steven and Kadavath, Saurav and Mazeika, Mantas and Arora, Akul and Guo, Ethan and Burns, Collin and Puranik, Samir and He, Horace and Song, Dawn and others},
  journal={Advances in Neural Information Processing Systems (NeurIPS) Datasets and Benchmarks Track},
  year={2021}
}

@article{codex,
  author    = {Mark Chen and
               Jerry Tworek and
               Heewoo Jun and
               Qiming Yuan and
               Henrique Ponde de Oliveira Pinto and
               Jared Kaplan and
               Harrison Edwards and
               Yuri Burda and
               Nicholas Joseph and
               Greg Brockman and
               Alex Ray and
               Raul Puri and
               Gretchen Krueger and
               Michael Petrov and
               Heidy Khlaaf and
               Girish Sastry and
               Pamela Mishkin and
               Brooke Chan and
               Scott Gray and
               Nick Ryder and
               Mikhail Pavlov and
               Alethea Power and
               Lukasz Kaiser and
               Mohammad Bavarian and
               Clemens Winter and
               Philippe Tillet and
               Felipe Petroski Such and
               Dave Cummings and
               Matthias Plappert and
               Fotios Chantzis and
               Elizabeth Barnes and
               Ariel Herbert{-}Voss and
               William Hebgen Guss and
               Alex Nichol and
               Alex Paino and
               Nikolas Tezak and
               Jie Tang and
               Igor Babuschkin and
               Suchir Balaji and
               Shantanu Jain and
               William Saunders and
               Christopher Hesse and
               Andrew N. Carr and
               Jan Leike and
               Joshua Achiam and
               Vedant Misra and
               Evan Morikawa and
               Alec Radford and
               Matthew Knight and
               Miles Brundage and
               Mira Murati and
               Katie Mayer and
               Peter Welinder and
               Bob McGrew and
               Dario Amodei and
               Sam McCandlish and
               Ilya Sutskever and
               Wojciech Zaremba},
  title     = {Evaluating Large Language Models Trained on Code},
  journal   = {CoRR},
  volume    = {abs/2107.03374},
  year      = {2021},
  url       = {https://arxiv.org/abs/2107.03374},
  eprinttype = {arXiv},
  eprint    = {2107.03374},
  timestamp = {Tue, 20 Jul 2021 15:08:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2107-03374.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@misc{c4,
  doi = {10.48550/ARXIV.1910.10683},
  url = {https://arxiv.org/abs/1910.10683},
  
  author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
  keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inbook{singh2019policy,
author = {Singh, Ashudeep and Joachims, Thorsten},
title = {Policy Learning for Fairness in Ranking},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Conventional Learning-to-Rank (LTR) methods optimize the utility of the rankings to the users, but they are oblivious to their impact on the ranked items. However, there has been a growing understanding that the latter is important to consider for a wide range of ranking applications (e.g. online marketplaces, job placement, admissions). To address this need, we propose a general LTR framework that can optimize a wide range of utility metrics (e.g. NDCG) while satisfying fairness of exposure constraints with respect to the items. This framework expands the class of learnable ranking functions to stochastic ranking policies, which provides a language for rigorously expressing fairness specifications. Furthermore, we provide a new LTR algorithm called FAIR-PG-RANK for directly searching the space of fair ranking policies via a policy-gradient approach. Beyond the theoretical evidence in deriving the framework and the algorithm, we provide empirical results on simulated and real-world datasets verifying the effectiveness of the approach in individual and group-fairness settings.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {487},
numpages = {11}
}

@inproceedings{Radford2019LanguageMA,
  title={Language Models are Unsupervised Multitask Learners},
  author={Alec Radford and Jeff Wu and Rewon Child and David Luan and Dario Amodei and Ilya Sutskever},
  year={2019}
}

@misc{ppo,
  doi = {10.48550/ARXIV.1707.06347},
  url = {https://arxiv.org/abs/1707.06347},
  author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Proximal Policy Optimization Algorithms},
  publisher = {arXiv},
  year = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{suzgun2019memory,
  title={Memory-augmented recurrent neural networks can learn generalized dyck languages},
  author={Suzgun, Mirac and Gehrmann, Sebastian and Belinkov, Yonatan and Shieber, Stuart M},
  journal={arXiv preprint arXiv:1911.03329},
  year={2019}
}

@article{buchanan2021truth,
  title={Truth, Lies, and Automation},
  author={Buchanan, Ben and Lohn, Andrew and Musser, Micah and Sedova, Katerina},
  year={2021}
}

@inproceedings{gabriel-etal-2022-misinfo,
    title = "Misinfo Reaction Frames: Reasoning about Readers{'} Reactions to News Headlines",
    author = "Gabriel, Saadia  and
      Hallinan, Skyler  and
      Sap, Maarten  and
      Nguyen, Pemi  and
      Roesner, Franziska  and
      Choi, Eunsol  and
      Choi, Yejin",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.222",
    doi = "10.18653/v1/2022.acl-long.222",
    pages = "3108--3127",
}

@article{ruis2022large,
  title={Large language models are not zero-shot communicators},
  author={Laura Ruis and Akbir Khan and Stella Rose Biderman and Sara Hooker and Tim Rocktaschel and Edward Grefenstette},
  journal={ArXiv},
  year={2022},
  volume={abs/2210.14986}
}

@article{biderman2022datasheet,
  title={Datasheet for the Pile},
  author={Stella Rose Biderman and Kieran Bicheno and Leo Gao},
  journal={ArXiv},
  year={2022},
  volume={abs/2201.07311}
}

@article{
wei2022emergent,
title={Emergent Abilities of Large Language Models},
author={Jason Wei and Yi Tay and Rishi Bommasani and Colin Raffel and Barret Zoph and Sebastian Borgeaud and Dani Yogatama and Maarten Bosma and Denny Zhou and Donald Metzler and Ed H. Chi and Tatsunori Hashimoto and Oriol Vinyals and Percy Liang and Jeff Dean and William Fedus},
journal={Transactions on Machine Learning Research},
year={2022},
url={https://openreview.net/forum?id=yzkSU5zdwD},
note={Survey Certification}
}

@inproceedings{diaz2022accounting,
    title = "Accounting for Offensive Speech as a Practice of Resistance",
    author = "Diaz, Mark  and
      Amironesei, Razvan  and
      Weidinger, Laura  and
      Gabriel, Iason",
    booktitle = "Proceedings of the Sixth Workshop on Online Abuse and Harms (WOAH)",
    month = jul,
    year = "2022",
    address = "Seattle, Washington (Hybrid)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.woah-1.18",
    doi = "10.18653/v1/2022.woah-1.18",
    pages = "192--202",
    abstract = "Tasks such as toxicity detection, hate speech detection, and online harassment detection have been developed for identifying interactions involving offensive speech. In this work we articulate the need for a relational understanding of offensiveness to help distinguish denotative offensive speech from offensive speech serving as a mechanism through which marginalized communities resist oppressive social norms. Using examples from the queer community, we argue that evaluations of offensive speech must focus on the impacts of language use. We call this the cynic perspective{--} or a characteristic of language with roots in Cynic philosophy that pertains to employing offensive speech as a practice of resistance. We also explore the degree to which NLP systems may encounter limits to modeling relational context.",
}

@inproceedings{kasirzadeh2022conversation,
  title={In conversation with Artificial Intelligence: aligning language models with human values},
  author={Atoosa Kasirzadeh and Iason Gabriel},
  year={2022}
}

@unpublished{grossman-disinformation,
title= {CRFM: Shelby Grossman on Disinformation},
author = {Shelby Grossman},
year = {2022},
note= {CRFM Seminar Meeting},
}

@misc{comicmix,
  author = {U.S. Copyright Office Fair Use Index},
  title = {{Dr. Seuss Enters., L.P. v. ComicMix LLC}},
  howpublished = "\url{https://www.copyright.gov/fair-use/summaries/drseuss-comicmix-9thcir2020.pdf}",
  year = {2020}, 
}

@article{kandpal2022deduplicating,
  title={Deduplicating training data mitigates privacy risks in language models},
  author={Kandpal, Nikhil and Wallace, Eric and Raffel, Colin},
  journal={arXiv preprint arXiv:2202.06539},
  year={2022}
}

@article{nimmer2017juries,
  title={Juries and the Development of Fair Use Standards},
  author={Nimmer, David},
  journal={Harv. JL \& Tech.},
  volume={31},
  pages={563},
  year={2017},
  publisher={HeinOnline}
}

@article{howard2017addressing,
  title={Addressing bias in machine learning algorithms: A pilot study on emotion recognition for intelligent systems},
  author={Ayanna M. Howard and Cha Zhang and Eric Horvitz},
  journal={2017 IEEE Workshop on Advanced Robotics and its Social Impacts (ARSO)},
  year={2017},
  pages={1-7}
}

@article{carlini2022quantifying,
  title={Quantifying memorization across neural language models},
  author={Carlini, Nicholas and Ippolito, Daphne and Jagielski, Matthew and Lee, Katherine and Tramer, Florian and Zhang, Chiyuan},
  journal={arXiv preprint arXiv:2202.07646},
  year={2022}
}

@article{horvitz2022horizon,
  title={On the Horizon: Interactive and Compositional Deepfakes},
  author={Eric Horvitz},
  journal={Proceedings of the 2022 International Conference on Multimodal Interaction},
  year={2022}
}

@inproceedings{carlini2021extracting,
  title={Extracting training data from large language models},
  author={Carlini, Nicholas and Tramer, Florian and Wallace, Eric and Jagielski, Matthew and Herbert-Voss, Ariel and Lee, Katherine and Roberts, Adam and Brown, Tom and Song, Dawn and Erlingsson, Ulfar and others},
  booktitle={30th USENIX Security Symposium (USENIX Security 21)},
  pages={2633--2650},
  year={2021}
}

@article{sun2021coprotector,
  title={CoProtector: Protect Open-Source Code against Unauthorized Training Usage with Data Poisoning},
  author={Sun, Zhensu and Du, Xiaoning and Song, Fu and Ni, Mingze and Li, Li},
  journal={arXiv preprint arXiv:2110.12925},
  year={2021}
}

@article{englund1990idea,
  title={Idea, Process, or Protected Expression?: Determining the Scope of Copyright Protection of the Structure of Computer Programs},
  author={Englund, Steven R},
  journal={Michigan Law Review},
  volume={88},
  number={4},
  pages={866--909},
  year={1990},
  publisher={JSTOR}
}

@article{heymann2008everything,
  title={Everything is transformative: Fair use and reader response},
  author={Heymann, Laura A},
  journal={Columbia Journal of Law \& the Arts},
  volume={31},
  pages={08--06},
  year={2008}
}

@article{manning2008introduction,
  title={Introduction to Information Retrieval},
  author={Manning, Christopher D and Raghavan, Prabhakar and Sch{\"u}tze, Hinrich},
  year={2008},
  publisher={Cambridge University Press}
}

@article{lester2017dilemma,
  title={The Dilemma of False Positives: Making Content ID Algorithms More Conducive to Fostering Innovative Fair Use in Media Creation},
  author={Lester, Toni and Pachamanova, Dessislava},
  journal={UCLA Ent. L. Rev.},
  volume={24},
  pages={51},
  year={2017},
  publisher={HeinOnline}
}

@inproceedings{wang2019talkdown,
    title = "{T}alk{D}own: A Corpus for Condescension Detection in Context",
    author = "Wang, Zijian  and
      Potts, Christopher",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1385",
    doi = "10.18653/v1/D19-1385",
    pages = "3711--3719",
    abstract = "Condescending language use is caustic; it can bring dialogues to an end and bifurcate communities. Thus, systems for condescension detection could have a large positive impact. A challenge here is that condescension is often impossible to detect from isolated utterances, as it depends on the discourse and social context. To address this, we present TalkDown, a new labeled dataset of condescending linguistic acts in context. We show that extending a language-only model with representations of the discourse improves performance, and we motivate techniques for dealing with the low rates of condescension overall. We also use our model to estimate condescension rates in various online communities and relate these differences to differing community norms.",
}

@article{kaplan2020scaling,
  title={Scaling Laws for Neural Language Models},
  author={Jared Kaplan and Sam McCandlish and T. J. Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeff Wu and Dario Amodei},
  journal={ArXiv},
  year={2020},
  volume={abs/2001.08361}
}

@ARTICLE{mendelsohn2020framework,
  
AUTHOR={Mendelsohn, Julia and Tsvetkov, Yulia and Jurafsky, Dan},   
	 
TITLE={A Framework for the Computational Linguistic Analysis of Dehumanization},      
	
JOURNAL={Frontiers in Artificial Intelligence},      
	
VOLUME={3},           
	
YEAR={2020},      
	  
URL={https://www.frontiersin.org/articles/10.3389/frai.2020.00055},       
	
DOI={10.3389/frai.2020.00055},      
	
ISSN={2624-8212},   
   
ABSTRACT={Dehumanization is a pernicious psychological process that often leads to extreme intergroup bias, hate speech, and violence aimed at targeted social groups. Despite these serious consequences and the wealth of available data, dehumanization has not yet been computationally studied on a large scale. Drawing upon social psychology research, we create a computational linguistic framework for analyzing dehumanizing language by identifying linguistic correlates of salient components of dehumanization. We then apply this framework to analyze discussions of LGBTQ people in the New York Times from 1986 to 2015. Overall, we find increasingly humanizing descriptions of LGBTQ people over time. However, we find that the label homosexual has emerged to be much more strongly associated with dehumanizing attitudes than other labels, such as gay. Our proposed techniques highlight processes of linguistic variation and change in discourses surrounding marginalized groups. Furthermore, the ability to analyze dehumanizing language at a large scale has implications for automatically detecting and understanding media bias as well as abusive language online.}
}

@article{yu2020can,
  title={Can Algorithms Promote Fair Use?},
  author={Yu, Peter K},
  journal={FIU L. Rev.},
  volume={14},
  pages={329},
  year={2020},
  publisher={HeinOnline}
}


@article{bartholomew2014death,
  title={The death of fair use in cyberspace: YouTube and the problem with content ID},
  author={Bartholomew, Taylor B},
  journal={Duke L. \& Tech. Rev.},
  volume={13},
  pages={66},
  year={2014},
  publisher={HeinOnline}
}


@article{samuelson2017saving,
  title={Saving Software's Fair Use Future},
  author={Samuelson, Pamela and Asay, Clark D},
  journal={Harv. JL \& Tech.},
  volume={31},
  pages={535},
  year={2017},
  publisher={HeinOnline}
}

@article{franceschelli2021copyright,
  title={Copyright in Generative Deep Learning},
  author={Franceschelli, Giorgio and Musolesi, Mirco},
  journal={arXiv preprint arXiv:2105.09266},
  year={2021}
}


@article{gray2020playing,
  title={Playing with machines: Using machine learning to understand automated copyright enforcement at scale},
  author={Gray, Joanne E and Suzor, Nicolas P},
  journal={Big Data \& Society},
  volume={7},
  number={1},
  pages={2053951720919963},
  year={2020},
  publisher={SAGE Publications Sage UK: London, England}
}

@inproceedings{saadatpanah2020adversarial,
  title={Adversarial attacks on copyright detection systems},
  author={Saadatpanah, Parsa and Shafahi, Ali and Goldstein, Tom},
  booktitle={International Conference on Machine Learning},
  pages={8307--8315},
  year={2020},
  organization={PMLR}
}


@article{gillotte2019copyright,
  title={Copyright infringement in ai-generated artworks},
  author={Gillotte, Jessica L},
  journal={UC Davis L. Rev.},
  volume={53},
  pages={2655},
  year={2019},
  publisher={HeinOnline}
}



@article{tu2020use,
  title={Use of Artificial Intelligence to Determine Copyright Liability for Musical Works},
  author={Tu, Shine Sean},
  journal={WVU College of Law Research Paper},
  number={2020-012},
  year={2020}
}


@article{nimmer2003fairest,
  title={" Fairest of them all" and other fairy tales of fair use},
  author={Nimmer, David},
  journal={Law and Contemporary Problems},
  volume={66},
  number={1/2},
  pages={263--287},
  year={2003},
  publisher={JSTOR}
}


@article{nimmer2017juries,
  title={Juries and the Development of Fair Use Standards},
  author={Nimmer, David},
  journal={Harv. JL \& Tech.},
  volume={31},
  pages={563},
  year={2017},
  publisher={HeinOnline}
}


@article{asay2020transformative,
  title={Is transformative use eating the world},
  author={Asay, Clark D and Sloan, Arielle and Sobczak, Dean},
  journal={BCL Rev.},
  volume={61},
  pages={905},
  year={2020},
  publisher={HeinOnline}
}


@inproceedings{carlini2019secret,
  title={The secret sharer: Evaluating and testing unintended memorization in neural networks},
  author={Carlini, Nicholas and Liu, Chang and Erlingsson, {\'U}lfar and Kos, Jernej and Song, Dawn},
  booktitle={28th $\{$USENIX$\}$ Security Symposium ($\{$USENIX$\}$ Security 19)},
  pages={267--284},
  year={2019}
}

@article{lee2022language,
  title={Do Language Models Plagiarize?},
  author={Lee, Jooyoung and Le, Thai and Chen, Jinghui and Lee, Dongwon},
  journal={arXiv preprint arXiv:2203.07618},
  year={2022}
}

@article{hoffmann2022chinchilla,
  title={Training Compute-Optimal Large Language Models},
  author={Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de Las Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and Tom Hennigan and Eric Noland and Katie Millican and George van den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and Karen Simonyan and Erich Elsen and Jack W. Rae and Oriol Vinyals and L. Sifre},
  journal={ArXiv},
  year={2022},
  volume={abs/2203.15556}
}

@article{lin2022conceptual,
author = {Lin, Jimmy},
title = {A Proposed Conceptual Framework for a Representational Approach to Information Retrieval},
year = {2022},
issue_date = {December 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {2},
issn = {0163-5840},
url = {https://doi.org/10.1145/3527546.3527552},
doi = {10.1145/3527546.3527552},
abstract = {This paper outlines a conceptual framework for understanding recent developments in information retrieval and natural language processing that attempts to integrate dense and sparse retrieval methods. I propose a representational approach that breaks the core text retrieval problem into a logical scoring model and a physical retrieval model. The scoring model is defined in terms of encoders, which map queries and documents into a representational space, and a comparison function that computes query-document scores. The physical retrieval model defines how a system produces the top-k scoring documents from an arbitrarily large corpus with respect to a query. The scoring model can be further analyzed along two dimensions: dense vs. sparse representations and supervised (learned) vs. unsupervised approaches. I show that many recently proposed retrieval methods, including multi-stage ranking designs, can be seen as different parameterizations in this framework, and that a unified view suggests a number of open research questions, providing a roadmap for future work. As a bonus, this conceptual framework establishes connections to sentence similarity tasks in natural language processing and information access "technologies" prior to the dawn of computing.},
journal = {SIGIR Forum},
month = {mar},
articleno = {4},
numpages = {29}
}

@article{li2022competition,
  title={Competition-level code generation with alphacode},
  author={Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate and Schrittwieser, Julian and Leblond, R{\'e}mi and Eccles, Tom and Keeling, James and Gimeno, Felix and Lago, Agustin Dal and others},
  journal={arXiv preprint arXiv:2203.07814},
  year={2022}
}


@article{lee2021deduplicating,
  title={Deduplicating training data makes language models better},
  author={Lee, Katherine and Ippolito, Daphne and Nystrom, Andrew and Zhang, Chiyuan and Eck, Douglas and Callison-Burch, Chris and Carlini, Nicholas},
  journal={arXiv preprint arXiv:2107.06499},
  year={2021}
}


@article{lemley2020fair,
  title={Fair Learning},
  author={Lemley, Mark A and Casey, Bryan},
  journal={Tex. L. Rev.},
  volume={99},
  pages={743},
  year={2020},
  publisher={HeinOnline}
}

@article{snow2019decides,
  title={Who Decides Fair Use-Judge or Jury},
  author={Snow, Ned},
  journal={Wash. L. Rev.},
  volume={94},
  pages={275},
  year={2019},
  publisher={HeinOnline}
}

@article{dhole2021nl,
  author    = {Kaustubh D. Dhole and
               Varun Gangal and
               Sebastian Gehrmann and
               Aadesh Gupta and
               Zhenhao Li and
               Saad Mahamood and
               Abinaya Mahendiran and
               Simon Mille and
               Ashish Srivastava and
               Samson Tan and
               Tongshuang Wu and
               Jascha Sohl{-}Dickstein and
               Jinho D. Choi and
               Eduard H. Hovy and
               Ondrej Dusek and
               Sebastian Ruder and
               Sajant Anand and
               Nagender Aneja and
               Rabin Banjade and
               Lisa Barthe and
               Hanna Behnke and
               Ian Berlot{-}Attwell and
               Connor Boyle and
               Caroline Brun and
               Marco Antonio Sobrevilla Cabezudo and
               Samuel Cahyawijaya and
               Emile Chapuis and
               Wanxiang Che and
               Mukund Choudhary and
               Christian Clauss and
               Pierre Colombo and
               Filip Cornell and
               Gautier Dagan and
               Mayukh Das and
               Tanay Dixit and
               Thomas Dopierre and
               Paul{-}Alexis Dray and
               Suchitra Dubey and
               Tatiana Ekeinhor and
               Marco Di Giovanni and
               Rishabh Gupta and
               Rishabh Gupta and
               Louanes Hamla and
               Sang Han and
               Fabrice Harel{-}Canada and
               Antoine Honore and
               Ishan Jindal and
               Przemyslaw K. Joniak and
               Denis Kleyko and
               Venelin Kovatchev and
               et al.},
  title     = {{NL}-{A}ugmenter: {A} Framework for Task-Sensitive Natural Language Augmentation},
   journal={arXiv preprint arXiv:2112.02721},
  year      = {2021},
}

@article{miller1995wordnet,
  title={WordNet: a lexical database for English},
  author={Miller, George A},
  journal={Communications of the ACM},
  volume={38},
  number={11},
  pages={39--41},
  year={1995},
  publisher={ACM New York, NY, USA}
}

@article{abedjan2016detecting,
  title={Detecting data errors: Where are we and what needs to be done?},
  author={Abedjan, Ziawasch and Chu, Xu and Deng, Dong and Fernandez, Raul Castro and Ilyas, Ihab F and Ouzzani, Mourad and Papotti, Paolo and Stonebraker, Michael and Tang, Nan},
  journal={Proceedings of the VLDB Endowment},
  volume={9},
  number={12},
  pages={993--1004},
  year={2016},
  publisher={VLDB Endowment}
}
@article{li2020deep,
  title={Deep entity matching with pre-trained language models},
  author={Li, Yuliang and Li, Jinfeng and Suhara, Yoshihiko and Doan, AnHai and Tan, Wang-Chiew},
  journal={arXiv preprint arXiv:2004.00584},
  year={2020}
}
@article{narayan2022can,
  title={Can Foundation Models Wrangle Your Data?},
  author={Narayan, Avanika and Chami, Ines and Orr, Laurel and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:2205.09911},
  year={2022}
}

@inproceedings{golshan2017data,
  title={Data integration: After the teenage years},
  author={Golshan, Behzad and Halevy, Alon and Mihaila, George and Tan, Wang-Chiew},
  booktitle={Proceedings of the 36th ACM SIGMOD-SIGACT-SIGAI symposium on principles of database systems},
  pages={101--106},
  year={2017}
}

@article{konda2016magellan,
  title={Magellan: toward building entity matching management systems over data science stacks},
  author={Konda, Pradap and Das, Sanjib and Doan, AnHai and Ardalan, Adel and Ballard, Jeffrey R and Li, Han and Panahi, Fatemah and Zhang, Haojun and Naughton, Jeff and Prasad, Shishir and others},
  journal={Proceedings of the VLDB Endowment},
  volume={9},
  number={13},
  pages={1581--1584},
  year={2016},
  publisher={VLDB Endowment}
}

@inproceedings{tejaswin2021summarization,
    title = "How well do you know your summarization datasets?",
    author = "Tejaswin, Priyam  and
      Naik, Dhruv  and
      Liu, Pengfei",
    booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-acl.303",
    doi = "10.18653/v1/2021.findings-acl.303",
    pages = "3436--3449",
}


@inproceedings{mei2021capturing,
  title={Capturing Semantics for Imputation with Pre-trained Language Models},
  author={Mei, Yinan and Song, Shaoxu and Fang, Chenguang and Yang, Haifeng and Fang, Jingyun and Long, Jiang},
  booktitle={2021 IEEE 37th International Conference on Data Engineering (ICDE)},
  pages={61--72},
  year={2021},
  organization={IEEE}
}

@article{warstadt-etal-2020-blimp-benchmark,
    title = "{BL}i{MP}: The Benchmark of Linguistic Minimal Pairs for {E}nglish",
    author = "Warstadt, Alex  and
      Parrish, Alicia  and
      Liu, Haokun  and
      Mohananey, Anhad  and
      Peng, Wei  and
      Wang, Sheng-Fu  and
      Bowman, Samuel R.",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "8",
    year = "2020",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2020.tacl-1.25",
    doi = "10.1162/tacl_a_00321",
    pages = "377--392",
    abstract = "We introduce The Benchmark of Linguistic Minimal Pairs (BLiMP),1 a challenge set for evaluating the linguistic knowledge of language models (LMs) on major grammatical phenomena in English. BLiMP consists of 67 individual datasets, each containing 1,000 minimal pairs{---}that is, pairs of minimally different sentences that contrast in grammatical acceptability and isolate specific phenomenon in syntax, morphology, or semantics. We generate the data according to linguist-crafted grammar templates, and human aggregate agreement with the labels is 96.4{\%}. We evaluate n-gram, LSTM, and Transformer (GPT-2 and Transformer-XL) LMs by observing whether they assign a higher probability to the acceptable sentence in each minimal pair. We find that state-of-the-art models identify morphological contrasts related to agreement reliably, but they struggle with some subtle semantic and syntactic phenomena, such as negative polarity items and extraction islands.",
}

@inproceedings{yatskar2019qualitative,
    title = "A Qualitative Comparison of {C}o{QA}, {SQ}u{AD} 2.0 and {Q}u{AC}",
    author = "Yatskar, Mark",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1241",
    doi = "10.18653/v1/N19-1241",
    pages = "2318--2323",
    abstract = "We compare three new datasets for question answering: SQuAD 2.0, QuAC, and CoQA, along several of their new features: (1) unanswerable questions, (2) multi-turn interactions, and (3) abstractive answers.We show that the datasets provide complementary coverage of the first two aspects, but weak coverage of the third.Because of the datasets{'} structural similarity, a single extractive model can be easily adapted to any of the datasets and we show improved baseline results on both SQuAD 2.0 and CoQA. Despite the similarity, models trained on one dataset are ineffective on another dataset, but we find moderate performance improvement through pretraining. To encourage cross-evaluation, we release code for conversion between datasets.",
}

@inproceedings{jung2019earlier,
    title = "Earlier Isn{'}t Always Better: Sub-aspect Analysis on Corpus and System Biases in Summarization",
    author = "Jung, Taehee  and
      Kang, Dongyeop  and
      Mentch, Lucas  and
      Hovy, Eduard",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1327",
    doi = "10.18653/v1/D19-1327",
    pages = "3324--3335",
    abstract = "Despite the recent developments on neural summarization systems, the underlying logic behind the improvements from the systems and its corpus-dependency remains largely unexplored. Position of sentences in the original text, for example, is a well known bias for news summarization. Following in the spirit of the claim that summarization is a combination of sub-functions, we define three sub-aspects of summarization: position, importance, and diversity and conduct an extensive analysis of the biases of each sub-aspect with respect to the domain of nine different summarization corpora (e.g., news, academic papers, meeting minutes, movie script, books, posts). We find that while position exhibits substantial bias in news articles, this is not the case, for example, with academic papers and meeting minutes. Furthermore, our empirical study shows that different types of summarization systems (e.g., neural-based) are composed of different degrees of the sub-aspects. Our study provides useful lessons regarding consideration of underlying sub-aspects when collecting a new summarization dataset or developing a new system.",
}


@article{DBLP:journals/corr/abs-1903-04561,
  author    = {Daniel Borkan and
               Lucas Dixon and
               Jeffrey Sorensen and
               Nithum Thain and
               Lucy Vasserman},
  title     = {Nuanced Metrics for Measuring Unintended Bias with Real Data for Text
               Classification},
  journal   = {CoRR},
  volume    = {abs/1903.04561},
  year      = {2019},
  url       = {http://arxiv.org/abs/1903.04561},
  archivePrefix = {arXiv},
  eprint    = {1903.04561},
  timestamp = {Sun, 31 Mar 2019 19:01:24 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1903-04561},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc {bloom2022bloom,
	author       = { BigScience Workshop },
	title        = { bloom (Revision 4ab0472) },
	year         = 2022,
	url          = { https://huggingface.co/bigscience/bloom },
	doi          = { 10.57967/hf/0003 },
	publisher    = { Hugging Face }
}

@article{luccioni2022estimating,
  title={Estimating the Carbon Footprint of BLOOM, a 176B Parameter Language Model},
  author={Alexandra Sasha Luccioni and Sylvain Viguier and Anne-Laure Ligozat},
  journal={ArXiv},
  year={2022},
  volume={abs/2211.02001},
  url={https://api.semanticscholar.org/CorpusID:253265387}
}

@article{scao2022bloom,
  doi = {10.48550/ARXIV.2211.05100},
  
  url = {https://arxiv.org/abs/2211.05100},
  
  author = {Le Scao, Teven and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ilić, Suzana and Hesslow, Daniel and Castagné, Roman and Luccioni, Alexandra Sasha and Yvon, François and Gallé, Matthias and Tow, Jonathan and Rush, Alexander M. and Biderman, Stella and Webson, Albert and Ammanamanchi, Pawan Sasanka and Wang, Thomas and Sagot, Benoît and Muennighoff, Niklas and del Moral, Albert Villanova and Ruwase, Olatunji and Bawden, Rachel and Bekman, Stas and McMillan-Major, Angelina and Beltagy, Iz and Nguyen, Huu and Saulnier, Lucile and Tan, Samson and Suarez, Pedro Ortiz and Sanh, Victor and Laurençon, Hugo and Jernite, Yacine and Launay, Julien and Mitchell, Margaret and Raffel, Colin and Gokaslan, Aaron and Simhi, Adi and Soroa, Aitor and Aji, Alham Fikri and Alfassy, Amit and Rogers, Anna and Nitzav, Ariel Kreisberg and Xu, Canwen and Mou, Chenghao and Emezue, Chris and Klamm, Christopher and Leong, Colin and van Strien, Daniel and Adelani, David Ifeoluwa and Radev, Dragomir and Ponferrada, Eduardo González and Levkovizh, Efrat and Kim, Ethan and Natan, Eyal Bar and De Toni, Francesco and Dupont, Gérard and Kruszewski, Germán and Pistilli, Giada and Elsahar, Hady and Benyamina, Hamza and Tran, Hieu and Yu, Ian and Abdulmumin, Idris and Johnson, Isaac and Gonzalez-Dios, Itziar and de la Rosa, Javier and Chim, Jenny and Dodge, Jesse and Zhu, Jian and Chang, Jonathan and Frohberg, Jörg and Tobing, Joseph and Bhattacharjee, Joydeep and Almubarak, Khalid and Chen, Kimbo and Lo, Kyle and Von Werra, Leandro and Weber, Leon and Phan, Long and allal, Loubna Ben and Tanguy, Ludovic and Dey, Manan and Muñoz, Manuel Romero and Masoud, Maraim and Grandury, María and Šaško, Mario and Huang, Max and Coavoux, Maximin and Singh, Mayank and Jiang, Mike Tian-Jian and Vu, Minh Chien and Jauhar, Mohammad A. and Ghaleb, Mustafa and Subramani, Nishant and Kassner, Nora and Khamis, Nurulaqilla and Nguyen, Olivier and Espejel, Omar and de Gibert, Ona and Villegas, Paulo and Henderson, Peter and Colombo, Pierre and Amuok, Priscilla and Lhoest, Quentin and Harliman, Rheza and Bommasani, Rishi and López, Roberto Luis and Ribeiro, Rui and Osei, Salomey and Pyysalo, Sampo and Nagel, Sebastian and Bose, Shamik and Muhammad, Shamsuddeen Hassan and Sharma, Shanya and Longpre, Shayne and Nikpoor, Somaieh and Silberberg, Stanislav and Pai, Suhas and Zink, Sydney and Torrent, Tiago Timponi and Schick, Timo and Thrush, Tristan and Danchev, Valentin and Nikoulina, Vassilina and Laippala, Veronika and Lepercq, Violette and Prabhu, Vrinda and Alyafeai, Zaid and Talat, Zeerak and Raja, Arun and Heinzerling, Benjamin and Si, Chenglei and Salesky, Elizabeth and Mielke, Sabrina J. and Lee, Wilson Y. and Sharma, Abheesht and Santilli, Andrea and Chaffin, Antoine and Stiegler, Arnaud and Datta, Debajyoti and Szczechla, Eliza and Chhablani, Gunjan and Wang, Han and Pandey, Harshit and Strobelt, Hendrik and Fries, Jason Alan and Rozen, Jos and Gao, Leo and Sutawika, Lintang and Bari, M Saiful and Al-shaibani, Maged S. and Manica, Matteo and Nayak, Nihal and Teehan, Ryan and Albanie, Samuel and Shen, Sheng and Ben-David, Srulik and Bach, Stephen H. and Kim, Taewoon and Bers, Tali and Fevry, Thibault and Neeraj, Trishala and Thakker, Urmish and Raunak, Vikas and Tang, Xiangru and Yong, Zheng-Xin and Sun, Zhiqing and Brody, Shaked and Uri, Yallow and Tojarieh, Hadar and Roberts, Adam and Chung, Hyung Won and Tae, Jaesung and Phang, Jason and Press, Ofir and Li, Conglong and Narayanan, Deepak and Bourfoune, Hatim and Casper, Jared and Rasley, Jeff and Ryabinin, Max and Mishra, Mayank and Zhang, Minjia and Shoeybi, Mohammad and Peyrounette, Myriam and Patry, Nicolas and Tazi, Nouamane and Sanseviero, Omar and von Platen, Patrick and Cornette, Pierre and Lavallée, Pierre François and Lacroix, Rémi and Rajbhandari, Samyam and Gandhi, Sanchit and Smith, Shaden and Requena, Stéphane and Patil, Suraj and Dettmers, Tim and Baruwa, Ahmed and Singh, Amanpreet and Cheveleva, Anastasia and Ligozat, Anne-Laure and Subramonian, Arjun and Névéol, Aurélie and Lovering, Charles and Garrette, Dan and Tunuguntla, Deepak and Reiter, Ehud and Taktasheva, Ekaterina and Voloshina, Ekaterina and Bogdanov, Eli and Winata, Genta Indra and Schoelkopf, Hailey and Kalo, Jan-Christoph and Novikova, Jekaterina and Forde, Jessica Zosa and Clive, Jordan and Kasai, Jungo and Kawamura, Ken and Hazan, Liam and Carpuat, Marine and Clinciu, Miruna and Kim, Najoung and Cheng, Newton and Serikov, Oleg and Antverg, Omer and van der Wal, Oskar and Zhang, Rui and Zhang, Ruochen and Gehrmann, Sebastian and Pais, Shani and Shavrina, Tatiana and Scialom, Thomas and Yun, Tian and Limisiewicz, Tomasz and Rieser, Verena and Protasov, Vitaly and Mikhailov, Vladislav and Pruksachatkun, Yada and Belinkov, Yonatan and Bamberger, Zachary and Kasner, Zdeněk and Rueda, Alice and Pestana, Amanda and Feizpour, Amir and Khan, Ammar and Faranak, Amy and Santos, Ana and Hevia, Anthony and Unldreaj, Antigona and Aghagol, Arash and Abdollahi, Arezoo and Tammour, Aycha and HajiHosseini, Azadeh and Behroozi, Bahareh and Ajibade, Benjamin and Saxena, Bharat and Ferrandis, Carlos Muñoz and Contractor, Danish and Lansky, David and David, Davis and Kiela, Douwe and Nguyen, Duong A. and Tan, Edward and Baylor, Emi and Ozoani, Ezinwanne and Mirza, Fatima and Ononiwu, Frankline and Rezanejad, Habib and Jones, Hessie and Bhattacharya, Indrani and Solaiman, Irene and Sedenko, Irina and Nejadgholi, Isar and Passmore, Jesse and Seltzer, Josh and Sanz, Julio Bonis and Fort, Karen and Dutra, Livia and Samagaio, Mairon and Elbadri, Maraim and Mieskes, Margot and Gerchick, Marissa and Akinlolu, Martha and McKenna, Michael and Qiu, Mike and Ghauri, Muhammed and Burynok, Mykola and Abrar, Nafis and Rajani, Nazneen and Elkott, Nour and Fahmy, Nour and Samuel, Olanrewaju and An, Ran and Kromann, Rasmus and Hao, Ryan and Alizadeh, Samira and Shubber, Sarmad and Wang, Silas and Roy, Sourav and Viguier, Sylvain and Le, Thanh and Oyebade, Tobi and Le, Trieu and Yang, Yoyo and Nguyen, Zach and Kashyap, Abhinav Ramesh and Palasciano, Alfredo and Callahan, Alison and Shukla, Anima and Miranda-Escalada, Antonio and Singh, Ayush and Beilharz, Benjamin and Wang, Bo and Brito, Caio and Zhou, Chenxi and Jain, Chirag and Xu, Chuxin and Fourrier, Clémentine and Periñán, Daniel León and Molano, Daniel and Yu, Dian and Manjavacas, Enrique and Barth, Fabio and Fuhrimann, Florian and Altay, Gabriel and Bayrak, Giyaseddin and Burns, Gully and Vrabec, Helena U. and Bello, Imane and Dash, Ishani and Kang, Jihyun and Giorgi, John and Golde, Jonas and Posada, Jose David and Sivaraman, Karthik Rangasai and Bulchandani, Lokesh and Liu, Lu and Shinzato, Luisa and de Bykhovetz, Madeleine Hahn and Takeuchi, Maiko and Pàmies, Marc and Castillo, Maria A and Nezhurina, Marianna and Sänger, Mario and Samwald, Matthias and Cullan, Michael and Weinberg, Michael and De Wolf, Michiel and Mihaljcic, Mina and Liu, Minna and Freidank, Moritz and Kang, Myungsun and Seelam, Natasha and Dahlberg, Nathan and Broad, Nicholas Michio and Muellner, Nikolaus and Fung, Pascale and Haller, Patrick and Chandrasekhar, Ramya and Eisenberg, Renata and Martin, Robert and Canalli, Rodrigo and Su, Rosaline and Su, Ruisi and Cahyawijaya, Samuel and Garda, Samuele and Deshmukh, Shlok S and Mishra, Shubhanshu and Kiblawi, Sid and Ott, Simon and Sang-aroonsiri, Sinee and Kumar, Srishti and Schweter, Stefan and Bharati, Sushil and Laud, Tanmay and Gigant, Théo and Kainuma, Tomoya and Kusa, Wojciech and Labrak, Yanis and Bajaj, Yash Shailesh and Venkatraman, Yash and Xu, Yifan and Xu, Yingxin and Xu, Yu and Tan, Zhe and Xie, Zhongli and Ye, Zifan and Bras, Mathilde and Belkada, Younes and Wolf, Thomas},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {BLOOM: A 176B-Parameter Open-Access Multilingual Language Model},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}



@inproceedings{Werra2022evaluate,
  title={Evaluate\&Evaluation on the Hub: Better Best Practices for Data and Model Measurements},
  author={Leandro von Werra and Lewis Tunstall and Abhishek Thakur and Alexandra Sasha Luccioni and Tristan Thrush and Aleksandra Piktus and Felix Marty and Nazneen Rajani and Victor Mustar and Helen Ngo and Omar Sanseviero and Mario vSavsko and Albert Villanova and Quentin Lhoest and Julien Chaumond and Margaret Mitchell and Alexander M. Rush and Thomas Wolf and Douwe Kiela},
  year={2022}
}

@inproceedings{helwe2021reasoning,
  title={Reasoning with transformer-based models: Deep learning, but shallow reasoning},
  author={Helwe, Chadi and Clavel, Chlo{\'e} and Suchanek, Fabian M},
  booktitle={3rd Conference on Automated Knowledge Base Construction},
  year={2021}
}

@article{wang2022lsat,
  title={From lsat: The progress and challenges of complex reasoning},
  author={Wang, Siyuan and Liu, Zhongkun and Zhong, Wanjun and Zhou, Ming and Wei, Zhongyu and Chen, Zhumin and Duan, Nan},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  year={2022},
  publisher={IEEE}
}

@inproceedings{zeng2022glm,
  title={GLM-130B: An Open Bilingual Pre-trained Model},
  author={Aohan Zeng and Xiao Liu and Zhengxiao Du and Zihan Wang and Hanyu Lai and Ming Ding and Zhuoyi Yang and Yifan Xu and Wendi Zheng and Xiao Xia and Weng Lam Tam and Zixuan Ma and Yufei Xue and Jidong Zhai and Wenguang Chen and P. Zhang and Yuxiao Dong and Jie Tang},
  year={2022}
}

@article{evans2022truthfulqa,
  title={How do new models from {OpenAI, DeepMind and Anthropic perform on TruthfulQA?}},
  author={Owain Evans and Stephanie Lin and Jacob Hilton},
  url = {https://www.lesswrong.com/posts/yYkrbS5iAwdEQyynW/how-do-new-models-from-openai-deepmind-and-anthropic-perform},
  journal={AI Alignment Forum},
  year={2022},
}

@inproceedings{caines2018aggressive,
    title = "Aggressive language in an online hacking forum",
    author = "Caines, Andrew  and
      Pastrana, Sergio  and
      Hutchings, Alice  and
      Buttery, Paula",
    booktitle = "Proceedings of the 2nd Workshop on Abusive Language Online ({ALW}2)",
    month = oct,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-5109",
    doi = "10.18653/v1/W18-5109",
    pages = "66--74",
    abstract = "We probe the heterogeneity in levels of abusive language in different sections of the Internet, using an annotated corpus of Wikipedia page edit comments to train a binary classifier for abuse detection. Our test data come from the CrimeBB Corpus of hacking-related forum posts and we find that (a) forum interactions are rarely abusive, (b) the abusive language which does exist tends to be relatively mild compared to that found in the Wikipedia comments domain, and tends to involve aggressive posturing rather than hate speech or threats of violence. We observe that the purpose of conversations in online forums tend to be more constructive and informative than those in Wikipedia page edit comments which are geared more towards adversarial interactions, and that this may explain the lower levels of abuse found in our forum data than in Wikipedia comments. Further work remains to be done to compare these results with other inter-domain classification experiments, and to understand the impact of aggressive language in forum conversations.",
}

@article{dehghani2018universal,
  title={Universal transformers},
  author={Dehghani, Mostafa and Gouws, Stephan and Vinyals, Oriol and Uszkoreit, Jakob and Kaiser, {\L}ukasz},
  journal={arXiv preprint arXiv:1807.03819},
  year={2018}
}

@inproceedings{kumar2016ask,
  title={Ask me anything: Dynamic memory networks for natural language processing},
  author={Kumar, Ankit and Irsoy, Ozan and Ondruska, Peter and Iyyer, Mohit and Bradbury, James and Gulrajani, Ishaan and Zhong, Victor and Paulus, Romain and Socher, Richard},
  booktitle={International conference on machine learning},
  pages={1378--1387},
  year={2016},
  organization={PMLR}
}

@article{babi,
  title={Towards AI-complete question answering: A set of prerequisite toy tasks},
  author={Weston, Jason and Bordes, Antoine and Chopra, Sumit and Rush, Alexander M and Van Merri{\"e}nboer, Bart and Joulin, Armand and Mikolov, Tomas},
  journal={arXiv preprint arXiv:1502.05698},
  year={2015}
}


@article{diresta2019potemkin,
  title={Potemkin pages \& personas: Assessing GRU online operations, 2014-2019},
  author={DiResta, Renee and Grossman, Shelby},
  journal={White Paper https://fsi-live. s3. us-west-1. amazonaws. com/s3fs-public/potemkin-pagespersonas-sio-wp. pdf},
  year={2019}
}

@inproceedings{sap2019socialiqa,
    title = "Social {IQ}a: Commonsense Reasoning about Social Interactions",
    author = "Sap, Maarten  and
      Rashkin, Hannah  and
      Chen, Derek  and
      Le Bras, Ronan  and
      Choi, Yejin",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1454",
    doi = "10.18653/v1/D19-1454",
    pages = "4463--4473",
    abstract = "We introduce Social IQa, the first large-scale benchmark for commonsense reasoning about social situations. Social IQa contains 38,000 multiple choice questions for probing emotional and social intelligence in a variety of everyday situations (e.g., Q: {``}Jordan wanted to tell Tracy a secret, so Jordan leaned towards Tracy. Why did Jordan do this?{''} A: {``}Make sure no one else could hear{''}). Through crowdsourcing, we collect commonsense questions along with correct and incorrect answers about social interactions, using a new framework that mitigates stylistic artifacts in incorrect answers by asking workers to provide the right answer to a different but related question. Empirical results show that our benchmark is challenging for existing question-answering models based on pretrained language models, compared to human performance ({\textgreater}20{\%} gap). Notably, we further establish Social IQa as a resource for transfer learning of commonsense knowledge, achieving state-of-the-art performance on multiple commonsense reasoning tasks (Winograd Schemas, COPA).",
}

@misc{election2021long,
  title={The Long Fuse: Misinformation and the 2020 Election},
  author={Election Integrity Partnership},
  year={2021}
}

@article{whitten2020poison,
  title={Poison if you don’t know how to use it: Facebook, democracy, and human rights in Myanmar},
  author={Whitten-Woodring, Jenifer and Kleinberg, Mona S and Thawnghmung, Ardeth and Thitsar, Myat The},
  journal={The International Journal of Press/Politics},
  volume={25},
  number={3},
  pages={407--425},
  year={2020},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}

@article{zarocostas2020fight,
  title={How to fight an infodemic},
  author={Zarocostas, John},
  journal={The lancet},
  volume={395},
  number={10225},
  pages={676},
  year={2020},
  publisher={Elsevier}
}


@incollection{benkler2018network,
    author = {Benkler, Yochai and Faris, Robert and Roberts, Hal},
    isbn = {9780190923624},
    title = "{3. Epistemic Crisis}",
    booktitle = "{Network Propaganda: Manipulation, Disinformation, and Radicalization in American Politics}",
    publisher = {Oxford University Press},
    year = {2018},
    month = {11},
    doi = {10.1093/oso/9780190923624.003.0001},
    url = {https://doi.org/10.1093/oso/9780190923624.003.0001},
    eprint = {https://academic.oup.com/book/0/chapter/194768451/chapter-pdf/44106104/oso-9780190923624-chapter-1.pdf},
}

@article{diresta2022house,
  title={In-House Vs. Outsourced Trolls: How Digital Mercenaries Shape State Influence Strategies},
  author={DiResta, Ren{\'e}e and Grossman, Shelby and Siegel, Alexandra},
  journal={Political Communication},
  pages={1--32},
  year={2022},
  publisher={Taylor \& Francis}
}

@inproceedings{kirk2022handling,
  title={Handling and Presenting Harmful Text in NLP Research},
  author={Hannah Rose Kirk and Abeba Birhane and Bertie Vidgen and Leon Derczynski},
  year={2022}
}

@article{pennycook2021guide,
    author = {Pennycook, Gordon and Binnendyk, Jabin and Newton, Christie and Rand, David G.},
    title = "{A Practical Guide to Doing Behavioral Research on Fake News and Misinformation}",
    journal = {Collabra: Psychology},
    volume = {7},
    number = {1},
    year = {2021},
    month = {07},
    issn = {2474-7394},
    url = {https://online.ucpress.edu/collabra/article/7/1/25293/117809/A-Practical-Guide-to-Doing-Behavioral-Research-on},
    note = {25293},
}

@article{metzler2021rethinking,
author = {Metzler, Donald and Tay, Yi and Bahri, Dara and Najork, Marc},
title = {Rethinking Search: Making Domain Experts out of Dilettantes},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {1},
issn = {0163-5840},
url = {https://doi.org/10.1145/3476415.3476428},
doi = {10.1145/3476415.3476428},
abstract = {When experiencing an information need, users want to engage with a domain expert, but often turn to an information retrieval system, such as a search engine, instead. Classical information retrieval systems do not answer information needs directly, but instead provide references to (hopefully authoritative) answers. Successful question answering systems offer a limited corpus created on-demand by human experts, which is neither timely nor scalable. Pre-trained language models, by contrast, are capable of directly generating prose that may be responsive to an information need, but at present they are dilettantes rather than domain experts - they do not have a true understanding of the world, they are prone to hallucinating, and crucially they are incapable of justifying their utterances by referring to supporting documents in the corpus they were trained over. This paper examines how ideas from classical information retrieval and pre-trained language models can be synthesized and evolved into systems that truly deliver on the promise of domain expert advice.},
journal = {SIGIR Forum},
month = {jul},
articleno = {13},
numpages = {27}
}

@unpublished{khattab2021HAI,
    author = {Khattab, Omar  and  Potts, Christopher  and  Zaharia, Matei},
    note = {Stanford HAI Blog},
    url={https://hai.stanford.edu/news/moderate-proposal-radically-better-ai-powered-web-search},
    year = {2021},
    title = {A Moderate Proposal for Radically Better {AI}-Powered {W}eb Search}}

@inproceedings{dai2018convolutional,
  title={Convolutional neural networks for soft-matching n-grams in ad-hoc search},
  author={Dai, Zhuyun and Xiong, Chenyan and Callan, Jamie and Liu, Zhiyuan},
  booktitle={Proceedings of the eleventh ACM international conference on web search and data mining},
  pages={126--134},
  year={2018}
}

@article{mitra2019updated,
  title={An updated duet model for passage re-ranking},
  author={Mitra, Bhaskar and Craswell, Nick},
  journal={arXiv preprint arXiv:1903.07666},
  year={2019}
}

@article{xiong2020approximate,
  title={Approximate nearest neighbor negative contrastive learning for dense text retrieval},
  author={Xiong, Lee and Xiong, Chenyan and Li, Ye and Tang, Kwok-Fung and Liu, Jialin and Bennett, Paul and Ahmed, Junaid and Overwijk, Arnold},
  journal={arXiv preprint arXiv:2007.00808},
  year={2020}
}

@article{santhanam2021colbertv2,
  title={Colbertv2: Effective and efficient retrieval via lightweight late interaction},
  author={Santhanam, Keshav and Khattab, Omar and Saad-Falcon, Jon and Potts, Christopher and Zaharia, Matei},
  journal={arXiv preprint arXiv:2112.01488},
  year={2021}
}

@inproceedings{gao2021complement,
  title={Complement lexical retrieval model with semantic residual embeddings},
  author={Gao, Luyu and Dai, Zhuyun and Chen, Tongfei and Fan, Zhen and Durme, Benjamin Van and Callan, Jamie},
  booktitle={European Conference on Information Retrieval},
  pages={146--160},
  year={2021},
  organization={Springer}
}

@article{wei2022chain,
  title={Chain of thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Chi, Ed and Le, Quoc and Zhou, Denny},
  journal={arXiv preprint arXiv:2201.11903},
  year={2022}
}

@inproceedings{yang2022capabilities,
  title={Capabilities for Better ML Engineering},
  author={Chenyang Yang and Rachel Brower-Sinning and Grace A. Lewis and Christian Kastner and Tongshuang Sherry Wu},
  year={2022}
}

@article{zhu2018texygenAB,
  title={Texygen: A Benchmarking Platform for Text Generation Models},
  author={Yaoming Zhu and Sidi Lu and Lei Zheng and Jiaxian Guo and Weinan Zhang and Jun Wang and Yong Yu},
  journal={The 41st International ACM SIGIR Conference on Research \& Development in Information Retrieval},
  year={2018}
}

@article{gruppi2022nela,
  title={NELA-GT-2021: A Large Multi-Labelled News Dataset for The Study of Misinformation in News Articles},
  author={Gruppi, Maur{\'\i}cio and Horne, Benjamin D and Adal{\i}, Sibel},
  journal={arXiv preprint arXiv:2203.05659},
  year={2022}
}

@article{nrregaard2019NELAGT2018AL,
  title={NELA-GT-2018: A Large Multi-Labelled News Dataset for The Study of Misinformation in News Articles},
  author={Jeppe N{\o}rregaard and Benjamin D. Horne and Sibel Adali},
  journal={ArXiv},
  year={2019},
  volume={abs/2203.05659}
}

@inproceedings{mishra2021neuralnere,
  title={NeuralNERE: Neural Named Entity Relationship Extraction for End-to-End Climate Change Knowledge Graph Construction},
  author={Mishra, Prakamya and Mittal, Rohan},
  booktitle={ICML 2021 Workshop on Tackling Climate Change with Machine Learning},
  url={https://www.climatechange.ai/papers/icml2021/76},
  year={2021}
}

@inproceedings{bulian2020climate-fever,
title	= {CLIMATE-FEVER: A Dataset for Verification of Real-World Climate Claims},
author	= {Jannis Bulian and Jordan Boyd-Graber and Markus Leippold and Massimiliano Ciaramita and Thomas Diggelmann},
year	= {2020},
booktitle	= {NeurIPS 2020 Workshop on Tackling Climate Change with Machine Learning}
}

@article{hao2018context,
  title={Context-Free Transductions with Neural Stacks},
  author={Hao, Yiding and Merrill, William and Angluin, Dana and Frank, Robert and Amsel, Noah and Benz, Andrew and Mendelsohn, Simon},
  journal={EMNLP 2018},
  pages={306},
  year={2018}
}

@inproceedings{hewitt2020rnns,
  title={RNNs can generate bounded hierarchical languages with optimal memory},
  author={Hewitt, John and Hahn, Michael and Ganguli, Surya and Liang, Percy and Manning, Christopher D},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={1978--2010},
  year={2020}
}

@article{hahn2020theoretical,
  title={Theoretical Limitations of Self-Attention in Neural Sequence Models},
  author={Hahn, Michael},
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={156--171},
  year={2020}
}

@inproceedings{suzgun2019lstm,
  title={LSTM Networks Can Perform Dynamic Counting},
  author={Suzgun, Mirac and Belinkov, Yonatan and Shieber, Stuart M and Gehrmann, Sebastian},
  booktitle={Proceedings of the Workshop on Deep Learning and Formal Languages: Building Bridges},
  pages={44--54},
  year={2019}
}

@inproceedings{sennhauser2018evaluating,
  title={Evaluating the Ability of LSTMs to Learn Context-Free Grammars},
  author={Sennhauser, Luzi and Berwick, Robert C},
  booktitle={Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},
  pages={115--124},
  year={2018}
}

@inproceedings{skachkova2018closing,
  title={Closing brackets with recurrent neural networks},
  author={Skachkova, Natalia and Trost, Thomas Alexander and Klakow, Dietrich},
  booktitle={Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},
  pages={232--239},
  year={2018}
}

@inproceedings{bhattamishra2020ability,
  title={On the Ability and Limitations of Transformers to Recognize Formal Languages},
  author={Bhattamishra, Satwik and Ahuja, Kabir and Goyal, Navin},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={7096--7116},
  year={2020}
}

@inproceedings{ebrahimi2020can,
  title={How Can Self-Attention Networks Recognize Dyck-n Languages?},
  author={Ebrahimi, Javid and Gelda, Dhruv and Zhang, Wei},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2020},
  pages={4301--4306},
  year={2020}
}

@article{merrill2021formal,
  title={Formal language theory meets modern NLP},
  author={Merrill, William},
  journal={arXiv preprint arXiv:2102.10094},
  year={2021}
}

@inproceedings{parrish-etal-2022-bbq,
    title = "{BBQ}: A hand-built bias benchmark for question answering",
    author = "Parrish, Alicia  and
      Chen, Angelica  and
      Nangia, Nikita  and
      Padmakumar, Vishakh  and
      Phang, Jason  and
      Thompson, Jana  and
      Htut, Phu Mon  and
      Bowman, Samuel",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.165",
    doi = "10.18653/v1/2022.findings-acl.165",
    pages = "2086--2105",
    abstract = "It is well documented that NLP models learn social biases, but little work has been done on how these biases manifest in model outputs for applied tasks like question answering (QA). We introduce the Bias Benchmark for QA (BBQ), a dataset of question-sets constructed by the authors that highlight attested social biases against people belonging to protected classes along nine social dimensions relevant for U.S. English-speaking contexts. Our task evaluate model responses at two levels: (i) given an under-informative context, we test how strongly responses reflect social biases, and (ii) given an adequately informative context, we test whether the model{'}s biases override a correct answer choice. We find that models often rely on stereotypes when the context is under-informative, meaning the model{'}s outputs consistently reproduce harmful biases in this setting. Though models are more accurate when the context provides an informative answer, they still rely on stereotypes and average up to 3.4 percentage points higher accuracy when the correct answer aligns with a social bias than when it conflicts, with this difference widening to over 5 points on examples targeting gender for most models tested.",
}

@ARTICLE{Greenbaum1991-js,
  title     = "{ICE}: The international corpus of English",
  author    = "Greenbaum, Sidney",
  abstract  = "An outline of a new worldwide project for the study of the
               language",
  journal   = "Engl. today",
  publisher = "Cambridge University Press (CUP)",
  volume    =  7,
  number    =  4,
  pages     = "3--7",
  month     =  oct,
  year      =  1991,
  language  = "en"
}

@misc{greenbaum1996international,
  title={The international corpus of English (ICE) project},
  author={Greenbaum, Sidney and Nelson, Gerald},
  journal={World Englishes},
  volume={15},
  number={1},
  pages={3--15},
  year={1996},
  publisher={Wiley Online Library}
}

@article{emnlp1996emnlp,
    author = {{EMNLP}},
    title = "Conference on Empirical Methods in Natural Language Processing",
    year = "1996",
    url = "https://aclanthology.org/W96-0200",
}

@inproceedings{grishman1996muc,
    title = "{M}essage {U}nderstanding {C}onference- 6: A Brief History",
    author = "Grishman, Ralph  and
      Sundheim, Beth",
    booktitle = "{COLING} 1996 Volume 1: The 16th International Conference on Computational Linguistics",
    year = "1996",
    url = "https://aclanthology.org/C96-1079",
}

@inproceedings{voorhees1998trec,
    title = "The {T}ext {RE}trieval {C}onferences ({TREC}s)",
    author = "Voorhees, Ellen M.  and
      Harman, Donna",
    booktitle = "TIPSTER TEXT PROGRAM PHASE III: Proceedings of a Workshop held at Baltimore, {M}aryland, October 13-15, 1998",
    month = oct,
    year = "1998",
    address = "Baltimore, Maryland, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/X98-1031",
    doi = "10.3115/1119089.1119127",
    pages = "241--273",
}



@misc{nist2023airmf,
	author = {Elham Tabassi},
	doi = {https://doi.org/10.6028/NIST.AI.100-1},
	language = {en},
	month = {2023-01-26 05:01:00},
	publisher = {NIST Trustworthy and Responsible AI, National Institute of Standards and Technology, Gaithersburg, MD},
	title = {Artificial Intelligence Risk Management Framework (AI RMF 1.0)},
	url = {https://tsapps.nist.gov/publication/get_pdf.cfm?pub_id=936225},
	year = {2023},
	bdsk-url-1 = {https://tsapps.nist.gov/publication/get_pdf.cfm?pub_id=936225},
	bdsk-url-2 = {https://doi.org/10.6028/NIST.AI.100-1}}

@book{jones1995evaluating,
  title={Evaluating Natural Language Processing Systems: An Analysis and Review},
  author={Sp\"arck Jones, Karen and Galliers, Julia R.},
  year={1995},
  address={Berlin},
  publisher={Springer Verlag},
  series={Lecture Notes in Computer Science},
  number={1083},
}

@ARTICLE{Szmrecsanyi2019-go,
  title     = "{Variation-Based} Distance and Similarity Modeling: A case study
               in world {E}nglishes",
  author    = "Szmrecsanyi, Benedikt and Grafmiller, Jason and Rosseel, Laura",
  abstract  = "Inspired by work in comparative sociolinguistics and
               quantitative dialectometry, we sketch a corpus-based method
               (Variation-Based Distance \& Similarity Modeling-VADIS for
               short) to rigorously quantify the similarity between varieties
               and dialects as a function of the correspondence of the ways in
               which language users choose between different ways of saying the
               same thing. To showcase the potential of the method, we present
               a case study that investigates three syntactic alternations in
               some nine international varieties of English. Key findings
               include that (a) probabilistic grammars are remarkably similar
               and stable across the varieties under study; (b) in many cases
               we see a cluster of ``native'' (a.k.a. Inner Circle) varieties,
               such as British English, whereas ``non-native'' (a.k.a. Outer
               Circle) varieties, such as Indian English, are a more
               heterogeneous group; and (c) coherence across alternations is
               less than perfect.",
  journal   = "Frontiers in Artificial Intelligence",
  publisher = "Frontiers Media SA",
  volume    =  2,
  pages     = "23",
  month     =  nov,
  year      =  2019,
  keywords  = "VADIS; comparative sociolinguistics; dialectometry;
               probabilistic grammar; variationist linguistics",
  copyright = "https://creativecommons.org/licenses/by/4.0/",
  language  = "en"
}

@book{kachru2009handbook,
  title={The handbook of world Englishes},
  author={Kachru, Braj B and Kachru, Yamuna and Nelson, Cecil L},
  volume={48},
  year={2009},
  publisher={John Wiley \& Sons}
}

@inproceedings{bender2009linguistically,
    title = {Linguistically Na{\"\i}ve != Language Independent: Why {NLP} Needs Linguistic Typology},
    author = "Bender, Emily M.",
    booktitle = "Proceedings of the {EACL} 2009 Workshop on the Interaction between Linguistics and Computational Linguistics: Virtuous, Vicious or Vacuous?",
    month = mar,
    year = "2009",
    address = "Athens, Greece",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W09-0106",
    pages = "26--32",
}

@article{liberman2010obituary,
author = {Liberman, Mark},
title = {Obituary: Fred Jelinek},
year = {2010},
issue_date = {December 2010},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {36},
number = {4},
issn = {0891-2017},
url = {https://doi.org/10.1162/coli_a_00032},
doi = {10.1162/coli_a_00032},
journal = {Comput. Linguist.},
month = {dec},
pages = {595–599},
numpages = {5}
}

@article{bender2011achieving,
  title={On Achieving and Evaluating Language-Independence in NLP},
  author={Emily M. Bender},
  journal={Linguistic Issues in Language Technology},
  year={2011},
  volume={6}
}

@book{glottolog,
  address      = {Leipzig},
  author       = {Harald Hammarström and Robert Forkel and Martin Haspelmath and Sebastian Bank},
  howpublished = {Max Planck Institute for Evolutionary Anthropology},
  title        = {Glottolog 4.4},
  url          = {https://glottolog.org/ accessed 2021-08-08},
  year         = {2021},
  doi          = {10.5281/zenodo.4761960}
}

@inproceedings{nordhoff2011glottolog,
  title={Glottolog/Langdoc: Defining dialects, languages, and language families as collections of resources},
  author={Nordhoff, Sebastian and Hammarstr{\"o}m, Harald},
  booktitle={First International Workshop on Linked Science 2011-In conjunction with the International Semantic Web Conference (ISWC 2011)},
  year={2011}
}

@inproceedings{wang2011summarizing,
    title = "Summarizing Decisions in Spoken Meetings",
    author = "Wang, Lu  and
      Cardie, Claire",
    booktitle = "Proceedings of the Workshop on Automatic Summarization for Different Genres, Media, and Languages",
    month = jun,
    year = "2011",
    address = "Portland, Oregon",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W11-0503",
    pages = "16--24",
}

@inproceedings{bender2012100,
    title = "100 Things You Always Wanted to Know about Linguistics But Were Afraid to Ask*",
    author = "Bender, Emily M.",
    booktitle = "Tutorial Abstracts at the Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2012",
    address = "Montr{\'e}al, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N12-4001",
}

@inproceedings{joshi2020state,
   title={The State and Fate of Linguistic Diversity and Inclusion in the NLP World},
    author={Pratik Joshi and Sebastin Santy and Amar Budhiraja and Kalika Bali and Monojit Choudhury},
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Seattle, Washington",
    publisher = "Association for Computational Linguistics",
    url={https://arxiv.org/abs/2004.09095}
}



@book{kirkpatrick2020routledge,
  title={The Routledge handbook of world Englishes},
  author={Kirkpatrick, Andy},
  year={2020},
  publisher={Routledge}
}

@inproceedings{tiedemann-2016-finding,
    title = "Finding Alternative Translations in a Large Corpus of Movie Subtitle",
    author = {Tiedemann, J{\"o}rg},
    booktitle = "Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16)",
    month = may,
    year = "2016",
    address = "Portoro{\v{z}}, Slovenia",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://aclanthology.org/L16-1559",
    pages = "3518--3522",
    abstract = "OpenSubtitles.org provides a large collection of user contributed subtitles in various languages for movies and TV programs. Subtitle translations are valuable resources for cross-lingual studies and machine translation research. A less explored feature of the collection is the inclusion of alternative translations, which can be very useful for training paraphrase systems or collecting multi-reference test suites for machine translation. However, differences in translation may also be due to misspellings, incomplete or corrupt data files, or wrongly aligned subtitles. This paper reports our efforts in recognising and classifying alternative subtitle translations with language independent techniques. We use time-based alignment with lexical re-synchronisation techniques and BLEU score filters and sort alternative translations into categories using edit distance metrics and heuristic rules. Our approach produces large numbers of sentence-aligned translation alternatives for over 50 languages provided via the OPUS corpus collection.",
}
@article{guha2022legalbench,
    title = {LegalBench: Prototyping a Collaborative Benchmark for Legal Reasoning},
    author = {Guha, Neel and Ho, Daniel E. and Nyarko, Julian and Ré, Christopher},
    journal = {arXiv},
    year = {2022},
    volume = {abs/2209.06120},
}

@inproceedings{hershcovich2022challenges,
    title = "Challenges and Strategies in Cross-Cultural {NLP}",
    author = "Hershcovich, Daniel  and
      Frank, Stella  and
      Lent, Heather  and
      de Lhoneux, Miryam  and
      Abdou, Mostafa  and
      Brandl, Stephanie  and
      Bugliarello, Emanuele  and
      Cabello Piqueras, Laura  and
      Chalkidis, Ilias  and
      Cui, Ruixiang  and
      Fierro, Constanza  and
      Margatina, Katerina  and
      Rust, Phillip  and
      S{\o}gaard, Anders",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.482",
    doi = "10.18653/v1/2022.acl-long.482",
    pages = "6997--7013",
    abstract = "Various efforts in the Natural Language Processing (NLP) community have been made to accommodate linguistic diversity and serve speakers of many different languages. However, it is important to acknowledge that speakers and the content they produce and require, vary not just by language, but also by culture. Although language and culture are tightly linked, there are important differences. Analogous to cross-lingual and multilingual NLP, cross-cultural and multicultural NLP considers these differences in order to better serve users of NLP systems. We propose a principled framework to frame these efforts, and survey existing and potential strategies.",
}



@InProceedings{liska2022streamingqa,
  title = 	 {{S}treaming{QA}: A Benchmark for Adaptation to New Knowledge over Time in Question Answering Models},
  author =       {Liska, Adam and Kocisky, Tomas and Gribovskaya, Elena and Terzi, Tayfun and Sezener, Eren and Agrawal, Devang and De Masson D'Autume, Cyprien and Scholtes, Tim and Zaheer, Manzil and Young, Susannah and Gilsenan-Mcmahon, Ellen and Austin, Sophia and Blunsom, Phil and Lazaridou, Angeliki},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {13604--13622},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/liska22a/liska22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/liska22a.html},
  abstract = 	 {Knowledge and language understanding of models evaluated through question answering (QA) has been usually studied on static snapshots of knowledge, like Wikipedia. However, our world is dynamic, evolves over time, and our models’ knowledge becomes outdated. To study how semi-parametric QA models and their underlying parametric language models (LMs) adapt to evolving knowledge, we construct a new large-scale dataset, StreamingQA, with human written and generated questions asked on a given date, to be answered from 14 years of time-stamped news articles. We evaluate our models quarterly as they read new articles not seen in pre-training. We show that parametric models can be updated without full retraining, while avoiding catastrophic forgetting. For semi-parametric models, adding new articles into the search space allows for rapid adaptation, however, models with an outdated underlying LM under-perform those with a retrained LM. For questions about higher-frequency named entities, parametric updates are particularly beneficial. In our dynamic world, the StreamingQA dataset enables a more realistic evaluation of QA models, and our experiments highlight several promising directions for future research.}
}




@inproceedings{lin-2004-rouge,
    title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
    author = "Lin, Chin-Yew",
    booktitle = "Text Summarization Branches Out",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W04-1013",
    pages = "74--81",
}

@inproceedings{fabbri-etal-2022-qafacteval,
    title = "{QAF}act{E}val: Improved {QA}-Based Factual Consistency Evaluation for Summarization",
    author = "Fabbri, Alexander  and
      Wu, Chien-Sheng  and
      Liu, Wenhao  and
      Xiong, Caiming",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.187",
    doi = "10.18653/v1/2022.naacl-main.187",
    pages = "2587--2601",
    abstract = "Factual consistency is an essential quality of text summarization models in practical settings. Existing work in evaluating this dimension can be broadly categorized into two lines of research, entailment-based and question answering (QA)-based metrics, and different experimental setups often lead to contrasting conclusions as to which paradigm performs the best. In this work, we conduct an extensive comparison of entailment and QA-based metrics, demonstrating that carefully choosing the components of a QA-based metric, especially question generation and answerability classification, is critical to performance. Building on those insights, we propose an optimized metric, which we call QAFactEval, that leads to a 14{\%} average improvement over previous QA-based metrics on the SummaC factual consistency benchmark, and also outperforms the best-performing entailment-based metric. Moreover, we find that QA-based and entailment-based metrics can offer complementary signals and be combined into a single metric for a further performance boost.",
}

@article{lin2021pretrained,
  title={Pretrained transformers for text ranking: Bert and beyond},
  author={Lin, Jimmy and Nogueira, Rodrigo and Yates, Andrew},
  journal={Synthesis Lectures on Human Language Technologies},
  volume={14},
  number={4},
  pages={1--325},
  year={2021},
  publisher={Morgan \& Claypool Publishers}
}

@phdthesis{tsipras2021learning,
  title={Learning Through the Lens of Robustness},
  author={Tsipras, Dimitris},
  year={2021},
  school={Massachusetts Institute of Technology},
  url={https://dspace.mit.edu/handle/1721.1/140148}
}

@inproceedings{lazaridou2021mind,
 author = {Lazaridou, Angeliki and Kuncoro, Adhi and Gribovskaya, Elena and Agrawal, Devang and Liska, Adam and Terzi, Tayfun and Gimenez, Mai and de Masson d\textquotesingle Autume, Cyprien and Kocisky, Tomas and Ruder, Sebastian and Yogatama, Dani and Cao, Kris and Young, Susannah and Blunsom, Phil},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {29348--29363},
 publisher = {Curran Associates, Inc.},
 title = {Mind the Gap: Assessing Temporal Generalization in Neural Language Models},
 url = {https://proceedings.neurips.cc/paper/2021/file/f5bf0ba0a17ef18f9607774722f5698c-Paper.pdf},
 volume = {34},
 year = {2021}
}



@inproceedings{
ma2021dynaboard,
title={Dynaboard: An Evaluation-As-A-Service Platform for Holistic Next-Generation Benchmarking},
author={Zhiyi Ma and Kawin Ethayarajh and Tristan Thrush and Somya Jain and Ledell Yu Wu and Robin Jia and Christopher Potts and Adina Williams and Douwe Kiela},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=TCarYAus7JL}
}

@inproceedings{jacobs2021measurement,
author = {Abigail Z. Jacobs and Hanna Wallach},
title = {Measurement and Fairness},
year = {2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://arxiv.org/abs/1912.05511},
booktitle = {Proceedings of the 2021 Conference on Fairness, Accountability, and Transparency},
location = {Online},
series = {FAccT '21}
}

@article{sachan2022improving,
  title={Improving Passage Retrieval with Zero-Shot Question Generation},
  author={Sachan, Devendra Singh and Lewis, Mike and Joshi, Mandar and Aghajanyan, Armen and Yih, Wen-tau and Pineau, Joelle and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2204.07496},
  year={2022}
}

@article{craswell2020overview,
  title={Overview of the TREC 2019 deep learning track},
  author={Craswell, Nick and Mitra, Bhaskar and Yilmaz, Emine and Campos, Daniel and Voorhees, Ellen M},
  journal={arXiv preprint arXiv:2003.07820},
  year={2020}
}

@article{bartolo2020beat,
    title = "Beat the {AI}: Investigating Adversarial Human Annotation for Reading Comprehension",
    author = "Bartolo, Max  and
      Roberts, Alastair  and
      Welbl, Johannes  and
      Riedel, Sebastian  and
      Stenetorp, Pontus",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "8",
    year = "2020",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2020.tacl-1.43",
    doi = "10.1162/tacl_a_00338",
    pages = "662--678",
    abstract = "Innovations in annotation methodology have been a catalyst for Reading Comprehension (RC) datasets and models. One recent trend to challenge current RC models is to involve a model in the annotation process: Humans create questions adversarially, such that the model fails to answer them correctly. In this work we investigate this annotation methodology and apply it in three different settings, collecting a total of 36,000 samples with progressively stronger models in the annotation loop. This allows us to explore questions such as the reproducibility of the adversarial effect, transfer from data collected with varying model-in-the-loop strengths, and generalization to data collected without a model. We find that training on adversarially collected samples leads to strong generalization to non-adversarially collected datasets, yet with progressive performance deterioration with increasingly stronger models-in-the-loop. Furthermore, we find that stronger models can still learn from datasets collected with substantially weaker models-in-the-loop. When trained on data collected with a BiDAF model in the loop, RoBERTa achieves 39.9F1 on questions that it cannot answer when trained on SQuAD{---}only marginally lower than when trained on data collected using RoBERTa itself (41.0F1).",
}

@inproceedings{wang2021textflint,
  title={Textflint: Unified multilingual robustness evaluation toolkit for natural language processing},
  author={Wang, Xiao and Liu, Qin and Gui, Tao and Zhang, Qi and Zou, Yicheng and Zhou, Xin and Ye, Jiacheng and Zhang, Yongxin and Zheng, Rui and Pang, Zexiong and others},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations},
  year={2021}
}

@article{wang2021adversarial,
  title={Adversarial glue: A multi-task benchmark for robustness evaluation of language models},
  author={Wang, Boxin and Xu, Chejian and Wang, Shuohang and Gan, Zhe and Cheng, Yu and Gao, Jianfeng and Awadallah, Ahmed Hassan and Li, Bo},
  journal={arXiv preprint arXiv:2111.02840},
  year={2021}
}

@article{morris2020textattack,
  title={Textattack: A framework for adversarial attacks, data augmentation, and adversarial training in nlp},
  author={Morris, John X and Lifland, Eli and Yoo, Jin Yong and Grigsby, Jake and Jin, Di and Qi, Yanjun},
  journal={arXiv preprint arXiv:2005.05909},
  year={2020}
}

@article{wang2021measure,
  title={Measure and Improve Robustness in NLP Models: A Survey},
  author={Wang, Xuezhi and Wang, Haohan and Yang, Diyi},
  journal={arXiv preprint arXiv:2112.08313},
  year={2021}
}


@inproceedings{narayanan2021efficient,
  title={{Efficient Large-Scale Language Model Training on GPU Clusters using Megatron-LM}},
  author={Narayanan, Deepak and Shoeybi, Mohammad and Casper, Jared and LeGresley, Patrick and Patwary, Mostofa and Korthikanti, Vijay and Vainbrand, Dmitri and Kashinkunti, Prethvi and Bernauer, Julie and Catanzaro, Bryan and others},
  booktitle={Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  year={2021}
}


@InCollection{sep-abduction,
	author       =	{Douven, Igor},
	title        =	{{Abduction}},
	booktitle    =	{The {Stanford} Encyclopedia of Philosophy},
	editor       =	{Edward N. Zalta},
	howpublished =	{\url{https://plato.stanford.edu/archives/sum2021/entries/abduction/}},
	year         =	{2021},
	edition      =	{{S}ummer 2021},
	publisher    =	{Metaphysics Research Lab, Stanford University}
}


@book{peirce1974collected,
  title={Collected papers of charles sanders peirce},
  author={Peirce, Charles Sanders},
  volume={5},
  year={1974},
  publisher={Harvard University Press}
}

@article{goldstein2022generative,
    title={Generative Language Models and Automated Influence Operations: Emerging Threats and Potential Mitigations},
    author={Goldstein, Josh A. and Musser, Micah and Sastry, Girish and DiResta, Ren{\'e}e and Gentzel, Matthew and Sedova, Katerina},
    year={Forthcoming}
}

@article{perez2022red,
  title={Red Teaming Language Models with Language Models},
  author={Ethan Perez and Saffron Huang and Francis Song and Trevor Cai and Roman Ring and John Aslanides and Amelia Glaese and Nathan McAleese and Geoffrey Irving},
  journal={ArXiv},
  year={2022},
  volume={abs/2202.03286}
}

@article{ganguli2022red,
  title={Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned},
  author={Deep Ganguli and Liane Lovitt and John Kernion and Amanda Askell and Yushi Bai and Saurav Kadavath and Benjamin Mann and Ethan Perez and Nicholas Schiefer and Kamal Ndousse and Andy Jones and Sam Bowman and Anna Chen and Tom Conerly and Nova DasSarma and Dawn Drain and Nelson Elhage and Sheer El-Showk and Stanislav Fort and Zachary Dodds and T. J. Henighan and Danny Hernandez and Tristan Hume and Josh Jacobson and Scott Johnston and Shauna Kravec and Catherine Olsson and Sam Ringer and Eli Tran-Johnson and Dario Amodei and Tom B. Brown and Nicholas Joseph and Sam McCandlish and Christopher Olah and Jared Kaplan and Jack Clark},
  journal={ArXiv},
  year={2022},
  volume={abs/2209.07858}
}

@misc{yamshchikov2022plutarch,
  title={{BERT} in {P}lutarch's Shadows},
  author={Ivan P. Yamshchikov and Alexey N. Tikhonov and Yorgos Pantis and Charlotte Schubert and J{\"u}rgen Jost},
  year={2022},
  url={https://arxiv.org/abs/2211.05673}
}

@inproceedings{macavaney:sigir2021-irds,
  author = {MacAvaney, Sean and Yates, Andrew and Feldman, Sergey and Downey, Doug and Cohan, Arman and Goharian, Nazli},
  title = {Simplified Data Wrangling with IR datasets},
  year = {2021},
  booktitle = {SIGIR}
}

@inproceedings{goel2021robustness,
    title = "Robustness Gym: Unifying the {NLP} Evaluation Landscape",
    author = "Goel, Karan  and
      Rajani, Nazneen Fatema  and
      Vig, Jesse  and
      Taschdjian, Zachary  and
      Bansal, Mohit  and
      R{\'e}, Christopher",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Demonstrations",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-demos.6",
    doi = "10.18653/v1/2021.naacl-demos.6",
    pages = "42--55",
    abstract = "Despite impressive performance on standard benchmarks, natural language processing (NLP) models are often brittle when deployed in real-world systems. In this work, we identify challenges with evaluating NLP systems and propose a solution in the form of Robustness Gym (RG), a simple and extensible evaluation toolkit that unifies 4 standard evaluation paradigms: subpopulations, transformations, evaluation sets, and adversarial attacks. By providing a common platform for evaluation, RG enables practitioners to compare results from disparate evaluation paradigms with a single click, and to easily develop and share novel evaluation methods using a built-in set of abstractions. RG is under active development and we welcome feedback {\&} contributions from the community.",
}


@inproceedings{welbl2021challenges,
    title = "Challenges in Detoxifying Language Models",
    author = "Welbl, Johannes  and
      Glaese, Amelia  and
      Uesato, Jonathan  and
      Dathathri, Sumanth  and
      Mellor, John  and
      Hendricks, Lisa Anne  and
      Anderson, Kirsty  and
      Kohli, Pushmeet  and
      Coppin, Ben  and
      Huang, Po-Sen",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.210",
    doi = "10.18653/v1/2021.findings-emnlp.210",
    pages = "2447--2469",
    abstract = "Large language models (LM) generate remarkably fluent text and can be efficiently adapted across NLP tasks. Measuring and guaranteeing the quality of generated text in terms of safety is imperative for deploying LMs in the real world; to this end, prior work often relies on automatic evaluation of LM toxicity. We critically discuss this approach, evaluate several toxicity mitigation strategies with respect to both automatic and human evaluation, and analyze consequences of toxicity mitigation in terms of model bias and LM quality. We demonstrate that while basic intervention strategies can effectively optimize previously established automatic metrics on the REALTOXICITYPROMPTS dataset, this comes at the cost of reduced LM coverage for both texts about, and dialects of, marginalized groups. Additionally, we find that human raters often disagree with high automatic toxicity scores after strong toxicity reduction interventions{---}highlighting further the nuances involved in careful evaluation of LM toxicity.",
}

@inproceedings{yang2017anserini,
  title={Anserini: Enabling the use of lucene for information retrieval research},
  author={Yang, Peilin and Fang, Hui and Lin, Jimmy},
  booktitle={Proceedings of the 40th international ACM SIGIR conference on research and development in information retrieval},
  pages={1253--1256},
  year={2017}
}

@inproceedings{gardent2017webnlg,
    title = "The {W}eb{NLG} Challenge: Generating Text from {RDF} Data",
    author = "Gardent, Claire  and
      Shimorina, Anastasia  and
      Narayan, Shashi  and
      Perez-Beltrachini, Laura",
    booktitle = "Proceedings of the 10th International Conference on Natural Language Generation",
    month = sep,
    year = "2017",
    address = "Santiago de Compostela, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W17-3518",
    doi = "10.18653/v1/W17-3518",
    pages = "124--133",
    abstract = "The WebNLG challenge consists in mapping sets of RDF triples to text. It provides a common benchmark on which to train, evaluate and compare {``}microplanners{''}, i.e. generation systems that verbalise a given content by making a range of complex interacting choices including referring expression generation, aggregation, lexicalisation, surface realisation and sentence segmentation. In this paper, we introduce the microplanning task, describe data preparation, introduce our evaluation methodology, analyse participant results and provide a brief description of the participating systems.",
}

@inproceedings{novikova2017e2e,
    title = "The {E}2{E} Dataset: New Challenges For End-to-End Generation",
    author = "Novikova, Jekaterina  and
      Du{\v{s}}ek, Ond{\v{r}}ej  and
      Rieser, Verena",
    booktitle = "Proceedings of the 18th Annual {SIG}dial Meeting on Discourse and Dialogue",
    month = aug,
    year = "2017",
    address = {Saarbr{\"u}cken, Germany},
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W17-5525",
    doi = "10.18653/v1/W17-5525",
    pages = "201--206",
    abstract = "This paper describes the E2E data, a new dataset for training end-to-end, data-driven natural language generation systems in the restaurant domain, which is ten times bigger than existing, frequently used datasets in this area. The E2E dataset poses new challenges: (1) its human reference texts show more lexical richness and syntactic variation, including discourse phenomena; (2) generating from this set requires content selection. As such, learning from this dataset promises more natural, varied and less template-like system utterances. We also establish a baseline on this dataset, which illustrates some of the difficulties associated with this data.",
}

@inproceedings{bert-score,
  title={BERTScore: Evaluating Text Generation with BERT},
  author={Tianyi Zhang and Varsha Kishore and Felix Wu and Kilian Q. Weinberger and Yoav Artzi},
  booktitle={International Conference on Learning Representations},
  year={2020},
  url={https://openreview.net/forum?id=SkeHuCVFDr}
}

@article{bamman2020latin,
  title={Latin BERT: A Contextual Language Model for Classical Philology},
  author={David Bamman and Patrick J. Burns},
  journal={ArXiv},
  year={2020},
  volume={abs/2009.10053}
}

@article{fabbri-etal-2021-summeval,
    title = "{S}umm{E}val: Re-evaluating Summarization Evaluation",
    author = "Fabbri, Alexander R.  and
      Kry{\'s}ci{\'n}ski, Wojciech  and
      McCann, Bryan  and
      Xiong, Caiming  and
      Socher, Richard  and
      Radev, Dragomir",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "9",
    year = "2021",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2021.tacl-1.24",
    doi = "10.1162/tacl_a_00373",
    pages = "391--409",
    abstract = "Abstract The scarcity of comprehensive up-to-date studies on evaluation metrics for text summarization and the lack of consensus regarding evaluation protocols continue to inhibit progress. We address the existing shortcomings of summarization evaluation methods along five dimensions: 1) we re-evaluate 14 automatic evaluation metrics in a comprehensive and consistent fashion using neural summarization model outputs along with expert and crowd-sourced human annotations; 2) we consistently benchmark 23 recent summarization models using the aforementioned automatic evaluation metrics; 3) we assemble the largest collection of summaries generated by models trained on the CNN/DailyMail news dataset and share it in a unified format; 4) we implement and share a toolkit that provides an extensible and unified API for evaluating summarization models across a broad range of automatic metrics; and 5) we assemble and share the largest and most diverse, in terms of model types, collection of human judgments of model-generated summaries on the CNN/Daily Mail dataset annotated by both expert judges and crowd-source workers. We hope that this work will help promote a more complete evaluation protocol for text summarization as well as advance research in developing evaluation metrics that better correlate with human judgments.",
}

@inproceedings{
tamkin2021dabs,
title={{DABS}: a Domain-Agnostic Benchmark for Self-Supervised Learning},
author={Alex Tamkin and Vincent Liu and Rongfei Lu and Daniel Fein and Colin Schultz and Noah Goodman},
booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1)},
year={2021},
url={https://openreview.net/forum?id=Uk2mymgn_LZ}
}

@inproceedings{
tamkin2022dabs2,
title={{DABS} 2.0: Improved Datasets and Algorithms for Universal Self-Supervision},
author={Alex Tamkin and Gaurab Banerjee and Mohamed Owda and Vincent Liu and Shashank Rammoorthy and Noah Goodman},
booktitle={Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
year={2022},
url={https://openreview.net/forum?id=ChWf1E43l4}
}



@inproceedings{liu-etal-2022-brio,
    title = "{BRIO}: Bringing Order to Abstractive Summarization",
    author = "Liu, Yixin  and
      Liu, Pengfei  and
      Radev, Dragomir  and
      Neubig, Graham",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.207",
    doi = "10.18653/v1/2022.acl-long.207",
    pages = "2890--2903",
    abstract = "Abstractive summarization models are commonly trained using maximum likelihood estimation, which assumes a deterministic (one-point) target distribution in which an ideal model will assign all the probability mass to the reference summary. This assumption may lead to performance degradation during inference, where the model needs to compare several system-generated (candidate) summaries that have deviated from the reference summary. To address this problem, we propose a novel training paradigm which assumes a non-deterministic distribution so that different candidate summaries are assigned probability mass according to their quality. Our method achieves a new state-of-the-art result on the CNN/DailyMail (47.78 ROUGE-1) and XSum (49.07 ROUGE-1) datasets. Further analysis also shows that our model can estimate probabilities of candidate summaries that are more correlated with their level of quality.",
}

@inproceedings{cao2018faithful,
  title={Faithful to the original: Fact aware neural abstractive summarization},
  author={Cao, Ziqiang and Wei, Furu and Li, Wenjie and Li, Sujian},
  booktitle={thirty-second AAAI conference on artificial intelligence},
  year={2018}
}

@inproceedings{durmus-etal-2020-feqa,
    title = "{FEQA}: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization",
    author = "Durmus, Esin  and
      He, He  and
      Diab, Mona",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.454",
    doi = "10.18653/v1/2020.acl-main.454",
    pages = "5055--5070",
    abstract = "Neural abstractive summarization models are prone to generate content inconsistent with the source document, i.e. unfaithful. Existing automatic metrics do not capture such mistakes effectively. We tackle the problem of evaluating faithfulness of a generated summary given its source document. We first collected human annotations of faithfulness for outputs from numerous models on two datasets. We find that current models exhibit a trade-off between abstractiveness and faithfulness: outputs with less word overlap with the source document are more likely to be unfaithful. Next, we propose an automatic question answering (QA) based metric for faithfulness, FEQA, which leverages recent advances in reading comprehension. Given question-answer pairs generated from the summary, a QA model extracts answers from the document; non-matched answers indicate unfaithful information in the summary. Among metrics based on word overlap, embedding similarity, and learned language understanding models, our QA-based metric has significantly higher correlation with human faithfulness scores, especially on highly abstractive summaries.",
}

@inproceedings{maynez-etal-2020-faithfulness,
    title = "On Faithfulness and Factuality in Abstractive Summarization",
    author = "Maynez, Joshua  and
      Narayan, Shashi  and
      Bohnet, Bernd  and
      McDonald, Ryan",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.173",
    doi = "10.18653/v1/2020.acl-main.173",
    pages = "1906--1919",
    abstract = "It is well known that the standard likelihood training and approximate decoding objectives in neural text generation models lead to less human-like responses for open-ended tasks such as language modeling and story generation. In this paper we have analyzed limitations of these models for abstractive document summarization and found that these models are highly prone to hallucinate content that is unfaithful to the input document. We conducted a large scale human evaluation of several neural abstractive summarization systems to better understand the types of hallucinations they produce. Our human annotators found substantial amounts of hallucinated content in all model generated summaries. However, our analysis does show that pretrained models are better summarizers not only in terms of raw metrics, i.e., ROUGE, but also in generating faithful and factual summaries as evaluated by humans. Furthermore, we show that textual entailment measures better correlate with faithfulness than standard metrics, potentially leading the way to automatic evaluation metrics as well as training and decoding criteria.",
}

@article{bates2023socially,
    author = {Bates, J. and Kennedy, H. and Medina Perea, I. et al.},
    title = {Socially meaningful transparency in data-based systems: reflections and proposals from practice},
    journal = {Journal of Documentation},
    year = {2023},
    issn = {0022-0418},
    doi = {10.1108/JD-01-2023-0006},
}

@article{ananny2016limits,
author = {Mike Ananny and Kate Crawford},
title ={Seeing without knowing: Limitations of the transparency ideal and its application to algorithmic accountability},

journal = {New Media \& Society},
volume = {20},
number = {3},
pages = {973-989},
year = {2018},
doi = {10.1177/1461444816676645},

URL = { 
    
        https://doi.org/10.1177/1461444816676645
    
    

},
eprint = { 
    
        https://doi.org/10.1177/1461444816676645
    
    

}
,
    abstract = { Models for understanding and holding systems accountable have long rested upon ideals and logics of transparency. Being able to see a system is sometimes equated with being able to know how it works and govern it—a pattern that recurs in recent work about transparency and computational systems. But can “black boxes’ ever be opened, and if so, would that ever be sufficient? In this article, we critically interrogate the ideal of transparency, trace some of its roots in scientific and sociotechnical epistemological cultures, and present 10 limitations to its application. We specifically focus on the inadequacy of transparency for understanding and governing algorithmic systems and sketch an alternative typology of algorithmic accountability grounded in constructive engagements with the limitations of transparency ideals. }
}


@inproceedings{ladhak-etal-2022-faithful,
    title = "Faithful or Extractive? On Mitigating the Faithfulness-Abstractiveness Trade-off in Abstractive Summarization",
    author = "Ladhak, Faisal  and
      Durmus, Esin  and
      He, He  and
      Cardie, Claire  and
      McKeown, Kathleen",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.100",
    doi = "10.18653/v1/2022.acl-long.100",
    pages = "1410--1421",
    abstract = "Despite recent progress in abstractive summarization, systems still suffer from faithfulness errors. While prior work has proposed models that improve faithfulness, it is unclear whether the improvement comes from an increased level of extractiveness of the model outputs as one naive way to improve faithfulness is to make summarization models more extractive. In this work, we present a framework for evaluating the effective faithfulness of summarization systems, by generating a faithfulness-abstractiveness trade-off curve that serves as a control at different operating points on the abstractiveness spectrum. We then show that the Maximum Likelihood Estimation (MLE) baseline as well as recently proposed methods for improving faithfulness, fail to consistently improve over the control at the same level of abstractiveness. Finally, we learn a selector to identify the most faithful and abstractive summary for a given document, and show that this system can attain higher faithfulness scores in human evaluations while being more abstractive than the baseline system on two datasets. Moreover, we show that our system is able to achieve a better faithfulness-abstractiveness trade-off than the control at the same level of abstractiveness.",
}

@inproceedings{mrini2021rewards,
  title={Rewards with Negative Examples for Reinforced Topic-Focused Abstractive Summarization},
  author={Mrini, Khalil and Liu, Can and Dreyer, Markus},
  booktitle={Proceedings of the Third Workshop on New Frontiers in Summarization},
  pages={33--38},
  year={2021}
}

 @inproceedings{whiting2019fair,
    title={Fair Work: Crowd Work Minimum Wage with One Line of Code},
    author={Whiting, Mark E and Hugh, Grant and Bernstein, Michael S},
    booktitle={Proceedings of the AAAI Conference on Human Computation and Crowdsourcing},
    volume={7},
    number={1},
    pages={197--206},
    year={2019}
  }
  
  @article{shoeybi2019megatron,
  title={{Megatron-LM: Training Multi-Billion Parameter Language Models using Model Parallelism}},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1909.08053},
  year={2019}
}

@book{BSR2018HRIA,
  author = {BSR},
  year = {2018},
  title = {Human Rights Impact Assessment: Facebook in Myanmar},
  publisher = {BSR}
}

@online{OHCHR2018report,
  author = {Human Rights Council},
  year = {2018},
  title = {Report of the independent international fact-finding mission on Myanmar. United Nations.},
  publisher = {United Nations},
  url = {https://www.ohchr.org/sites/default/files/Documents/HRBodies/HRCouncil/FFM-Myanmar/A_HRC_39_64.pdf},
 urldate = {2022-11-14}
}

@Online{stecklow2018report,
 author = {Steve Stecklow},
 year = {2018},
 title = {Special Report: Why Facebook is losing the war on hate speech in Myanmar},
 journal = {Reuters},
 url = {https://www.reuters.com/article/us-myanmar-facebook-hate-specialreport/special-report-why-facebook-is-losing-the-war-on-hate-speech-in-myanmar-idUSKBN1L01JY},
 urldate = {2022-11-14}
}

@article{Quaranto2022-QUADWC,
	title = {Dog Whistles, Covertly Coded Speech, and the Practices That Enable Them},
	volume = {200},
	pages = {1--34},
	journal = {Synthese},
	author = {Anne Quaranto},
	number = {4},
	publisher = {Springer Verlag},
	year = {2022},
	doi = {10.1007/s11229-022-03791-y}
}

@book{persily_tucker_2020,
    place={Cambridge},
    editor={Persily, Nathaniel and Tucker, Joshua A.},
    series={SSRC Anxieties of Democracy},
    title={Social Media and Democracy: The State of the Field, Prospects for Reform},
    DOI={10.1017/9781108890960},
    publisher={Cambridge University Press},
    year={2020},
    collection={SSRC Anxieties of Democracy}
}

@book{Fogal2018-FOGNWO,
	title = {New Work on Speech Acts},
	year = {2018},
	publisher = {Oxford University Press},
	author = {Daniel Fogal and Daniel W. Harris and Matt Moss}
}

@article{kantor2000trec,
  title={The TREC-5 Confusion Track},
  author={Kantor, Paul and Voorhees, Ellen},
  journal={Information Retrieval},
  volume={2},
  number={2-3},
  pages={165--176},
  year={2000}
}

@article{jarvelin2002,
  author = {J{\"a}rvelin, Kalervo and Kek{\"a}l{\"a}inen, Jaana},
  title = {Cumulated Gain-based Evaluation of IR Techniques},
  journal = {ACM Trans. Inf. Syst.},
  volume = {20},
  number = {4},
  year = {2002},
  pages = {422--446},
  numpages = {25},
  url = {http://doi.acm.org/10.1145/582415.582418},
}

@article{bommasani2023ecosystem,
  title={Ecosystem Graphs: The Social Footprint of Foundation Models},
  author={Rishi Bommasani and Dilara Soylu and Thomas Liao and Kathleen A. Creel and Percy Liang},
  journal={ArXiv},
  year={2023},
  volume={abs/2303.15772},
  url={https://api.semanticscholar.org/CorpusID:257771875}
}

@article{bresnahan1995gpt,
title = {General purpose technologies ‘Engines of growth’?},
journal = {Journal of Econometrics},
volume = {65},
number = {1},
pages = {83-108},
year = {1995},
issn = {0304-4076},
doi = {https://doi.org/10.1016/0304-4076(94)01598-T},
url = {https://www.sciencedirect.com/science/article/pii/030440769401598T},
author = {Timothy F. Bresnahan and M. Trajtenberg},
keywords = {Technical change, Growth, Social returns, Coordination},
abstract = {Whole eras of technical progress and growth appear to be driven by a few ‘General Purpose Technologies’ (GPT's), such as the steam engine, the electric motor, and semiconductors. GPT's are characterized by pervasiveness, inherent potential for technical improvements, and ‘innovational complementarities’, giving rise to increasing returns-to-scale. However, a decentralized economy will have difficulty in fully exploiting the growth opportunities of GPT's: arms-length market transactions between the GPT and its users may result in ‘too little, too late’ innovation. Likewise, difficulties in forecasting the technological developments of the other side can lower the rate of technical advance of all sectors.}
}

@misc{openx2023openx,
title={Open {X-E}mbodiment: Robotic Learning Datasets and {RT-X} Models},
author = {{Open X-Embodiment Collaboration} and Abhishek Padalkar and Acorn Pooley and Ajinkya Jain and Alex Bewley and Alex Herzog and Alex Irpan and Alexander Khazatsky and Anant Rai and Anikait Singh and Anthony Brohan and Antonin Raffin and Ayzaan Wahid and Ben Burgess-Limerick and Beomjoon Kim and Bernhard Schölkopf and Brian Ichter and Cewu Lu and Charles Xu and Chelsea Finn and Chenfeng Xu and Cheng Chi and Chenguang Huang and Christine Chan and Chuer Pan and Chuyuan Fu and Coline Devin and Danny Driess and Deepak Pathak and Dhruv Shah and Dieter Büchler and Dmitry Kalashnikov and Dorsa Sadigh and Edward Johns and Federico Ceola and Fei Xia and Freek Stulp and Gaoyue Zhou and Gaurav S. Sukhatme and Gautam Salhotra and Ge Yan and Giulio Schiavi and Hao Su and Hao-Shu Fang and Haochen Shi and Heni Ben Amor and Henrik I Christensen and Hiroki Furuta and Homer Walke and Hongjie Fang and Igor Mordatch and Ilija Radosavovic and Isabel Leal and Jacky Liang and Jaehyung Kim and Jan Schneider and Jasmine Hsu and Jeannette Bohg and Jeffrey Bingham and Jiajun Wu and Jialin Wu and Jianlan Luo and Jiayuan Gu and Jie Tan and Jihoon Oh and Jitendra Malik and Jonathan Tompson and Jonathan Yang and Joseph J. Lim and João Silvério and Junhyek Han and Kanishka Rao and Karl Pertsch and Karol Hausman and Keegan Go and Keerthana Gopalakrishnan and Ken Goldberg and Kendra Byrne and Kenneth Oslund and Kento Kawaharazuka and Kevin Zhang and Keyvan Majd and Krishan Rana and Krishnan Srinivasan and Lawrence Yunliang Chen and Lerrel Pinto and Liam Tan and Lionel Ott and Lisa Lee and Masayoshi Tomizuka and Maximilian Du and Michael Ahn and Mingtong Zhang and Mingyu Ding and Mohan Kumar Srirama and Mohit Sharma and Moo Jin Kim and Naoaki Kanazawa and Nicklas Hansen and Nicolas Heess and Nikhil J Joshi and Niko Suenderhauf and Norman Di Palo and Nur Muhammad Mahi Shafiullah and Oier Mees and Oliver Kroemer and Pannag R Sanketi and Paul Wohlhart and Peng Xu and Pierre Sermanet and Priya Sundaresan and Quan Vuong and Rafael Rafailov and Ran Tian and Ria Doshi and Roberto Martín-Martín and Russell Mendonca and Rutav Shah and Ryan Hoque and Ryan Julian and Samuel Bustamante and Sean Kirmani and Sergey Levine and Sherry Moore and Shikhar Bahl and Shivin Dass and Shuran Song and Sichun Xu and Siddhant Haldar and Simeon Adebola and Simon Guist and Soroush Nasiriany and Stefan Schaal and Stefan Welker and Stephen Tian and Sudeep Dasari and Suneel Belkhale and Takayuki Osa and Tatsuya Harada and Tatsuya Matsushima and Ted Xiao and Tianhe Yu and Tianli Ding and Todor Davchev and Tony Z. Zhao and Travis Armstrong and Trevor Darrell and Vidhi Jain and Vincent Vanhoucke and Wei Zhan and Wenxuan Zhou and Wolfram Burgard and Xi Chen and Xiaolong Wang and Xinghao Zhu and Xuanlin Li and Yao Lu and Yevgen Chebotar and Yifan Zhou and Yifeng Zhu and Ying Xu and Yixuan Wang and Yonatan Bisk and Yoonyoung Cho and Youngwoon Lee and Yuchen Cui and Yueh-hua Wu and Yujin Tang and Yuke Zhu and Yunzhu Li and Yusuke Iwasawa and Yutaka Matsuo and Zhuo Xu and Zichen Jeff Cui},
howpublished  = {\url{https://robotics-transformer-x.github.io}},
year = {2023},
}


@article{chambon2022roentgen,
  title={RoentGen: Vision-Language Foundation Model for Chest X-ray Generation},
  author={Pierre Chambon and Christian Bl{\"u}thgen and Jean-Benoit Delbrouck and Rogier van der Sluijs and Malgorzata Polacin and Juan Manuel Zambrano Chaves and T. Abraham and Shivanshu Purohit and Curt P. Langlotz and Akshay Chaudhari},
  journal={ArXiv},
  year={2022},
  volume={abs/2211.12737},
  url={https://api.semanticscholar.org/CorpusID:253801600}
}

@article{nguyen2023astrollama,
  title={AstroLLaMA: Towards Specialized Foundation Models in Astronomy},
  author={Tuan Dung Nguyen and Yuan-Sen Ting and Ioana Ciucă and Charlie O'Neill and Ze-Chang Sun and Maja Jablo'nska and Sandor Kruk and Ernest Perkowski and Jack W. Miller and Jason Li and Josh Peek and Kartheik Iyer and Tomasz R'o.za'nski and Pranav Khetarpal and Sharaf Zaman and David Brodrick and Sergio J. Rodr'iguez M'endez and Thang Bui and Alyssa Goodman and Alberto Accomazzi and Jill P. Naiman and Jesse Cranney and Kevin Schawinski and UniverseTBD},
  journal={ArXiv},
  year={2023},
  volume={abs/2309.06126},
  url={https://api.semanticscholar.org/CorpusID:261696577}
}

@article{lacoste2023geobench,
  title={GEO-Bench: Toward Foundation Models for Earth Monitoring},
  author={Alexandre Lacoste and Nils Lehmann and Pau Rodr{\'i}guez L{\'o}pez and Evan D. Sherwin and Hannah Rae Kerner and Bjorn Lutjens and Jeremy A. Irvin and David Dao and Hamed Alemohammad and Alexandre Drouin and Mehmet Gunturkun and Gabriel Huang and David V{\'a}zquez and Dava Newman and Yoshua Bengio and Stefano Ermon and Xiao Xiang Zhu},
  journal={ArXiv},
  year={2023},
  volume={abs/2306.03831},
  url={https://api.semanticscholar.org/CorpusID:259088736}
}







@article{parasuraman2010complacency,
author = {Raja Parasuraman and Dietrich H. Manzey},
title ={Complacency and Bias in Human Use of Automation: An Attentional Integration},

journal = {Human Factors},
volume = {52},
number = {3},
pages = {381-410},
year = {2010},
doi = {10.1177/0018720810376055},
    note ={PMID: 21077562},

URL = { 
    
        https://doi.org/10.1177/0018720810376055
    
    

},
eprint = { 
    
        https://doi.org/10.1177/0018720810376055
    
    

}
,
    abstract = { Objective: Our aim was to review empirical studies of complacency and bias in human interaction with automated and decision support systems and provide an integrated theoretical model for their explanation.Background: Automation-related complacency and automation bias have typically been considered separately and independently.Methods: Studies on complacency and automation bias were analyzed with respect to the cognitive processes involved.Results: Automation complacency occurs under conditions of multiple-task load, when manual tasks compete with the automated task for the operator’s attention. Automation complacency is found in both naive and expert participants and cannot be overcome with simple practice. Automation bias results in making both omission and commission errors when decision aids are imperfect.Automation bias occurs in both naive and expert participants, cannot be prevented by training or instructions, and can affect decision making in individuals as well as in teams.While automation bias has been conceived of as a special case of decision bias, our analysis suggests that it also depends on attentional processes similar to those involved in automation-related complacency.Conclusion: Complacency and automation bias represent different manifestations of overlapping automation-induced phenomena, with attention playing a central role. An integrated model of complacency and automation bias shows that they result from the dynamic interaction of personal, situational, and automation-related characteristics.Application: The integrated model and attentional synthesis provides a heuristic framework for further research on complacency and automation bias and design options for mitigating such effects in automated and decision support systems. }
}




@article{Mittelstadt2019,
  author       = {Brent Mittelstadt},
  title        = {Principles alone cannot guarantee ethical AI},
  journal      = {Nature Machine Intelligence},
  volume       = {1},
  number       = {11},
  pages        = {501--507},
  year         = {2019},
  month        = {November},
  doi          = {10.1038/s42256-019-0114-4},
  url          = {https://doi.org/10.1038/s42256-019-0114-4},
  abstract     = {Artificial intelligence (AI) ethics is now a global topic of discussion in academic and policy circles. At least 84 public–private initiatives have produced statements describing high-level principles, values and other tenets to guide the ethical development, deployment and governance of AI. According to recent meta-analyses, AI ethics has seemingly converged on a set of principles that closely resemble the four classic principles of medical ethics. Despite the initial credibility granted to a principled approach to AI ethics by the connection to principles in medical ethics, there are reasons to be concerned about its future impact on AI development and governance. Significant differences exist between medicine and AI development that suggest a principled approach for the latter may not enjoy success comparable to the former. Compared to medicine, AI development lacks (1) common aims and fiduciary duties, (2) professional history and norms, (3) proven methods to translate principles into practice, and (4) robust legal and professional accountability mechanisms. These differences suggest we should not yet celebrate consensus around high-level principles that hide deep political and normative disagreement.},
  issn         = {2522-5839},
}



@article{lin2023evolutionary,
author = {Zeming Lin  and Halil Akin  and Roshan Rao  and Brian Hie  and Zhongkai Zhu  and Wenting Lu  and Nikita Smetanin  and Robert Verkuil  and Ori Kabeli  and Yaniv Shmueli  and Allan dos Santos Costa  and Maryam Fazel-Zarandi  and Tom Sercu  and Salvatore Candido  and Alexander Rives },
title = {Evolutionary-scale prediction of atomic-level protein structure with a language model},
journal = {Science},
volume = {379},
number = {6637},
pages = {1123-1130},
year = {2023},
doi = {10.1126/science.ade2574},
URL = {https://www.science.org/doi/abs/10.1126/science.ade2574},
eprint = {https://www.science.org/doi/pdf/10.1126/science.ade2574},
abstract = {Recent advances in machine learning have leveraged evolutionary information in multiple sequence alignments to predict protein structure. We demonstrate direct inference of full atomic-level protein structure from primary sequence using a large language model. As language models of protein sequences are scaled up to 15 billion parameters, an atomic-resolution picture of protein structure emerges in the learned representations. This results in an order-of-magnitude acceleration of high-resolution structure prediction, which enables large-scale structural characterization of metagenomic proteins. We apply this capability to construct the ESM Metagenomic Atlas by predicting structures for \&gt;617 million metagenomic protein sequences, including \&gt;225 million that are predicted with high confidence, which gives a view into the vast breadth and diversity of natural proteins. Machine learning methods for protein structure prediction have taken advantage of the evolutionary information present in multiple sequence alignments to derive accurate structural information, but predicting structure accurately from a single sequence is much more difficult. Lin et al. trained transformer protein language models with up to 15 billion parameters on experimental and high-quality predicted structures and found that information about atomic-level structure emerged in the model as it was scaled up. They created ESMFold, a sequence-to-structure predictor that is nearly as accurate as alignment-based methods and considerably faster. The increased speed permitted the generation of a database, the ESM Metagenomic Atlas, containing more than 600 million metagenomic proteins. —MAF A protein language model enables structure prediction and analysis of more than 600 million metagenomic proteins.}}



@article{brown2023allocating,
  title={Expert explainer: Allocating accountability in AI supply chains},
  author={Ian Brown},
  journal={The Ada Lovelace Institute},
  year={2023},
  url={https://www.adalovelaceinstitute.org/resource/ai-supply-chains/}
}

@article{Kuditipudi2023RobustDW,
  title={Robust Distortion-free Watermarks for Language Models},
  author={Rohith Kuditipudi and John Thickstun and Tatsunori Hashimoto and Percy Liang},
  journal={ArXiv},
  year={2023},
  volume={abs/2307.15593},
  url={https://api.semanticscholar.org/CorpusID:260315804}
}

@article{vipra2023concentration,
  title={Market concentration implications of foundation models: The Invisible Hand of ChatGPT},
  author={Jai Vipra and Anton Korinek},
  journal={The Brookings Institution},
  year={2023},
  url={https://www.brookings.edu/articles/market-concentration-implications-of-foundation-models-the-invisible-hand-of-chatgpt}
}

@article{cen2023supplychain,
  title={AI supply chains and why they matter},
  author={Sarah H. Cen and Aspen Hopkins and Andrew Ilyas and Aleksander Madry and Isabella Struckman and Luis Videgaray},
  journal={AI Policy Substack},
  year={2023},
  url={https://aipolicy.substack.com/p/supply-chains-2}
}

@article{jones2023foundationmodels,
  title={Explainer: What is a foundation model?},
  author={Elliot Jones},
  journal={Ada Lovelace Institute},
  year={2023},
  url={https://www.adalovelaceinstitute.org/resource/foundation-models-explainer/}
}

@article{genlaw2023,
  title={Machine Learning and Artificial Intelligence: Legal Concepts},
  author={Cooper, A. Feder and Mimno, David and Choksi, Madiha and Lee, Katherine},
  journal={Generative AI and Law Workshop at the International Conference of Machine Learning},
  url={https://genlaw.github.io/glossary.html#legal-concepts},
  year={2023}
}

@article{henderson2023foundation,
  title={Foundation models and fair use},
  author={Henderson, Peter and Li, Xuechen and Jurafsky, Dan and Hashimoto, Tatsunori and Lemley, Mark A and Liang, Percy},
  journal={arXiv preprint arXiv:2303.15715},
  year={2023}
}

@inproceedings{brown2022does,
  title={What does it mean for a language model to preserve privacy?},
  author={Brown, Hannah and Lee, Katherine and Mireshghallah, Fatemehsadat and Shokri, Reza and Tram{\`e}r, Florian},
  booktitle={Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
  pages={2280--2292},
  year={2022}
}

@article{longpre2023pretrainer,
  title={A Pretrainer's Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, \& Toxicity},
  author={Longpre, Shayne and Yauney, Gregory and Reif, Emily and Lee, Katherine and Roberts, Adam and Zoph, Barret and Zhou, Denny and Wei, Jason and Robinson, Kevin and Mimno, David and others},
  journal={arXiv preprint arXiv:2305.13169},
  year={2023}
}

@article{bandy2021addressing,
  title={Addressing" documentation debt" in machine learning research: A retrospective datasheet for bookcorpus},
  author={Bandy, Jack and Vincent, Nicholas},
  journal={arXiv preprint arXiv:2105.05241},
  year={2021}
}

@inproceedings{sambasivan2021everyone,
  title={“Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI},
  author={Sambasivan, Nithya and Kapania, Shivani and Highfill, Hannah and Akrong, Diana and Paritosh, Praveen and Aroyo, Lora M},
  booktitle={proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
  pages={1--15},
  year={2021}
}

@inproceedings{hutchinson2021towards,
  title={Towards accountability for machine learning datasets: Practices from software engineering and infrastructure},
  author={Hutchinson, Ben and Smart, Andrew and Hanna, Alex and Denton, Emily and Greer, Christina and Kjartansson, Oddur and Barnes, Parker and Mitchell, Margaret},
  booktitle={Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
  pages={560--575},
  year={2021}
}

@article{gebru2021datasheets,
  title={Datasheets for datasets},
  author={Gebru, Timnit and Morgenstern, Jamie and Vecchione, Briana and Vaughan, Jennifer Wortman and Wallach, Hanna and Iii, Hal Daum{\'e} and Crawford, Kate},
  journal={Communications of the ACM},
  volume={64},
  number={12},
  pages={86--92},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@inproceedings{pushkarna2022data,
  title={Data cards: Purposeful and transparent dataset documentation for responsible ai},
  author={Pushkarna, Mahima and Zaldivar, Andrew and Kjartansson, Oddur},
  booktitle={2022 ACM Conference on Fairness, Accountability, and Transparency},
  pages={1776--1826},
  year={2022}
}

@inproceedings{mitchell2019model,
  title={Model cards for model reporting},
  author={Mitchell, Margaret and Wu, Simone and Zaldivar, Andrew and Barnes, Parker and Vasserman, Lucy and Hutchinson, Ben and Spitzer, Elena and Raji, Inioluwa Deborah and Gebru, Timnit},
  booktitle={Proceedings of the conference on fairness, accountability, and transparency},
  pages={220--229},
  year={2019}
}

@article{cdt2021,
  title={Making Transparency Meaningful: A Framework for Policymakers},
  author={Vogus, Caitlin and Llansó, Emma},
  journal={Center for Democracy and Technology},
  url={https://cdt.org/insights/report-making-transparency-meaningful-a-framework-for-policymakers/},
  year={2021},
}

@article{dsa2022,
  title={The Digital Services Act: ensuring a safe and accountable online environment},
  author={European Commission},
  journal={European Commission},
  url={https://commission.europa.eu/strategy-and-policy/priorities-2019-2024/europe-fit-digital-age/digital-services-act-ensuring-safe-and-accountable-online-environment_en},
  year={2022},
}

@article{pata2021,
  title={THE PLATFORM ACCOUNTABILITY AND TRANSPARENCY ACT},
  author={Coons, Chris and Cassidy, Bill and Klobuchar, Amy and Blumenthal, Richard and Romney, Mitt},
  journal={Congressional Bill},
  url={https://www.coons.senate.gov/imo/media/doc/pata_one_pager_118th_congress_june_2023.pdf},
  year={2021},
}

@inproceedings{edelson2021universal,
    title={Universal Digital Ad Transparency},
    author={Edelson, Laura and Chuang, Jason and Franklin Fowler, Erika and Franz, Michael and Ridout, Travis N.},
    booktitle={TPRC49: The 49th Research Conference on Communication, Information and Internet Policy},
    year={2021},
    month={August},
    note={Available at SSRN: \url{https://ssrn.com/abstract=3898214} or \url{http://dx.doi.org/10.2139/ssrn.3898214}}
}

@misc{santaclara2023,
    title={The Santa Clara Principles On Transparency and Accountability in Content Moderation},
    author={ACLU Foundation of Northern California and Center for Democracy \& Technology and Electronic Frontier Foundation and New America’s Open Technology Institute and Raicu, Irina and Suzor, Nicolas and West, Sarah Myers and Roberts, Sarah T.},
    year={2021},
}

@techreport{aspen2021commission,
    title={Commission on Information Disorder Final Report},
    author={boyd, danah and DiResta, Renée and Donovan, Joan and douek, evelyn and Frye, Emily and Gleicher, Nathaniel and Raji, Deborah and Rid, Thomas and Roth, Yoel and Wanless, Alicia and Wolf, Clement},
    institution={Aspen Institute},
    year={2021},
    month={November},
    url={https://www.aspeninstitute.org/wp-content/uploads/2021/11/Aspen-Institute_Commission-on-Information-Disorder_Final-Report.pdf},
    note={Recommendations for transparency}
}

@misc{hartzog2023oversight,
  title={Oversight of A.I.: Legislating on Artificial Intelligence},
  author={Hartzog, Woodrow},
  year={2023},
  month={Sep},
  day={12},
  howpublished={Prepared Testimony and Statement for the Record before the U.S. Senate Committee on the Judiciary, Subcommittee on Privacy, Technology, and the Law},
  url={https://www.judiciary.senate.gov/imo/media/doc/2023-09-12_pm_-_testimony_-_hartzog.pdf}
}

@misc{gregory2023testimony,
  title={The Need for Transparency in Artificial Intelligence},
  author={Gregory, Sam},
  year={2023},
  month={Sep},
  day={12},
  howpublished={Testimony before the U.S. Senate Committee on Commerce, Science and Transportation, Subcommittee on Consumer Protection, Product Safety and Data Security},
  url={https://www.commerce.senate.gov/services/files/DAD2163A-EF02-41B5-B7BA-2BA8B568C977},
  note={Executive Director, WITNESS}
}

@online{miller2021radical,
  title={Radical Proposal: Third-Party Auditor Access for AI Accountability},
  author={Miller, Katharine},
  year={2021},
  month={Oct},
  url={https://hai.stanford.edu/news/radical-proposal-third-party-auditor-access-ai-accountability},
  publisher={Stanford Institute for Human-Centered Artificial Intelligence}
}

@misc{raji2022mozilla,
  title={Mozilla Open Source Audit Tooling (OAT) Project},
  author={Raji, Deb},
  year={2022},
  month={Feb},
  day={2},
  publisher={Mozilla},
  note={Project documentation or description at Mozilla.}
}



@article{heikkila2023high,
  title={It’s high time for more AI transparency},
  author={Heikkilä, Melissa},
  journal={MIT Technology Review},
  year={2023},
  month={Jul},
  day={25},
  url={https://www.technologyreview.com/2023/07/25/1076698/its-high-time-for-more-ai-transparency/}
}


@book{belli2023igf,
    author = {Luca Belli and Walter Gaspar},
    title = {The Quest for AI Sovereignty, Transparency and Accountability},
    publisher = {UN Internet Governance Forum Data and Artificial Intelligence Governance Coalition},
    year = {2023}
}

@article{brown2020language,
  author = {Tom B. Brown and
                  Benjamin Mann and
                  Nick Ryder and
                  Melanie Subbiah and
                  Jared Kaplan and
                  Prafulla Dhariwal and
                  Arvind Neelakantan and
                  Pranav Shyam and
                  Girish Sastry and
                  Amanda Askell and
                  Sandhini Agarwal and
                  Ariel Herbert{-}Voss and
                  Gretchen Krueger and
                  Tom Henighan and
                  Rewon Child and
                  Aditya Ramesh and
                  Daniel M. Ziegler and
                  Jeffrey Wu and
                  Clemens Winter and
                  Christopher Hesse and
                  Mark Chen and
                  Eric Sigler and
                  Mateusz Litwin and
                  Scott Gray and
                  Benjamin Chess and
                  Jack Clark and
                  Christopher Berner and
                  Sam McCandlish and
                  Alec Radford and
                  Ilya Sutskever and
                  Dario Amodei},
  title        = {Language Models are Few-Shot Learners},
  journal      = {CoRR},
  volume       = {abs/2005.14165},
  year         = {2020},
  url          = {https://arxiv.org/abs/2005.14165},
  eprinttype    = {arXiv},
  eprint       = {2005.14165},
  timestamp    = {Thu, 25 May 2023 10:38:31 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2005-14165.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@inproceedings{solaiman2023gradient,
  title={The gradient of generative AI release: Methods and considerations},
  author={Solaiman, Irene},
  booktitle={Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency},
  pages={111--122},
  year={2023}
}

@article{serrano2019attention,
  title={Is attention interpretable?},
  author={Serrano, Sofia and Smith, Noah A},
  journal={arXiv preprint arXiv:1906.03731},
  year={2019}
}

@online{liang2022thetime,
  title={The Time Is Now to Develop Community Norms for the Release of Foundation Models},
  author={Liang, Percy},
  year={2022},
  month={May},
  url={https://hai.stanford.edu/news/time-now-develop-community-norms-release-foundation-models},
  publisher={Stanford Institute for Human-Centered Artificial Intelligence}
}

@article{kumar2022language,
  title={Language generation models can cause harm: So what can we do about it? An actionable survey},
  author={Kumar, Sachin and Balachandran, Vidhisha and Njoo, Lucille and Anastasopoulos, Antonios and Tsvetkov, Yulia},
  journal={arXiv preprint arXiv:2210.07700},
  year={2022}
}

@article{weidinger2021ethical,
  title={Ethical and social risks of harm from language models},
  author={Weidinger, Laura and Mellor, John and Rauh, Maribeth and Griffin, Conor and Uesato, Jonathan and Huang, Po-Sen and Cheng, Myra and Glaese, Mia and Balle, Borja and Kasirzadeh, Atoosa and others},
  journal={arXiv preprint arXiv:2112.04359},
  year={2021}
}

@article{
liang2023holistic,
title={Holistic Evaluation of Language Models},
author={Percy Liang and Rishi Bommasani and Tony Lee and Dimitris Tsipras and Dilara Soylu and Michihiro Yasunaga and Yian Zhang and Deepak Narayanan and Yuhuai Wu and Ananya Kumar and Benjamin Newman and Binhang Yuan and Bobby Yan and Ce Zhang and Christian Alexander Cosgrove and Christopher D Manning and Christopher Re and Diana Acosta-Navas and Drew Arad Hudson and Eric Zelikman and Esin Durmus and Faisal Ladhak and Frieda Rong and Hongyu Ren and Huaxiu Yao and Jue WANG and Keshav Santhanam and Laurel Orr and Lucia Zheng and Mert Yuksekgonul and Mirac Suzgun and Nathan Kim and Neel Guha and Niladri S. Chatterji and Omar Khattab and Peter Henderson and Qian Huang and Ryan Andrew Chi and Sang Michael Xie and Shibani Santurkar and Surya Ganguli and Tatsunori Hashimoto and Thomas Icard and Tianyi Zhang and Vishrav Chaudhary and William Wang and Xuechen Li and Yifan Mai and Yuhui Zhang and Yuta Koreeda},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2023},
url={https://openreview.net/forum?id=iO4LZibEqW},
note={Featured Certification, Expert Certification}
}

@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{gehmanetal2020realtoxicityprompts,
    title = "{R}eal{T}oxicity{P}rompts: Evaluating Neural Toxic Degeneration in Language Models",
    author = "Gehman, Samuel  and
      Gururangan, Suchin  and
      Sap, Maarten  and
      Choi, Yejin  and
      Smith, Noah A.",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.301",
    doi = "10.18653/v1/2020.findings-emnlp.301",
    pages = "3356--3369",
    abstract = "Pretrained neural language models (LMs) are prone to generating racist, sexist, or otherwise toxic language which hinders their safe deployment. We investigate the extent to which pretrained LMs can be prompted to generate toxic language, and the effectiveness of controllable text generation algorithms at preventing such toxic degeneration. We create and release RealToxicityPrompts, a dataset of 100K naturally occurring, sentence-level prompts derived from a large corpus of English web text, paired with toxicity scores from a widely-used toxicity classifier. Using RealToxicityPrompts, we find that pretrained LMs can degenerate into toxic text even from seemingly innocuous prompts. We empirically assess several controllable generation methods, and find that while data- or compute-intensive methods (e.g., adaptive pretraining on non-toxic data) are more effective at steering away from toxicity than simpler solutions (e.g., banning {``}bad{''} words), no current method is failsafe against neural toxic degeneration. To pinpoint the potential cause of such persistent toxic degeneration, we analyze two web text corpora used to pretrain several LMs (including GPT-2; Radford et. al, 2019), and find a significant amount of offensive, factually unreliable, and otherwise toxic content. Our work provides a test bed for evaluating toxic generations by LMs and stresses the need for better data selection processes for pretraining.",
}

@article{dathathri2019plug,
  title={Plug and play language models: A simple approach to controlled text generation},
  author={Dathathri, Sumanth and Madotto, Andrea and Lan, Janice and Hung, Jane and Frank, Eric and Molino, Piero and Yosinski, Jason and Liu, Rosanne},
  journal={arXiv preprint arXiv:1912.02164},
  year={2019}
}

@article{brundage2020toward,
  title={Toward trustworthy AI development: mechanisms for supporting verifiable claims},
  author={Brundage, Miles and Avin, Shahar and Wang, Jasmine and Belfield, Haydn and Krueger, Gretchen and Hadfield, Gillian and Khlaaf, Heidy and Yang, Jingying and Toner, Helen and Fong, Ruth and others},
  journal={arXiv preprint arXiv:2004.07213},
  year={2020}
}

@article{cammarota2020trustworthy,
  title={Trustworthy AI inference systems: An industry research view},
  author={Cammarota, Rosario and Schunter, Matthias and Rajan, Anand and Boemer, Fabian and Kiss, {\'A}gnes and Treiber, Amos and Weinert, Christian and Schneider, Thomas and Stapf, Emmanuel and Sadeghi, Ahmad-Reza and others},
  journal={arXiv preprint arXiv:2008.04449},
  year={2020}
}

@article{liu2022trustworthy,
  title={Trustworthy ai: A computational perspective},
  author={Liu, Haochen and Wang, Yiqi and Fan, Wenqi and Liu, Xiaorui and Li, Yaxin and Jain, Shaili and Liu, Yunhao and Jain, Anil and Tang, Jiliang},
  journal={ACM Transactions on Intelligent Systems and Technology},
  volume={14},
  number={1},
  pages={1--59},
  year={2022},
  publisher={ACM New York, NY}
}

@article{radford2022whisper,
  title={Robust Speech Recognition via Large-Scale Weak Supervision},
  author={Alec Radford and Jong Wook Kim and Tao Xu and Greg Brockman and Christine McLeavey and Ilya Sutskever},
  journal={ArXiv},
  year={2022},
  volume={abs/2212.04356},
  url={https://api.semanticscholar.org/CorpusID:252923993}
}

@inproceedings{kumar2020trustworthy,
  title={Trustworthy AI in the age of pervasive computing and big data},
  author={Kumar, Abhishek and Braud, Tristan and Tarkoma, Sasu and Hui, Pan},
  booktitle={2020 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops)},
  pages={1--6},
  year={2020},
  organization={IEEE}
}

@article{shneiderman2020bridging,
author = {Shneiderman, Ben},
title = {Bridging the Gap Between Ethics and Practice: Guidelines for Reliable, Safe, and Trustworthy Human-Centered AI Systems},
year = {2020},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {2160-6455},
url = {https://doi.org/10.1145/3419764},
doi = {10.1145/3419764},
journal = {ACM Trans. Interact. Intell. Syst.},
month = {oct},
articleno = {26},
numpages = {31},
keywords = {management strategies, Artificial Intelligence, safe, trustworthy, Human-centered AI, independent oversight, reliable, software engineering practices, design, Human-Computer Interaction}
}

@inproceedings{reddi2020mlperf,
  title={Mlperf inference benchmark},
  author={Reddi, Vijay Janapa and Cheng, Christine and Kanter, David and Mattson, Peter and Schmuelling, Guenther and Wu, Carole-Jean and Anderson, Brian and Breughe, Maximilien and Charlebois, Mark and Chou, William and others},
  booktitle={2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)},
  pages={446--459},
  year={2020},
  organization={IEEE}
}

% Try to use refdb instead of adding to this file!
@article{
lee2023evaluating,
title={Evaluating Human-Language Model Interaction},
author={Mina Lee and Megha Srivastava and Amelia Hardy and John Thickstun and Esin Durmus and Ashwin Paranjape and Ines Gerard-Ursin and Xiang Lisa Li and Faisal Ladhak and Frieda Rong and Rose E Wang and Minae Kwon and Joon Sung Park and Hancheng Cao and Tony Lee and Rishi Bommasani and Michael S. Bernstein and Percy Liang},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2023},
url={https://openreview.net/forum?id=hjDYJUn9l1},
note={}
}

@article{narayanan2022evaluating,
  title={{Evaluating Efficiency-Capability Tradeoffs for Black-Box Autoregressive Transformer APIs}},
  author={Deepak Narayanan and Keshav Santhanam and Peter Henderson and Rishi Bommasani and Tony Lee and Percy Liang},
  year={2022}
}

@article{seger2023open,
  title={{Open-Sourcing Highly Capable Foundation Models: An Evaluation of Risks, Benefits, and Alternative Methods for Pursuing Open-Source Objectives}},
  author={Seger, Elizabeth and Dreksler, Noemi and Moulange, Richard and Dardaman, Emily and Schuett, Jonas and Wei, K. and Winter, Christoph and Arnold, Mackenzie and Ó hÉigeartaigh, Seán and Korinek, Anton and Anderljung, Markus and Bucknall, Ben and Chan, Alan and Stafford, Eoghan and Koessler, Leonie and Ovadya, Aviv and Garfinkel, Ben and Bluemke, Emma and Aird, Michael and Levermore, Patrick and Hazell, Julian and Gupta, Abhishek},
  year={2023}
}

@article{shannon1948mathematical,
  title={A mathematical theory of communication},
  author={Shannon, Claude Elwood},
  journal={The Bell system technical journal},
  volume={27},
  number={3},
  pages={379--423},
  year={1948},
  publisher={Nokia Bell Labs}
}

@article{lounsbury1954transitional,
  title={ Transitional probability, linguistic structure and systems of habitfamily hierarchies.},
  author={Floyd G. Lounsburg},
  journal={Psycholinguistics: a survey of
theory and research},
  year={1954},
  publisher={Indiana University Press}
}


@article{franceschelli2022copyright,
  title={Copyright in generative deep learning},
  author={Franceschelli, Giorgio and Musolesi, Mirco},
  journal={Data \& Policy},
  volume={4},
  year={2022},
  publisher={Cambridge University Press}
}

@inproceedings{taylor2022galactica,
    title={GALACTICA: A Large Language Model for Science},
    author={Ross Taylor and Marcin Kardas and Guillem Cucurull and Thomas Scialom and Anthony Hartshorn and Elvis Saravia and Andrew Poulton and Viktor Kerkez and Robert Stojnic},
    year={2022},
    url={https://galactica.org/static/paper.pdf}
}

@article{khashabi2022unified,
  title={UnifiedQA-v2: Stronger Generalization via Broader Cross-Format Training},
  author={Daniel Khashabi and Yeganeh Kordi and Hannaneh Hajishirzi},
  journal={ArXiv},
  year={2022},
  volume={abs/2202.12359}
}

@inproceedings{costanzachock2022audits,
author = {Costanza-Chock, Sasha and Raji, Inioluwa Deborah and Buolamwini, Joy},
title = {Who Audits the Auditors? Recommendations from a Field Scan of the Algorithmic Auditing Ecosystem},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533213},
doi = {10.1145/3531146.3533213},
abstract = {Algorithmic audits (or ‘AI audits’) are an increasingly popular mechanism for algorithmic accountability; however, they remain poorly defined. Without a clear understanding of audit practices, let alone widely used standards or regulatory guidance, claims that an AI product or system has been audited, whether by first-, second-, or third-party auditors, are difficult to verify and may potentially exacerbate, rather than mitigate, bias and harm. To address this knowledge gap, we provide the first comprehensive field scan of the AI audit ecosystem. We share a catalog of individuals (N=438) and organizations (N=189) who engage in algorithmic audits or whose work is directly relevant to algorithmic audits; conduct an anonymous survey of the group (N=152); and interview industry leaders (N=10). We identify emerging best practices as well as methods and tools that are becoming commonplace, and enumerate common barriers to leveraging algorithmic audits as effective accountability mechanisms. We outline policy recommendations to improve the quality and impact of these audits, and highlight proposals with wide support from algorithmic auditors as well as areas of debate. Our recommendations have implications for lawmakers, regulators, internal company policymakers, and standards-setting bodies, as well as for auditors. They are: 1) require the owners and operators of AI systems to engage in independent algorithmic audits against clearly defined standards; 2) notify individuals when they are subject to algorithmic decision-making systems; 3) mandate disclosure of key components of audit findings for peer review; 4) consider real-world harm in the audit process, including through standardized harm incident reporting and response mechanisms; 5) directly involve the stakeholders most likely to be harmed by AI systems in the algorithmic audit process; and 6) formalize evaluation and, potentially, accreditation of algorithmic auditors.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1571–1583},
numpages = {13},
keywords = {algorithm audit, AI policy, AI harm, ethical AI, AI bias, audit, AI audit, algorithmic accountability},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}

@article{bhatt2022recontextualizing,
  title={Re-contextualizing Fairness in NLP: The Case of India},
  author={Shaily Bhatt and Sunipa Dev and Partha P. Talukdar and Shachi Dave and Vinodkumar Prabhakaran},
  journal={ArXiv},
  year={2022},
  volume={abs/2209.12226}
}

@article{kleinberg2021monoculture,
author = {Jon Kleinberg  and Manish Raghavan },
title = {Algorithmic monoculture and social welfare},
journal = {Proceedings of the National Academy of Sciences},
volume = {118},
number = {22},
pages = {e2018340118},
year = {2021},
doi = {10.1073/pnas.2018340118},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.2018340118},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.2018340118},
abstract = {As algorithms are increasingly applied to screen applicants for high-stakes decisions in employment, lending, and other domains, concerns have been raised about the effects of algorithmic monoculture, in which many decision-makers all rely on the same algorithm. This concern invokes analogies to agriculture, where a monocultural system runs the risk of severe harm from unexpected shocks. Here, we show that the dangers of algorithmic monoculture run much deeper, in that monocultural convergence on a single algorithm by a group of decision-making agents, even when the algorithm is more accurate for any one agent in isolation, can reduce the overall quality of the decisions being made by the full collection of agents. Unexpected shocks are therefore not needed to expose the risks of monoculture; it can hurt accuracy even under “normal” operations and even for algorithms that are more accurate when used by only a single decision-maker. Our results rely on minimal assumptions and involve the development of a probabilistic framework for analyzing systems that use multiple noisy estimates of a set of alternatives.}}

@article{metaxa2021audit,
url = {http://dx.doi.org/10.1561/1100000083},
year = {2021},
volume = {14},
journal = {Foundations and Trends® in Human–Computer Interaction},
title = {Auditing Algorithms: Understanding Algorithmic Systems from the Outside In},
doi = {10.1561/1100000083},
issn = {1551-3955},
number = {4},
pages = {272-344},
author = {Danaë Metaxa and Joon Sung Park and Ronald E. Robertson and Karrie Karahalios and Christo Wilson and Jeff Hancock and Christian Sandvig}
}

@misc{rombach2021highresolution,
      title={High-Resolution Image Synthesis with Latent Diffusion Models}, 
      author={Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Björn Ommer},
      year={2021},
      eprint={2112.10752},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{openai2023chatgptapi,
  title={Introducing ChatGPT and Whisper APIs},
  author={OpenAI},
  year={2023},
  url={https://openai.com/blog/introducing-chatgpt-and-whisper-apis}
}


@article{madry2023supplychain,
  title={Advances in AI: Are We Ready For a Tech Revolution?},
  author={Aleksander Mądry},
  journal={Cybersecurity, Information Technology, and Government Innovation Subcommittee},
  year={2023},
  url={https://oversight.house.gov/wp-content/uploads/2023/03/madry_written_statement100.pdf}
}

@article{whitehouse2021cybersecurity,
  title={Executive Order on Improving the Nation’s Cybersecurity},
  author={{White House Executive Order}},
  year={2021},
  url={https://www.whitehouse.gov/briefing-room/presidential-actions/2021/05/12/executive-order-on-improving-the-nations-cybersecurity/}
}

@article{scott2023atlanticcouncil,
  title={Avoiding the success trap: Toward policy for open-source software as infrastructure},
  author={Stewart Scott and Sara Ann Brackett and Trey Herr and Maia Hamin},
  year={2023},
  journal={Atlantic Council},
  url={https://www.atlanticcouncil.org/in-depth-research-reports/report/open-source-software-as-infrastructure/}
}

@article{ramaswami2021securing,
  title={Securing Open Source Software at the Source},
  author={Ashwin Ramaswami},
  year={2021},
  journal={Plaintext Group by Schmidt Futures},
  url={https://www.plaintextgroup.com/reports/securing-open-source-software-at-the-source}
}

@TECHREPORT{korinek2023language,
title = {Language Models and Cognitive Automation for Economic Research},
author = {Korinek, Anton},
year = {2023},
institution = {National Bureau of Economic Research, Inc},
type = {NBER Working Papers},
number = {30957},
abstract = {Large language models (LLMs) such as ChatGPT have the potential to revolutionize research in economics and other disciplines. I describe 25 use cases along six domains in which LLMs are starting to become useful as both research assistants and tutors: ideation, writing, background research, data analysis, coding, and mathematical derivations. I provide general instructions and demonstrate specific examples for how to take advantage of each of these, classifying the LLM capabilities from experimental to highly useful. I hypothesize that ongoing advances will improve the performance of LLMs across all of these domains, and that economic researchers who take advantage of LLMs to automate micro tasks will become significantly more productive. Finally, I speculate on the longer-term implications of cognitive automation via LLMs for economic research.},
url = {https://EconPapers.repec.org/RePEc:nbr:nberwo:30957}
}

@misc{eloundou2023gpts,
      title={GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models}, 
      author={Tyna Eloundou and Sam Manning and Pamela Mishkin and Daniel Rock},
      year={2023},
      eprint={2303.10130},
      archivePrefix={arXiv},
      primaryClass={econ.GN}
}

@article{brynjolfsson2021jcurve,
Author = {Brynjolfsson, Erik and Rock, Daniel and Syverson, Chad},
Title = {The Productivity J-Curve: How Intangibles Complement General Purpose Technologies},
Journal = {American Economic Journal: Macroeconomics},
Volume = {13},
Number = {1},
Year = {2021},
Month = {January},
Pages = {333-72},
DOI = {10.1257/mac.20180386},
URL = {https://www.aeaweb.org/articles?id=10.1257/mac.20180386}}

@book{aristotle350deanima,
  title={De Anima: On the Soul},
  author={Aristotle},
  year={350 B.C.E},
}

@techreport{zalnieriute2021transparency,
abstract = {Contemporary discourse on the regulation and governance of the digital environment has often focused on the procedural value of transparency. This article traces the prominence of the concept of transparency in contemporary regulatory debates to the corporate agenda of technology companies. Looking at the latest transparency initiatives of IBM, Google and Facebook, I introduce the concept of \textquotedblleft{}transparency-washing,\textquotedblright{} whereby a focus on transparency acts as an obfuscation and redirection from more substantive and fundamental questions about the concentration of power, substantial policies and actions of technology behemoths. While the \textquotedblleft{}ethics-washing\textquotedblright{} of the tech giants has become widely acknowledged, \textquotedblleft{}transparency washing\textquotedblright{} presents a wider critique of corporate discourse and neoliberal governmentality based on procedural fetishism, which detracts from the questions of substantial accountability and obligations by diverting the attention to procedural micro-issues that have little chance of changing the political or legal status quo},
author = {Monika Zalnieriute},
booktitle = {\textquotedblleft{}Transparency-Washing\textquotedblright{} in the Digital Age : A Corporate Agenda of Procedural Fetishism},
copyright = {https://zbw.eu/econis-archiv/termsofuse},
language = {eng},
publisher = {[S.l.] : SSRN},
title = {\textquotedblleft{}Transparency-Washing\textquotedblright{} in the Digital Age : A Corporate Agenda of Procedural Fetishism},
url = {http://hdl.handle.net/11159/468588},
year = {2021}
}

@misc{bommasani2023eu-ai-act, 
    author = {Rishi Bommasani and Kevin Klyman and Daniel Zhang and Percy Liang}, 
    title  = {Do Foundation Model Providers Comply with the EU AI Act?}, 
    url    = {https://crfm.stanford.edu/2023/06/15/eu-ai-act.html}, 
    year   = {2023}
}

@inproceedings{sandvig2014auditing,
  title={Auditing Algorithms : Research Methods for Detecting Discrimination on Internet Platforms},
  author={Christian Sandvig and Kevin Hamilton and Karrie Karahalios and C{\'e}dric Langbort},
  year={2014},
  url={https://api.semanticscholar.org/CorpusID:15686114}
}

@misc{tabassi2023airmf,
  author = {Elham Tabassi},
  title = {Artificial Intelligence Risk Management Framework (AI RMF 1.0)},
  year = {2023},
  month = {2023-01-26 05:01:00},
  publisher = {NIST Trustworthy and Responsible AI, National Institute of Standards and Technology, Gaithersburg, MD},
  url = {https://tsapps.nist.gov/publication/get_pdf.cfm?pub_id=936225},
  doi = {https://doi.org/10.6028/NIST.AI.100-1},
  language = {en},
}

@article{kapoor2023leakage,
title = {Leakage and the reproducibility crisis in machine-learning-based science},
journal = {Patterns},
volume = {4},
number = {9},
pages = {100804},
year = {2023},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2023.100804},
url = {https://www.sciencedirect.com/science/article/pii/S2666389923001599},
author = {Sayash Kapoor and Arvind Narayanan},
keywords = {reproducibility, machine learning, leakage},
abstract = {Summary
Machine-learning (ML) methods have gained prominence in the quantitative sciences. However, there are many known methodological pitfalls, including data leakage, in ML-based science. We systematically investigate reproducibility issues in ML-based science. Through a survey of literature in fields that have adopted ML methods, we find 17 fields where leakage has been found, collectively affecting 294 papers and, in some cases, leading to wildly overoptimistic conclusions. Based on our survey, we introduce a detailed taxonomy of eight types of leakage, ranging from textbook errors to open research problems. We propose that researchers test for each type of leakage by filling out model info sheets, which we introduce. Finally, we conduct a reproducibility study of civil war prediction, where complex ML models are believed to vastly outperform traditional statistical models such as logistic regression (LR). When the errors are corrected, complex ML models do not perform substantively better than decades-old LR models.}
}

@article{widder2023open,
  title={Open (For Business): Big Tech, Concentrated Power, and the Political Economy of Open AI},
  author={Widder, David Gray and West, Sarah and Whittaker, Meredith},
  year={2023}
}


@article{landis1977agreement,
 ISSN = {0006341X, 15410420},
 URL = {http://www.jstor.org/stable/2529310},
 abstract = {This paper presents a general statistical methodology for the analysis of multivariate categorical data arising from observer reliability studies. The procedure essentially involves the construction of functions of the observed proportions which are directed at the extent to which the observers agree among themselves and the construction of test statistics for hypotheses involving these functions. Tests for interobserver bias are presented in terms of first-order marginal homogeneity and measures of interobserver agreement are developed as generalized kappa-type statistics. These procedures are illustrated with a clinical diagnosis example from the epidemiological literature.},
 author = {J. Richard Landis and Gary G. Koch},
 journal = {Biometrics},
 number = {1},
 pages = {159--174},
 publisher = {[Wiley, International Biometric Society]},
 title = {The Measurement of Observer Agreement for Categorical Data},
 urldate = {2023-10-07},
 volume = {33},
 year = {1977}
}



@misc{kapoor2023reforms,
      title={REFORMS: Reporting Standards for Machine Learning Based Science}, 
      author={Sayash Kapoor and Emily Cantrell and Kenny Peng and Thanh Hien Pham and Christopher A. Bail and Odd Erik Gundersen and Jake M. Hofman and Jessica Hullman and Michael A. Lones and Momin M. Malik and Priyanka Nanayakkara and Russell A. Poldrack and Inioluwa Deborah Raji and Michael Roberts and Matthew J. Salganik and Marta Serra-Garcia and Brandon M. Stewart and Gilles Vandewiele and Arvind Narayanan},
      year={2023},
      eprint={2308.07832},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{lipton2019troubling,
author = {Lipton, Zachary C. and Steinhardt, Jacob},
title = {Troubling Trends in Machine Learning Scholarship: Some ML Papers Suffer from Flaws That Could Mislead the Public and Stymie Future Research.},
year = {2019},
issue_date = {January-February 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {1},
issn = {1542-7730},
url = {https://doi.org/10.1145/3317287.3328534},
doi = {10.1145/3317287.3328534},
abstract = {Flawed scholarship threatens to mislead the public and stymie future research by compromising ML’s intellectual foundations. Indeed, many of these problems have recurred cyclically throughout the history of AI and, more broadly, in scientific research. In 1976, Drew McDermott chastised the AI community for abandoning self-discipline, warning prophetically that "if we can’t criticize ourselves, someone else will save us the trouble." The current strength of machine learning owes to a large body of rigorous research to date, both theoretical and empirical. By promoting clear scientific thinking and communication, our community can sustain the trust and investment it currently enjoys.},
journal = {Queue},
month = {feb},
pages = {45–77},
numpages = {33}
}

@article{lam2022user,
author = {Lam, Michelle S. and Gordon, Mitchell L. and Metaxa, Dana\"{e} and Hancock, Jeffrey T. and Landay, James A. and Bernstein, Michael S.},
title = {End-User Audits: A System Empowering Communities to Lead Large-Scale Investigations of Harmful Algorithmic Behavior},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW2},
url = {https://doi.org/10.1145/3555625},
doi = {10.1145/3555625},
abstract = {Because algorithm audits are conducted by technical experts, audits are necessarily limited to the hypotheses that experts think to test. End users hold the promise to expand this purview, as they inhabit spaces and witness algorithmic impacts that auditors do not. In pursuit of this goal, we propose end-user audits-system-scale audits led by non-technical users-and present an approach that scaffolds end users in hypothesis generation, evidence identification, and results communication. Today, performing a system-scale audit requires substantial user effort to label thousands of system outputs, so we introduce a collaborative filtering technique that leverages the algorithmic system's own disaggregated training data to project from a small number of end user labels onto the full test set. Our end-user auditing tool, IndieLabel, employs these predicted labels so that users can rapidly explore where their opinions diverge from the algorithmic system's outputs. By highlighting topic areas where the system is under-performing for the user and surfacing sets of likely error cases, the tool guides the user in authoring an audit report. In an evaluation of end-user audits on a popular comment toxicity model with 17 non-technical participants, participants both replicated issues that formal audits had previously identified and also raised previously underreported issues such as under-flagging on veiled forms of hate that perpetuate stigma and over-flagging of slurs that have been reclaimed by marginalized communities.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {nov},
articleno = {512},
numpages = {34},
keywords = {algorithm auditing, machine learning, human-centered ai, interactive visualization, algorithmic fairness}
}

@article{metaxa2021auditing,
url = {http://dx.doi.org/10.1561/1100000083},
year = {2021},
volume = {14},
journal = {Foundations and Trends® in Human–Computer Interaction},
title = {Auditing Algorithms: Understanding Algorithmic Systems from the Outside In},
doi = {10.1561/1100000083},
issn = {1551-3955},
number = {4},
pages = {272-344},
author = {Danaë Metaxa and Joon Sung Park and Ronald E. Robertson and Karrie Karahalios and Christo Wilson and Jeff Hancock and Christian Sandvig}
}

@inproceedings{costanzachock2022audit,
author = {Costanza-Chock, Sasha and Raji, Inioluwa Deborah and Buolamwini, Joy},
title = {Who Audits the Auditors? Recommendations from a Field Scan of the Algorithmic Auditing Ecosystem},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533213},
doi = {10.1145/3531146.3533213},
abstract = {Algorithmic audits (or ‘AI audits’) are an increasingly popular mechanism for algorithmic accountability; however, they remain poorly defined. Without a clear understanding of audit practices, let alone widely used standards or regulatory guidance, claims that an AI product or system has been audited, whether by first-, second-, or third-party auditors, are difficult to verify and may potentially exacerbate, rather than mitigate, bias and harm. To address this knowledge gap, we provide the first comprehensive field scan of the AI audit ecosystem. We share a catalog of individuals (N=438) and organizations (N=189) who engage in algorithmic audits or whose work is directly relevant to algorithmic audits; conduct an anonymous survey of the group (N=152); and interview industry leaders (N=10). We identify emerging best practices as well as methods and tools that are becoming commonplace, and enumerate common barriers to leveraging algorithmic audits as effective accountability mechanisms. We outline policy recommendations to improve the quality and impact of these audits, and highlight proposals with wide support from algorithmic auditors as well as areas of debate. Our recommendations have implications for lawmakers, regulators, internal company policymakers, and standards-setting bodies, as well as for auditors. They are: 1) require the owners and operators of AI systems to engage in independent algorithmic audits against clearly defined standards; 2) notify individuals when they are subject to algorithmic decision-making systems; 3) mandate disclosure of key components of audit findings for peer review; 4) consider real-world harm in the audit process, including through standardized harm incident reporting and response mechanisms; 5) directly involve the stakeholders most likely to be harmed by AI systems in the algorithmic audit process; and 6) formalize evaluation and, potentially, accreditation of algorithmic auditors.},
booktitle = {Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1571–1583},
numpages = {13},
keywords = {audit, algorithm audit, AI policy, AI bias, ethical AI, AI harm, algorithmic accountability, AI audit},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}

@inproceedings{
narayanan2023cheaply,
title={Cheaply Evaluating Inference Efficiency Metrics for Autoregressive Transformer {API}s},
author={Deepak Narayanan and Keshav Santhanam and Peter Henderson and Rishi Bommasani and Tony Lee and Percy Liang},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=RJpAz15D0S}
}

@misc{wef2023presidio,
    author = {WEF},
    title = {The Presidio Recommendations on Responsible Generative AI},
    year = {2023},
    url = {https://www3.weforum.org/docs/WEF_Presidio_Recommendations_on_Responsible_Generative_AI_2023.pdf}
}

@article{liao2023transparency,
  title={AI Transparency in the Age of LLMs: A Human-Centered Research Roadmap},
  author={Qingzi Vera Liao and Jennifer Wortman Vaughan},
  journal={ArXiv},
  year={2023},
  volume={abs/2306.01941},
  url={https://api.semanticscholar.org/CorpusID:259075521}
}

@misc{zou2023representation,
      title={Representation Engineering: A Top-Down Approach to AI Transparency}, 
      author={Andy Zou and Long Phan and Sarah Chen and James Campbell and Phillip Guo and Richard Ren and Alexander Pan and Xuwang Yin and Mantas Mazeika and Ann-Kathrin Dombrowski and Shashwat Goel and Nathaniel Li and Michael J. Byun and Zifan Wang and Alex Mallen and Steven Basart and Sanmi Koyejo and Dawn Song and Matt Fredrikson and J. Zico Kolter and Dan Hendrycks},
      year={2023},
      eprint={2310.01405},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@incollection{kalderon2015transparency,
    author = {Kalderon, Mark Eli},
    isbn = {9780198717904},
    title = "{Transparency}",
    booktitle = "{Form without Matter: Empedocles and Aristotle on Color Perception}",
    publisher = {Oxford University Press},
    year = {2015},
    month = {01},
    abstract = "{This chapter examines and defends Aristotle’s views about transparency as presented in De Anima and De Sensu in light of the Empedoclean puzzlement about the presentation of remote objects of perception. Specifically, the De Anima discussion sheds light on how states and properties lack space occupancy and so undermines a spatial presupposition of Empedoclean puzzlement. And the De Sensu discussion explores the sense in which the transparent may be seen through and so highlights the relevance of transparency to Empedoclean puzzlement.}",
    doi = {10.1093/acprof:oso/9780198717904.003.0003},
    url = {https://doi.org/10.1093/acprof:oso/9780198717904.003.0003},
    eprint = {https://academic.oup.com/book/0/chapter/151154008/chapter-ag-pdf/44977694/book\_6913\_section\_151154008.ag.pdf},
}




@book{autor2022work,
  title={The Work of the Future: Building Better Jobs in an Age of Intelligent Machines},
  author={Autor, D.H. and Mindell, D.A. and Reynolds, E. and Solow, R.M.},
  isbn={9780262046367},
  lccn={2021010583},
  url={https://books.google.com/books?id=3tKMEAAAQBAJ},
  year={2022},
  publisher={MIT Press}
}

@article{brynjolfsson2017ml,
author = {Erik Brynjolfsson  and Tom Mitchell },
title = {What can machine learning do? Workforce implications},
journal = {Science},
volume = {358},
number = {6370},
pages = {1530-1534},
year = {2017},
doi = {10.1126/science.aap8062},
URL = {https://www.science.org/doi/abs/10.1126/science.aap8062},
eprint = {https://www.science.org/doi/pdf/10.1126/science.aap8062},
abstract = {Profound change is coming, but roles for humans remain Digital computers have transformed work in almost every sector of the economy over the past several decades (1). We are now at the beginning of an even larger and more rapid transformation due to recent advances in machine learning (ML), which is capable of accelerating the pace of automation itself. However, although it is clear that ML is a “general purpose technology,” like the steam engine and electricity, which spawns a plethora of additional innovations and capabilities (2), there is no widely shared agreement on the tasks where ML systems excel, and thus little agreement on the specific expected impacts on the workforce and on the economy more broadly. We discuss what we see to be key implications for the workforce, drawing on our rubric of what the current generation of ML systems can and cannot do [see the supplementary materials (SM)]. Although parts of many jobs may be “suitable for ML” (SML), other tasks within these same jobs do not fit the criteria for ML well; hence, effects on employment are more complex than the simple replacement and substitution story emphasized by some. Although economic effects of ML are relatively limited today, and we are not facing the imminent “end of work” as is sometimes proclaimed, the implications for the economy and the workforce going forward are profound.}}

@techreport{acemoglu2020ai,
 title = "AI and Jobs: Evidence from Online Vacancies",
 author = "Acemoglu, Daron and Autor, David and Hazell, Jonathon and Restrepo, Pascual",
 institution = "National Bureau of Economic Research",
 type = "Working Paper",
 series = "Working Paper Series",
 number = "28257",
 year = "2020",
 month = "December",
 doi = {10.3386/w28257},
 URL = "http://www.nber.org/papers/w28257",
 abstract = {We study the impact of AI on labor markets using establishment-level data on vacancies with detailed occupation and skill information comprising the near-universe of online vacancies in the US from 2010 onwards. There is rapid growth in AI related vacancies over 2010-2018 that is greater in AI-exposed establishments. AI-exposed establishments are reducing hiring in non-AI positions. We find no discernible relationship between AI exposure and employment or wage growth at the occupation or industry level, however, implying that AI is currently substituting for humans in a subset of tasks but it is not yet having detectable aggregate labor market consequences.},
}

@techreport{acemoglu2010skills,
 title = "Skills, Tasks and Technologies: Implications for Employment and Earnings",
 author = "Acemoglu, Daron and Autor, David",
 institution = "National Bureau of Economic Research",
 type = "Working Paper",
 series = "Working Paper Series",
 number = "16082",
 year = "2010",
 month = "June",
 doi = {10.3386/w16082},
 URL = "http://www.nber.org/papers/w16082",
 abstract = {A central organizing framework of the voluminous recent literature studying changes in the returns to skills and the evolution of earnings inequality is what we refer to as the canonical model, which elegantly and powerfully operationalizes the supply and demand for skills by assuming two distinct skill groups that perform two different and imperfectly substitutable tasks or produce two imperfectly substitutable goods. Technology is assumed to take a factor-augmenting form, which, by complementing either high or low skill workers, can generate skill biased demand shifts. In this paper, we argue that despite its notable successes, the canonical model is largely silent on a number of central empirical developments of the last three decades, including: (1) significant declines in real wages of low skill workers, particularly low skill males; (2) non-monotone changes in wages at different parts of the earnings distribution during different decades; (3) broad-based increases in employment in high skill and low skill occupations relative to middle skilled occupations (i.e., job 'polarization'); (4) rapid diffusion of new technologies that directly substitute capital for labor in tasks previously performed by moderately-skilled workers; and (5) expanding offshoring opportunities, enabled by technology, which allow foreign labor to substitute for domestic workers in specific tasks. Motivated by these patterns, we argue that it is valuable to consider a richer framework for analyzing how recent changes in the earnings and employment distribution in the United States and other advanced economies are shaped by the interactions among worker skills, job tasks, evolving technologies, and shifting trading opportunities. We propose a tractable task-based model in which the assignment of skills to tasks is endogenous and technical change may involve the substitution of machines for certain tasks previously performed by labor. We further consider how the evolution of technology in this task-based setting may be endogenized. We show how such a framework can be used to interpret several central recent trends, and we also suggest further directions for empirical exploration.},
}

@article{acemoglu2018race,
Author = {Acemoglu, Daron and Restrepo, Pascual},
Title = {The Race between Man and Machine: Implications of Technology for Growth, Factor Shares, and Employment},
Journal = {American Economic Review},
Volume = {108},
Number = {6},
Year = {2018},
Month = {June},
Pages = {1488-1542},
DOI = {10.1257/aer.20160696},
URL = {https://www.aeaweb.org/articles?id=10.1257/aer.20160696}}

@techreport{agrawal2021ai,
 title = "AI Adoption and System-Wide Change",
 author = "Agrawal, Ajay K and Gans, Joshua S and Goldfarb, Avi",
 institution = "National Bureau of Economic Research",
 type = "Working Paper",
 series = "Working Paper Series",
 number = "28811",
 year = "2021",
 month = "May",
 doi = {10.3386/w28811},
 URL = "http://www.nber.org/papers/w28811",
 abstract = {Analyses of AI adoption focus on its adoption at the individual task level. What has received significantly less attention is how AI adoption is shaped by the fact that organisations are composed of many interacting tasks. AI adoption may, therefore, require system-wide change which is both a constraint and an opportunity. We provide the first formal analysis where multiple tasks may be part of a modular or non-modular system. We find that reliance on AI, a prediction tool, increases decision variation which, in turn, raises challenges if decisions across the organisation interact. Modularity, which leads to task independence rather than system-level inter-dependencies,  softens that impact. Thus, modularity can facilitate AI adoption. However, it does this at the expense of synergies. By contrast, when there are mechanisms for inter-decision coordination, AI adoption is enhanced when there is a non-modular environment. Consequently, we show that there are important cases where AI adoption will be enhanced when it can be adopted beyond tasks but as part of a designed organisational system.},
}

@article{carlini2023poisoning,
  title={Poisoning Web-Scale Training Datasets is Practical},
  author={Nicholas Carlini and Matthew Jagielski and Christopher A. Choquette-Choo and Daniel Paleka and Will Pearce and H. Anderson and A. Terzis and Kurt Thomas and Florian Tram{\`e}r},
  journal={ArXiv},
  year={2023},
  volume={abs/2302.10149}
}

@article{turow2023madrona,
  title={Foundation Models: The future (still) isn’t happening fast enough},
  author={Jon Turow and Palak Goel and Tim Porter},
  journal={Madrona},
  year={2023},
  url={https://www.madrona.com/foundation-models/}
}

@article{felten2023chatgpt,
  title={How will Language Modelers like ChatGPT Affect Occupations and Industries?},
  author={Edward W. Felten and Manav Raj and Robert C. Seamans},
  journal={SSRN Electronic Journal},
  year={2023}
}

@article{noy2023experimental,
  title={Experimental Evidence on the Productivity Effects of Generative Artificial Intelligence},
  author={Shakked Noy and Whitney Zhang},
  journal={SSRN Electronic Journal},
  year={2023}
}

@article{peng2023copilot,
  title={The Impact of AI on Developer Productivity: Evidence from GitHub Copilot},
  author={Sida Peng and Eirini Kalliamvakou and Peter Cihon and Mert Demirer},
  journal={ArXiv},
  year={2023},
  volume={abs/2302.06590}
}

@article{weingast1988industrial,
author = {Weingast, Barry R. and Marshall, William J.},
title = {The Industrial Organization of Congress; or, Why Legislatures, Like Firms, Are Not Organized as Markets},
journal = {Journal of Political Economy},
volume = {96},
number = {1},
pages = {132-163},
year = {1988},
doi = {10.1086/261528},

URL = { 
    
        https://doi.org/10.1086/261528
    
    

},
eprint = { 
    
        https://doi.org/10.1086/261528
    
    

}
,
    abstract = { This paper provides a theory of legislative institutions that parallels the theory of the firm and the theory of contractual institutions. Like market institutions, legislative institutions reflect two key components: the goals or preferences of individuals (here, representatives seeking reelection) and the relevant transactions costs. We present three conclusions. First, we show how the legislative institutions enforce bargains among legislators. Second, we explain why, given the peculiar form of bargaining problems found in legislatures, specific forms of nonmarket exchange prove superior to market exchange. Third, our approach shows how the committee system limits the types of coalitions that may form on a particular issue. }
}

@article{einav2010empirical,
Author = {Einav, Liran and Levin, Jonathan},
Title = {Empirical Industrial Organization: A Progress Report},
Journal = {Journal of Economic Perspectives},
Volume = {24},
Number = {2},
Year = {2010},
Month = {June},
Pages = {145-62},
DOI = {10.1257/jep.24.2.145},
URL = {https://www.aeaweb.org/articles?id=10.1257/jep.24.2.145}}


@article{van2013network,
  title={Network hubs in the human brain},
  author={Van den Heuvel, Martijn P and Sporns, Olaf},
  journal={Trends in cognitive sciences},
  volume={17},
  number={12},
  pages={683--696},
  year={2013},
  publisher={Elsevier}
}

@article{martin2020extending,
  title={Extending the machine learning abstraction boundary: A Complex systems approach to incorporate societal context},
  author={Martin Jr, Donald and Prabhakaran, Vinodkumar and Kuhlberg, Jill and Smart, Andrew and Isaac, William S},
  journal={arXiv preprint arXiv:2006.09663},
  year={2020}
}


@article{kleinberg1999hubs,
author = {Kleinberg, Jon M.},
title = {Hubs, Authorities, and Communities},
year = {1999},
issue_date = {Dec. 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {4es},
issn = {0360-0300},
url = {https://doi.org/10.1145/345966.345982},
doi = {10.1145/345966.345982},
journal = {ACM Comput. Surv.},
month = {dec},
pages = {5–es},
numpages = {3},
keywords = {graph algorithms, hypertext structure, link analysis, World Wide Web}
}

@ARTICLE{amironesei2021notes,
  author={Amironesei, Razvan and Denton, Emily and Hanna, Alex},
  journal={IEEE Technology and Society Magazine}, 
  title={Notes on Problem Formulation in Machine Learning}, 
  year={2021},
  volume={40},
  number={3},
  pages={80-83},
  doi={10.1109/MTS.2021.3104380}}


@article{hendricks1995hubs,
    author = {Hendricks, Ken and Piccione, Michele and Tan, Guofu},
    title = "{The Economics of Hubs: The Case of Monopoly}",
    journal = {The Review of Economic Studies},
    volume = {62},
    number = {1},
    pages = {83-99},
    year = {1995},
    month = {01},
    abstract = "{In this paper, we study the optimization problem of an unregulated air carrier which is given the exclusive right to satisfy demand for air travel between any pair of cities. It chooses a network of connections and a set of prices to maximize profits. Thus, both network design and prices are endogenous. We characterize the solution to this optimization problem when demands and costs are symmetric. Our main result is that, if there are economies of density in the number of individuals travelling between two directly connected cities, the optimal network is either a hub of size n − 1 or one in which every pair of cities is connected directly.}",
    issn = {0034-6527},
    doi = {10.2307/2297842},
    url = {https://doi.org/10.2307/2297842},
    eprint = {https://academic.oup.com/restud/article-pdf/62/1/83/4464379/62-1-83.pdf},
}

@article{franks2008extremism,
  title={Extremism propagation in social networks with hubs},
  author={Franks, Daniel W and Noble, Jason and Kaufmann, Peter and Stagl, Sigrid},
  journal={Adaptive Behavior},
  volume={16},
  number={4},
  pages={264--274},
  year={2008},
  publisher={Sage Publications Sage UK: London, England}
}

@book{fleury2014sociology,
  title={Sociology of culture and cultural practices: The transformative power of institutions},
  author={Fleury, Laurent},
  year={2014},
  publisher={Lexington Books}
}

@article{dequech2006institutions,
  title={Institutions and norms in institutional economics and sociology},
  author={Dequech, David},
  journal={Journal of Economic Issues},
  volume={40},
  number={2},
  pages={473--481},
  year={2006},
  publisher={Taylor \& Francis}
}

@book{frickel2006new,
  title={The new political sociology of science: Institutions, networks, and power},
  author={Frickel, Scott and Moore, Kelly},
  year={2006},
  publisher={Univ of Wisconsin Press}
}

@article{zhang2021ai,
  title={The AI index 2021 annual report},
  author={Zhang, Daniel and Mishra, Saurabh and Brynjolfsson, Erik and Etchemendy, John and Ganguli, Deep and Grosz, Barbara and Lyons, Terah and Manyika, James and Niebles, Juan Carlos and Sellitto, Michael and others},
  journal={arXiv preprint arXiv:2103.06312},
  year={2021}
}

@article{stone2022artificial,
  title={Artificial intelligence and life in 2030: the one hundred year study on artificial intelligence},
  author={Stone, Peter and Brooks, Rodney and Brynjolfsson, Erik and Calo, Ryan and Etzioni, Oren and Hager, Greg and Hirschberg, Julia and Kalyanakrishnan, Shivaram and Kamar, Ece and Kraus, Sarit and others},
  journal={arXiv preprint arXiv:2211.06318},
  year={2022}
}

@article{lhoest2021datasets,
  title={Datasets: A Community Library for Natural Language Processing},
  author={Quentin Lhoest and Albert Villanova del Moral and Yacine Jernite and Abhishek Thakur and Patrick von Platen and Suraj Patil and Julien Chaumond and Mariama Drame and Julien Plu and Lewis Tunstall and Joe Davison and Mario vSavsko and Gunjan Chhablani and Bhavitvya Malik and Simon Brandeis and Teven Le Scao and Victor Sanh and Canwen Xu and Nicolas Patry and Angelina McMillan-Major and Philipp Schmid and Sylvain Gugger and Clement Delangue and Th'eo Matussiere and Lysandre Debut and Stas Bekman and Pierric Cistac and Thibault Goehringer and Victor Mustar and François Lagunas and Alexander M. Rush and Thomas Wolf},
  journal={ArXiv},
  year={2021},
  volume={abs/2109.02846}
}

@misc{taori2023alpaca, 
    author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li* and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto}, 
    title  = {Alpaca: A Strong, Replicable Instruction-Following Model}, 
    url    = {https://crfm.stanford.edu/2023/03/13/alpaca.html}, 
    year = {2023}
}

@misc{sastry2021release, 
    author = {Girish Sastry}, 
    title  = {Beyond “Release” vs. “Not Release”}, 
    url    = {https://crfm.stanford.edu/commentary/2021/10/18/sastry.html}, 
    year = {2021}
}

@article{solaiman2019release,
  title={Release Strategies and the Social Impacts of Language Models},
  author={Irene Solaiman and Miles Brundage and Jack Clark and Amanda Askell and Ariel Herbert-Voss and Jeff Wu and Alec Radford and Jasmine Wang},
  journal={ArXiv},
  year={2019},
  volume={abs/1908.09203}
}

@book{rowlinson1997organisations,
  title={Organisations and institutions: perspectives in economics and sociology},
  author={Rowlinson, Michael},
  year={1997},
  publisher={Springer}
}

@article{zhou2022large,

      title={Large Language Models Are Human-Level Prompt Engineers}, 

      author={Yongchao Zhou and Andrei Ioan Muresanu and Ziwen Han and Keiran Paster and Silviu Pitis and Harris Chan and Jimmy Ba},

      year={2022},

      eprint={2211.01910},

      archivePrefix={arXiv},

      primaryClass={cs.LG}

}

@article{zhao2022inherent,
  author  = {Han Zhao and Geoffrey J. Gordon},
  title   = {Inherent Tradeoffs in Learning Fair Representations},
  journal = {Journal of Machine Learning Research},
  year    = {2022},
  volume  = {23},
  number  = {57},
  pages   = {1--26},
  url     = {http://jmlr.org/papers/v23/21-1427.html}
}

@article{liu2022prompt,
author = {Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
title = {Pre-Train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {0360-0300},
url = {https://doi.org/10.1145/3560815},
doi = {10.1145/3560815},
abstract = {This paper surveys and organizes research works in a new paradigm in natural language processing, which we dub “prompt-based learning”. Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P(y|x), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x′ that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string (hat{bm {x}} ) , from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: it allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this paper we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g.&nbsp;the choice of pre-trained language models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts, but also release other resources, e.g., a website NLPedia–Pretrain including constantly-updated survey, and paperlist.},
note = {Just Accepted},
journal = {ACM Comput. Surv.},
month = {sep},
keywords = {pre-trained language models, prompting}
}

@article{burk2019algorithmic,
  title={Algorithmic fair use},
  author={Burk, Dan L},
  journal={U. Chi. L. Rev.},
  volume={86},
  pages={283},
  year={2019},
  publisher={HeinOnline}
}

@inbook{HarmanRationality,
author = {Harman, Gilbert},
publisher = {John Wiley \& Sons, Ltd},
title = {Rationality},
booktitle = {International Encyclopedia of Ethics},
year = {2013}
}

@inproceedings{wang2013domain,
    title = "Domain-Independent Abstract Generation for Focused Meeting Summarization",
    author = "Wang, Lu  and
      Cardie, Claire",
    booktitle = "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2013",
    address = "Sofia, Bulgaria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P13-1137",
    pages = "1395--1405",
}

@book{MercierSperber,
    author = {Hugo Mercier and Dan Sperber},
    title = {The Enigma of Reason},
    year = {2017},
    publisher = {Harvard University Press}
}

@book{Dehaene,
    author = {Stanislas Dehaene},
    title = {How We Learn: Why Brains are Better than Any Machine \dots for now},
    year = {2020},
    publisher = {Penguin Random House}
}

@inproceedings{nguer2020sencorpus,
  title={SENCORPUS: A French-Wolof Parallel Corpus},
  author={Nguer, Elhadji Mamadou and Lo, Alla and Dione, Cheikh M Bamba and Ba, Sileye O and Lo, Moussa},
  booktitle={Proceedings of the 12th Language Resources and Evaluation Conference},
  pages={2803--2811},
  year={2020}
}

@inproceedings{dutta2020tradeoff,
author = {Dutta, Sanghamitra and Wei, Dennis and Yueksel, Hazar and Chen, Pin-Yu and Liu, Sijia and Varshney, Kush R.},
title = {Is There a Trade-off between Fairness and Accuracy? A Perspective Using Mismatched Hypothesis Testing},
year = {2020},
publisher = {JMLR.org},
abstract = {A trade-off between accuracy and fairness is almost taken as a given in the existing literature on fairness in machine learning. Yet, it is not preordained that accuracy should decrease with increased fairness. Novel to this work, we examine fair classification through the lens of mismatched hypothesis testing: trying to find a classifier that distinguishes between two ideal distributions when given two mismatched distributions that are biased. Using Chernoff information, a tool in information theory, we theoretically demonstrate that, contrary to popular belief, there always exist ideal distributions such that optimal fairness and accuracy (with respect to the ideal distributions) are achieved simultaneously: there is no trade-off. Moreover, the same classifier yields the lack of a trade-off with respect to ideal distributions while yielding a trade-off when accuracy is measured with respect to the given (possibly biased) dataset. To complement our main result, we formulate an optimization to find ideal distributions and derive fundamental limits to explain why a trade-off exists on the given biased dataset. We also derive conditions under which active data collection can alleviate the fairness-accuracy trade-off in the real world. Our results lead us to contend that it is problematic to measure accuracy with respect to data that reflects bias, and instead, we should be considering accuracy with respect to ideal, unbiased data.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {263},
numpages = {11},
series = {ICML'20}
}

@article{wang2021understanding,
  title={Understanding and Improving Fairness-Accuracy Trade-offs in Multi-Task Learning},
  author={Yuyan Wang and Xuezhi Wang and Alex Beutel and Flavien Prost and Jilin Chen and Ed H. Chi},
  journal={Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \& Data Mining},
  year={2021}
}

@inproceedings{
he2021deberta,
title={{\{}DEBERTA{\}}: {\{}DECODING{\}}-{\{}ENHANCED{\}} {\{}BERT{\}} {\{}WITH{\}} {\{}DISENTANGLED{\}} {\{}ATTENTION{\}}},
author={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=XPZIaotutsD}
}

@inproceedings{sambasivan2021reimagining,
author = {Sambasivan, Nithya and Arnesen, Erin and Hutchinson, Ben and Doshi, Tulsee and Prabhakaran, Vinodkumar},
title = {Re-Imagining Algorithmic Fairness in India and Beyond},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445896},
doi = {10.1145/3442188.3445896},
abstract = {Conventional algorithmic fairness is West-centric, as seen in its subgroups, values, and methods. In this paper, we de-center algorithmic fairness and analyse AI power in India. Based on 36 qualitative interviews and a discourse analysis of algorithmic deployments in India, we find that several assumptions of algorithmic fairness are challenged. We find that in India, data is not always reliable due to socio-economic factors, ML makers appear to follow double standards, and AI evokes unquestioning aspiration. We contend that localising model fairness alone can be window dressing in India, where the distance between models and oppressed communities is large. Instead, we re-imagine algorithmic fairness in India and provide a roadmap to re-contextualise data and models, empower oppressed communities, and enable Fair-ML ecosystems.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {315–328},
numpages = {14},
keywords = {India, decoloniality, ability, algorithmic fairness, gender, anti-caste politics, religion, caste, feminism, class, critical algorithmic studies},
location = {Virtual Event, Canada},
series = {FAccT '21}
}

@article{vspetko2021dgx,
  title={DGX-A100 Face to Face DGX-2—Performance, Power and Thermal Behavior Evaluation},
  author={{\v{S}}pet'ko, Matej and Vysock{\`y}, Ond{\v{r}}ej and Jans{\'\i}k, Branislav and {\v{R}}{\'\i}ha, Lubom{\'\i}r},
  journal={Energies},
  volume={14},
  number={2},
  pages={376},
  year={2021},
  publisher={MDPI}
}

@inproceedings{kiela2021dynabench,
    title = "Dynabench: Rethinking Benchmarking in {NLP}",
    author = "Kiela, Douwe  and
      Bartolo, Max  and
      Nie, Yixin  and
      Kaushik, Divyansh  and
      Geiger, Atticus  and
      Wu, Zhengxuan  and
      Vidgen, Bertie  and
      Prasad, Grusha  and
      Singh, Amanpreet  and
      Ringshia, Pratik  and
      Ma, Zhiyi  and
      Thrush, Tristan  and
      Riedel, Sebastian  and
      Waseem, Zeerak  and
      Stenetorp, Pontus  and
      Jia, Robin  and
      Bansal, Mohit  and
      Potts, Christopher  and
      Williams, Adina",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.324",
    doi = "10.18653/v1/2021.naacl-main.324",
    pages = "4110--4124",
    abstract = "We introduce Dynabench, an open-source platform for dynamic dataset creation and model benchmarking. Dynabench runs in a web browser and supports human-and-model-in-the-loop dataset creation: annotators seek to create examples that a target model will misclassify, but that another person will not. In this paper, we argue that Dynabench addresses a critical need in our community: contemporary models quickly achieve outstanding performance on benchmark tasks but nonetheless fail on simple challenge examples and falter in real-world scenarios. With Dynabench, dataset creation, model development, and model assessment can directly inform each other, leading to more robust and informative benchmarks. We report on four initial NLP tasks, illustrating these concepts and highlighting the promise of the platform, and address potential objections to dynamic benchmarking as a new standard for the field.",
}

@article{loevinger1957objective,
author = {Jane Loevinger},
title ={Objective Tests as Instruments of Psychological Theory},
journal = {Psychological Reports},
volume = {3},
number = {3},
pages = {635-694},
year = {1957},
doi = {10.2466/pr0.1957.3.3.635},
URL = {https://doi.org/10.2466/pr0.1957.3.3.635},
eprint = {https://doi.org/10.2466/pr0.1957.3.3.635}
}

@article{goldman1958speech,
author = {Frieda Goldman-Eisler},
title ={Speech Production and the Predictability of Words in Context},
journal = {Quarterly Journal of Experimental Psychology},
volume = {10},
number = {2},
pages = {96-106},
year = {1958},
doi = {10.1080/17470215808416261},

URL = { 
        https://doi.org/10.1080/17470215808416261
    
},
eprint = { 
        https://doi.org/10.1080/17470215808416261
    
}
,
    abstract = { The purpose of the experiments was to examine the function of hesitation pauses in speech. Pauses were conceived of as anticipating increase of information in subsequent speech and as involving acts of choice.This hypothesis was tested by relating the incidence of pauses within sentences to the transition probabilities of the words constituting them. Estimates of these probabilities were obtained experimentally by an adaptation of Shannon's guessing technique and were based on reverse as well as forward guessing. The hypothesis was borne out by the facts; hesitancy in speech was shown to be closely related to uncertainty of prediction (entropy) and fluency of utterance to redundancy. These results are shown to be in line with the facts of language statistics. Their theoretical implications for the concept of information and for understanding the processes of speech organization is discussed. }
}

@ARTICLE{baker1975dragon,
    author = {James K. Baker},
    title = {The dragon system – an overview},
    journal = {IEEE Transactions on Acoustic Speech Signal Processing},
    year = {1975}
}

@inbook{baker1975stochastic,
author = {Baker, James K.},
title = {Stochastic Modeling for Automatic Speech Understanding},
year = {1975},
isbn = {1558601244},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
booktitle = {Readings in Speech Recognition},
pages = {297–307},
numpages = {11}
}

@article{jelinek1976continuous,
  title={Continuous speech recognition by statistical methods},
  author={Frederick Jelinek},
  journal={Proceedings of the IEEE},
  year={1976},
  volume={64},
  pages={532-556}
}

@inbook{jelinek1990self,
author = {Jelinek, Frederick},
title = {Self-Organized Language Modeling for Speech Recognition},
year = {1990},
isbn = {1558601244},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
booktitle = {Readings in Speech Recognition},
pages = {450–506},
numpages = {57}
}

@article{hendrycksmath2021,
  title={Measuring Mathematical Problem Solving With the MATH Dataset},
  author={Dan Hendrycks and Collin Burns and Saurav Kadavath and Akul Arora and Steven Basart and Eric Tang and Dawn Song and Jacob Steinhardt},
  journal={NeurIPS},
  year={2021}
}

@inproceedings{
liao2021are,
title={Are We Learning Yet? A Meta Review of Evaluation Failures Across Machine Learning},
author={Thomas Liao and Rohan Taori and Inioluwa Deborah Raji and Ludwig Schmidt},
booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)},
year={2021},
url={https://openreview.net/forum?id=mPducS1MsEK}
}

@inproceedings{Clark2020TransformersAS,
  title={Transformers as Soft Reasoners over Language},
  author={Peter Clark and Oyvind Tafjord and Kyle Richardson},
  booktitle={IJCAI},
  year={2020}
}

@article{chomsky1957syntactic,
  title={Syntactic structures},
  author={Chomsky, Noam},
  journal={The Hague, The Netherlands},
  year={1957}
}

@article{anthony2020carbontracker,
  title={Carbontracker: Tracking and predicting the carbon footprint of training deep learning models},
  author={Anthony, Lasse F Wolff and Kanding, Benjamin and Selvan, Raghavendra},
  journal={arXiv preprint arXiv:2007.03051},
  year={2020}
}


@article{cao2020towards,
  title={Towards accurate and reliable energy measurement of NLP models},
  author={Cao, Qingqing and Balasubramanian, Aruna and Balasubramanian, Niranjan},
  journal={arXiv preprint arXiv:2010.05248},
  year={2020}
}


@book{salton1971smart,
  title={The SMART retrieval system—experiments in automatic document processing},
  author={Salton, Gerard},
  year={1971},
  publisher={Prentice-Hall, Inc.}
}

@article{jones1972statistical,
  title={A statistical interpretation of term specificity and its application in retrieval},
  author={Sp\"arck Jones, Karen},
  journal={Journal of documentation},
  year={1972},
  publisher={MCB UP Ltd}
}

@book{salton1983introduction,
  title={Introduction to modern information retrieval},
  author={Salton, Gerard and McGill, Michael J},
  year={1983},
  publisher={mcgraw-hill}
}


@article{messick1987validity,
  title={Validity},
  author={Messick, Samuel},
  journal={ETS Research Report Series},
  volume={1987},
  number={2},
  pages={i--208},
  year={1987},
  publisher={Wiley Online Library},
  url={https://onlinelibrary.wiley.com/doi/abs/10.1002/j.2330-8516.1987.tb00244.x}
}

@article{messick1988assessing,
  title={The once and future issues of validity: Assessing the meaning and consequences of measurement.},
  author={Messick, Samuel},
  year={1988},
  publisher={Lawrence Erlbaum Associates, Inc},
  url={https://onlinelibrary.wiley.com/doi/abs/10.1002/j.2330-8516.1986.tb00185.x},
  journal = {ETS Research Report Series}
}

@article{crenshaw1989intersectional,
  title={Demarginalizing the intersection of race and sex: A black feminist critique of antidiscrimination doctrine, feminist theory and antiracist politics},
  author={Crenshaw, Kimberl{\'e}},
  journal={University of Chicago Legal Forum},
  volume={Vol.1989, Article 8},
  year={1989},
  url={https://chicagounbound.uchicago.edu/cgi/viewcontent.cgi?article=1052&context=uclf}
}

@article{friedman1996bias,
author = {Friedman, Batya and Nissenbaum, Helen},
title = {Bias in Computer Systems},
year = {1996},
issue_date = {July 1996},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/230538.230561},
doi = {10.1145/230538.230561},
abstract = {From an analysis of actual cases, three categories of bias in computer systems have been developed: preexisting, technical, and emergent. Preexisting bias has its roots in social institutions, practices, and attitudes. Technical bias arises from technical constraints of considerations. Emergent bias arises in a context of use. Although others have pointed to bias inparticular computer systems and have noted the general problem, we know of no comparable work that examines this phenomenon comprehensively and which offers a framework for understanding and remedying it. We conclude by suggesting that freedom from bias should by counted amoung the select set of criteria—including reliability, accuracy, and efficiency—according to which the quality of systems in use in society should be judged.},
journal = {ACM Transactions on Information Systems},
month = jul,
pages = {330–347},
numpages = {18},
keywords = {computers and society, standards, human values, universal design, ethics, computer ethics, social impact, design methods, system design, bias, social computing, values}
}

@article{strathern1997improving,
  title={{‘Improving ratings’: audit in the British University system}},
  author={Strathern, Marilyn},
  journal={European Review},
  volume={5},
  number={3},
  pages={305--321},
  year={1997},
  publisher={Cambridge University Press},
  url={https://www.cambridge.org/core/journals/european-review/article/abs/improving-ratings-audit-in-the-british-university-system/FC2EE640C0C44E3DB87C29FB666E9AAB}
}

@inproceedings{yang1997comparative,
  title={A Comparative Study on Feature Selection in Text Categorization},
  author={Yang, Yiming and Pedersen, Jan O.},
  booktitle={Proceedings of the Fourteenth International Conference on Machine Learning},
  pages={412--420},
  year={1997}
}

@inproceedings{joachims1998svm,
  title={Text categorization with support vector machines: Learning with many relevant features},
  author={Joachims, Thorsten},
  booktitle={European conference on machine learning},
  pages={137--142},
  year={1998},
  organization={Springer}
}

@article{yang1999evaluation,
  title={An evaluation of statistical approaches to text categorization},
  author={Yang, Yiming},
  journal={Information retrieval},
  volume={1},
  number={1-2},
  pages={69--90},
  year={1999},
  publisher={Springer}
}

@book{mani1999advances,
 author = {Mani, Inderjeet},
 editor = {Maybury, Mark T.},
 title = {Advances in Automatic Text Summarization},
 year = {1999},
 isbn = {0262133598},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 

@article{sparck1999automatic,
  title={Automatic summarizing: factors and directions},
  author={Sp\"arck Jones, Karen},
  journal={Advances in automatic text summarization},
  pages={1--12},
  year={1999},
  publisher={Cambridge, MA: MIT Press}
}

@inproceedings{hale2001probabilistic,
    title = "A Probabilistic {E}arley Parser as a Psycholinguistic Model",
    author = "Hale, John",
    booktitle = "Second Meeting of the North {A}merican Chapter of the Association for Computational Linguistics",
    year = "2001",
    url = "https://aclanthology.org/N01-1021",
}

@inproceedings{turney2002thumbs,
    title = "Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews",
    author = "Turney, Peter",
    booktitle = "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2002",
    address = "Philadelphia, Pennsylvania, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P02-1053",
    doi = "10.3115/1073083.1073153",
    pages = "417--424",
}

@inproceedings{pang2002thumbs,
    title = "Thumbs up? Sentiment Classification using Machine Learning Techniques",
    author = "Pang, Bo  and
      Lee, Lillian  and
      Vaithyanathan, Shivakumar",
    booktitle = "Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing ({EMNLP} 2002)",
    month = jul,
    year = "2002",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W02-1011",
    doi = "10.3115/1118693.1118704",
    pages = "79--86",
}

@article{wiebe2005annotating,
  title={Annotating Expressions of Opinions and Emotions in Language},
  author={Janyce Wiebe and Theresa Wilson and Claire Cardie},
  journal={Language Resources and Evaluation},
  year={2005},
  volume={39},
  pages={165-210}
}

@article{jones2005acl,
    title = "{ACL} Lifetime Achievement Award: Some Points in a Time",
    author = {Sp{\"a}rck Jones, Karen},
    journal = "Computational Linguistics",
    volume = "31",
    number = "1",
    year = "2005",
    url = "https://aclanthology.org/J05-1001",
    doi = "10.1162/0891201053630237",
    pages = "1--14",
}

@article{pang2008opinion,
author = {Pang, Bo and Lee, Lillian},
title = {Opinion Mining and Sentiment Analysis},
year = {2008},
issue_date = {January 2008},
publisher = {Now Publishers Inc.},
address = {Hanover, MA, USA},
volume = {2},
number = {1–2},
issn = {1554-0669},
url = {https://doi.org/10.1561/1500000011},
doi = {10.1561/1500000011},
abstract = {An important part of our information-gathering behavior has always been to find out what other people think. With the growing availability and popularity of opinion-rich resources such as online review sites and personal blogs, new opportunities and challenges arise as people now can, and do, actively use information technologies to seek out and understand the opinions of others. The sudden eruption of activity in the area of opinion mining and sentiment analysis, which deals with the computational treatment of opinion, sentiment, and subjectivity in text, has thus occurred at least in part as a direct response to the surge of interest in new systems that deal directly with opinions as a first-class object.This survey covers techniques and approaches that promise to directly enable opinion-oriented information-seeking systems. Our focus is on methods that seek to address the new challenges raised by sentiment-aware applications, as compared to those that are already present in more traditional fact-based analysis. We include material on summarization of evaluative text and on broader issues regarding privacy, manipulation, and economic impact that the development of opinion-oriented information-access services gives rise to. To facilitate future work, a discussion of available resources, benchmark datasets, and evaluation campaigns is also provided.},
journal = {Found. Trends Inf. Retr.},
month = {jan},
pages = {1–135},
numpages = {135}
}

@book{jackman2008measurement,
  title={Measurement},
  author={Jackman, Simon},
  series={The Oxford Handbook of Political Methodology},
  url={https://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780199286546.001.0001/oxfordhb-9780199286546-e-6},
  year={2008},
  publisher={Oxford Handbooks}
}

@article{levy2008expectation,
title = {Expectation-based syntactic comprehension},
journal = {Cognition},
volume = {106},
number = {3},
pages = {1126-1177},
year = {2008},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2007.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S0010027707001436},
author = {Roger Levy},
keywords = {Parsing, Frequency, Sentence processing, Information theory, Prediction, Syntax, Word order, Syntactic complexity},
abstract = {This paper investigates the role of resource allocation as a source of processing difficulty in human sentence comprehension. The paper proposes a simple information-theoretic characterization of processing difficulty as the work incurred by resource reallocation during parallel, incremental, probabilistic disambiguation in sentence comprehension, and demonstrates its equivalence to the theory of Hale [Hale, J. (2001). A probabilistic Earley parser as a psycholinguistic model. In Proceedings of NAACL (Vol. 2, pp. 159–166)], in which the difficulty of a word is proportional to its surprisal (its negative log-probability) in the context within which it appears. This proposal subsumes and clarifies findings that high-constraint contexts can facilitate lexical processing, and connects these findings to well-known models of parallel constraint-based comprehension. In addition, the theory leads to a number of specific predictions about the role of expectation in syntactic comprehension, including the reversal of locality-based difficulty patterns in syntactically constrained contexts, and conditions under which increased ambiguity facilitates processing. The paper examines a range of established results bearing on these predictions, and shows that they are largely consistent with the surprisal theory.}
}

@incollection{aggarwal2012survey,
  title={A survey of text classification algorithms},
  author={Aggarwal, Charu C. and Zhai, ChengXiang},
  booktitle={Mining text data},
  pages={163--222},
  year={2012},
  publisher={Springer}
}

@incollection{nenkova2012survey,
  title={A survey of text summarization techniques},
  author={Nenkova, Ani and McKeown, Kathleen},
  booktitle={Mining text data},
  pages={43--76},
  year={2012},
  publisher={Springer}
}

@article{mcauley2012learning,
  title={Learning Attitudes and Attributes from Multi-aspect Reviews},
  author={Julian McAuley and Jure Leskovec and Dan Jurafsky},
  journal={2012 IEEE 12th International Conference on Data Mining},
  year={2012},
  pages={1020-1025}
}

@article{sweeney2013discrimination,
 author = {Sweeney, Latanya},
 title = {Discrimination in Online Ad Delivery},
 journal = {Queue},
 issue_date = {March 2013},
 volume = {11},
 number = {3},
 month = mar,
 year = {2013},
 issn = {1542-7730},
 pages = {10:10--10:29},
 articleno = {10},
 numpages = {20},
 url = {http://doi.acm.org/10.1145/2460276.2460278},
 doi = {10.1145/2460276.2460278},
 acmid = {2460278},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@inproceedings{nakov2016semeval,
    title = "{S}em{E}val-2016 Task 4: Sentiment Analysis in {T}witter",
    author = "Nakov, Preslav  and
      Ritter, Alan  and
      Rosenthal, Sara  and
      Sebastiani, Fabrizio  and
      Stoyanov, Veselin",
    booktitle = "Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016)",
    month = jun,
    year = "2016",
    address = "San Diego, California",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/S16-1001",
    doi = "10.18653/v1/S16-1001",
    pages = "1--18",
}

@article{linzen2016lstm,
    author = {Linzen, Tal and Dupoux, Emmanuel and Goldberg, Yoav},
    title = "{Assessing the Ability of LSTMs to Learn Syntax-Sensitive
                    Dependencies}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {4},
    pages = {521-535},
    year = {2016},
    month = {12},
    abstract = "{The success of long short-term memory (LSTM) neural networks in language
                    processing is typically attributed to their ability to capture long-distance
                    statistical regularities. Linguistic regularities are often sensitive to
                    syntactic structure; can such dependencies be captured by LSTMs, which do not
                    have explicit structural representations? We begin addressing this question
                    using number agreement in English subject-verb dependencies. We probe the
                    architecture’s grammatical competence both using training objectives with an
                    explicit grammatical target (number prediction, grammaticality judgments) and
                    using language models. In the strongly supervised settings, the LSTM achieved
                    very high overall accuracy (less than 1\\% errors), but errors increased when
                    sequential and structural information conflicted. The frequency of such errors
                    rose sharply in the language-modeling setting. We conclude that LSTMs can
                    capture a non-trivial amount of grammatical structure given targeted
                    supervision, but stronger architectures may be required to further reduce
                    errors; furthermore, the language modeling signal is insufficient for capturing
                    syntax-sensitive dependencies, and should be supplemented with more direct
                    supervision if such dependencies need to be captured.}",
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00115},
    url = {https://doi.org/10.1162/tacl\_a\_00115},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00115/1567418/tacl\_a\_00115.pdf},
}

@inproceedings{schmidt2017survey,
    title = "A Survey on Hate Speech Detection using Natural Language Processing",
    author = "Schmidt, Anna  and
      Wiegand, Michael",
    booktitle = "Proceedings of the Fifth International Workshop on Natural Language Processing for Social Media",
    month = apr,
    year = "2017",
    address = "Valencia, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W17-1101",
    doi = "10.18653/v1/W17-1101",
    pages = "1--10",
    abstract = "This paper presents a survey on hate speech detection. Given the steadily growing body of social media content, the amount of online hate speech is also increasing. Due to the massive scale of the web, methods that automatically detect hate speech are required. Our survey describes key areas that have been explored to automatically recognize these types of utterances using natural language processing. We also discuss limits of those approaches.",
}

@article{merity2018analysis,
  title={An Analysis of Neural Language Modeling at Multiple Scales},
  author={Stephen Merity and Nitish Shirish Keskar and Richard Socher},
  journal={ArXiv},
  year={2018},
  volume={abs/1803.08240}
}

@inproceedings{grusky2018newsroom,
    title = "{N}ewsroom: A Dataset of 1.3 Million Summaries with Diverse Extractive Strategies",
    author = "Grusky, Max  and
      Naaman, Mor  and
      Artzi, Yoav",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N18-1065",
    doi = "10.18653/v1/N18-1065",
    pages = "708--719",
    abstract = "We present NEWSROOM, a summarization dataset of 1.3 million articles and summaries written by authors and editors in newsrooms of 38 major news publications. Extracted from search and social media metadata between 1998 and 2017, these high-quality summaries demonstrate high diversity of summarization styles. In particular, the summaries combine abstractive and extractive strategies, borrowing words and phrases from articles at varying rates. We analyze the extraction strategies used in NEWSROOM summaries against other datasets to quantify the diversity and difficulty of our new data, and train existing methods on the data to evaluate its utility and challenges. The dataset is available online at summari.es.",
}

@misc{gokaslan2019openwebtext,  
	title={OpenWebText Corpus},
	author={Aaron Gokaslan and Vanya Cohen},
	howpublished={\url{http://Skylion007.github.io/OpenWebTextCorpus}}, 
	year={2019}
}

@inproceedings{peyrard2019simple,
    title = "A Simple Theoretical Model of Importance for Summarization",
    author = "Peyrard, Maxime",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1101",
    pages = "1059--1073",
    abstract = "Research on summarization has mainly been driven by empirical approaches, crafting systems to perform well on standard datasets with the notion of information Importance remaining latent. We argue that establishing theoretical models of Importance will advance our understanding of the task and help to further improve summarization systems. To this end, we propose simple but rigorous definitions of several concepts that were previously used only intuitively in summarization: Redundancy, Relevance, and Informativeness. Importance arises as a single quantity naturally unifying these concepts. Additionally, we provide intuitions to interpret the proposed quantities and experiments to demonstrate the potential of the framework to inform and guide subsequent works.",
}

@book{benjamin2019race,
  title={Race after Technology},
  author={Benjamin, Ruha},
  publisher={Polity Press},
  year= {2019},
}

@InProceedings{chen2020imagegpt,
  title = 	 {Generative Pretraining From Pixels},
  author =       {Chen, Mark and Radford, Alec and Child, Rewon and Wu, Jeffrey and Jun, Heewoo and Luan, David and Sutskever, Ilya},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {1691--1703},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/chen20s/chen20s.pdf},
  url = 	 {http://proceedings.mlr.press/v119/chen20s.html},
  abstract = 	 {Inspired by progress in unsupervised representation learning for natural language, we examine whether similar models can learn useful representations for images. We train a sequence Transformer to auto-regressively predict pixels, without incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels, we find that a GPT-2 scale model learns strong image representations as measured by linear probing, fine-tuning, and low-data classification. On CIFAR-10, we achieve 96.3% accuracy with a linear probe, outperforming a supervised Wide ResNet, and 99.0% accuracy with full fine-tuning, matching the top supervised pre-trained models. We are also competitive with self-supervised benchmarks on ImageNet when substituting pixels for a VQVAE encoding, achieving 69.0% top-1 accuracy on a linear probe of our features.}
}


@InProceedings{ramesh2021dalle,
  title = 	 {Zero-Shot Text-to-Image Generation},
  author =       {Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8821--8831},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/ramesh21a/ramesh21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/ramesh21a.html},
  abstract = 	 {Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.}
}


@article{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  journal={arXiv preprint arXiv:2103.00020},
  year={2021}
}

@inproceedings{bordia2019bias,
    title = "Identifying and Reducing Gender Bias in Word-Level Language Models",
    author = "Bordia, Shikha  and
      Bowman, Samuel R.",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Student Research Workshop",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-3002",
    doi = "10.18653/v1/N19-3002",
    pages = "7--15",
    abstract = "Many text corpora exhibit socially problematic biases, which can be propagated or amplified in the models trained on such data. For example, doctor cooccurs more frequently with male pronouns than female pronouns. In this study we (i) propose a metric to measure gender bias; (ii) measure bias in a text corpus and the text generated from a recurrent neural network language model trained on the text corpus; (iii) propose a regularization loss term for the language model that minimizes the projection of encoder-trained embeddings onto an embedding subspace that encodes gender; (iv) finally, evaluate efficacy of our proposed method on reducing gender bias. We find this regularization method to be effective in reducing gender bias up to an optimal weight assigned to the loss term, beyond which the model becomes unstable as the perplexity increases. We replicate this study on three training corpora{---}Penn Treebank, WikiText-2, and CNN/Daily Mail{---}resulting in similar conclusions.",
}

@inproceedings{ethayarajh2019undesirable,
    title = "Understanding Undesirable Word Embedding Associations",
    author = "Ethayarajh, Kawin  and
      Duvenaud, David  and
      Hirst, Graeme",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1166",
    doi = "10.18653/v1/P19-1166",
    pages = "1696--1705",
    abstract = "Word embeddings are often criticized for capturing undesirable word associations such as gender stereotypes. However, methods for measuring and removing such biases remain poorly understood. We show that for any embedding model that implicitly does matrix factorization, debiasing vectors post hoc using subspace projection (Bolukbasi et al., 2016) is, under certain conditions, equivalent to training on an unbiased corpus. We also prove that WEAT, the most common association test for word embeddings, systematically overestimates bias. Given that the subspace projection method is provably effective, we use it to derive a new measure of association called the relational inner product association (RIPA). Experiments with RIPA reveal that, on average, skipgram with negative sampling (SGNS) does not make most words any more gendered than they are in the training corpus. However, for gender-stereotyped words, SGNS actually amplifies the gender association in the corpus.",
}

@inproceedings{bommasani2020intrinsic,
    title = "Intrinsic Evaluation of Summarization Datasets",
    author = "Bommasani, Rishi  and
      Cardie, Claire",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.649",
    doi = "10.18653/v1/2020.emnlp-main.649",
    pages = "8075--8096",
    abstract = "High quality data forms the bedrock for building meaningful statistical models in NLP. Consequently, data quality must be evaluated either during dataset construction or *post hoc*. Almost all popular summarization datasets are drawn from natural sources and do not come with inherent quality assurance guarantees. In spite of this, data quality has gone largely unquestioned for many of these recent datasets. We perform the first large-scale evaluation of summarization datasets by introducing 5 intrinsic metrics and applying them to 10 popular datasets. We find that data usage in recent summarization research is sometimes inconsistent with the underlying properties of the data. Further, we discover that our metrics can serve the additional purpose of being inexpensive heuristics for detecting generically low quality examples.",
}

@inproceedings{linzen2020accelerate,
    title = "How Can We Accelerate Progress Towards Human-like Linguistic Generalization?",
    author = "Linzen, Tal",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.465",
    doi = "10.18653/v1/2020.acl-main.465",
    pages = "5210--5217",
    abstract = "This position paper describes and critiques the Pretraining-Agnostic Identically Distributed (PAID) evaluation paradigm, which has become a central tool for measuring progress in natural language understanding. This paradigm consists of three stages: (1) pre-training of a word prediction model on a corpus of arbitrary size; (2) fine-tuning (transfer learning) on a training set representing a classification task; (3) evaluation on a test set drawn from the same distribution as that training set. This paradigm favors simple, low-bias architectures, which, first, can be scaled to process vast amounts of data, and second, can capture the fine-grained statistical properties of a particular data set, regardless of whether those properties are likely to generalize to examples of the task outside the data set. This contrasts with humans, who learn language from several orders of magnitude less data than the systems favored by this evaluation paradigm, and generalize to new tasks in a consistent way. We advocate for supplementing or replacing PAID with paradigms that reward architectures that generalize as quickly and robustly as humans.",
}

@inproceedings{gauthier2020syntaxgym,
    title = "{S}yntax{G}ym: An Online Platform for Targeted Evaluation of Language Models",
    author = "Gauthier, Jon  and
      Hu, Jennifer  and
      Wilcox, Ethan  and
      Qian, Peng  and
      Levy, Roger",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-demos.10",
    doi = "10.18653/v1/2020.acl-demos.10",
    pages = "70--76",
    abstract = "Targeted syntactic evaluations have yielded insights into the generalizations learned by neural network language models. However, this line of research requires an uncommon confluence of skills: both the theoretical knowledge needed to design controlled psycholinguistic experiments, and the technical proficiency needed to train and deploy large-scale language models. We present SyntaxGym, an online platform designed to make targeted evaluations accessible to both experts in NLP and linguistics, reproducible across computing environments, and standardized following the norms of psycholinguistic experimental design. This paper releases two tools of independent value for the computational linguistics community: 1. A website, syntaxgym.org, which centralizes the process of targeted syntactic evaluation and provides easy tools for analysis and visualization; 2. Two command-line tools, {`}syntaxgym{`} and {`}lm-zoo{`}, which allow any user to reproduce targeted syntactic evaluations and general language model inference on their own machine.",
}

@inproceedings{zhang2020pegasus,
author = {Zhang, Jingqing and Zhao, Yao and Saleh, Mohammad and Liu, Peter J.},
title = {PEGASUS: Pre-Training with Extracted Gap-Sentences for Abstractive Summarization},
year = {2020},
publisher = {JMLR.org},
abstract = {Recent work pre-training Transformers with self-supervised objectives on large text corpora has shown great success when fine-tuned on downstream NLP tasks including text summarization. However, pre-training objectives tailored for abstractive text summarization have not been explored. Furthermore there is a lack of systematic evaluation across diverse domains. In this work, we propose pretraining large Transformer-based encoder-decoder models on massive text corpora with a new self-supervised objective. In PEGASUS, important sentences are removed/masked from an input document and are generated together as one output sequence from the remaining sentences, similar to an extractive summary. We evaluated our best PEGASUS model on 12 downstream summarization tasks spanning news, science, stories, instructions, emails, patents, and legislative bills. Experiments demonstrate it achieves state-of-the-art performance on all 12 downstream datasets measured by ROUGE scores. Our model also shows surprising performance on low-resource summarization, surpassing previous state-of-the-art results on 6 datasets with only 1000 examples. Finally we validated our results using human evaluation and show that our model summaries achieve human performance on multiple datasets.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {1051},
numpages = {12},
series = {ICML'20}
}

@inproceedings{blodgett2020critical,
    title = "Language (Technology) is Power: A Critical Survey of {``}Bias{''} in {NLP}",
    author = "Blodgett, Su Lin  and
      Barocas, Solon  and
      Daum{\'e} III, Hal  and
      Wallach, Hanna",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.485",
    doi = "10.18653/v1/2020.acl-main.485",
    pages = "5454--5476",
    abstract = "We survey 146 papers analyzing {``}bias{''} in NLP systems, finding that their motivations are often vague, inconsistent, and lacking in normative reasoning, despite the fact that analyzing {``}bias{''} is an inherently normative process. We further find that these papers{'} proposed quantitative techniques for measuring or mitigating {``}bias{''} are poorly matched to their motivations and do not engage with the relevant literature outside of NLP. Based on these findings, we describe the beginnings of a path forward by proposing three recommendations that should guide work analyzing {``}bias{''} in NLP systems. These recommendations rest on a greater recognition of the relationships between language and social hierarchies, encouraging researchers and practitioners to articulate their conceptualizations of {``}bias{''}---i.e., what kinds of system behaviors are harmful, in what ways, to whom, and why, as well as the normative reasoning underlying these statements{---}and to center work around the lived experiences of members of communities affected by NLP systems, while interrogating and reimagining the power relations between technologists and such communities.",
}

@inproceedings{jo2020archives,
author = {Jo, Eun Seo and Gebru, Timnit},
title = {Lessons from Archives: Strategies for Collecting Sociocultural Data in Machine Learning},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372829},
doi = {10.1145/3351095.3372829},
abstract = {A growing body of work shows that many problems in fairness, accountability, transparency, and ethics in machine learning systems are rooted in decisions surrounding the data collection and annotation process. In spite of its fundamental nature however, data collection remains an overlooked part of the machine learning (ML) pipeline. In this paper, we argue that a new specialization should be formed within ML that is focused on methodologies for data collection and annotation: efforts that require institutional frameworks and procedures. Specifically for sociocultural data, parallels can be drawn from archives and libraries. Archives are the longest standing communal effort to gather human information and archive scholars have already developed the language and procedures to address and discuss many challenges pertaining to data collection such as consent, power, inclusivity, transparency, and ethics &amp; privacy. We discuss these five key approaches in document collection practices in archives that can inform data collection in sociocultural ML. By showing data collection practices from another field, we encourage ML research to be more cognizant and systematic in data collection and draw from interdisciplinary expertise.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {306–316},
numpages = {11},
keywords = {datasets, machine learning, ML fairness, data collection, sociocultural data, archives},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{shin2020autoprompt,
    title = "{A}uto{P}rompt: {E}liciting {K}nowledge from {L}anguage {M}odels with {A}utomatically {G}enerated {P}rompts",
    author = "Shin, Taylor  and
      Razeghi, Yasaman  and
      Logan IV, Robert L.  and
      Wallace, Eric  and
      Singh, Sameer",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.346",
    doi = "10.18653/v1/2020.emnlp-main.346",
    pages = "4222--4235",
    abstract = "The remarkable success of pretrained language models has motivated the study of what kinds of knowledge these models learn during pretraining. Reformulating tasks as fill-in-the-blanks problems (e.g., cloze tests) is a natural approach for gauging such knowledge, however, its usage is limited by the manual effort and guesswork required to write suitable prompts. To address this, we develop AutoPrompt, an automated method to create prompts for a diverse set of tasks, based on a gradient-guided search. Using AutoPrompt, we show that masked language models (MLMs) have an inherent capability to perform sentiment analysis and natural language inference without additional parameters or finetuning, sometimes achieving performance on par with recent state-of-the-art supervised models. We also show that our prompts elicit more accurate factual knowledge from MLMs than the manually created prompts on the LAMA benchmark, and that MLMs can be used as relation extractors more effectively than supervised relation extraction models. These results demonstrate that automatically generated prompts are a viable parameter-free alternative to existing probing methods, and as pretrained LMs become more sophisticated and capable, potentially a replacement for finetuning.",
}

@inproceedings{hovy2021importance,
    title = "The Importance of Modeling Social Factors of Language: Theory and Practice",
    author = "Hovy, Dirk  and
      Yang, Diyi",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.49",
    doi = "10.18653/v1/2021.naacl-main.49",
    pages = "588--602",
    abstract = "Natural language processing (NLP) applications are now more powerful and ubiquitous than ever before. With rapidly developing (neural) models and ever-more available data, current NLP models have access to more information than any human speaker during their life. Still, it would be hard to argue that NLP models have reached human-level capacity. In this position paper, we argue that the reason for the current limitations is a focus on information content while ignoring language{'}s social factors. We show that current NLP systems systematically break down when faced with interpreting the social factors of language. This limits applications to a subset of information-related tasks and prevents NLP from reaching human-level performance. At the same time, systems that incorporate even a minimum of social factors already show remarkable improvements. We formalize a taxonomy of seven social factors based on linguistic theory and exemplify current failures and emerging successes for each of them. We suggest that the NLP community address social factors to get closer to the goal of human-like language understanding.",
}

@inproceedings{bowman2021fix,
    title = "What Will it Take to Fix Benchmarking in Natural Language Understanding?",
    author = "Bowman, Samuel R.  and
      Dahl, George",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2021.naacl-main.385",
    doi = "10.18653/v1/2021.naacl-main.385",
    pages = "4843--4855",
    abstract = "Evaluation for many natural language understanding (NLU) tasks is broken: Unreliable and biased systems score so highly on standard benchmarks that there is little room for researchers who develop better systems to demonstrate their improvements. The recent trend to abandon IID benchmarks in favor of adversarially-constructed, out-of-distribution test sets ensures that current models will perform poorly, but ultimately only obscures the abilities that we want our benchmarks to measure. In this position paper, we lay out four criteria that we argue NLU benchmarks should meet. We argue most current benchmarks fail at these criteria, and that adversarial data collection does not meaningfully address the causes of these failures. Instead, restoring a healthy evaluation ecosystem will require significant progress in the design of benchmark datasets, the reliability with which they are annotated, their size, and the ways they handle social bias.",
}

@inproceedings{alex2021raft,
 author = {Alex, Neel and Lifland, Eli and Tunstall, Lewis and Thakur, Abhishek and Maham, Pegah and Riedel, C. and Hine, Emmie and Ashurst, Carolyn and Sedille, Paul and Carlier, Alexis and Noetel, Michael and Stuhlm\"{u}ller, Andreas},
 booktitle = {Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks},
 editor = {J. Vanschoren and S. Yeung},
 pages = {},
 title = {RAFT: A Real-World Few-Shot Text Classification Benchmark},
 url = {https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/file/ca46c1b9512a7a8315fa3c5a946e8265-Paper-round2.pdf},
 volume = {1},
 year = {2021}
}

@inproceedings{potts2021dynasent,
    title = "{D}yna{S}ent: A Dynamic Benchmark for Sentiment Analysis",
    author = "Potts, Christopher  and
      Wu, Zhengxuan  and
      Geiger, Atticus  and
      Kiela, Douwe",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.186",
    doi = "10.18653/v1/2021.acl-long.186",
    pages = "2388--2404",
    abstract = "We introduce DynaSent ({`}Dynamic Sentiment{'}), a new English-language benchmark task for ternary (positive/negative/neutral) sentiment analysis. DynaSent combines naturally occurring sentences with sentences created using the open-source Dynabench Platform, which facilities human-and-model-in-the-loop dataset creation. DynaSent has a total of 121,634 sentences, each validated by five crowdworkers, and its development and test splits are designed to produce chance performance for even the best models we have been able to develop; when future models solve this task, we will use them to create DynaSent version 2, continuing the dynamic evolution of this benchmark. Here, we report on the dataset creation effort, focusing on the steps we took to increase quality and reduce artifacts. We also present evidence that DynaSent{'}s Neutral category is more coherent than the comparable category in other benchmarks, and we motivate training models from scratch for each round over successive fine-tuning.",
}

@article{paullada2021data,
title = {Data and its (dis)contents: A survey of dataset development and use in machine learning research},
journal = {Patterns},
volume = {2},
number = {11},
pages = {100336},
year = {2021},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2021.100336},
url = {https://www.sciencedirect.com/science/article/pii/S2666389921001847},
author = {Amandalynne Paullada and Inioluwa Deborah Raji and Emily M. Bender and Emily Denton and Alex Hanna},
keywords = {datasets machine learning},
abstract = {Summary
In this work, we survey a breadth of literature that has revealed the limitations of predominant practices for dataset collection and use in the field of machine learning. We cover studies that critically review the design and development of datasets with a focus on negative societal impacts and poor outcomes for system performance. We also cover approaches to filtering and augmenting data and modeling techniques aimed at mitigating the impact of bias in datasets. Finally, we discuss works that have studied data practices, cultures, and disciplinary norms and discuss implications for the legal, ethical, and functional challenges the field continues to face. Based on these findings, we advocate for the use of both qualitative and quantitative approaches to more carefully document and analyze datasets during the creation and usage phases.}
}

@inproceedings{lescao2021prompt,
    title = "How many data points is a prompt worth?",
    author = "Le Scao, Teven  and
      Rush, Alexander",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.208",
    doi = "10.18653/v1/2021.naacl-main.208",
    pages = "2627--2636",
    abstract = "When fine-tuning pretrained models for classification, researchers either use a generic model head or a task-specific prompt for prediction. Proponents of prompting have argued that prompts provide a method for injecting task-specific guidance, which is beneficial in low-data regimes. We aim to quantify this benefit through rigorous testing of prompts in a fair setting: comparing prompted and head-based fine-tuning in equal conditions across many tasks and data sizes. By controlling for many sources of advantage, we find that prompting does indeed provide a benefit, and that this benefit can be quantified per task. Results show that prompting is often worth 100s of data points on average across classification tasks.",
}

@inproceedings{antoniak2021seeds,
    title = "Bad Seeds: Evaluating Lexical Methods for Bias Measurement",
    author = "Antoniak, Maria  and
      Mimno, David",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.148",
    doi = "10.18653/v1/2021.acl-long.148",
    pages = "1889--1904",
    abstract = "A common factor in bias measurement methods is the use of hand-curated seed lexicons, but there remains little guidance for their selection. We gather seeds used in prior work, documenting their common sources and rationales, and in case studies of three English-language corpora, we enumerate the different types of social biases and linguistic features that, once encoded in the seeds, can affect subsequent bias measurements. Seeds developed in one context are often re-used in other contexts, but documentation and evaluation remain necessary precursors to relying on seeds for sensitive measurements.",
}

@inproceedings{blodgett2021norwegian,
    title = "Stereotyping {N}orwegian Salmon: An Inventory of Pitfalls in Fairness Benchmark Datasets",
    author = "Blodgett, Su Lin  and
      Lopez, Gilsinia  and
      Olteanu, Alexandra  and
      Sim, Robert  and
      Wallach, Hanna",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.81",
    doi = "10.18653/v1/2021.acl-long.81",
    pages = "1004--1015",
    abstract = "Auditing NLP systems for computational harms like surfacing stereotypes is an elusive goal. Several recent efforts have focused on benchmark datasets consisting of pairs of contrastive sentences, which are often accompanied by metrics that aggregate an NLP system{'}s behavior on these pairs into measurements of harms. We examine four such benchmarks constructed for two NLP tasks: language modeling and coreference resolution. We apply a measurement modeling lens{---}originating from the social sciences{---}to inventory a range of pitfalls that threaten these benchmarks{'} validity as measurement models for stereotyping. We find that these benchmarks frequently lack clear articulations of what is being measured, and we highlight a range of ambiguities and unstated assumptions that affect how these benchmarks conceptualize and operationalize stereotyping.",
}

@inproceedings{goldfarb2021intrinsic,
    title = "Intrinsic Bias Metrics Do Not Correlate with Application Bias",
    author = "Goldfarb-Tarrant, Seraphina  and
      Marchant, Rebecca  and
      Mu{\~n}oz S{\'a}nchez, Ricardo  and
      Pandya, Mugdha  and
      Lopez, Adam",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.150",
    doi = "10.18653/v1/2021.acl-long.150",
    pages = "1926--1940",
    abstract = "Natural Language Processing (NLP) systems learn harmful societal biases that cause them to amplify inequality as they are deployed in more and more situations. To guide efforts at debiasing these systems, the NLP community relies on a variety of metrics that quantify bias in models. Some of these metrics are intrinsic, measuring bias in word embedding spaces, and some are extrinsic, measuring bias in downstream tasks that the word embeddings enable. Do these intrinsic and extrinsic metrics correlate with each other? We compare intrinsic and extrinsic metrics across hundreds of trained models covering different tasks and experimental conditions. Our results show no reliable correlation between these metrics that holds in all scenarios across tasks and languages. We urge researchers working on debiasing to focus on extrinsic measures of bias, and to make using these measures more feasible via creation of new challenge sets and annotated test data. To aid this effort, we release code, a new intrinsic metric, and an annotated test set focused on gender bias in hate speech.",
}

@article{askell2021general,
  title={A General Language Assistant as a Laboratory for Alignment},
  author={Amanda Askell and Yushi Bai and Anna Chen and Dawn Drain and Deep Ganguli and T. J. Henighan and Andy Jones and Nicholas Joseph and Benjamin Mann and Nova DasSarma and Nelson Elhage and Zac Hatfield-Dodds and Danny Hernandez and John Kernion and Kamal Ndousse and Catherine Olsson and Dario Amodei and Tom B. Brown and Jack Clark and Sam McCandlish and Christopher Olah and Jared Kaplan},
  journal={ArXiv},
  year={2021},
  volume={abs/2112.00861}
}

@inproceedings{rogers2021changing,
    title = "Changing the World by Changing the Data",
    author = "Rogers, Anna",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.170",
    doi = "10.18653/v1/2021.acl-long.170",
    pages = "2182--2194",
    abstract = "NLP community is currently investing a lot more research and resources into development of deep learning models than training data. While we have made a lot of progress, it is now clear that our models learn all kinds of spurious patterns, social biases, and annotation artifacts. Algorithmic solutions have so far had limited success. An alternative that is being actively discussed is more careful design of datasets so as to deliver specific signals. This position paper maps out the arguments for and against data curation, and argues that fundamentally the point is moot: curation already is and will be happening, and it is changing the world. The question is only how much thought we want to invest into that process.",
}

@article{chen2021codex,
  title={Evaluating Large Language Models Trained on Code},
  author={Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde and Jared Kaplan and Harrison Edwards and Yura Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and David W. Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William H. Guss and Alex Nichol and Igor Babuschkin and S. Arun Balaji and Shantanu Jain and Andrew Carr and Jan Leike and Joshua Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew M. Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
  journal={ArXiv},
  year={2021},
  volume={abs/2107.03374}
}

@article{lieber2021jurassic,
  title={Jurassic-1: Technical details and evaluation},
  author={Opher Lieber and Or Sharir and Barak Lenz and Yoav Shoham},
  journal={White Paper, AI21 Labs},
  year={2021},
  url={https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf}
}

@article{jumper2021alphafold,
  title={Highly accurate protein structure prediction with AlphaFold},
  author={John M. Jumper and Richard Evans and Alexander Pritzel and Tim Green and Michael Figurnov and Olaf Ronneberger and Kathryn Tunyasuvunakool and Russ Bates and Augustin Z{\'i}dek and Anna Potapenko and Alex Bridgland and Clemens Meyer and Simon A A Kohl and Andy Ballard and Andrew Cowie and Bernardino Romera-Paredes and Stanislav Nikolov and Rishub Jain and Jonas Adler and Trevor Back and Stig Petersen and David A. Reiman and Ellen Clancy and Michal Zielinski and Martin Steinegger and Michalina Pacholska and Tamas Berghammer and Sebastian Bodenstein and David Silver and Oriol Vinyals and Andrew W. Senior and Koray Kavukcuoglu and Pushmeet Kohli and Demis Hassabis},
  journal={Nature},
  year={2021},
  volume={596},
  pages={583 - 589}
}

@article {verkuil2022esm2,
	author = {Verkuil, Robert and Kabeli, Ori and Du, Yilun and Wicky, Basile I. M. and Milles, Lukas F. and Dauparas, Justas and Baker, David and Ovchinnikov, Sergey and Sercu, Tom and Rives, Alexander},
	title = {Language models generalize beyond natural proteins},
	elocation-id = {2022.12.21.521521},
	year = {2022},
	doi = {10.1101/2022.12.21.521521},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {Learning the design patterns of proteins from sequences across evolution may have promise toward generative protein design. However it is unknown whether language models, trained on sequences of natural proteins, will be capable of more than memorization of existing protein families. Here we show that language models generalize beyond natural proteins to generate de novo proteins. We focus on two protein design tasks: fixed backbone design where the structure is specified, and unconstrained generation where the structure is sampled from the model. Remarkably although the models are trained only on sequences, we find that they are capable of designing structure. A total of 228 generated proteins are evaluated experimentally with high overall success rates (152/228 or 67\%) in producing a soluble and monomeric species by size exclusion chromatography. Out of 152 experimentally successful designs, 35 have no significant sequence match to known natural proteins. Of the remaining 117, sequence identity to the nearest sequence match is at median 27\%, below 20\% for 6 designs, and as low as 18\% for 3 designs. For fixed backbone design, the language model generates successful designs for each of eight experimentally evaluated artificially created fixed backbone targets. For unconstrained generation, sampled proteins cover diverse topologies and secondary structure compositions, and have high experimental success rate (71/129 or 55\%). The designs reflect deep patterns linking sequence and structure, including motifs that occur in related natural structures, and motifs that are not observed in similar structural contexts in known protein families. The results show that language models, though only trained on sequences, learn a deep grammar that enables the design of protein structure, extending beyond natural proteins.Competing Interest StatementThe authors have declared no competing interest.},
	URL = {https://www.biorxiv.org/content/early/2022/12/22/2022.12.21.521521},
	eprint = {https://www.biorxiv.org/content/early/2022/12/22/2022.12.21.521521.full.pdf},
	journal = {bioRxiv}
}


@article{tay2022unifying,
  title={Unifying Language Learning Paradigms},
  author={Yi Tay and Mostafa Dehghani and Vinh Q. Tran and Xavier Garc{\'i}a and Dara Bahri and Tal Schuster and Huaixiu Zheng and Neil Houlsby and Donald Metzler},
  journal={ArXiv},
  year={2022},
  volume={abs/2205.05131}
}

@article{wang2022internvideo,
  title={InternVideo: General Video Foundation Models via Generative and Discriminative Learning},
  author={Yi Wang and Kunchang Li and Yizhuo Li and Yinan He and Bingkun Huang and Zhiyu Zhao and Hongjie Zhang and Jilan Xu and Yi Liu and Zun Wang and Sen Xing and Guo Chen and Junting Pan and Jiashuo Yu and Yali Wang and Limin Wang and Yu Qiao},
  journal={ArXiv},
  year={2022},
  volume={abs/2212.03191}
}

@article{singer2022makeavideo,
  title={Make-A-Video: Text-to-Video Generation without Text-Video Data},
  author={Uriel Singer and Adam Polyak and Thomas Hayes and Xiaoyue Yin and Jie An and Songyang Zhang and Qiyuan Hu and Harry Yang and Oron Ashual and Oran Gafni and Devi Parikh and Sonal Gupta and Yaniv Taigman},
  journal={ArXiv},
  year={2022},
  volume={abs/2209.14792}
}

@inproceedings{
wei2022flan,
title={Finetuned Language Models are Zero-Shot Learners},
author={Jason Wei and Maarten Bosma and Vincent Zhao and Kelvin Guu and Adams Wei Yu and Brian Lester and Nan Du and Andrew M. Dai and Quoc V Le},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=gEZrGCozdqR}
}

@article{soltan2022alexa,
  title={AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model},
  author={Saleh Soltan and Shankar Ananthakrishnan and Jack G. M. FitzGerald and Rahul Gupta and Wael Hamza and Haidar Khan and Charith S. Peris and Stephen Rawls and Andrew Rosenbaum and Anna Rumshisky and Chandan Prakash and Mukund Sridhar and Fabian Triefenbach and Apurv Verma and Gokhan Tur and Premkumar Natarajan},
  journal={ArXiv},
  year={2022},
  volume={abs/2208.01448}
}

@inproceedings{wang2022supernatural,
  title={Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks},
  author={Yizhong Wang and Swaroop Mishra and Pegah Alipoormolabashi and Yeganeh Kordi and Amirreza Mirzaei and A. Arunkumar and Arjun Ashok and Arut Selvan Dhanasekaran and Atharva Naik and David Stap and Eshaan Pathak and Giannis Karamanolakis and Haizhi Gary Lai and Ishan Purohit and Ishani Mondal and Jacob Anderson and Kirby Kuznia and Krima Doshi and Maitreya Patel and Kuntal Kumar Pal and M. Moradshahi and Mihir Parmar and Mirali Purohit and Neeraj Varshney and Phani Rohitha Kaza and Pulkit Verma and Ravsehaj Singh Puri and Rushang Karia and Shailaja Keyur Sampat and Savan Doshi and Siddharth Deepak Mishra and Sujan Reddy and Sumanta Patro and Tanay Dixit and Xudong Shen and Chitta Baral and Yejin Choi and Noah A. Smith and Hanna Hajishirzi and Daniel Khashabi},
  year={2022}
}

@article{efrat2022lmentry,
  doi = {10.48550/ARXIV.2211.02069},
  author = {Efrat, Avia and Honovich, Or and Levy, Omer},
    keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {LMentry: A Language Model Benchmark of Elementary Language Tasks},
    publisher = {arXiv},
    year = {2022},
  url = {https://arxiv.org/abs/2211.02069},
  copyright = {Creative Commons Attribution 4.0 International}
}

@article{tay2022transcending,
  title={Transcending Scaling Laws with 0.1\% Extra Compute},
  author={Yi Tay and Jason Wei and Hyung Won Chung and Vinh Q. Tran and David R. So and Siamak Shakeri and Xavier Garc{\'i}a and Huaixiu Zheng and Jinfeng Rao and Aakanksha Chowdhery and Denny Zhou and Donald Metzler and Slav Petrov and Neil Houlsby and Quoc V. Le and Mostafa Dehghani},
  journal={ArXiv},
  year={2022},
  volume={abs/2210.11399}
}

@article{qian2022perturbation,
  title={Perturbation Augmentation for Fairer NLP},
  author={Rebecca Qian and Candace Ross and Jude Fernandes and Eric Michael Smith and Douwe Kiela and Adina Williams},
  journal={ArXiv},
  year={2022},
  volume={abs/2205.12586}
}

@article{goyal2022news,
  title={News Summarization and Evaluation in the Era of GPT-3},
  author={Tanya Goyal and Junyi Jessy Li and Greg Durrett},
  journal={ArXiv},
  year={2022},
  volume={abs/2209.12356}
}

@inproceedings{wu2022aichains,
author = {Wu, Tongshuang and Terry, Michael and Cai, Carrie Jun},
title = {AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3517582},
doi = {10.1145/3491102.3517582},
abstract = {Although large language models (LLMs) have demonstrated impressive potential on simple tasks, their breadth of scope, lack of transparency, and insufficient controllability can make them less effective when assisting humans on more complex tasks. In response, we introduce the concept of Chaining LLM steps together, where the output of one step becomes the input for the next, thus aggregating the gains per step. We first define a set of LLM primitive operations useful for Chain construction, then present an interactive system where users can modify these Chains, along with their intermediate results, in a modular way. In a 20-person user study, we found that Chaining not only improved the quality of task outcomes, but also significantly enhanced system transparency, controllability, and sense of collaboration. Additionally, we saw that users developed new ways of interacting with LLMs through Chains: they leveraged sub-tasks to calibrate model expectations, compared and contrasted alternative strategies by observing parallel downstream effects, and debugged unexpected model outputs by “unit-testing” sub-components of a Chain. In two case studies, we further explore how LLM Chains may be used in future applications.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {385},
numpages = {22},
keywords = {Large Language Models, Human-AI Interaction, Natural Language Processing},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{
he2022towards,
title={Towards a Unified View of Parameter-Efficient Transfer Learning},
author={Junxian He and Chunting Zhou and Xuezhe Ma and Taylor Berg-Kirkpatrick and Graham Neubig},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=0RDcd5Axok}
}

@article{press2022selfask,
  title={Measuring and Narrowing the Compositionality Gap in Language Models},
  author={Ofir Press and Muru Zhang and Sewon Min and Ludwig Schmidt and Noah A. Smith and Mike Lewis},
  journal={ArXiv},
  year={2022},
  volume={abs/2210.03350}
}

@article{arora2022ama,
  title={Ask Me Anything: A simple strategy for prompting language models},
  author={Simran Arora and Avanika Narayan and Mayee F. Chen and Laurel J. Orr and Neel Guha and Kush S Bhatia and Ines Chami and Frederic Sala and Christopher R'e},
  journal={ArXiv},
  year={2022},
  volume={abs/2210.02441}
}

@inproceedings{
chen2022hapi,
title={{HAPI}: A Large-scale Longitudinal Dataset of Commercial {ML} {API} Predictions},
author={Lingjiao Chen and Zhihua Jin and Sabri Eyuboglu and Christopher Re and Matei Zaharia and James Y. Zou},
booktitle={Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
year={2022},
url={https://openreview.net/forum?id=CZeIOfCjMf}
}

@article{gehrmann2022repairing,
  title={Repairing the Cracked Foundation: A Survey of Obstacles in Evaluation Practices for Generated Text},
  author={Sebastian Gehrmann and Elizabeth Clark and Thibault Sellam},
  journal={ArXiv},
  year={2022},
  volume={abs/2202.06935}
}

@article{reiter2022summarization,
  title={Summarisation datasets should contain summaries!},
  author={Ehud Reiter},
  url={https://ehudreiter.com/2022/10/13/summarisation-datasets/},
  year={2022},
}


@article{suzgun2022challenging,
  doi = {10.48550/ARXIV.2210.09261},
  url = {https://arxiv.org/abs/2210.09261},
  author = {Suzgun, Mirac and Scales, Nathan and Schärli, Nathanael and Gehrmann, Sebastian and Tay, Yi and Chung, Hyung Won and Chowdhery, Aakanksha and Le, Quoc V. and Chi, Ed H. and Zhou, Denny and Wei, Jason},
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}

@article{liu2022fewshot,
  title={Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning},
  author={Haokun Liu and Derek Tam and Mohammed Muqeeth and Jay Mohta and Tenghao Huang and Mohit Bansal and Colin Raffel},
  journal={ArXiv},
  year={2022},
  volume={abs/2205.05638}
}


@inproceedings{bommasani2022homogenization,
  title={Picking on the Same Person: Does Algorithmic Monoculture lead to Outcome Homogenization?},
  author={Rishi Bommasani and Kathleen A. Creel and Ananya Kumar and Dan Jurafsky and Percy Liang},
    booktitle = {Advances in Neural Information Processing Systems},
  year={2022}
}

@article{thoppilan2022lamda,
  title={LaMDA: Language Models for Dialog Applications},
  author={Romal Thoppilan and Daniel De Freitas and Jamie Hall and Noam M. Shazeer and Apoorv Kulshreshtha and Heng-Tze Cheng and Alicia Jin and Taylor Bos and Leslie Baker and Yu Du and Yaguang Li and Hongrae Lee and Huaixiu Zheng and Amin Ghafouri and Marcelo Menegali and Yanping Huang and Maxim Krikun and Dmitry Lepikhin and James Qin and Dehao Chen and Yuanzhong Xu and Zhifeng Chen and Adam Roberts and Maarten Bosma and Yanqi Zhou and Chung-Ching Chang and I. A. Krivokon and Willard James Rusch and Marc Pickett and Kathleen S. Meier-Hellstern and Meredith Ringel Morris and Tulsee Doshi and Renelito Delos Santos and Toju Duke and Johnny Hartz S{\o}raker and Ben Zevenbergen and Vinodkumar Prabhakaran and Mark D{\'i}az and Ben Hutchinson and Kristen Olson and Alejandra Molina and Erin Hoffman-John and Josh Lee and Lora Aroyo and Ravindran Rajakumar and Alena Butryna and Matthew Lamm and V. O. Kuzmina and Joseph Fenton and Aaron Cohen and Rachel Bernstein and Ray Kurzweil and Blaise Aguera-Arcas and Claire Cui and Marian Croak and Ed Chi and Quoc Le},
  journal={ArXiv},
  year={2022},
  volume={abs/2201.08239}
}

@InProceedings{borgeaud2022retro,
  title = 	 {Improving Language Models by Retrieving from Trillions of Tokens},
  author =       {Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie and Van Den Driessche, George Bm and Lespiau, Jean-Baptiste and Damoc, Bogdan and Clark, Aidan and De Las Casas, Diego and Guy, Aurelia and Menick, Jacob and Ring, Roman and Hennigan, Tom and Huang, Saffron and Maggiore, Loren and Jones, Chris and Cassirer, Albin and Brock, Andy and Paganini, Michela and Irving, Geoffrey and Vinyals, Oriol and Osindero, Simon and Simonyan, Karen and Rae, Jack and Elsen, Erich and Sifre, Laurent},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {2206--2240},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/borgeaud22a/borgeaud22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/borgeaud22a.html},
  abstract = 	 {We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a 2 trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25{\texttimes} fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale.}
}


@inproceedings{steed2022upstream,
    title = "{U}pstream {M}itigation {I}s \textit{ {N}ot} {A}ll {Y}ou {N}eed: {T}esting the {B}ias {T}ransfer {H}ypothesis in {P}re-{T}rained {L}anguage {M}odels",
    author = "Steed, Ryan  and
      Panda, Swetasudha  and
      Kobren, Ari  and
      Wick, Michael",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.247",
    doi = "10.18653/v1/2022.acl-long.247",
    pages = "3524--3542",
    abstract = "A few large, homogenous, pre-trained models undergird many machine learning systems {---} and often, these models contain harmful stereotypes learned from the internet. We investigate the \textit{bias transfer hypothesis}: the theory that social biases (such as stereotypes) internalized by large language models during pre-training transfer into harmful task-specific behavior after fine-tuning. For two classification tasks, we find that reducing intrinsic bias with controlled interventions \textit{before} fine-tuning does little to mitigate the classifier{'}s discriminatory behavior \textit{after} fine-tuning. Regression analysis suggests that downstream disparities are better explained by biases in the fine-tuning dataset. Still, pre-training plays a role: simple alterations to co-occurrence rates in the fine-tuning dataset are ineffective when the model has been pre-trained. Our results encourage practitioners to focus more on dataset quality and context-specific harms.",
}

@inproceedings{bach2022promptsource,
    title = "{P}rompt{S}ource: An Integrated Development Environment and Repository for Natural Language Prompts",
    author = "Bach, Stephen  and
      Sanh, Victor  and
      Yong, Zheng Xin  and
      Webson, Albert  and
      Raffel, Colin  and
      Nayak, Nihal V.  and
      Sharma, Abheesht  and
      Kim, Taewoon  and
      Bari, M Saiful  and
      Fevry, Thibault  and
      Alyafeai, Zaid  and
      Dey, Manan  and
      Santilli, Andrea  and
      Sun, Zhiqing  and
      Ben-david, Srulik  and
      Xu, Canwen  and
      Chhablani, Gunjan  and
      Wang, Han  and
      Fries, Jason  and
      Al-shaibani, Maged  and
      Sharma, Shanya  and
      Thakker, Urmish  and
      Almubarak, Khalid  and
      Tang, Xiangru  and
      Radev, Dragomir  and
      Jiang, Mike Tian-jian  and
      Rush, Alexander",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-demo.9",
    doi = "10.18653/v1/2022.acl-demo.9",
    pages = "93--104",
    abstract = "PromptSource is a system for creating, sharing, and using natural language prompts. Prompts are functions that map an example from a dataset to a natural language input and target output. Using prompts to train and query language models is an emerging area in NLP that requires new tools that let users develop and refine these prompts collaboratively. PromptSource addresses the emergent challenges in this new setting with (1) a templating language for defining data-linked prompts, (2) an interface that lets users quickly iterate on prompt development by observing outputs of their prompts on many examples, and (3) a community-driven set of guidelines for contributing new prompts to a common pool. Over 2,000 prompts for roughly 170 datasets are already available in PromptSource. PromptSource is available at https://github.com/bigscience-workshop/promptsource.",
}

@inproceedings{liu2022incontext,
    title = "What Makes Good In-Context Examples for {GPT}-3?",
    author = "Liu, Jiachang  and
      Shen, Dinghan  and
      Zhang, Yizhe  and
      Dolan, Bill  and
      Carin, Lawrence  and
      Chen, Weizhu",
    booktitle = "Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures",
    month = may,
    year = "2022",
    address = "Dublin, Ireland and Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.deelio-1.10",
    doi = "10.18653/v1/2022.deelio-1.10",
    pages = "100--114",
    abstract = "GPT-3 has attracted lots of attention due to its superior performance across a wide range of NLP tasks, especially with its in-context learning abilities. Despite its success, we found that the empirical results of GPT-3 depend heavily on the choice of in-context examples. In this work, we investigate whether there are more effective strategies for judiciously selecting in-context examples (relative to random sampling) that better leverage GPT-3{'}s in-context learning capabilities.Inspired by the recent success of leveraging a retrieval module to augment neural networks, we propose to retrieve examples that are semantically-similar to a test query sample to formulate its corresponding prompt. Intuitively, the examples selected with such a strategy may serve as more informative inputs to unleash GPT-3{'}s power of text generation. We evaluate the proposed approach on several natural language understanding and generation benchmarks, where the retrieval-based prompt selection approach consistently outperforms the random selection baseline. Moreover, it is observed that the sentence encoders fine-tuned on task-related datasets yield even more helpful retrieval results. Notably, significant gains are observed on tasks such as table-to-text generation (44.3{\%} on the ToTTo dataset) and open-domain question answering (45.5{\%} on the NQ dataset).",
}

@misc{kilcher2022gpt4chan,
    author = {Yannic Kilcher},
    title  = {GPT-4chan},
    url    = {https://github.com/yk/gpt-4chan-public},
    year   = {2022}
}

@misc{narayanan2023transparencyreports,
    author  = {Narayanan, Arvind and Kapoor, Sayash},
    title   = {Generative AI companies must publish transparency reports},
    url     = {https://knightcolumbia.org/blog/generative-ai-companies-must-publish-transparency-reports},
    year    = {2023},
    journal = {The Knight First Amendment Institute at Columbia University}
}

@inproceedings{xu2021detoxifying,
    title = "Detoxifying Language Models Risks Marginalizing Minority Voices",
    author = "Xu, Albert  and
      Pathak, Eshaan  and
      Wallace, Eric  and
      Gururangan, Suchin  and
      Sap, Maarten  and
      Klein, Dan",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.190",
    doi = "10.18653/v1/2021.naacl-main.190",
    pages = "2390--2397",
    abstract = "Language models (LMs) must be both safe and equitable to be responsibly deployed in practice. With safety in mind, numerous detoxification techniques (e.g., Dathathri et al. 2020; Krause et al. 2020) have been proposed to mitigate toxic LM generations. In this work, we show that these detoxification techniques hurt equity: they decrease the utility of LMs on language used by marginalized groups (e.g., African-American English and minority identity mentions). In particular, we perform automatic and human evaluations of text generation quality when LMs are conditioned on inputs with different dialects and group identifiers. We find that detoxification makes LMs more brittle to distribution shift, especially on language used by marginalized groups. We identify that these failures stem from detoxification methods exploiting spurious correlations in toxicity datasets. Overall, our results highlight the tension between the controllability and distributional robustness of LMs.",
}

@inproceedings{perez2021true,
 author = {Perez, Ethan and Kiela, Douwe and Cho, Kyunghyun},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {11054--11070},
 publisher = {Curran Associates, Inc.},
 title = {True Few-Shot Learning with Language Models},
 url = {https://proceedings.neurips.cc/paper/2021/file/5c04925674920eb58467fb52ce4ef728-Paper.pdf},
 volume = {34},
 year = {2021}
}

@inproceedings{roller2021recipes,
    title = "Recipes for Building an Open-Domain Chatbot",
    author = "Roller, Stephen  and
      Dinan, Emily  and
      Goyal, Naman  and
      Ju, Da  and
      Williamson, Mary  and
      Liu, Yinhan  and
      Xu, Jing  and
      Ott, Myle  and
      Smith, Eric Michael  and
      Boureau, Y-Lan  and
      Weston, Jason",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.24",
    doi = "10.18653/v1/2021.eacl-main.24",
    pages = "300--325",
    abstract = "Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we highlight other ingredients. Good conversation requires blended skills: providing engaging talking points, and displaying knowledge, empathy and personality appropriately, while maintaining a consistent persona. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter models, and make our models and code publicly available. Human evaluations show our best models outperform existing approaches in multi-turn dialogue on engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.",
}

@inproceedings{baumgartner2020pushshift,
  title={The Pushshift Reddit Dataset},
  author={Jason Baumgartner and Savvas Zannettou and Brian Keegan and Megan Squire and Jeremy Blackburn},
  booktitle={ICWSM},
  year={2020}
}

@inproceedings{liesenfeld2023opening,
author = {Liesenfeld, Andreas and Lopez, Alianda and Dingemanse, Mark},
title = {Opening up ChatGPT: Tracking Openness, Transparency, and Accountability in Instruction-Tuned Text Generators},
year = {2023},
isbn = {9798400700149},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3571884.3604316},
doi = {10.1145/3571884.3604316},
abstract = {Large language models that exhibit instruction-following behaviour represent one of the biggest recent upheavals in conversational interfaces, a trend in large part fuelled by the release of OpenAI’s ChatGPT, a proprietary large language model for text generation fine-tuned through reinforcement learning from human feedback (LLM+RLHF). We review the risks of relying on proprietary software and survey the first crop of open-source projects of comparable architecture and functionality. The main contribution of this paper is to show that openness is differentiated, and to offer scientific documentation of degrees of openness in this fast-moving field. We evaluate projects in terms of openness of code, training data, model weights, RLHF data, licensing, scientific documentation, and access methods. We find that while there is a fast-growing list of projects billing themselves as ‘open source’, many inherit undocumented data of dubious legality, few share the all-important instruction-tuning (a key site where human annotation labour is involved), and careful scientific documentation is exceedingly rare. Degrees of openness are relevant to fairness and accountability at all points, from data collection and curation to model architecture, and from training and fine-tuning to release and deployment.},
booktitle = {Proceedings of the 5th International Conference on Conversational User Interfaces},
articleno = {47},
numpages = {6},
keywords = {large language models, chatGPT, open source, RLHF, survey},
location = {Eindhoven, Netherlands},
series = {CUI '23}
}



@article{kaack2021aligning,
  title={Aligning artificial intelligence with climate change mitigation},
  author={Kaack, Lynn and Donti, Priya and Strubell, Emma and Kamiya, George and Creutzig, Felix and Rolnick, David},
  year={2021}
}

@article{Hendrycks2021MeasuringCC,
  title={Measuring Coding Challenge Competence With APPS},
  author={Dan Hendrycks and Steven Basart and Saurav Kadavath and Mantas Mazeika and Akul Arora and Ethan Guo and Collin Burns and Samir Puranik and Horace He and Dawn Xiaodong Song and Jacob Steinhardt},
  journal={ArXiv},
  year={2021},
  volume={abs/2105.09938}
}

@article{Chen2021EvaluatingLL,
  title={Evaluating Large Language Models Trained on Code},
  author={Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde and Jared Kaplan and Harrison Edwards and Yura Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and David W. Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William H. Guss and Alex Nichol and Igor Babuschkin and S. Arun Balaji and Shantanu Jain and Andrew Carr and Jan Leike and Joshua Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew M. Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
  journal={ArXiv},
  year={2021},
  volume={abs/2107.03374}
}

@article{gardner2019qa,
  title={Question Answering is a Format; When is it Useful?},
  author={Matt Gardner and Jonathan Berant and Hannaneh Hajishirzi and Alon Talmor and Sewon Min},
  journal={ArXiv},
  year={2019},
  volume={abs/1909.11291}
}

@inproceedings{conneau2019xlm,
 author = {Conneau, Alexis and Lample, Guillaume},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Cross-lingual Language Model Pretraining},
 url = {https://proceedings.neurips.cc/paper/2019/file/c04c19c2c2474dbf5f7ac4372c5b9af1-Paper.pdf},
 volume = {32},
 year = {2019}
}

@article{beukeboom2019stereotypes,
  title={{How stereotypes are shared through language: a review and introduction of the aocial categories and stereotypes communication (SCSC) framework}},
  author={Beukeboom, Camiel J. and Burgers, Christian},
  journal={Review of Communication Research},
  volume={7},
  pages={1--37},
  year={2019},
  publisher={ESP},
  url={https://research.vu.nl/en/publications/how-stereotypes-are-shared-through-language-a-review-and-introduc}
}

@misc{https://doi.org/10.48550/arxiv.1912.08777,
  doi = {10.48550/ARXIV.1912.08777},
  
  url = {https://arxiv.org/abs/1912.08777},
  
  author = {Zhang, Jingqing and Zhao, Yao and Saleh, Mohammad and Liu, Peter J.},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{du2022glm,
    title = "{GLM}: General Language Model Pretraining with Autoregressive Blank Infilling",
    author = "Du, Zhengxiao  and
      Qian, Yujie  and
      Liu, Xiao  and
      Ding, Ming  and
      Qiu, Jiezhong  and
      Yang, Zhilin  and
      Tang, Jie",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.26",
    doi = "10.18653/v1/2022.acl-long.26",
    pages = "320--335",
    abstract = "There have been various types of pretraining architectures including autoencoding models (e.g., BERT), autoregressive models (e.g., GPT), and encoder-decoder models (e.g., T5). However, none of the pretraining frameworks performs the best for all tasks of three main categories including natural language understanding (NLU), unconditional generation, and conditional generation. We propose a General Language Model (GLM) based on autoregressive blank infilling to address this challenge. GLM improves blank filling pretraining by adding 2D positional encodings and allowing an arbitrary order to predict spans, which results in performance gains over BERT and T5 on NLU tasks. Meanwhile, GLM can be pretrained for different types of tasks by varying the number and lengths of blanks. On a wide range of tasks across NLU, conditional and unconditional generation, GLM outperforms BERT, T5, and GPT given the same model sizes and data, and achieves the best performance from a single pretrained model with 1.25{\mbox{$\times$}} parameters of BERT Large , demonstrating its generalizability to different downstream tasks.",
}

@inproceedings{wang2022deepstruct,
    title = "{D}eep{S}truct: Pretraining of Language Models for Structure Prediction",
    author = "Wang, Chenguang  and
      Liu, Xiao  and
      Chen, Zui  and
      Hong, Haoyun  and
      Tang, Jie  and
      Song, Dawn",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.67",
    doi = "10.18653/v1/2022.findings-acl.67",
    pages = "803--823",
    abstract = "We introduce a method for improving the structural understanding abilities of language models. Unlike previous approaches that finetune the models with task-specific augmentation, we pretrain language models to generate structures from the text on a collection of task-agnostic corpora. Our structure pretraining enables zero-shot transfer of the learned knowledge that models have about the structure tasks. We study the performance of this approach on 28 datasets, spanning 10 structure prediction tasks including open information extraction, joint entity and relation extraction, named entity recognition, relation classification, semantic role labeling, event extraction, coreference resolution, factual probe, intent detection, and dialogue state tracking. We further enhance the pretraining with the task-specific training sets. We show that a 10B parameter language model transfers non-trivially to most tasks and obtains state-of-the-art performance on 21 of 28 datasets that we evaluate. Our code and datasets will be made publicly available.",
}

@inproceedings{durmus-etal-2022-spurious,
    title = "Spurious Correlations in Reference-Free Evaluation of Text Generation",
    author = "Durmus, Esin  and
      Ladhak, Faisal  and
      Hashimoto, Tatsunori",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.102",
    doi = "10.18653/v1/2022.acl-long.102",
    pages = "1443--1454",
    abstract = "Model-based, reference-free evaluation metricshave been proposed as a fast and cost-effectiveapproach to evaluate Natural Language Generation(NLG) systems. Despite promising recentresults, we find evidence that reference-freeevaluation metrics of summarization and dialoggeneration may be relying on spuriouscorrelations with measures such as word overlap,perplexity, and length. We further observethat for text summarization, these metrics havehigh error rates when ranking current state-ofthe-art abstractive summarization systems. Wedemonstrate that these errors can be mitigatedby explicitly designing evaluation metrics toavoid spurious features in reference-free evaluation.",
}
@article{10.1162/tacl_a_00453,
    author = {Laban, Philippe and Schnabel, Tobias and Bennett, Paul N. and Hearst, Marti A.},
    title = "{SummaC: Re-Visiting NLI-based Models for Inconsistency Detection in Summarization}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {10},
    pages = {163-177},
    year = {2022},
    month = {02},
    abstract = "{In the summarization domain, a key requirement for summaries is to be factually consistent with the input document. Previous work has found that natural language inference (NLI) models do not perform competitively when applied to inconsistency detection. In this work, we revisit the use of NLI for inconsistency detection, finding that past work suffered from a mismatch in input granularity between NLI datasets (sentence-level), and inconsistency detection (document level). We provide a highly effective and light-weight method called SummaCConv that enables NLI models to be successfully used for this task by segmenting documents into sentence units and aggregating scores between pairs of sentences. We furthermore introduce a new benchmark called SummaC (Summary Consistency) which consists of six large inconsistency detection datasets. On this dataset, SummaCConv obtains state-of-the-art results with a balanced accuracy of 74.4\\%, a 5\\% improvement compared with prior work.}",
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00453},
    url = {https://doi.org/10.1162/tacl\_a\_00453},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00453/1987014/tacl\_a\_00453.pdf},
}

@InProceedings{wu2021lime,
  title = 	 {LIME: Learning Inductive Bias for Primitives of Mathematical Reasoning},
  author =       {Wu, Yuhuai and Rabe, Markus N and Li, Wenda and Ba, Jimmy and Grosse, Roger B and Szegedy, Christian},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {11251--11262},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/wu21c/wu21c.pdf},
  url = 	 {https://proceedings.mlr.press/v139/wu21c.html},
  abstract = 	 {While designing inductive bias in neural architectures has been widely studied, we hypothesize that transformer networks are flexible enough to learn inductive bias from suitable generic tasks. Here, we replace architecture engineering by encoding inductive bias in the form of datasets. Inspired by Peirce’s view that deduction, induction, and abduction are the primitives of reasoning, we design three synthetic tasks that are intended to require the model to have these three abilities. We specifically design these tasks to be synthetic and devoid of mathematical knowledge to ensure that only the fundamental reasoning biases can be learned from these tasks. This defines a new pre-training methodology called "LIME" (Learning Inductive bias for Mathematical rEasoning). Models trained with LIME significantly outperform vanilla transformers on four very different large mathematical reasoning benchmarks. Unlike dominating the computation cost as traditional pre-training approaches, LIME requires only a small fraction of the computation cost of the typical downstream task. The code for generating LIME tasks is available at https://github.com/tonywu95/LIME.}
}


@article{gao2021sizes,
  title={On the sizes of OpenAI API models},
  author={Gao, Leo},
  journal={EleutherAI Blog},
  year={2021}
}

@inproceedings{gehrmann2021gem,
    title = "The {GEM} Benchmark: Natural Language Generation, its Evaluation and Metrics",
    author = "Gehrmann, Sebastian  and
      Adewumi, Tosin  and
      Aggarwal, Karmanya  and
      Ammanamanchi, Pawan Sasanka  and
      Aremu, Anuoluwapo  and
      Bosselut, Antoine  and
      Chandu, Khyathi Raghavi  and
      Clinciu, Miruna-Adriana  and
      Das, Dipanjan  and
      Dhole, Kaustubh  and
      Du, Wanyu  and
      Durmus, Esin  and
      Du{\v{s}}ek, Ond{\v{r}}ej  and
      Emezue, Chris Chinenye  and
      Gangal, Varun  and
      Garbacea, Cristina  and
      Hashimoto, Tatsunori  and
      Hou, Yufang  and
      Jernite, Yacine  and
      Jhamtani, Harsh  and
      Ji, Yangfeng  and
      Jolly, Shailza  and
      Kale, Mihir  and
      Kumar, Dhruv  and
      Ladhak, Faisal  and
      Madaan, Aman  and
      Maddela, Mounica  and
      Mahajan, Khyati  and
      Mahamood, Saad  and
      Majumder, Bodhisattwa Prasad  and
      Martins, Pedro Henrique  and
      McMillan-Major, Angelina  and
      Mille, Simon  and
      van Miltenburg, Emiel  and
      Nadeem, Moin  and
      Narayan, Shashi  and
      Nikolaev, Vitaly  and
      Niyongabo Rubungo, Andre  and
      Osei, Salomey  and
      Parikh, Ankur  and
      Perez-Beltrachini, Laura  and
      Rao, Niranjan Ramesh  and
      Raunak, Vikas  and
      Rodriguez, Juan Diego  and
      Santhanam, Sashank  and
      Sedoc, Jo{\~a}o  and
      Sellam, Thibault  and
      Shaikh, Samira  and
      Shimorina, Anastasia  and
      Sobrevilla Cabezudo, Marco Antonio  and
      Strobelt, Hendrik  and
      Subramani, Nishant  and
      Xu, Wei  and
      Yang, Diyi  and
      Yerukola, Akhila  and
      Zhou, Jiawei",
    booktitle = "Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.gem-1.10",
    doi = "10.18653/v1/2021.gem-1.10",
    pages = "96--120",
    abstract = "We introduce GEM, a living benchmark for natural language Generation (NLG), its Evaluation, and Metrics. Measuring progress in NLG relies on a constantly evolving ecosystem of automated metrics, datasets, and human evaluation standards. Due to this moving target, new models often still evaluate on divergent anglo-centric corpora with well-established, but flawed, metrics. This disconnect makes it challenging to identify the limitations of current models and opportunities for progress. Addressing this limitation, GEM provides an environment in which models can easily be applied to a wide set of tasks and in which evaluation strategies can be tested. Regular updates to the benchmark will help NLG research become more multilingual and evolve the challenge alongside models. This paper serves as the description of the data for the 2021 shared task at the associated GEM Workshop.",
}























@article{donoho2017fifty,
author = {David Donoho},
title = {50 Years of Data Science},
journal = {Journal of Computational and Graphical Statistics},
volume = {26},
number = {4},
pages = {745-766},
year = {2017},
publisher = {Taylor & Francis},
doi = {10.1080/10618600.2017.1384734},


URL = { 
    
        https://doi.org/10.1080/10618600.2017.1384734
    
    

},
eprint = { 
    
        https://doi.org/10.1080/10618600.2017.1384734
    
    

}

}




@article{greco2019methodological,
author = {Greco, Salvatore and Ishizaka, Alessio and Tasiou, Menelaos and Torrisi, Gianpiero},
year = {2019},
month = {01},
pages = {1-34},
title = {On the Methodological Framework of Composite Indices: A Review of the Issues of Weighting, Aggregation, and Robustness},
volume = {141},
journal = {Social Indicators Research},
doi = {10.1007/s11205-017-1832-9}
}

@book{oecd2008handbook,
   author = "OECD and European Union and Joint Research Centre - European Commission",
   title = "Handbook on Constructing Composite Indicators: Methodology and User Guide",
   year = "2008",
   pages = 162,
   url = "https://www.oecd-ilibrary.org/content/publication/9789264043466-en",
   doi = "https://doi.org/https://doi.org/10.1787/9789264043466-en" 
}



@inproceedings{dhamala2021bold,
author = {Dhamala, Jwala and Sun, Tony and Kumar, Varun and Krishna, Satyapriya and Pruksachatkun, Yada and Chang, Kai-Wei and Gupta, Rahul},
title = {BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445924},
doi = {10.1145/3442188.3445924},
abstract = {Recent advances in deep learning techniques have enabled machines to generate cohesive
open-ended text when prompted with a sequence of words as context. While these models
now empower many downstream applications from conversation bots to automatic storytelling,
they have been shown to generate texts that exhibit social biases. To systematically
study and benchmark social biases in open-ended language generation, we introduce
the Bias in Open-Ended Language Generation Dataset (BOLD), a large-scale dataset that
consists of 23,679 English text generation prompts for bias benchmarking across five
domains: profession, gender, race, religion, and political ideology. We also propose
new automated metrics for toxicity, psycholinguistic norms, and text gender polarity
to measure social biases in open-ended text generation from multiple angles. An examination
of text generated from three popular language models reveals that the majority of
these models exhibit a larger social bias than human-written Wikipedia text across
all domains. With these results we highlight the need to benchmark biases in open-ended
language generation and caution users of language generation models on downstream
tasks to be cognizant of these embedded prejudices.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {862–872},
numpages = {11},
keywords = {Fairness, natural language generation},
location = {Virtual Event, Canada},
series = {FAccT '21}
}


@article{rauh2022characteristics,
  title={Characteristics of Harmful Text: Towards Rigorous Benchmarking of Language Models},
  author={Maribeth Rauh and John F. J. Mellor and Jonathan Uesato and Po-Sen Huang and Johannes Welbl and Laura Weidinger and Sumanth Dathathri and Amelia Glaese and Geoffrey Irving and Iason Gabriel and William S. Isaac and Lisa Anne Hendricks},
  journal={ArXiv},
  year={2022},
  volume={abs/2206.08325}
}

@article{gehrmann2022gemv2,
  title={GEMv2: Multilingual NLG Benchmarking in a Single Line of Code},
  author={Sebastian Gehrmann and Abhik Bhattacharjee and Abinaya Mahendiran and Alex Wang and Alexandros Papangelis and Aman Madaan and Angelina McMillan-Major and Anna V. Shvets and Ashish Upadhyay and Bingsheng Yao and Bryan Wilie and Chandra Bhagavatula and Chaobin You and Craig Thomson and Cristina Garbacea and Dakuo Wang and Daniel Deutsch and Deyi Xiong and Di Jin and Dimitra Gkatzia and Dragomir Radev and Elizabeth Clark and Esin Durmus and Faisal Ladhak and Filip Ginter and Genta Indra Winata and Hendrik Strobelt and Hiroaki Hayashi and Jekaterina Novikova and Jenna Kanerva and Jenny Chim and Jiawei Zhou and Jordan Clive and Joshua Maynez and Jo{\~a}o Sedoc and Juraj Juraska and Kaustubh D. Dhole and Khyathi Raghavi Chandu and Leonardo F. R. Ribeiro and Lewis Tunstall and Li Zhang and Mahima Pushkarna and Mathias Creutz and Michael White and Mihir Kale and Moussa Kamal Eddine and Nico Daheim and Nishant Subramani and Ondrej Dusek and Paul Pu Liang and Pawan Sasanka Ammanamanchi and Qinqin Zhu and Ratish Puduppully and Reno Kriz and Rifat Shahriyar and Ronald Cardenas and Saad Mahamood and Salomey Osei and Samuel Cahyawijaya and Sanja vStajner and S{\'e}bastien Montella and Shailza and Shailza Jolly and Simon Mille and Tahmid Hasan and Tianhao Shen and Tosin P. Adewumi and Vikas Raunak and Vipul Raheja and Vitaly Nikolaev and Vivian Tsai and Yacine Jernite and Yi Xu and Yisi Sang and Yixin Liu and Yufang Hou},
  journal={ArXiv},
  year={2022},
  volume={abs/2206.11249}
}

@article{ziegler2019finetuning,
  title={Fine-Tuning Language Models from Human Preferences},
  author={Daniel M. Ziegler and Nisan Stiennon and Jeff Wu and Tom B. Brown and Alec Radford and Dario Amodei and Paul Christiano and Geoffrey Irving},
  journal={ArXiv},
  year={2019},
  volume={abs/1909.08593},
  url={https://api.semanticscholar.org/CorpusID:202660943}
}


@article{ji2022hallucination,
  title={Survey of Hallucination in Natural Language Generation},
  author={Ziwei Ji and Nayeon Lee and Rita Frieske and Tiezheng Yu and Dan Su and Yan Xu and Etsuko Ishii and Yejin Bang and Andrea Madotto and Pascale Fung},
  journal={ArXiv},
  year={2022},
  volume={abs/2202.03629}
}

@inproceedings{goyal-durrett-2021-annotating,
    title = "Annotating and Modeling Fine-grained Factuality in Summarization",
    author = "Goyal, Tanya  and
      Durrett, Greg",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.114",
    doi = "10.18653/v1/2021.naacl-main.114",
    pages = "1449--1462",
    abstract = "Recent pre-trained abstractive summarization systems have started to achieve credible performance, but a major barrier to their use in practice is their propensity to output summaries that are not faithful to the input and that contain factual errors. While a number of annotated datasets and statistical models for assessing factuality have been explored, there is no clear picture of what errors are most important to target or where current techniques are succeeding and failing. We explore both synthetic and human-labeled data sources for training models to identify factual errors in summarization, and study factuality at the word-, dependency-, and sentence-level. Our observations are threefold. First, exhibited factual errors differ significantly across datasets, and commonly-used training sets of simple synthetic errors do not reflect errors made on abstractive datasets like XSum. Second, human-labeled data with fine-grained annotations provides a more effective training signal than sentence-level annotations or synthetic data. Finally, we show that our best factuality detection model enables training of more factual XSum summarization models by allowing us to identify non-factual tokens in the training data.",
}

@inproceedings{lewis-etal-2020-bart,
    title = "{BART}: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
    author = "Lewis, Mike  and
      Liu, Yinhan  and
      Goyal, Naman  and
      Ghazvininejad, Marjan  and
      Mohamed, Abdelrahman  and
      Levy, Omer  and
      Stoyanov, Veselin  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.703",
    doi = "10.18653/v1/2020.acl-main.703",
    pages = "7871--7880",
    abstract = "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.",
}

@article{mitchell2018modelcards,
  title={Model Cards for Model Reporting},
  author={Margaret Mitchell and Simone Wu and Andrew Zaldivar and Parker Barnes and Lucy Vasserman and Ben Hutchinson and Elena Spitzer and Inioluwa Deborah Raji and Timnit Gebru},
  journal={Proceedings of the Conference on Fairness, Accountability, and Transparency},
  year={2018}
}

@article{narayanan2018fairness,
title = "{21 Fairness Definitions and their Politics}",
author = "Arvind Narayanan",
year = "2018",
note = "{Tutorial presented at the Conference on Fairness, Accountability,
  and Transparency}",
  url={https://www.youtube.com/watch?v=jIXIuYdnyyk}
}

@book{noble2018algorithms,
  title={Algorithms of Oppression},
  author={Noble, Safiya Umoja},
  year={2018},
  publisher={New York University Press}
}


@inproceedings{narayan-etal-2018-dont,
    title = "Don{'}t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization",
    author = "Narayan, Shashi  and
      Cohen, Shay B.  and
      Lapata, Mirella",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1206",
    doi = "10.18653/v1/D18-1206",
    pages = "1797--1807",
    abstract = "We introduce {``}extreme summarization{''}, a new single-document summarization task which does not favor extractive strategies and calls for an abstractive modeling approach. The idea is to create a short, one-sentence news summary answering the question {``}What is the article about?{''}. We collect a real-world, large-scale dataset for this task by harvesting online articles from the British Broadcasting Corporation (BBC). We propose a novel abstractive model which is conditioned on the article{'}s topics and based entirely on convolutional neural networks. We demonstrate experimentally that this architecture captures long-range dependencies in a document and recognizes pertinent content, outperforming an oracle extractive system and state-of-the-art abstractive approaches when evaluated automatically and by humans.",
}

@inproceedings{10.5555/2969239.2969428,
author = {Hermann, Karl Moritz and Ko\v{c}isk\'{y}, Tom\'{a}\v{s} and Grefenstette, Edward and Espeholt, Lasse and Kay, Will and Suleyman, Mustafa and Blunsom, Phil},
title = {Teaching Machines to Read and Comprehend},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1693–1701},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}


@inproceedings{bommasani2020interpreting,
    title = "{I}nterpreting {P}retrained {C}ontextualized {R}epresentations via {R}eductions to {S}tatic {E}mbeddings",
    author = "Bommasani, Rishi  and
      Davis, Kelly  and
      Cardie, Claire",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.431",
    doi = "10.18653/v1/2020.acl-main.431",
    pages = "4758--4781",
    abstract = "Contextualized representations (e.g. ELMo, BERT) have become the default pretrained representations for downstream NLP applications. In some settings, this transition has rendered their static embedding predecessors (e.g. Word2Vec, GloVe) obsolete. As a side-effect, we observe that older interpretability methods for static embeddings {---} while more diverse and mature than those available for their dynamic counterparts {---} are underutilized in studying newer contextualized representations. Consequently, we introduce simple and fully general methods for converting from contextualized representations to static lookup-table embeddings which we apply to 5 popular pretrained models and 9 sets of pretrained weights. Our analysis of the resulting static embeddings notably reveals that pooling over many contexts significantly improves representational quality under intrinsic evaluation. Complementary to analyzing representational quality, we consider social biases encoded in pretrained representations with respect to gender, race/ethnicity, and religion and find that bias is encoded disparately across pretrained models and internal layers even for models with the same training data. Concerningly, we find dramatic inconsistencies between social bias estimators for word embeddings.",
}


@inproceedings{nangia2020crows,
    title = "{C}row{S}-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models",
    author = "Nangia, Nikita  and
      Vania, Clara  and
      Bhalerao, Rasika  and
      Bowman, Samuel R.",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.154",
    doi = "10.18653/v1/2020.emnlp-main.154",
    pages = "1953--1967",
    abstract = "Pretrained language models, especially masked language models (MLMs) have seen success across many NLP tasks. However, there is ample evidence that they use the cultural biases that are undoubtedly present in the corpora they are trained on, implicitly creating harm with biased representations. To measure some forms of social bias in language models against protected demographic groups in the US, we introduce the Crowdsourced Stereotype Pairs benchmark (CrowS-Pairs). CrowS-Pairs has 1508 examples that cover stereotypes dealing with nine types of bias, like race, religion, and age. In CrowS-Pairs a model is presented with two sentences: one that is more stereotyping and another that is less stereotyping. The data focuses on stereotypes about historically disadvantaged groups and contrasts them with advantaged groups. We find that all three of the widely-used MLMs we evaluate substantially favor sentences that express stereotypes in every category in CrowS-Pairs. As work on building less biased models advances, this dataset can be used as a benchmark to evaluate progress.",
}


@article{nissim2020fair,
    title = "Fair Is Better than Sensational: Man Is to Doctor as Woman Is to Doctor",
    author = "Nissim, Malvina  and
      van Noord, Rik  and
      van der Goot, Rob",
    journal = "Computational Linguistics",
    volume = "46",
    number = "2",
    month = jun,
    year = "2020",
    url = "https://aclanthology.org/2020.cl-2.7",
    doi = "10.1162/coli_a_00379",
    pages = "487--497",
    abstract = "Analogies such as man is to king as woman is to X are often used to illustrate the amazing power of word embeddings. Concurrently, they have also been used to expose how strongly human biases are encoded in vector spaces trained on natural language, with examples like man is to computer programmer as woman is to homemaker. Recent work has shown that analogies are in fact not an accurate diagnostic for bias, but this does not mean that they are not used anymore, or that their legacy is fading. Instead of focusing on the intrinsic problems of the analogy task as a bias detection tool, we discuss a series of issues involving implementation as well as subjective choices that might have yielded a distorted picture of bias in word embeddings. We stand by the truth that human biases are present in word embeddings, and, of course, the need to address them. But analogies are not an accurate tool to do so, and the way they have been most often used has exacerbated some possibly non-existing biases and perhaps hidden others. Because they are still widely popular, and some of them have become classics within and outside the NLP community, we deem it important to provide a series of clarifications that should put well-known, and potentially new analogies, into the right perspective.",
}

@inproceedings{ethayarajh2019understanding,
    title = "Understanding Undesirable Word Embedding Associations",
    author = "Ethayarajh, Kawin  and
      Duvenaud, David  and
      Hirst, Graeme",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1166",
    doi = "10.18653/v1/P19-1166",
    pages = "1696--1705",
    abstract = "Word embeddings are often criticized for capturing undesirable word associations such as gender stereotypes. However, methods for measuring and removing such biases remain poorly understood. We show that for any embedding model that implicitly does matrix factorization, debiasing vectors post hoc using subspace projection (Bolukbasi et al., 2016) is, under certain conditions, equivalent to training on an unbiased corpus. We also prove that WEAT, the most common association test for word embeddings, systematically overestimates bias. Given that the subspace projection method is provably effective, we use it to derive a new measure of association called the relational inner product association (RIPA). Experiments with RIPA reveal that, on average, skipgram with negative sampling (SGNS) does not make most words any more gendered than they are in the training corpus. However, for gender-stereotyped words, SGNS actually amplifies the gender association in the corpus.",
}

@inproceedings{selbst2019fairness,
author = {Selbst, Andrew D. and Boyd, Danah and Friedler, Sorelle A. and Venkatasubramanian, Suresh and Vertesi, Janet},
title = {Fairness and Abstraction in Sociotechnical Systems},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287598},
doi = {10.1145/3287560.3287598},
abstract = {A key goal of the fair-ML community is to develop machine-learning based systems that, once introduced into a social context, can achieve social and legal outcomes such as fairness, justice, and due process. Bedrock concepts in computer science---such as abstraction and modular design---are used to define notions of fairness and discrimination, to produce fairness-aware learning algorithms, and to intervene at different stages of a decision-making pipeline to produce "fair" outcomes. In this paper, however, we contend that these concepts render technical interventions ineffective, inaccurate, and sometimes dangerously misguided when they enter the societal context that surrounds decision-making systems. We outline this mismatch with five "traps" that fair-ML work can fall into even as it attempts to be more context-aware in comparison to traditional data science. We draw on studies of sociotechnical systems in Science and Technology Studies to explain why such traps occur and how to avoid them. Finally, we suggest ways in which technical designers can mitigate the traps through a refocusing of design in terms of process rather than solutions, and by drawing abstraction boundaries to include social actors rather than purely technical ones.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {59–68},
numpages = {10},
keywords = {Sociotechnical Systems, Interdisciplinary, Fairness-aware Machine Learning},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}

@inproceedings{pavlopoulos2020toxicity,
    title = "Toxicity Detection: Does Context Really Matter?",
    author = "Pavlopoulos, John  and
      Sorensen, Jeffrey  and
      Dixon, Lucas  and
      Thain, Nithum  and
      Androutsopoulos, Ion",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.396",
    doi = "10.18653/v1/2020.acl-main.396",
    pages = "4296--4305",
    abstract = "Moderation is crucial to promoting healthy online discussions. Although several {`}toxicity{'} detection datasets and models have been published, most of them ignore the context of the posts, implicitly assuming that comments may be judged independently. We investigate this assumption by focusing on two questions: (a) does context affect the human judgement, and (b) does conditioning on context improve performance of toxicity detection systems? We experiment with Wikipedia conversations, limiting the notion of context to the previous post in the thread and the discussion title. We find that context can both amplify or mitigate the perceived toxicity of posts. Moreover, a small but significant subset of manually labeled posts (5{\%} in one of our experiments) end up having the opposite toxicity labels if the annotators are not provided with context. Surprisingly, we also find no evidence that context actually improves the performance of toxicity classifiers, having tried a range of classifiers and mechanisms to make them context aware. This points to the need for larger datasets of comments annotated in context. We make our code and data publicly available.",
}

@inproceedings{blodgett2021stereotyping,
    title = "Stereotyping {N}orwegian Salmon: An Inventory of Pitfalls in Fairness Benchmark Datasets",
    author = "Blodgett, Su Lin  and
      Lopez, Gilsinia  and
      Olteanu, Alexandra  and
      Sim, Robert  and
      Wallach, Hanna",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.81",
    doi = "10.18653/v1/2021.acl-long.81",
    pages = "1004--1015",
    abstract = "Auditing NLP systems for computational harms like surfacing stereotypes is an elusive goal. Several recent efforts have focused on benchmark datasets consisting of pairs of contrastive sentences, which are often accompanied by metrics that aggregate an NLP system{'}s behavior on these pairs into measurements of harms. We examine four such benchmarks constructed for two NLP tasks: language modeling and coreference resolution. We apply a measurement modeling lens{---}originating from the social sciences{---}to inventory a range of pitfalls that threaten these benchmarks{'} validity as measurement models for stereotyping. We find that these benchmarks frequently lack clear articulations of what is being measured, and we highlight a range of ambiguities and unstated assumptions that affect how these benchmarks conceptualize and operationalize stereotyping.",
}

@inproceedings{du2021assessing,
    title = "Assessing the Reliability of Word Embedding Gender Bias Measures",
    author = "Du, Yupei  and
      Fang, Qixiang  and
      Nguyen, Dong",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.785",
    doi = "10.18653/v1/2021.emnlp-main.785",
    pages = "10012--10034",
    abstract = "Various measures have been proposed to quantify human-like social biases in word embeddings. However, bias scores based on these measures can suffer from measurement error. One indication of measurement quality is reliability, concerning the extent to which a measure produces consistent results. In this paper, we assess three types of reliability of word embedding gender bias measures, namely test-retest reliability, inter-rater consistency and internal consistency. Specifically, we investigate the consistency of bias scores across different choices of random seeds, scoring rules and words. Furthermore, we analyse the effects of various factors on these measures{'} reliability scores. Our findings inform better design of word embedding gender bias measures. Moreover, we urge researchers to be more critical about the application of such measures",
}

@inproceedings{hede2021toxicity,
    title = "From Toxicity in Online Comments to Incivility in {A}merican News: Proceed with Caution",
    author = "Hede, Anushree  and
      Agarwal, Oshin  and
      Lu, Linda  and
      Mutz, Diana C.  and
      Nenkova, Ani",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.225",
    doi = "10.18653/v1/2021.eacl-main.225",
    pages = "2620--2630",
    abstract = "The ability to quantify incivility online, in news and in congressional debates is of great interest to political scientists. Computational tools for detecting online incivility for English are now fairly accessible and potentially could be applied more broadly. We test the Jigsaw Perspective API for its ability to detect the degree of incivility on a corpus that we developed, consisting of manual annotations of civility in American news. We demonstrate that toxicity models, as exemplified by Perspective, are inadequate for the analysis of incivility in news. We carry out error analysis that points to the need to develop methods to remove spurious correlations between words often mentioned in the news, especially identity descriptors and incivility. Without such improvements, applying Perspective or similar models on news is likely to lead to wrong conclusions, that are not aligned with the human perception of incivility.",
}


@misc{stephanielin2021trutfulqa,
  doi = {10.48550/ARXIV.2109.07958},
  url = {https://arxiv.org/abs/2109.07958},
  author = {Lin, Stephanie and Hilton, Jacob and Evans, Owain},
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Computers and Society (cs.CY), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {TruthfulQA: Measuring How Models Mimic Human Falsehoods},
  publisher = {arXiv},
  year = {2021},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{liang2022helm,
  title={Holistic Evaluation of Language Models},
  author={Percy Liang and Rishi Bommasani and Tony Lee and Dimitris Tsipras and Dilara Soylu and Michihiro Yasunaga and Yian Zhang and Deepak Narayanan and Yuhuai Wu and Ananya Kumar and Benjamin Newman and Binhang Yuan and Bobby Yan and Ce Zhang and Christian Cosgrove and Christopher D. Manning and Christopher R'e and Diana Acosta-Navas and Drew A. Hudson and E. Zelikman and Esin Durmus and Faisal Ladhak and Frieda Rong and Hongyu Ren and Huaxiu Yao and Jue Wang and Keshav Santhanam and Laurel J. Orr and Lucia Zheng and Mert Yuksekgonul and Mirac Suzgun and Nathan S. Kim and Neel Guha and Niladri S. Chatterji and O. Khattab and Peter Henderson and Qian Huang and Ryan Chi and Sang Michael Xie and Shibani Santurkar and Surya Ganguli and Tatsunori Hashimoto and Thomas F. Icard and Tianyi Zhang and Vishrav Chaudhary and William Wang and Xuechen Li and Yifan Mai and Yuhui Zhang and Yuta Koreeda},
  journal={ArXiv},
  year={2022},
  volume={abs/2211.09110}
}

@inproceedings{gururangan2022whose,
  title={Whose Language Counts as High Quality? Measuring Language Ideologies in Text Data Selection},
  author={Suchin Gururangan and Dallas Card and Sarah K. Drier and Emily Kalah Gade and Leroy Z. Wang and Zeyu Wang and Luke Zettlemoyer and Noah A. Smith},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2022}
}

@misc{ziems2022value,
  doi = {10.48550/ARXIV.2204.03031},
  url = {https://arxiv.org/abs/2204.03031},
  author = {Ziems, Caleb and Chen, Jiaao and Harris, Camille and Anderson, Jessica and Yang, Diyi},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {VALUE: Understanding Dialect Disparity in NLU},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution 4.0 International}
}

@article{lees2022perspective,
  title={A New Generation of Perspective API: Efficient Multilingual Character-level Transformers},
  author={Alyssa Lees and Vinh Q. Tran and Yi Tay and Jeffrey Scott Sorensen and Jai Gupta and Donald Metzler and Lucy Vasserman},
  journal={Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  year={2022}
}

@inproceedings{gordon2022jury,
author = {Gordon, Mitchell L. and Lam, Michelle S. and Park, Joon Sung and Patel, Kayur and Hancock, Jeff and Hashimoto, Tatsunori and Bernstein, Michael S.},
title = {Jury Learning: Integrating Dissenting Voices into Machine Learning Models},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3502004},
doi = {10.1145/3491102.3502004},
abstract = {Whose labels should a machine learning (ML) algorithm learn to emulate? For ML tasks ranging from online comment toxicity to misinformation detection to medical diagnosis, different groups in society may have irreconcilable disagreements about ground truth labels. Supervised ML today resolves these label disagreements implicitly using majority vote, which overrides minority groups’ labels. We introduce jury learning, a supervised ML approach that resolves these disagreements explicitly through the metaphor of a jury: defining which people or groups, in what proportion, determine the classifier’s prediction. For example, a jury learning model for online toxicity might centrally feature women and Black jurors, who are commonly targets of online harassment. To enable jury learning, we contribute a deep learning architecture that models every annotator in a dataset, samples from annotators’ models to populate the jury, then runs inference to classify. Our architecture enables juries that dynamically adapt their composition, explore counterfactuals, and visualize dissent. A field evaluation finds that practitioners construct diverse juries that alter 14% of classification outcomes.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {115},
numpages = {19},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@misc{lauscher2022pronouns,
  doi = {10.48550/ARXIV.2202.11923},
  url = {https://arxiv.org/abs/2202.11923},
  author = {Lauscher, Anne and Crowley, Archie and Hovy, Dirk},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Welcome to the Modern World of Pronouns: Identity-Inclusive Natural Language Processing beyond Gender},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution 4.0 International}
}

@article{garg2018stereotypes,
  author = {Nikhil Garg  and Londa Schiebinger  and Dan Jurafsky  and James Zou},
  title = {Word embeddings quantify 100 years of gender and ethnic stereotypes},
  journal = {Proceedings of the National Academy of Sciences},
  number = {16},
  pages = {E3635-E3644},
  year = {2018},
  doi = {10.1073/pnas.1720347115},
  URL = {https://www.pnas.org/doi/abs/10.1073/pnas.1720347115},
  eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.1720347115},
}

@article{coleman2017dawnbench,
  title={Dawnbench: An end-to-end deep learning benchmark and competition},
  author={Coleman, Cody and Narayanan, Deepak and Kang, Daniel and Zhao, Tian and Zhang, Jian and Nardi, Luigi and Bailis, Peter and Olukotun, Kunle and R{\'e}, Chris and Zaharia, Matei},
  year=2017,
}

@inproceedings{zehlike2017fair,
author = {Zehlike, Meike and Bonchi, Francesco and Castillo, Carlos and Hajian, Sara and Megahed, Mohamed and Baeza-Yates, Ricardo},
title = {FA*IR: A Fair Top-k Ranking Algorithm},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132938},
doi = {10.1145/3132847.3132938},
abstract = {In this work, we define and solve the Fair Top-k Ranking problem, in which we want to determine a subset of k candidates from a large pool of n » k candidates, maximizing utility (i.e., select the "best" candidates) subject to group fairness criteria.Our ranked group fairness definition extends group fairness using the standard notion of protected groups and is based on ensuring that the proportion of protected candidates in every prefix of the top-k ranking remains statistically above or indistinguishable from a given minimum. Utility is operationalized in two ways: (i) every candidate included in the top-k should be more qualified than every candidate not included; and (ii) for every pair of candidates in the top-k, the more qualified candidate should be ranked above.An efficient algorithm is presented for producing the Fair Top-k Ranking, and tested experimentally on existing datasets as well as new datasets released with this paper, showing that our approach yields small distortions with respect to rankings that maximize utility without considering fairness criteria. To the best of our knowledge, this is the first algorithm grounded in statistical tests that can mitigate biases in the representation of an under-represented group along a ranked list.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1569–1578},
numpages = {10},
keywords = {algorithmic fairness, ranking, top-k selection, bias in computer systems},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@misc{chalabi2017dear,
  title={Dear Mona, what’s the most common name in America},
  author={Chalabi, Mona and Flowers, Andrew},
  year={2017}
}

@inproceedings{talat2017understanding,
    title = "Understanding Abuse: A Typology of Abusive Language Detection Subtasks",
    author = "Talat, Zeerak  and
      Davidson, Thomas  and
      Warmsley, Dana  and
      Weber, Ingmar",
    booktitle = "Proceedings of the First Workshop on Abusive Language Online",
    month = aug,
    year = "2017",
    address = "Vancouver, BC, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W17-3012",
    doi = "10.18653/v1/W17-3012",
    pages = "78--84",
    abstract = "As the body of research on abusive language detection and analysis grows, there is a need for critical consideration of the relationships between different subtasks that have been grouped under this label. Based on work on hate speech, cyberbullying, and online abuse we propose a typology that captures central similarities and differences between subtasks and discuss the implications of this for data annotation and feature construction. We emphasize the practical actions that can be taken by researchers to best approach their abusive language detection subtask of interest.",
}

@article{greenwald1998measuring,
  title={Measuring individual differences in implicit cognition: the implicit association test.},
  author={Greenwald, Anthony G and McGhee, Debbie E and Schwartz, Jordan LK},
  journal={Journal of personality and social psychology},
  volume={74},
  number={6},
  pages={1464},
  year={1998},
  publisher={American Psychological Association}
}


@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@inproceedings{halevy2021mitigating,
author = {Halevy, Matan and Harris, Camille and Bruckman, Amy and Yang, Diyi and Howard, Ayanna},
title = {Mitigating Racial Biases in Toxic Language Detection with an Equity-Based Ensemble Framework},
year = {2021},
isbn = {9781450385534},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465416.3483299},
doi = {10.1145/3465416.3483299},
abstract = {Recent research has demonstrated how racial biases against users who write African American English exists in popular toxic language datasets. While previous work has focused on a single fairness criteria, we propose to use additional descriptive fairness metrics to better understand the source of these biases. We demonstrate that different benchmark classifiers, as well as two in-process bias-remediation techniques, propagate racial biases even in a larger corpus. We then propose a novel ensemble-framework that uses a specialized classifier that is fine-tuned to the African American English dialect. We show that our proposed framework substantially reduces the racial biases that the model learns from these datasets. We demonstrate how the ensemble framework improves fairness metrics across all sample datasets with minimal impact on the classification performance, and provide empirical evidence for its ability to unlearn the annotation biases towards authors who use African American English. ** Please note that this work may contain examples of offensive words and phrases.},
booktitle = {Equity and Access in Algorithms, Mechanisms, and Optimization},
articleno = {7},
numpages = {11},
keywords = {moderation, bias mitigation, hate speech detection, AI fairness},
location = {--, NY, USA},
series = {EAAMO '21}
}

@misc{zhong2021arlsat,
      title={AR-LSAT: Investigating Analytical Reasoning of Text}, 
      author={Wanjun Zhong and Siyuan Wang and Duyu Tang and Zenan Xu and Daya Guo and Jiahai Wang and Jian Yin and Ming Zhou and Nan Duan},
      year={2021},
      eprint={2104.06598},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{hendrycks2021measuringcoding,
  title={Measuring coding challenge competence with apps},
  author={Hendrycks, Dan and Basart, Steven and Kadavath, Saurav and Mazeika, Mantas and Arora, Akul and Guo, Ethan and Burns, Collin and Puranik, Samir and He, Horace and Song, Dawn and others},
  journal={Advances in Neural Information Processing Systems (NeurIPS) Datasets and Benchmarks Track},
  year={2021}
}

@article{codex,
  author    = {Mark Chen and
               Jerry Tworek and
               Heewoo Jun and
               Qiming Yuan and
               Henrique Ponde de Oliveira Pinto and
               Jared Kaplan and
               Harrison Edwards and
               Yuri Burda and
               Nicholas Joseph and
               Greg Brockman and
               Alex Ray and
               Raul Puri and
               Gretchen Krueger and
               Michael Petrov and
               Heidy Khlaaf and
               Girish Sastry and
               Pamela Mishkin and
               Brooke Chan and
               Scott Gray and
               Nick Ryder and
               Mikhail Pavlov and
               Alethea Power and
               Lukasz Kaiser and
               Mohammad Bavarian and
               Clemens Winter and
               Philippe Tillet and
               Felipe Petroski Such and
               Dave Cummings and
               Matthias Plappert and
               Fotios Chantzis and
               Elizabeth Barnes and
               Ariel Herbert{-}Voss and
               William Hebgen Guss and
               Alex Nichol and
               Alex Paino and
               Nikolas Tezak and
               Jie Tang and
               Igor Babuschkin and
               Suchir Balaji and
               Shantanu Jain and
               William Saunders and
               Christopher Hesse and
               Andrew N. Carr and
               Jan Leike and
               Joshua Achiam and
               Vedant Misra and
               Evan Morikawa and
               Alec Radford and
               Matthew Knight and
               Miles Brundage and
               Mira Murati and
               Katie Mayer and
               Peter Welinder and
               Bob McGrew and
               Dario Amodei and
               Sam McCandlish and
               Ilya Sutskever and
               Wojciech Zaremba},
  title     = {Evaluating Large Language Models Trained on Code},
  journal   = {CoRR},
  volume    = {abs/2107.03374},
  year      = {2021},
  url       = {https://arxiv.org/abs/2107.03374},
  eprinttype = {arXiv},
  eprint    = {2107.03374},
  timestamp = {Tue, 20 Jul 2021 15:08:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2107-03374.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@misc{c4,
  doi = {10.48550/ARXIV.1910.10683},
  url = {https://arxiv.org/abs/1910.10683},
  
  author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
  keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inbook{singh2019policy,
author = {Singh, Ashudeep and Joachims, Thorsten},
title = {Policy Learning for Fairness in Ranking},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Conventional Learning-to-Rank (LTR) methods optimize the utility of the rankings to the users, but they are oblivious to their impact on the ranked items. However, there has been a growing understanding that the latter is important to consider for a wide range of ranking applications (e.g. online marketplaces, job placement, admissions). To address this need, we propose a general LTR framework that can optimize a wide range of utility metrics (e.g. NDCG) while satisfying fairness of exposure constraints with respect to the items. This framework expands the class of learnable ranking functions to stochastic ranking policies, which provides a language for rigorously expressing fairness specifications. Furthermore, we provide a new LTR algorithm called FAIR-PG-RANK for directly searching the space of fair ranking policies via a policy-gradient approach. Beyond the theoretical evidence in deriving the framework and the algorithm, we provide empirical results on simulated and real-world datasets verifying the effectiveness of the approach in individual and group-fairness settings.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {487},
numpages = {11}
}

@inproceedings{Radford2019LanguageMA,
  title={Language Models are Unsupervised Multitask Learners},
  author={Alec Radford and Jeff Wu and Rewon Child and David Luan and Dario Amodei and Ilya Sutskever},
  year={2019}
}

@misc{ppo,
  doi = {10.48550/ARXIV.1707.06347},
  url = {https://arxiv.org/abs/1707.06347},
  author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Proximal Policy Optimization Algorithms},
  publisher = {arXiv},
  year = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{suzgun2019memory,
  title={Memory-augmented recurrent neural networks can learn generalized dyck languages},
  author={Suzgun, Mirac and Gehrmann, Sebastian and Belinkov, Yonatan and Shieber, Stuart M},
  journal={arXiv preprint arXiv:1911.03329},
  year={2019}
}

@article{buchanan2021truth,
  title={Truth, Lies, and Automation},
  author={Buchanan, Ben and Lohn, Andrew and Musser, Micah and Sedova, Katerina},
  year={2021}
}

@inproceedings{gabriel-etal-2022-misinfo,
    title = "Misinfo Reaction Frames: Reasoning about Readers{'} Reactions to News Headlines",
    author = "Gabriel, Saadia  and
      Hallinan, Skyler  and
      Sap, Maarten  and
      Nguyen, Pemi  and
      Roesner, Franziska  and
      Choi, Eunsol  and
      Choi, Yejin",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.222",
    doi = "10.18653/v1/2022.acl-long.222",
    pages = "3108--3127",
}

@article{ruis2022large,
  title={Large language models are not zero-shot communicators},
  author={Laura Ruis and Akbir Khan and Stella Rose Biderman and Sara Hooker and Tim Rocktaschel and Edward Grefenstette},
  journal={ArXiv},
  year={2022},
  volume={abs/2210.14986}
}

@article{biderman2022datasheet,
  title={Datasheet for the Pile},
  author={Stella Rose Biderman and Kieran Bicheno and Leo Gao},
  journal={ArXiv},
  year={2022},
  volume={abs/2201.07311}
}

@article{
wei2022emergent,
title={Emergent Abilities of Large Language Models},
author={Jason Wei and Yi Tay and Rishi Bommasani and Colin Raffel and Barret Zoph and Sebastian Borgeaud and Dani Yogatama and Maarten Bosma and Denny Zhou and Donald Metzler and Ed H. Chi and Tatsunori Hashimoto and Oriol Vinyals and Percy Liang and Jeff Dean and William Fedus},
journal={Transactions on Machine Learning Research},
year={2022},
url={https://openreview.net/forum?id=yzkSU5zdwD},
note={Survey Certification}
}

@inproceedings{diaz2022accounting,
    title = "Accounting for Offensive Speech as a Practice of Resistance",
    author = "Diaz, Mark  and
      Amironesei, Razvan  and
      Weidinger, Laura  and
      Gabriel, Iason",
    booktitle = "Proceedings of the Sixth Workshop on Online Abuse and Harms (WOAH)",
    month = jul,
    year = "2022",
    address = "Seattle, Washington (Hybrid)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.woah-1.18",
    doi = "10.18653/v1/2022.woah-1.18",
    pages = "192--202",
    abstract = "Tasks such as toxicity detection, hate speech detection, and online harassment detection have been developed for identifying interactions involving offensive speech. In this work we articulate the need for a relational understanding of offensiveness to help distinguish denotative offensive speech from offensive speech serving as a mechanism through which marginalized communities resist oppressive social norms. Using examples from the queer community, we argue that evaluations of offensive speech must focus on the impacts of language use. We call this the cynic perspective{--} or a characteristic of language with roots in Cynic philosophy that pertains to employing offensive speech as a practice of resistance. We also explore the degree to which NLP systems may encounter limits to modeling relational context.",
}

@inproceedings{kasirzadeh2022conversation,
  title={In conversation with Artificial Intelligence: aligning language models with human values},
  author={Atoosa Kasirzadeh and Iason Gabriel},
  year={2022}
}

@unpublished{grossman-disinformation,
title= {CRFM: Shelby Grossman on Disinformation},
author = {Shelby Grossman},
year = {2022},
note= {CRFM Seminar Meeting},
}

@misc{comicmix,
  author = {U.S. Copyright Office Fair Use Index},
  title = {{Dr. Seuss Enters., L.P. v. ComicMix LLC}},
  howpublished = "\url{https://www.copyright.gov/fair-use/summaries/drseuss-comicmix-9thcir2020.pdf}",
  year = {2020}, 
}

@article{nimmer2017juries,
  title={Juries and the Development of Fair Use Standards},
  author={Nimmer, David},
  journal={Harv. JL \& Tech.},
  volume={31},
  pages={563},
  year={2017},
  publisher={HeinOnline}
}

@article{howard2017addressing,
  title={Addressing bias in machine learning algorithms: A pilot study on emotion recognition for intelligent systems},
  author={Ayanna M. Howard and Cha Zhang and Eric Horvitz},
  journal={2017 IEEE Workshop on Advanced Robotics and its Social Impacts (ARSO)},
  year={2017},
  pages={1-7}
}

@article{carlini2022quantifying,
  title={Quantifying memorization across neural language models},
  author={Carlini, Nicholas and Ippolito, Daphne and Jagielski, Matthew and Lee, Katherine and Tramer, Florian and Zhang, Chiyuan},
  journal={arXiv preprint arXiv:2202.07646},
  year={2022}
}

@article{horvitz2022horizon,
  title={On the Horizon: Interactive and Compositional Deepfakes},
  author={Eric Horvitz},
  journal={Proceedings of the 2022 International Conference on Multimodal Interaction},
  year={2022}
}

@inproceedings{carlini2021extracting,
  title={Extracting training data from large language models},
  author={Carlini, Nicholas and Tramer, Florian and Wallace, Eric and Jagielski, Matthew and Herbert-Voss, Ariel and Lee, Katherine and Roberts, Adam and Brown, Tom and Song, Dawn and Erlingsson, Ulfar and others},
  booktitle={30th USENIX Security Symposium (USENIX Security 21)},
  pages={2633--2650},
  year={2021}
}

@article{sun2021coprotector,
  title={CoProtector: Protect Open-Source Code against Unauthorized Training Usage with Data Poisoning},
  author={Sun, Zhensu and Du, Xiaoning and Song, Fu and Ni, Mingze and Li, Li},
  journal={arXiv preprint arXiv:2110.12925},
  year={2021}
}

@article{englund1990idea,
  title={Idea, Process, or Protected Expression?: Determining the Scope of Copyright Protection of the Structure of Computer Programs},
  author={Englund, Steven R},
  journal={Michigan Law Review},
  volume={88},
  number={4},
  pages={866--909},
  year={1990},
  publisher={JSTOR}
}

@article{heymann2008everything,
  title={Everything is transformative: Fair use and reader response},
  author={Heymann, Laura A},
  journal={Columbia Journal of Law \& the Arts},
  volume={31},
  pages={08--06},
  year={2008}
}

@article{manning2008introduction,
  title={Introduction to Information Retrieval},
  author={Manning, Christopher D and Raghavan, Prabhakar and Sch{\"u}tze, Hinrich},
  year={2008},
  publisher={Cambridge University Press}
}

@article{lester2017dilemma,
  title={The Dilemma of False Positives: Making Content ID Algorithms More Conducive to Fostering Innovative Fair Use in Media Creation},
  author={Lester, Toni and Pachamanova, Dessislava},
  journal={UCLA Ent. L. Rev.},
  volume={24},
  pages={51},
  year={2017},
  publisher={HeinOnline}
}

@inproceedings{wang2019talkdown,
    title = "{T}alk{D}own: A Corpus for Condescension Detection in Context",
    author = "Wang, Zijian  and
      Potts, Christopher",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1385",
    doi = "10.18653/v1/D19-1385",
    pages = "3711--3719",
    abstract = "Condescending language use is caustic; it can bring dialogues to an end and bifurcate communities. Thus, systems for condescension detection could have a large positive impact. A challenge here is that condescension is often impossible to detect from isolated utterances, as it depends on the discourse and social context. To address this, we present TalkDown, a new labeled dataset of condescending linguistic acts in context. We show that extending a language-only model with representations of the discourse improves performance, and we motivate techniques for dealing with the low rates of condescension overall. We also use our model to estimate condescension rates in various online communities and relate these differences to differing community norms.",
}

@article{kaplan2020scaling,
  title={Scaling Laws for Neural Language Models},
  author={Jared Kaplan and Sam McCandlish and T. J. Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeff Wu and Dario Amodei},
  journal={ArXiv},
  year={2020},
  volume={abs/2001.08361}
}

@ARTICLE{mendelsohn2020framework,
  
AUTHOR={Mendelsohn, Julia and Tsvetkov, Yulia and Jurafsky, Dan},   
	 
TITLE={A Framework for the Computational Linguistic Analysis of Dehumanization},      
	
JOURNAL={Frontiers in Artificial Intelligence},      
	
VOLUME={3},           
	
YEAR={2020},      
	  
URL={https://www.frontiersin.org/articles/10.3389/frai.2020.00055},       
	
DOI={10.3389/frai.2020.00055},      
	
ISSN={2624-8212},   
   
ABSTRACT={Dehumanization is a pernicious psychological process that often leads to extreme intergroup bias, hate speech, and violence aimed at targeted social groups. Despite these serious consequences and the wealth of available data, dehumanization has not yet been computationally studied on a large scale. Drawing upon social psychology research, we create a computational linguistic framework for analyzing dehumanizing language by identifying linguistic correlates of salient components of dehumanization. We then apply this framework to analyze discussions of LGBTQ people in the New York Times from 1986 to 2015. Overall, we find increasingly humanizing descriptions of LGBTQ people over time. However, we find that the label homosexual has emerged to be much more strongly associated with dehumanizing attitudes than other labels, such as gay. Our proposed techniques highlight processes of linguistic variation and change in discourses surrounding marginalized groups. Furthermore, the ability to analyze dehumanizing language at a large scale has implications for automatically detecting and understanding media bias as well as abusive language online.}
}

@article{yu2020can,
  title={Can Algorithms Promote Fair Use?},
  author={Yu, Peter K},
  journal={FIU L. Rev.},
  volume={14},
  pages={329},
  year={2020},
  publisher={HeinOnline}
}


@article{bartholomew2014death,
  title={The death of fair use in cyberspace: YouTube and the problem with content ID},
  author={Bartholomew, Taylor B},
  journal={Duke L. \& Tech. Rev.},
  volume={13},
  pages={66},
  year={2014},
  publisher={HeinOnline}
}


@article{samuelson2017saving,
  title={Saving Software's Fair Use Future},
  author={Samuelson, Pamela and Asay, Clark D},
  journal={Harv. JL \& Tech.},
  volume={31},
  pages={535},
  year={2017},
  publisher={HeinOnline}
}

@article{franceschelli2021copyright,
  title={Copyright in Generative Deep Learning},
  author={Franceschelli, Giorgio and Musolesi, Mirco},
  journal={arXiv preprint arXiv:2105.09266},
  year={2021}
}


@article{gray2020playing,
  title={Playing with machines: Using machine learning to understand automated copyright enforcement at scale},
  author={Gray, Joanne E and Suzor, Nicolas P},
  journal={Big Data \& Society},
  volume={7},
  number={1},
  pages={2053951720919963},
  year={2020},
  publisher={SAGE Publications Sage UK: London, England}
}

@inproceedings{saadatpanah2020adversarial,
  title={Adversarial attacks on copyright detection systems},
  author={Saadatpanah, Parsa and Shafahi, Ali and Goldstein, Tom},
  booktitle={International Conference on Machine Learning},
  pages={8307--8315},
  year={2020},
  organization={PMLR}
}


@article{gillotte2019copyright,
  title={Copyright infringement in ai-generated artworks},
  author={Gillotte, Jessica L},
  journal={UC Davis L. Rev.},
  volume={53},
  pages={2655},
  year={2019},
  publisher={HeinOnline}
}



@article{tu2020use,
  title={Use of Artificial Intelligence to Determine Copyright Liability for Musical Works},
  author={Tu, Shine Sean},
  journal={WVU College of Law Research Paper},
  number={2020-012},
  year={2020}
}


@article{nimmer2003fairest,
  title={" Fairest of them all" and other fairy tales of fair use},
  author={Nimmer, David},
  journal={Law and Contemporary Problems},
  volume={66},
  number={1/2},
  pages={263--287},
  year={2003},
  publisher={JSTOR}
}


@article{nimmer2017juries,
  title={Juries and the Development of Fair Use Standards},
  author={Nimmer, David},
  journal={Harv. JL \& Tech.},
  volume={31},
  pages={563},
  year={2017},
  publisher={HeinOnline}
}


@article{asay2020transformative,
  title={Is transformative use eating the world},
  author={Asay, Clark D and Sloan, Arielle and Sobczak, Dean},
  journal={BCL Rev.},
  volume={61},
  pages={905},
  year={2020},
  publisher={HeinOnline}
}


@inproceedings{carlini2019secret,
  title={The secret sharer: Evaluating and testing unintended memorization in neural networks},
  author={Carlini, Nicholas and Liu, Chang and Erlingsson, {\'U}lfar and Kos, Jernej and Song, Dawn},
  booktitle={28th USENIX Security 19)},
  pages={267--284},
  year={2019}
}

@article{lee2022language,
  title={Do Language Models Plagiarize?},
  author={Lee, Jooyoung and Le, Thai and Chen, Jinghui and Lee, Dongwon},
  journal={arXiv preprint arXiv:2203.07618},
  year={2022}
}


@article{lin2022conceptual,
author = {Lin, Jimmy},
title = {A Proposed Conceptual Framework for a Representational Approach to Information Retrieval},
year = {2022},
issue_date = {December 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {2},
issn = {0163-5840},
url = {https://doi.org/10.1145/3527546.3527552},
doi = {10.1145/3527546.3527552},
abstract = {This paper outlines a conceptual framework for understanding recent developments in information retrieval and natural language processing that attempts to integrate dense and sparse retrieval methods. I propose a representational approach that breaks the core text retrieval problem into a logical scoring model and a physical retrieval model. The scoring model is defined in terms of encoders, which map queries and documents into a representational space, and a comparison function that computes query-document scores. The physical retrieval model defines how a system produces the top-k scoring documents from an arbitrarily large corpus with respect to a query. The scoring model can be further analyzed along two dimensions: dense vs. sparse representations and supervised (learned) vs. unsupervised approaches. I show that many recently proposed retrieval methods, including multi-stage ranking designs, can be seen as different parameterizations in this framework, and that a unified view suggests a number of open research questions, providing a roadmap for future work. As a bonus, this conceptual framework establishes connections to sentence similarity tasks in natural language processing and information access "technologies" prior to the dawn of computing.},
journal = {SIGIR Forum},
month = {mar},
articleno = {4},
numpages = {29}
}

@article{li2022competition,
  title={Competition-level code generation with alphacode},
  author={Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate and Schrittwieser, Julian and Leblond, R{\'e}mi and Eccles, Tom and Keeling, James and Gimeno, Felix and Lago, Agustin Dal and others},
  journal={arXiv preprint arXiv:2203.07814},
  year={2022}
}


@article{lee2021deduplicating,
  title={Deduplicating training data makes language models better},
  author={Lee, Katherine and Ippolito, Daphne and Nystrom, Andrew and Zhang, Chiyuan and Eck, Douglas and Callison-Burch, Chris and Carlini, Nicholas},
  journal={arXiv preprint arXiv:2107.06499},
  year={2021}
}


@article{snow2019decides,
  title={Who Decides Fair Use-Judge or Jury},
  author={Snow, Ned},
  journal={Wash. L. Rev.},
  volume={94},
  pages={275},
  year={2019},
  publisher={HeinOnline}
}

@article{dhole2021nl,
  author    = {Kaustubh D. Dhole and
               Varun Gangal and
               Sebastian Gehrmann and
               Aadesh Gupta and
               Zhenhao Li and
               Saad Mahamood and
               Abinaya Mahendiran and
               Simon Mille and
               Ashish Srivastava and
               Samson Tan and
               Tongshuang Wu and
               Jascha Sohl{-}Dickstein and
               Jinho D. Choi and
               Eduard H. Hovy and
               Ondrej Dusek and
               Sebastian Ruder and
               Sajant Anand and
               Nagender Aneja and
               Rabin Banjade and
               Lisa Barthe and
               Hanna Behnke and
               Ian Berlot{-}Attwell and
               Connor Boyle and
               Caroline Brun and
               Marco Antonio Sobrevilla Cabezudo and
               Samuel Cahyawijaya and
               Emile Chapuis and
               Wanxiang Che and
               Mukund Choudhary and
               Christian Clauss and
               Pierre Colombo and
               Filip Cornell and
               Gautier Dagan and
               Mayukh Das and
               Tanay Dixit and
               Thomas Dopierre and
               Paul{-}Alexis Dray and
               Suchitra Dubey and
               Tatiana Ekeinhor and
               Marco Di Giovanni and
               Rishabh Gupta and
               Rishabh Gupta and
               Louanes Hamla and
               Sang Han and
               Fabrice Harel{-}Canada and
               Antoine Honore and
               Ishan Jindal and
               Przemyslaw K. Joniak and
               Denis Kleyko and
               Venelin Kovatchev and
               et al.},
  title     = {{NL}-{A}ugmenter: {A} Framework for Task-Sensitive Natural Language Augmentation},
   journal={arXiv preprint arXiv:2112.02721},
  year      = {2021},
}

@article{miller1995wordnet,
  title={WordNet: a lexical database for English},
  author={Miller, George A},
  journal={Communications of the ACM},
  volume={38},
  number={11},
  pages={39--41},
  year={1995},
  publisher={ACM New York, NY, USA}
}

@article{abedjan2016detecting,
  title={Detecting data errors: Where are we and what needs to be done?},
  author={Abedjan, Ziawasch and Chu, Xu and Deng, Dong and Fernandez, Raul Castro and Ilyas, Ihab F and Ouzzani, Mourad and Papotti, Paolo and Stonebraker, Michael and Tang, Nan},
  journal={Proceedings of the VLDB Endowment},
  volume={9},
  number={12},
  pages={993--1004},
  year={2016},
  publisher={VLDB Endowment}
}
@article{li2020deep,
  title={Deep entity matching with pre-trained language models},
  author={Li, Yuliang and Li, Jinfeng and Suhara, Yoshihiko and Doan, AnHai and Tan, Wang-Chiew},
  journal={arXiv preprint arXiv:2004.00584},
  year={2020}
}
@article{narayan2022can,
  title={Can Foundation Models Wrangle Your Data?},
  author={Narayan, Avanika and Chami, Ines and Orr, Laurel and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:2205.09911},
  year={2022}
}

@inproceedings{golshan2017data,
  title={Data integration: After the teenage years},
  author={Golshan, Behzad and Halevy, Alon and Mihaila, George and Tan, Wang-Chiew},
  booktitle={Proceedings of the 36th ACM SIGMOD-SIGACT-SIGAI symposium on principles of database systems},
  pages={101--106},
  year={2017}
}

@article{konda2016magellan,
  title={Magellan: toward building entity matching management systems over data science stacks},
  author={Konda, Pradap and Das, Sanjib and Doan, AnHai and Ardalan, Adel and Ballard, Jeffrey R and Li, Han and Panahi, Fatemah and Zhang, Haojun and Naughton, Jeff and Prasad, Shishir and others},
  journal={Proceedings of the VLDB Endowment},
  volume={9},
  number={13},
  pages={1581--1584},
  year={2016},
  publisher={VLDB Endowment}
}

@inproceedings{tejaswin2021summarization,
    title = "How well do you know your summarization datasets?",
    author = "Tejaswin, Priyam  and
      Naik, Dhruv  and
      Liu, Pengfei",
    booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-acl.303",
    doi = "10.18653/v1/2021.findings-acl.303",
    pages = "3436--3449",
}


@inproceedings{mei2021capturing,
  title={Capturing Semantics for Imputation with Pre-trained Language Models},
  author={Mei, Yinan and Song, Shaoxu and Fang, Chenguang and Yang, Haifeng and Fang, Jingyun and Long, Jiang},
  booktitle={2021 IEEE 37th International Conference on Data Engineering (ICDE)},
  pages={61--72},
  year={2021},
  organization={IEEE}
}

@article{warstadt-etal-2020-blimp-benchmark,
    title = "{BL}i{MP}: The Benchmark of Linguistic Minimal Pairs for {E}nglish",
    author = "Warstadt, Alex  and
      Parrish, Alicia  and
      Liu, Haokun  and
      Mohananey, Anhad  and
      Peng, Wei  and
      Wang, Sheng-Fu  and
      Bowman, Samuel R.",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "8",
    year = "2020",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2020.tacl-1.25",
    doi = "10.1162/tacl_a_00321",
    pages = "377--392",
    abstract = "We introduce The Benchmark of Linguistic Minimal Pairs (BLiMP),1 a challenge set for evaluating the linguistic knowledge of language models (LMs) on major grammatical phenomena in English. BLiMP consists of 67 individual datasets, each containing 1,000 minimal pairs{---}that is, pairs of minimally different sentences that contrast in grammatical acceptability and isolate specific phenomenon in syntax, morphology, or semantics. We generate the data according to linguist-crafted grammar templates, and human aggregate agreement with the labels is 96.4{\%}. We evaluate n-gram, LSTM, and Transformer (GPT-2 and Transformer-XL) LMs by observing whether they assign a higher probability to the acceptable sentence in each minimal pair. We find that state-of-the-art models identify morphological contrasts related to agreement reliably, but they struggle with some subtle semantic and syntactic phenomena, such as negative polarity items and extraction islands.",
}

@inproceedings{yatskar2019qualitative,
    title = "A Qualitative Comparison of {C}o{QA}, {SQ}u{AD} 2.0 and {Q}u{AC}",
    author = "Yatskar, Mark",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1241",
    doi = "10.18653/v1/N19-1241",
    pages = "2318--2323",
    abstract = "We compare three new datasets for question answering: SQuAD 2.0, QuAC, and CoQA, along several of their new features: (1) unanswerable questions, (2) multi-turn interactions, and (3) abstractive answers.We show that the datasets provide complementary coverage of the first two aspects, but weak coverage of the third.Because of the datasets{'} structural similarity, a single extractive model can be easily adapted to any of the datasets and we show improved baseline results on both SQuAD 2.0 and CoQA. Despite the similarity, models trained on one dataset are ineffective on another dataset, but we find moderate performance improvement through pretraining. To encourage cross-evaluation, we release code for conversion between datasets.",
}

@inproceedings{jung2019earlier,
    title = "Earlier Isn{'}t Always Better: Sub-aspect Analysis on Corpus and System Biases in Summarization",
    author = "Jung, Taehee  and
      Kang, Dongyeop  and
      Mentch, Lucas  and
      Hovy, Eduard",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1327",
    doi = "10.18653/v1/D19-1327",
    pages = "3324--3335",
    abstract = "Despite the recent developments on neural summarization systems, the underlying logic behind the improvements from the systems and its corpus-dependency remains largely unexplored. Position of sentences in the original text, for example, is a well known bias for news summarization. Following in the spirit of the claim that summarization is a combination of sub-functions, we define three sub-aspects of summarization: position, importance, and diversity and conduct an extensive analysis of the biases of each sub-aspect with respect to the domain of nine different summarization corpora (e.g., news, academic papers, meeting minutes, movie script, books, posts). We find that while position exhibits substantial bias in news articles, this is not the case, for example, with academic papers and meeting minutes. Furthermore, our empirical study shows that different types of summarization systems (e.g., neural-based) are composed of different degrees of the sub-aspects. Our study provides useful lessons regarding consideration of underlying sub-aspects when collecting a new summarization dataset or developing a new system.",
}


@article{DBLP:journals/corr/abs-1903-04561,
  author    = {Daniel Borkan and
               Lucas Dixon and
               Jeffrey Sorensen and
               Nithum Thain and
               Lucy Vasserman},
  title     = {Nuanced Metrics for Measuring Unintended Bias with Real Data for Text
               Classification},
  journal   = {CoRR},
  volume    = {abs/1903.04561},
  year      = {2019},
  url       = {http://arxiv.org/abs/1903.04561},
  archivePrefix = {arXiv},
  eprint    = {1903.04561},
  timestamp = {Sun, 31 Mar 2019 19:01:24 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1903-04561},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc {bloom2022bloom,
	author       = { BigScience Workshop },
	title        = { bloom (Revision 4ab0472) },
	year         = 2022,
	url          = { https://huggingface.co/bigscience/bloom },
	doi          = { 10.57967/hf/0003 },
	publisher    = { Hugging Face }
}

@inproceedings{Werra2022evaluate,
  title={Evaluate\&Evaluation on the Hub: Better Best Practices for Data and Model Measurements},
  author={Leandro von Werra and Lewis Tunstall and Abhishek Thakur and Alexandra Sasha Luccioni and Tristan Thrush and Aleksandra Piktus and Felix Marty and Nazneen Rajani and Victor Mustar and Helen Ngo and Omar Sanseviero and Mario vSavsko and Albert Villanova and Quentin Lhoest and Julien Chaumond and Margaret Mitchell and Alexander M. Rush and Thomas Wolf and Douwe Kiela},
  year={2022}
}

@inproceedings{helwe2021reasoning,
  title={Reasoning with transformer-based models: Deep learning, but shallow reasoning},
  author={Helwe, Chadi and Clavel, Chlo{\'e} and Suchanek, Fabian M},
  booktitle={3rd Conference on Automated Knowledge Base Construction},
  year={2021}
}

@article{wang2022lsat,
  title={From lsat: The progress and challenges of complex reasoning},
  author={Wang, Siyuan and Liu, Zhongkun and Zhong, Wanjun and Zhou, Ming and Wei, Zhongyu and Chen, Zhumin and Duan, Nan},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  year={2022},
  publisher={IEEE}
}

@inproceedings{zeng2022glm,
  title={GLM-130B: An Open Bilingual Pre-trained Model},
  author={Aohan Zeng and Xiao Liu and Zhengxiao Du and Zihan Wang and Hanyu Lai and Ming Ding and Zhuoyi Yang and Yifan Xu and Wendi Zheng and Xiao Xia and Weng Lam Tam and Zixuan Ma and Yufei Xue and Jidong Zhai and Wenguang Chen and P. Zhang and Yuxiao Dong and Jie Tang},
  year={2022}
}

@article{evans2022truthfulqa,
  title={How do new models from {OpenAI, DeepMind and Anthropic perform on TruthfulQA?}},
  author={Owain Evans and Stephanie Lin and Jacob Hilton},
  url = {https://www.lesswrong.com/posts/yYkrbS5iAwdEQyynW/how-do-new-models-from-openai-deepmind-and-anthropic-perform},
  journal={AI Alignment Forum},
  year={2022},
}

@inproceedings{caines2018aggressive,
    title = "Aggressive language in an online hacking forum",
    author = "Caines, Andrew  and
      Pastrana, Sergio  and
      Hutchings, Alice  and
      Buttery, Paula",
    booktitle = "Proceedings of the 2nd Workshop on Abusive Language Online ({ALW}2)",
    month = oct,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-5109",
    doi = "10.18653/v1/W18-5109",
    pages = "66--74",
    abstract = "We probe the heterogeneity in levels of abusive language in different sections of the Internet, using an annotated corpus of Wikipedia page edit comments to train a binary classifier for abuse detection. Our test data come from the CrimeBB Corpus of hacking-related forum posts and we find that (a) forum interactions are rarely abusive, (b) the abusive language which does exist tends to be relatively mild compared to that found in the Wikipedia comments domain, and tends to involve aggressive posturing rather than hate speech or threats of violence. We observe that the purpose of conversations in online forums tend to be more constructive and informative than those in Wikipedia page edit comments which are geared more towards adversarial interactions, and that this may explain the lower levels of abuse found in our forum data than in Wikipedia comments. Further work remains to be done to compare these results with other inter-domain classification experiments, and to understand the impact of aggressive language in forum conversations.",
}

@article{dehghani2018universal,
  title={Universal transformers},
  author={Dehghani, Mostafa and Gouws, Stephan and Vinyals, Oriol and Uszkoreit, Jakob and Kaiser, {\L}ukasz},
  journal={arXiv preprint arXiv:1807.03819},
  year={2018}
}

@inproceedings{kumar2016ask,
  title={Ask me anything: Dynamic memory networks for natural language processing},
  author={Kumar, Ankit and Irsoy, Ozan and Ondruska, Peter and Iyyer, Mohit and Bradbury, James and Gulrajani, Ishaan and Zhong, Victor and Paulus, Romain and Socher, Richard},
  booktitle={International conference on machine learning},
  pages={1378--1387},
  year={2016},
  organization={PMLR}
}

@article{babi,
  title={Towards AI-complete question answering: A set of prerequisite toy tasks},
  author={Weston, Jason and Bordes, Antoine and Chopra, Sumit and Rush, Alexander M and Van Merri{\"e}nboer, Bart and Joulin, Armand and Mikolov, Tomas},
  journal={arXiv preprint arXiv:1502.05698},
  year={2015}
}


@article{diresta2019potemkin,
  title={Potemkin pages \& personas: Assessing GRU online operations, 2014-2019},
  author={DiResta, Renee and Grossman, Shelby},
  journal={White Paper https://fsi-live. s3. us-west-1. amazonaws. com/s3fs-public/potemkin-pagespersonas-sio-wp. pdf},
  year={2019}
}

@inproceedings{sap2019socialiqa,
    title = "Social {IQ}a: Commonsense Reasoning about Social Interactions",
    author = "Sap, Maarten  and
      Rashkin, Hannah  and
      Chen, Derek  and
      Le Bras, Ronan  and
      Choi, Yejin",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1454",
    doi = "10.18653/v1/D19-1454",
    pages = "4463--4473",
    abstract = "We introduce Social IQa, the first large-scale benchmark for commonsense reasoning about social situations. Social IQa contains 38,000 multiple choice questions for probing emotional and social intelligence in a variety of everyday situations (e.g., Q: {``}Jordan wanted to tell Tracy a secret, so Jordan leaned towards Tracy. Why did Jordan do this?{''} A: {``}Make sure no one else could hear{''}). Through crowdsourcing, we collect commonsense questions along with correct and incorrect answers about social interactions, using a new framework that mitigates stylistic artifacts in incorrect answers by asking workers to provide the right answer to a different but related question. Empirical results show that our benchmark is challenging for existing question-answering models based on pretrained language models, compared to human performance ({\textgreater}20{\%} gap). Notably, we further establish Social IQa as a resource for transfer learning of commonsense knowledge, achieving state-of-the-art performance on multiple commonsense reasoning tasks (Winograd Schemas, COPA).",
}

@misc{election2021long,
  title={The Long Fuse: Misinformation and the 2020 Election},
  author={Election Integrity Partnership},
  year={2021}
}

@article{whitten2020poison,
  title={Poison if you don’t know how to use it: Facebook, democracy, and human rights in Myanmar},
  author={Whitten-Woodring, Jenifer and Kleinberg, Mona S and Thawnghmung, Ardeth and Thitsar, Myat The},
  journal={The International Journal of Press/Politics},
  volume={25},
  number={3},
  pages={407--425},
  year={2020},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}

@article{zarocostas2020fight,
  title={How to fight an infodemic},
  author={Zarocostas, John},
  journal={The lancet},
  volume={395},
  number={10225},
  pages={676},
  year={2020},
  publisher={Elsevier}
}


@incollection{benkler2018network,
    author = {Benkler, Yochai and Faris, Robert and Roberts, Hal},
    isbn = {9780190923624},
    title = "{3. Epistemic Crisis}",
    booktitle = "{Network Propaganda: Manipulation, Disinformation, and Radicalization in American Politics}",
    publisher = {Oxford University Press},
    year = {2018},
    month = {11},
    doi = {10.1093/oso/9780190923624.003.0001},
    url = {https://doi.org/10.1093/oso/9780190923624.003.0001},
    eprint = {https://academic.oup.com/book/0/chapter/194768451/chapter-pdf/44106104/oso-9780190923624-chapter-1.pdf},
}

@article{diresta2022house,
  title={In-House Vs. Outsourced Trolls: How Digital Mercenaries Shape State Influence Strategies},
  author={DiResta, Ren{\'e}e and Grossman, Shelby and Siegel, Alexandra},
  journal={Political Communication},
  pages={1--32},
  year={2022},
  publisher={Taylor \& Francis}
}

@inproceedings{kirk2022handling,
  title={Handling and Presenting Harmful Text in NLP Research},
  author={Hannah Rose Kirk and Abeba Birhane and Bertie Vidgen and Leon Derczynski},
  year={2022}
}

@article{pennycook2021guide,
    author = {Pennycook, Gordon and Binnendyk, Jabin and Newton, Christie and Rand, David G.},
    title = "{A Practical Guide to Doing Behavioral Research on Fake News and Misinformation}",
    journal = {Collabra: Psychology},
    volume = {7},
    number = {1},
    year = {2021},
    month = {07},
    issn = {2474-7394},
    url = {https://online.ucpress.edu/collabra/article/7/1/25293/117809/A-Practical-Guide-to-Doing-Behavioral-Research-on},
    note = {25293},
}

@article{metzler2021rethinking,
author = {Metzler, Donald and Tay, Yi and Bahri, Dara and Najork, Marc},
title = {Rethinking Search: Making Domain Experts out of Dilettantes},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {1},
issn = {0163-5840},
url = {https://doi.org/10.1145/3476415.3476428},
doi = {10.1145/3476415.3476428},
abstract = {When experiencing an information need, users want to engage with a domain expert, but often turn to an information retrieval system, such as a search engine, instead. Classical information retrieval systems do not answer information needs directly, but instead provide references to (hopefully authoritative) answers. Successful question answering systems offer a limited corpus created on-demand by human experts, which is neither timely nor scalable. Pre-trained language models, by contrast, are capable of directly generating prose that may be responsive to an information need, but at present they are dilettantes rather than domain experts - they do not have a true understanding of the world, they are prone to hallucinating, and crucially they are incapable of justifying their utterances by referring to supporting documents in the corpus they were trained over. This paper examines how ideas from classical information retrieval and pre-trained language models can be synthesized and evolved into systems that truly deliver on the promise of domain expert advice.},
journal = {SIGIR Forum},
month = {jul},
articleno = {13},
numpages = {27}
}

@unpublished{khattab2021HAI,
    author = {Khattab, Omar  and  Potts, Christopher  and  Zaharia, Matei},
    note = {Stanford HAI Blog},
    url={https://hai.stanford.edu/news/moderate-proposal-radically-better-ai-powered-web-search},
    year = {2021},
    title = {A Moderate Proposal for Radically Better {AI}-Powered {W}eb Search}}

@inproceedings{dai2018convolutional,
  title={Convolutional neural networks for soft-matching n-grams in ad-hoc search},
  author={Dai, Zhuyun and Xiong, Chenyan and Callan, Jamie and Liu, Zhiyuan},
  booktitle={Proceedings of the eleventh ACM international conference on web search and data mining},
  pages={126--134},
  year={2018}
}

@article{mitra2019updated,
  title={An updated duet model for passage re-ranking},
  author={Mitra, Bhaskar and Craswell, Nick},
  journal={arXiv preprint arXiv:1903.07666},
  year={2019}
}

@article{xiong2020approximate,
  title={Approximate nearest neighbor negative contrastive learning for dense text retrieval},
  author={Xiong, Lee and Xiong, Chenyan and Li, Ye and Tang, Kwok-Fung and Liu, Jialin and Bennett, Paul and Ahmed, Junaid and Overwijk, Arnold},
  journal={arXiv preprint arXiv:2007.00808},
  year={2020}
}

@article{santhanam2021colbertv2,
  title={Colbertv2: Effective and efficient retrieval via lightweight late interaction},
  author={Santhanam, Keshav and Khattab, Omar and Saad-Falcon, Jon and Potts, Christopher and Zaharia, Matei},
  journal={arXiv preprint arXiv:2112.01488},
  year={2021}
}

@inproceedings{gao2021complement,
  title={Complement lexical retrieval model with semantic residual embeddings},
  author={Gao, Luyu and Dai, Zhuyun and Chen, Tongfei and Fan, Zhen and Durme, Benjamin Van and Callan, Jamie},
  booktitle={European Conference on Information Retrieval},
  pages={146--160},
  year={2021},
  organization={Springer}
}

@article{wei2022chain,
  title={Chain of thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Chi, Ed and Le, Quoc and Zhou, Denny},
  journal={arXiv preprint arXiv:2201.11903},
  year={2022}
}

@inproceedings{yang2022capabilities,
  title={Capabilities for Better ML Engineering},
  author={Chenyang Yang and Rachel Brower-Sinning and Grace A. Lewis and Christian Kastner and Tongshuang Sherry Wu},
  year={2022}
}

@article{zhu2018texygenAB,
  title={Texygen: A Benchmarking Platform for Text Generation Models},
  author={Yaoming Zhu and Sidi Lu and Lei Zheng and Jiaxian Guo and Weinan Zhang and Jun Wang and Yong Yu},
  journal={The 41st International ACM SIGIR Conference on Research \& Development in Information Retrieval},
  year={2018}
}

@article{gruppi2022nela,
  title={NELA-GT-2021: A Large Multi-Labelled News Dataset for The Study of Misinformation in News Articles},
  author={Gruppi, Maur{\'\i}cio and Horne, Benjamin D and Adal{\i}, Sibel},
  journal={arXiv preprint arXiv:2203.05659},
  year={2022}
}

@article{nrregaard2019NELAGT2018AL,
  title={NELA-GT-2018: A Large Multi-Labelled News Dataset for The Study of Misinformation in News Articles},
  author={Jeppe N{\o}rregaard and Benjamin D. Horne and Sibel Adali},
  journal={ArXiv},
  year={2019},
  volume={abs/2203.05659}
}

@inproceedings{mishra2021neuralnere,
  title={NeuralNERE: Neural Named Entity Relationship Extraction for End-to-End Climate Change Knowledge Graph Construction},
  author={Mishra, Prakamya and Mittal, Rohan},
  booktitle={ICML 2021 Workshop on Tackling Climate Change with Machine Learning},
  url={https://www.climatechange.ai/papers/icml2021/76},
  year={2021}
}

@inproceedings{bulian2020climate-fever,
title	= {CLIMATE-FEVER: A Dataset for Verification of Real-World Climate Claims},
author	= {Jannis Bulian and Jordan Boyd-Graber and Markus Leippold and Massimiliano Ciaramita and Thomas Diggelmann},
year	= {2020},
booktitle	= {NeurIPS 2020 Workshop on Tackling Climate Change with Machine Learning}
}

@article{hao2018context,
  title={Context-Free Transductions with Neural Stacks},
  author={Hao, Yiding and Merrill, William and Angluin, Dana and Frank, Robert and Amsel, Noah and Benz, Andrew and Mendelsohn, Simon},
  journal={EMNLP 2018},
  pages={306},
  year={2018}
}

@inproceedings{hewitt2020rnns,
  title={RNNs can generate bounded hierarchical languages with optimal memory},
  author={Hewitt, John and Hahn, Michael and Ganguli, Surya and Liang, Percy and Manning, Christopher D},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={1978--2010},
  year={2020}
}

@article{hahn2020theoretical,
  title={Theoretical Limitations of Self-Attention in Neural Sequence Models},
  author={Hahn, Michael},
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={156--171},
  year={2020}
}

@inproceedings{suzgun2019lstm,
  title={LSTM Networks Can Perform Dynamic Counting},
  author={Suzgun, Mirac and Belinkov, Yonatan and Shieber, Stuart M and Gehrmann, Sebastian},
  booktitle={Proceedings of the Workshop on Deep Learning and Formal Languages: Building Bridges},
  pages={44--54},
  year={2019}
}

@inproceedings{sennhauser2018evaluating,
  title={Evaluating the Ability of LSTMs to Learn Context-Free Grammars},
  author={Sennhauser, Luzi and Berwick, Robert C},
  booktitle={Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},
  pages={115--124},
  year={2018}
}

@inproceedings{skachkova2018closing,
  title={Closing brackets with recurrent neural networks},
  author={Skachkova, Natalia and Trost, Thomas Alexander and Klakow, Dietrich},
  booktitle={Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},
  pages={232--239},
  year={2018}
}

@inproceedings{bhattamishra2020ability,
  title={On the Ability and Limitations of Transformers to Recognize Formal Languages},
  author={Bhattamishra, Satwik and Ahuja, Kabir and Goyal, Navin},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={7096--7116},
  year={2020}
}

@inproceedings{ebrahimi2020can,
  title={How Can Self-Attention Networks Recognize Dyck-n Languages?},
  author={Ebrahimi, Javid and Gelda, Dhruv and Zhang, Wei},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2020},
  pages={4301--4306},
  year={2020}
}

@article{merrill2021formal,
  title={Formal language theory meets modern NLP},
  author={Merrill, William},
  journal={arXiv preprint arXiv:2102.10094},
  year={2021}
}

@inproceedings{parrish-etal-2022-bbq,
    title = "{BBQ}: A hand-built bias benchmark for question answering",
    author = "Parrish, Alicia  and
      Chen, Angelica  and
      Nangia, Nikita  and
      Padmakumar, Vishakh  and
      Phang, Jason  and
      Thompson, Jana  and
      Htut, Phu Mon  and
      Bowman, Samuel",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.165",
    doi = "10.18653/v1/2022.findings-acl.165",
    pages = "2086--2105",
    abstract = "It is well documented that NLP models learn social biases, but little work has been done on how these biases manifest in model outputs for applied tasks like question answering (QA). We introduce the Bias Benchmark for QA (BBQ), a dataset of question-sets constructed by the authors that highlight attested social biases against people belonging to protected classes along nine social dimensions relevant for U.S. English-speaking contexts. Our task evaluate model responses at two levels: (i) given an under-informative context, we test how strongly responses reflect social biases, and (ii) given an adequately informative context, we test whether the model{'}s biases override a correct answer choice. We find that models often rely on stereotypes when the context is under-informative, meaning the model{'}s outputs consistently reproduce harmful biases in this setting. Though models are more accurate when the context provides an informative answer, they still rely on stereotypes and average up to 3.4 percentage points higher accuracy when the correct answer aligns with a social bias than when it conflicts, with this difference widening to over 5 points on examples targeting gender for most models tested.",
}

@ARTICLE{Greenbaum1991-js,
  title     = "{ICE}: The international corpus of English",
  author    = "Greenbaum, Sidney",
  abstract  = "An outline of a new worldwide project for the study of the
               language",
  journal   = "Engl. today",
  publisher = "Cambridge University Press (CUP)",
  volume    =  7,
  number    =  4,
  pages     = "3--7",
  month     =  oct,
  year      =  1991,
  language  = "en"
}

@misc{greenbaum1996international,
  title={The international corpus of English (ICE) project},
  author={Greenbaum, Sidney and Nelson, Gerald},
  journal={World Englishes},
  volume={15},
  number={1},
  pages={3--15},
  year={1996},
  publisher={Wiley Online Library}
}

@article{emnlp1996emnlp,
    author = {{EMNLP}},
    title = "Conference on Empirical Methods in Natural Language Processing",
    year = "1996",
    url = "https://aclanthology.org/W96-0200",
}

@inproceedings{grishman1996muc,
    title = "{M}essage {U}nderstanding {C}onference- 6: A Brief History",
    author = "Grishman, Ralph  and
      Sundheim, Beth",
    booktitle = "{COLING} 1996 Volume 1: The 16th International Conference on Computational Linguistics",
    year = "1996",
    url = "https://aclanthology.org/C96-1079",
}

@inproceedings{voorhees1998trec,
    title = "The {T}ext {RE}trieval {C}onferences ({TREC}s)",
    author = "Voorhees, Ellen M.  and
      Harman, Donna",
    booktitle = "TIPSTER TEXT PROGRAM PHASE III: Proceedings of a Workshop held at Baltimore, {M}aryland, October 13-15, 1998",
    month = oct,
    year = "1998",
    address = "Baltimore, Maryland, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/X98-1031",
    doi = "10.3115/1119089.1119127",
    pages = "241--273",
}

@book{jones1995evaluating,
  title={Evaluating Natural Language Processing Systems: An Analysis and Review},
  author={Sp\"arck Jones, Karen and Galliers, Julia R.},
  year={1995},
  address={Berlin},
  publisher={Springer Verlag},
  series={Lecture Notes in Computer Science},
  number={1083},
}

@ARTICLE{Szmrecsanyi2019-go,
  title     = "{Variation-Based} Distance and Similarity Modeling: A case study
               in world {E}nglishes",
  author    = "Szmrecsanyi, Benedikt and Grafmiller, Jason and Rosseel, Laura",
  abstract  = "Inspired by work in comparative sociolinguistics and
               quantitative dialectometry, we sketch a corpus-based method
               (Variation-Based Distance \& Similarity Modeling-VADIS for
               short) to rigorously quantify the similarity between varieties
               and dialects as a function of the correspondence of the ways in
               which language users choose between different ways of saying the
               same thing. To showcase the potential of the method, we present
               a case study that investigates three syntactic alternations in
               some nine international varieties of English. Key findings
               include that (a) probabilistic grammars are remarkably similar
               and stable across the varieties under study; (b) in many cases
               we see a cluster of ``native'' (a.k.a. Inner Circle) varieties,
               such as British English, whereas ``non-native'' (a.k.a. Outer
               Circle) varieties, such as Indian English, are a more
               heterogeneous group; and (c) coherence across alternations is
               less than perfect.",
  journal   = "Frontiers in Artificial Intelligence",
  publisher = "Frontiers Media SA",
  volume    =  2,
  pages     = "23",
  month     =  nov,
  year      =  2019,
  keywords  = "VADIS; comparative sociolinguistics; dialectometry;
               probabilistic grammar; variationist linguistics",
  copyright = "https://creativecommons.org/licenses/by/4.0/",
  language  = "en"
}

@book{kachru2009handbook,
  title={The handbook of world Englishes},
  author={Kachru, Braj B and Kachru, Yamuna and Nelson, Cecil L},
  volume={48},
  year={2009},
  publisher={John Wiley \& Sons}
}

@inproceedings{bender2009linguistically,
    title = {Linguistically Na{\"\i}ve != Language Independent: Why {NLP} Needs Linguistic Typology},
    author = "Bender, Emily M.",
    booktitle = "Proceedings of the {EACL} 2009 Workshop on the Interaction between Linguistics and Computational Linguistics: Virtuous, Vicious or Vacuous?",
    month = mar,
    year = "2009",
    address = "Athens, Greece",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W09-0106",
    pages = "26--32",
}

@article{liberman2010obituary,
author = {Liberman, Mark},
title = {Obituary: Fred Jelinek},
year = {2010},
issue_date = {December 2010},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {36},
number = {4},
issn = {0891-2017},
url = {https://doi.org/10.1162/coli_a_00032},
doi = {10.1162/coli_a_00032},
journal = {Comput. Linguist.},
month = {dec},
pages = {595–599},
numpages = {5}
}

@article{bender2011achieving,
  title={On Achieving and Evaluating Language-Independence in NLP},
  author={Emily M. Bender},
  journal={Linguistic Issues in Language Technology},
  year={2011},
  volume={6}
}

@book{glottolog,
  address      = {Leipzig},
  author       = {Harald Hammarström and Robert Forkel and Martin Haspelmath and Sebastian Bank},
  howpublished = {Max Planck Institute for Evolutionary Anthropology},
  title        = {Glottolog 4.4},
  url          = {https://glottolog.org/ accessed 2021-08-08},
  year         = {2021},
  doi          = {10.5281/zenodo.4761960}
}

@inproceedings{nordhoff2011glottolog,
  title={Glottolog/Langdoc: Defining dialects, languages, and language families as collections of resources},
  author={Nordhoff, Sebastian and Hammarstr{\"o}m, Harald},
  booktitle={First International Workshop on Linked Science 2011-In conjunction with the International Semantic Web Conference (ISWC 2011)},
  year={2011}
}

@inproceedings{wang2011summarizing,
    title = "Summarizing Decisions in Spoken Meetings",
    author = "Wang, Lu  and
      Cardie, Claire",
    booktitle = "Proceedings of the Workshop on Automatic Summarization for Different Genres, Media, and Languages",
    month = jun,
    year = "2011",
    address = "Portland, Oregon",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W11-0503",
    pages = "16--24",
}

@inproceedings{bender2012100,
    title = "100 Things You Always Wanted to Know about Linguistics But Were Afraid to Ask*",
    author = "Bender, Emily M.",
    booktitle = "Tutorial Abstracts at the Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2012",
    address = "Montr{\'e}al, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N12-4001",
}

@inproceedings{joshi2020state,
   title={The State and Fate of Linguistic Diversity and Inclusion in the NLP World},
    author={Pratik Joshi and Sebastin Santy and Amar Budhiraja and Kalika Bali and Monojit Choudhury},
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Seattle, Washington",
    publisher = "Association for Computational Linguistics",
    url={https://arxiv.org/abs/2004.09095}
}



@book{kirkpatrick2020routledge,
  title={The Routledge handbook of world Englishes},
  author={Kirkpatrick, Andy},
  year={2020},
  publisher={Routledge}
}

@inproceedings{tiedemann-2016-finding,
    title = "Finding Alternative Translations in a Large Corpus of Movie Subtitle",
    author = {Tiedemann, J{\"o}rg},
    booktitle = "Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16)",
    month = may,
    year = "2016",
    address = "Portoro{\v{z}}, Slovenia",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://aclanthology.org/L16-1559",
    pages = "3518--3522",
    abstract = "OpenSubtitles.org provides a large collection of user contributed subtitles in various languages for movies and TV programs. Subtitle translations are valuable resources for cross-lingual studies and machine translation research. A less explored feature of the collection is the inclusion of alternative translations, which can be very useful for training paraphrase systems or collecting multi-reference test suites for machine translation. However, differences in translation may also be due to misspellings, incomplete or corrupt data files, or wrongly aligned subtitles. This paper reports our efforts in recognising and classifying alternative subtitle translations with language independent techniques. We use time-based alignment with lexical re-synchronisation techniques and BLEU score filters and sort alternative translations into categories using edit distance metrics and heuristic rules. Our approach produces large numbers of sentence-aligned translation alternatives for over 50 languages provided via the OPUS corpus collection.",
}
@article{guha2022legalbench,
    title = {LegalBench: Prototyping a Collaborative Benchmark for Legal Reasoning},
    author = {Guha, Neel and Ho, Daniel E. and Nyarko, Julian and Ré, Christopher},
    journal = {arXiv},
    year = {2022},
    volume = {abs/2209.06120},
}

@inproceedings{hershcovich2022challenges,
    title = "Challenges and Strategies in Cross-Cultural {NLP}",
    author = "Hershcovich, Daniel  and
      Frank, Stella  and
      Lent, Heather  and
      de Lhoneux, Miryam  and
      Abdou, Mostafa  and
      Brandl, Stephanie  and
      Bugliarello, Emanuele  and
      Cabello Piqueras, Laura  and
      Chalkidis, Ilias  and
      Cui, Ruixiang  and
      Fierro, Constanza  and
      Margatina, Katerina  and
      Rust, Phillip  and
      S{\o}gaard, Anders",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.482",
    doi = "10.18653/v1/2022.acl-long.482",
    pages = "6997--7013",
    abstract = "Various efforts in the Natural Language Processing (NLP) community have been made to accommodate linguistic diversity and serve speakers of many different languages. However, it is important to acknowledge that speakers and the content they produce and require, vary not just by language, but also by culture. Although language and culture are tightly linked, there are important differences. Analogous to cross-lingual and multilingual NLP, cross-cultural and multicultural NLP considers these differences in order to better serve users of NLP systems. We propose a principled framework to frame these efforts, and survey existing and potential strategies.",
}



@InProceedings{liska2022streamingqa,
  title = 	 {{S}treaming{QA}: A Benchmark for Adaptation to New Knowledge over Time in Question Answering Models},
  author =       {Liska, Adam and Kocisky, Tomas and Gribovskaya, Elena and Terzi, Tayfun and Sezener, Eren and Agrawal, Devang and De Masson D'Autume, Cyprien and Scholtes, Tim and Zaheer, Manzil and Young, Susannah and Gilsenan-Mcmahon, Ellen and Austin, Sophia and Blunsom, Phil and Lazaridou, Angeliki},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {13604--13622},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/liska22a/liska22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/liska22a.html},
  abstract = 	 {Knowledge and language understanding of models evaluated through question answering (QA) has been usually studied on static snapshots of knowledge, like Wikipedia. However, our world is dynamic, evolves over time, and our models’ knowledge becomes outdated. To study how semi-parametric QA models and their underlying parametric language models (LMs) adapt to evolving knowledge, we construct a new large-scale dataset, StreamingQA, with human written and generated questions asked on a given date, to be answered from 14 years of time-stamped news articles. We evaluate our models quarterly as they read new articles not seen in pre-training. We show that parametric models can be updated without full retraining, while avoiding catastrophic forgetting. For semi-parametric models, adding new articles into the search space allows for rapid adaptation, however, models with an outdated underlying LM under-perform those with a retrained LM. For questions about higher-frequency named entities, parametric updates are particularly beneficial. In our dynamic world, the StreamingQA dataset enables a more realistic evaluation of QA models, and our experiments highlight several promising directions for future research.}
}




@inproceedings{lin-2004-rouge,
    title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
    author = "Lin, Chin-Yew",
    booktitle = "Text Summarization Branches Out",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W04-1013",
    pages = "74--81",
}

@inproceedings{fabbri-etal-2022-qafacteval,
    title = "{QAF}act{E}val: Improved {QA}-Based Factual Consistency Evaluation for Summarization",
    author = "Fabbri, Alexander  and
      Wu, Chien-Sheng  and
      Liu, Wenhao  and
      Xiong, Caiming",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.187",
    doi = "10.18653/v1/2022.naacl-main.187",
    pages = "2587--2601",
    abstract = "Factual consistency is an essential quality of text summarization models in practical settings. Existing work in evaluating this dimension can be broadly categorized into two lines of research, entailment-based and question answering (QA)-based metrics, and different experimental setups often lead to contrasting conclusions as to which paradigm performs the best. In this work, we conduct an extensive comparison of entailment and QA-based metrics, demonstrating that carefully choosing the components of a QA-based metric, especially question generation and answerability classification, is critical to performance. Building on those insights, we propose an optimized metric, which we call QAFactEval, that leads to a 14{\%} average improvement over previous QA-based metrics on the SummaC factual consistency benchmark, and also outperforms the best-performing entailment-based metric. Moreover, we find that QA-based and entailment-based metrics can offer complementary signals and be combined into a single metric for a further performance boost.",
}

@article{lin2021pretrained,
  title={Pretrained transformers for text ranking: Bert and beyond},
  author={Lin, Jimmy and Nogueira, Rodrigo and Yates, Andrew},
  journal={Synthesis Lectures on Human Language Technologies},
  volume={14},
  number={4},
  pages={1--325},
  year={2021},
  publisher={Morgan \& Claypool Publishers}
}

@phdthesis{tsipras2021learning,
  title={Learning Through the Lens of Robustness},
  author={Tsipras, Dimitris},
  year={2021},
  school={Massachusetts Institute of Technology},
  url={https://dspace.mit.edu/handle/1721.1/140148}
}

@inproceedings{lazaridou2021mind,
 author = {Lazaridou, Angeliki and Kuncoro, Adhi and Gribovskaya, Elena and Agrawal, Devang and Liska, Adam and Terzi, Tayfun and Gimenez, Mai and de Masson d\textquotesingle Autume, Cyprien and Kocisky, Tomas and Ruder, Sebastian and Yogatama, Dani and Cao, Kris and Young, Susannah and Blunsom, Phil},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {29348--29363},
 publisher = {Curran Associates, Inc.},
 title = {Mind the Gap: Assessing Temporal Generalization in Neural Language Models},
 url = {https://proceedings.neurips.cc/paper/2021/file/f5bf0ba0a17ef18f9607774722f5698c-Paper.pdf},
 volume = {34},
 year = {2021}
}



@inproceedings{
ma2021dynaboard,
title={Dynaboard: An Evaluation-As-A-Service Platform for Holistic Next-Generation Benchmarking},
author={Zhiyi Ma and Kawin Ethayarajh and Tristan Thrush and Somya Jain and Ledell Yu Wu and Robin Jia and Christopher Potts and Adina Williams and Douwe Kiela},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=TCarYAus7JL}
}

@inproceedings{jacobs2021measurement,
author = {Abigail Z. Jacobs and Hanna Wallach},
title = {Measurement and Fairness},
year = {2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://arxiv.org/abs/1912.05511},
booktitle = {Proceedings of the 2021 Conference on Fairness, Accountability, and Transparency},
location = {Online},
series = {FAccT '21}
}

@article{sachan2022improving,
  title={Improving Passage Retrieval with Zero-Shot Question Generation},
  author={Sachan, Devendra Singh and Lewis, Mike and Joshi, Mandar and Aghajanyan, Armen and Yih, Wen-tau and Pineau, Joelle and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2204.07496},
  year={2022}
}

@article{craswell2020overview,
  title={Overview of the TREC 2019 deep learning track},
  author={Craswell, Nick and Mitra, Bhaskar and Yilmaz, Emine and Campos, Daniel and Voorhees, Ellen M},
  journal={arXiv preprint arXiv:2003.07820},
  year={2020}
}

@article{bartolo2020beat,
    title = "Beat the {AI}: Investigating Adversarial Human Annotation for Reading Comprehension",
    author = "Bartolo, Max  and
      Roberts, Alastair  and
      Welbl, Johannes  and
      Riedel, Sebastian  and
      Stenetorp, Pontus",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "8",
    year = "2020",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2020.tacl-1.43",
    doi = "10.1162/tacl_a_00338",
    pages = "662--678",
    abstract = "Innovations in annotation methodology have been a catalyst for Reading Comprehension (RC) datasets and models. One recent trend to challenge current RC models is to involve a model in the annotation process: Humans create questions adversarially, such that the model fails to answer them correctly. In this work we investigate this annotation methodology and apply it in three different settings, collecting a total of 36,000 samples with progressively stronger models in the annotation loop. This allows us to explore questions such as the reproducibility of the adversarial effect, transfer from data collected with varying model-in-the-loop strengths, and generalization to data collected without a model. We find that training on adversarially collected samples leads to strong generalization to non-adversarially collected datasets, yet with progressive performance deterioration with increasingly stronger models-in-the-loop. Furthermore, we find that stronger models can still learn from datasets collected with substantially weaker models-in-the-loop. When trained on data collected with a BiDAF model in the loop, RoBERTa achieves 39.9F1 on questions that it cannot answer when trained on SQuAD{---}only marginally lower than when trained on data collected using RoBERTa itself (41.0F1).",
}

@inproceedings{wang2021textflint,
  title={Textflint: Unified multilingual robustness evaluation toolkit for natural language processing},
  author={Wang, Xiao and Liu, Qin and Gui, Tao and Zhang, Qi and Zou, Yicheng and Zhou, Xin and Ye, Jiacheng and Zhang, Yongxin and Zheng, Rui and Pang, Zexiong and others},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations},
  year={2021}
}

@article{wang2021adversarial,
  title={Adversarial glue: A multi-task benchmark for robustness evaluation of language models},
  author={Wang, Boxin and Xu, Chejian and Wang, Shuohang and Gan, Zhe and Cheng, Yu and Gao, Jianfeng and Awadallah, Ahmed Hassan and Li, Bo},
  journal={arXiv preprint arXiv:2111.02840},
  year={2021}
}

@article{morris2020textattack,
  title={Textattack: A framework for adversarial attacks, data augmentation, and adversarial training in nlp},
  author={Morris, John X and Lifland, Eli and Yoo, Jin Yong and Grigsby, Jake and Jin, Di and Qi, Yanjun},
  journal={arXiv preprint arXiv:2005.05909},
  year={2020}
}

@article{wang2021measure,
  title={Measure and Improve Robustness in NLP Models: A Survey},
  author={Wang, Xuezhi and Wang, Haohan and Yang, Diyi},
  journal={arXiv preprint arXiv:2112.08313},
  year={2021}
}


@inproceedings{narayanan2021efficient,
  title={{Efficient Large-Scale Language Model Training on GPU Clusters using Megatron-LM}},
  author={Narayanan, Deepak and Shoeybi, Mohammad and Casper, Jared and LeGresley, Patrick and Patwary, Mostofa and Korthikanti, Vijay and Vainbrand, Dmitri and Kashinkunti, Prethvi and Bernauer, Julie and Catanzaro, Bryan and others},
  booktitle={Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  year={2021}
}


@InCollection{sep-abduction,
	author       =	{Douven, Igor},
	title        =	{{Abduction}},
	booktitle    =	{The {Stanford} Encyclopedia of Philosophy},
	editor       =	{Edward N. Zalta},
	howpublished =	{\url{https://plato.stanford.edu/archives/sum2021/entries/abduction/}},
	year         =	{2021},
	edition      =	{{S}ummer 2021},
	publisher    =	{Metaphysics Research Lab, Stanford University}
}


@book{peirce1974collected,
  title={Collected papers of charles sanders peirce},
  author={Peirce, Charles Sanders},
  volume={5},
  year={1974},
  publisher={Harvard University Press}
}

@article{goldstein2022generative,
    title={Generative Language Models and Automated Influence Operations: Emerging Threats and Potential Mitigations},
    author={Goldstein, Josh A. and Musser, Micah and Sastry, Girish and DiResta, Ren{\'e}e and Gentzel, Matthew and Sedova, Katerina},
    year={Forthcoming}
}


@article{ganguli2022red,
  title={Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned},
  author={Deep Ganguli and Liane Lovitt and John Kernion and Amanda Askell and Yushi Bai and Saurav Kadavath and Benjamin Mann and Ethan Perez and Nicholas Schiefer and Kamal Ndousse and Andy Jones and Sam Bowman and Anna Chen and Tom Conerly and Nova DasSarma and Dawn Drain and Nelson Elhage and Sheer El-Showk and Stanislav Fort and Zachary Dodds and T. J. Henighan and Danny Hernandez and Tristan Hume and Josh Jacobson and Scott Johnston and Shauna Kravec and Catherine Olsson and Sam Ringer and Eli Tran-Johnson and Dario Amodei and Tom B. Brown and Nicholas Joseph and Sam McCandlish and Christopher Olah and Jared Kaplan and Jack Clark},
  journal={ArXiv},
  year={2022},
  volume={abs/2209.07858}
}

@misc{yamshchikov2022plutarch,
  title={{BERT} in {P}lutarch's Shadows},
  author={Ivan P. Yamshchikov and Alexey N. Tikhonov and Yorgos Pantis and Charlotte Schubert and J{\"u}rgen Jost},
  year={2022},
  url={https://arxiv.org/abs/2211.05673}
}

@inproceedings{macavaney:sigir2021-irds,
  author = {MacAvaney, Sean and Yates, Andrew and Feldman, Sergey and Downey, Doug and Cohan, Arman and Goharian, Nazli},
  title = {Simplified Data Wrangling with IR datasets},
  year = {2021},
  booktitle = {SIGIR}
}

@inproceedings{goel2021robustness,
    title = "Robustness Gym: Unifying the {NLP} Evaluation Landscape",
    author = "Goel, Karan  and
      Rajani, Nazneen Fatema  and
      Vig, Jesse  and
      Taschdjian, Zachary  and
      Bansal, Mohit  and
      R{\'e}, Christopher",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Demonstrations",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-demos.6",
    doi = "10.18653/v1/2021.naacl-demos.6",
    pages = "42--55",
    abstract = "Despite impressive performance on standard benchmarks, natural language processing (NLP) models are often brittle when deployed in real-world systems. In this work, we identify challenges with evaluating NLP systems and propose a solution in the form of Robustness Gym (RG), a simple and extensible evaluation toolkit that unifies 4 standard evaluation paradigms: subpopulations, transformations, evaluation sets, and adversarial attacks. By providing a common platform for evaluation, RG enables practitioners to compare results from disparate evaluation paradigms with a single click, and to easily develop and share novel evaluation methods using a built-in set of abstractions. RG is under active development and we welcome feedback {\&} contributions from the community.",
}


@inproceedings{welbl2021challenges,
    title = "Challenges in Detoxifying Language Models",
    author = "Welbl, Johannes  and
      Glaese, Amelia  and
      Uesato, Jonathan  and
      Dathathri, Sumanth  and
      Mellor, John  and
      Hendricks, Lisa Anne  and
      Anderson, Kirsty  and
      Kohli, Pushmeet  and
      Coppin, Ben  and
      Huang, Po-Sen",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.210",
    doi = "10.18653/v1/2021.findings-emnlp.210",
    pages = "2447--2469",
    abstract = "Large language models (LM) generate remarkably fluent text and can be efficiently adapted across NLP tasks. Measuring and guaranteeing the quality of generated text in terms of safety is imperative for deploying LMs in the real world; to this end, prior work often relies on automatic evaluation of LM toxicity. We critically discuss this approach, evaluate several toxicity mitigation strategies with respect to both automatic and human evaluation, and analyze consequences of toxicity mitigation in terms of model bias and LM quality. We demonstrate that while basic intervention strategies can effectively optimize previously established automatic metrics on the REALTOXICITYPROMPTS dataset, this comes at the cost of reduced LM coverage for both texts about, and dialects of, marginalized groups. Additionally, we find that human raters often disagree with high automatic toxicity scores after strong toxicity reduction interventions{---}highlighting further the nuances involved in careful evaluation of LM toxicity.",
}

@inproceedings{yang2017anserini,
  title={Anserini: Enabling the use of lucene for information retrieval research},
  author={Yang, Peilin and Fang, Hui and Lin, Jimmy},
  booktitle={Proceedings of the 40th international ACM SIGIR conference on research and development in information retrieval},
  pages={1253--1256},
  year={2017}
}

@inproceedings{gardent2017webnlg,
    title = "The {W}eb{NLG} Challenge: Generating Text from {RDF} Data",
    author = "Gardent, Claire  and
      Shimorina, Anastasia  and
      Narayan, Shashi  and
      Perez-Beltrachini, Laura",
    booktitle = "Proceedings of the 10th International Conference on Natural Language Generation",
    month = sep,
    year = "2017",
    address = "Santiago de Compostela, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W17-3518",
    doi = "10.18653/v1/W17-3518",
    pages = "124--133",
    abstract = "The WebNLG challenge consists in mapping sets of RDF triples to text. It provides a common benchmark on which to train, evaluate and compare {``}microplanners{''}, i.e. generation systems that verbalise a given content by making a range of complex interacting choices including referring expression generation, aggregation, lexicalisation, surface realisation and sentence segmentation. In this paper, we introduce the microplanning task, describe data preparation, introduce our evaluation methodology, analyse participant results and provide a brief description of the participating systems.",
}

@inproceedings{novikova2017e2e,
    title = "The {E}2{E} Dataset: New Challenges For End-to-End Generation",
    author = "Novikova, Jekaterina  and
      Du{\v{s}}ek, Ond{\v{r}}ej  and
      Rieser, Verena",
    booktitle = "Proceedings of the 18th Annual {SIG}dial Meeting on Discourse and Dialogue",
    month = aug,
    year = "2017",
    address = {Saarbr{\"u}cken, Germany},
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W17-5525",
    doi = "10.18653/v1/W17-5525",
    pages = "201--206",
    abstract = "This paper describes the E2E data, a new dataset for training end-to-end, data-driven natural language generation systems in the restaurant domain, which is ten times bigger than existing, frequently used datasets in this area. The E2E dataset poses new challenges: (1) its human reference texts show more lexical richness and syntactic variation, including discourse phenomena; (2) generating from this set requires content selection. As such, learning from this dataset promises more natural, varied and less template-like system utterances. We also establish a baseline on this dataset, which illustrates some of the difficulties associated with this data.",
}

@inproceedings{bert-score,
  title={BERTScore: Evaluating Text Generation with BERT},
  author={Tianyi Zhang and Varsha Kishore and Felix Wu and Kilian Q. Weinberger and Yoav Artzi},
  booktitle={International Conference on Learning Representations},
  year={2020},
  url={https://openreview.net/forum?id=SkeHuCVFDr}
}

@article{bamman2020latin,
  title={Latin BERT: A Contextual Language Model for Classical Philology},
  author={David Bamman and Patrick J. Burns},
  journal={ArXiv},
  year={2020},
  volume={abs/2009.10053}
}

@article{fabbri-etal-2021-summeval,
    title = "{S}umm{E}val: Re-evaluating Summarization Evaluation",
    author = "Fabbri, Alexander R.  and
      Kry{\'s}ci{\'n}ski, Wojciech  and
      McCann, Bryan  and
      Xiong, Caiming  and
      Socher, Richard  and
      Radev, Dragomir",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "9",
    year = "2021",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2021.tacl-1.24",
    doi = "10.1162/tacl_a_00373",
    pages = "391--409",
    abstract = "Abstract The scarcity of comprehensive up-to-date studies on evaluation metrics for text summarization and the lack of consensus regarding evaluation protocols continue to inhibit progress. We address the existing shortcomings of summarization evaluation methods along five dimensions: 1) we re-evaluate 14 automatic evaluation metrics in a comprehensive and consistent fashion using neural summarization model outputs along with expert and crowd-sourced human annotations; 2) we consistently benchmark 23 recent summarization models using the aforementioned automatic evaluation metrics; 3) we assemble the largest collection of summaries generated by models trained on the CNN/DailyMail news dataset and share it in a unified format; 4) we implement and share a toolkit that provides an extensible and unified API for evaluating summarization models across a broad range of automatic metrics; and 5) we assemble and share the largest and most diverse, in terms of model types, collection of human judgments of model-generated summaries on the CNN/Daily Mail dataset annotated by both expert judges and crowd-source workers. We hope that this work will help promote a more complete evaluation protocol for text summarization as well as advance research in developing evaluation metrics that better correlate with human judgments.",
}

@inproceedings{
tamkin2021dabs,
title={{DABS}: a Domain-Agnostic Benchmark for Self-Supervised Learning},
author={Alex Tamkin and Vincent Liu and Rongfei Lu and Daniel Fein and Colin Schultz and Noah Goodman},
booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1)},
year={2021},
url={https://openreview.net/forum?id=Uk2mymgn_LZ}
}

@inproceedings{
tamkin2022dabs2,
title={{DABS} 2.0: Improved Datasets and Algorithms for Universal Self-Supervision},
author={Alex Tamkin and Gaurab Banerjee and Mohamed Owda and Vincent Liu and Shashank Rammoorthy and Noah Goodman},
booktitle={Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
year={2022},
url={https://openreview.net/forum?id=ChWf1E43l4}
}



@inproceedings{liu-etal-2022-brio,
    title = "{BRIO}: Bringing Order to Abstractive Summarization",
    author = "Liu, Yixin  and
      Liu, Pengfei  and
      Radev, Dragomir  and
      Neubig, Graham",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.207",
    doi = "10.18653/v1/2022.acl-long.207",
    pages = "2890--2903",
    abstract = "Abstractive summarization models are commonly trained using maximum likelihood estimation, which assumes a deterministic (one-point) target distribution in which an ideal model will assign all the probability mass to the reference summary. This assumption may lead to performance degradation during inference, where the model needs to compare several system-generated (candidate) summaries that have deviated from the reference summary. To address this problem, we propose a novel training paradigm which assumes a non-deterministic distribution so that different candidate summaries are assigned probability mass according to their quality. Our method achieves a new state-of-the-art result on the CNN/DailyMail (47.78 ROUGE-1) and XSum (49.07 ROUGE-1) datasets. Further analysis also shows that our model can estimate probabilities of candidate summaries that are more correlated with their level of quality.",
}

@inproceedings{cao2018faithful,
  title={Faithful to the original: Fact aware neural abstractive summarization},
  author={Cao, Ziqiang and Wei, Furu and Li, Wenjie and Li, Sujian},
  booktitle={thirty-second AAAI conference on artificial intelligence},
  year={2018}
}

@inproceedings{durmus-etal-2020-feqa,
    title = "{FEQA}: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization",
    author = "Durmus, Esin  and
      He, He  and
      Diab, Mona",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.454",
    doi = "10.18653/v1/2020.acl-main.454",
    pages = "5055--5070",
    abstract = "Neural abstractive summarization models are prone to generate content inconsistent with the source document, i.e. unfaithful. Existing automatic metrics do not capture such mistakes effectively. We tackle the problem of evaluating faithfulness of a generated summary given its source document. We first collected human annotations of faithfulness for outputs from numerous models on two datasets. We find that current models exhibit a trade-off between abstractiveness and faithfulness: outputs with less word overlap with the source document are more likely to be unfaithful. Next, we propose an automatic question answering (QA) based metric for faithfulness, FEQA, which leverages recent advances in reading comprehension. Given question-answer pairs generated from the summary, a QA model extracts answers from the document; non-matched answers indicate unfaithful information in the summary. Among metrics based on word overlap, embedding similarity, and learned language understanding models, our QA-based metric has significantly higher correlation with human faithfulness scores, especially on highly abstractive summaries.",
}

@inproceedings{maynez-etal-2020-faithfulness,
    title = "On Faithfulness and Factuality in Abstractive Summarization",
    author = "Maynez, Joshua  and
      Narayan, Shashi  and
      Bohnet, Bernd  and
      McDonald, Ryan",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.173",
    doi = "10.18653/v1/2020.acl-main.173",
    pages = "1906--1919",
    abstract = "It is well known that the standard likelihood training and approximate decoding objectives in neural text generation models lead to less human-like responses for open-ended tasks such as language modeling and story generation. In this paper we have analyzed limitations of these models for abstractive document summarization and found that these models are highly prone to hallucinate content that is unfaithful to the input document. We conducted a large scale human evaluation of several neural abstractive summarization systems to better understand the types of hallucinations they produce. Our human annotators found substantial amounts of hallucinated content in all model generated summaries. However, our analysis does show that pretrained models are better summarizers not only in terms of raw metrics, i.e., ROUGE, but also in generating faithful and factual summaries as evaluated by humans. Furthermore, we show that textual entailment measures better correlate with faithfulness than standard metrics, potentially leading the way to automatic evaluation metrics as well as training and decoding criteria.",
}

@inproceedings{ladhak-etal-2022-faithful,
    title = "Faithful or Extractive? On Mitigating the Faithfulness-Abstractiveness Trade-off in Abstractive Summarization",
    author = "Ladhak, Faisal  and
      Durmus, Esin  and
      He, He  and
      Cardie, Claire  and
      McKeown, Kathleen",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.100",
    doi = "10.18653/v1/2022.acl-long.100",
    pages = "1410--1421",
    abstract = "Despite recent progress in abstractive summarization, systems still suffer from faithfulness errors. While prior work has proposed models that improve faithfulness, it is unclear whether the improvement comes from an increased level of extractiveness of the model outputs as one naive way to improve faithfulness is to make summarization models more extractive. In this work, we present a framework for evaluating the effective faithfulness of summarization systems, by generating a faithfulness-abstractiveness trade-off curve that serves as a control at different operating points on the abstractiveness spectrum. We then show that the Maximum Likelihood Estimation (MLE) baseline as well as recently proposed methods for improving faithfulness, fail to consistently improve over the control at the same level of abstractiveness. Finally, we learn a selector to identify the most faithful and abstractive summary for a given document, and show that this system can attain higher faithfulness scores in human evaluations while being more abstractive than the baseline system on two datasets. Moreover, we show that our system is able to achieve a better faithfulness-abstractiveness trade-off than the control at the same level of abstractiveness.",
}

@inproceedings{mrini2021rewards,
  title={Rewards with Negative Examples for Reinforced Topic-Focused Abstractive Summarization},
  author={Mrini, Khalil and Liu, Can and Dreyer, Markus},
  booktitle={Proceedings of the Third Workshop on New Frontiers in Summarization},
  pages={33--38},
  year={2021}
}

 @inproceedings{whiting2019fair,
    title={Fair Work: Crowd Work Minimum Wage with One Line of Code},
    author={Whiting, Mark E and Hugh, Grant and Bernstein, Michael S},
    booktitle={Proceedings of the AAAI Conference on Human Computation and Crowdsourcing},
    volume={7},
    number={1},
    pages={197--206},
    year={2019}
  }
  
  @article{shoeybi2019megatron,
  title={{Megatron-LM: Training Multi-Billion Parameter Language Models using Model Parallelism}},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1909.08053},
  year={2019}
}

@book{BSR2018HRIA,
  author = {BSR},
  year = {2018},
  title = {Human Rights Impact Assessment: Facebook in Myanmar},
  publisher = {BSR}
}

@online{OHCHR2018report,
  author = {Human Rights Council},
  year = {2018},
  title = {Report of the independent international fact-finding mission on Myanmar. United Nations.},
  publisher = {United Nations},
  url = {https://www.ohchr.org/sites/default/files/Documents/HRBodies/HRCouncil/FFM-Myanmar/A_HRC_39_64.pdf},
 urldate = {2022-11-14}
}

@Online{stecklow2018report,
 author = {Steve Stecklow},
 year = {2018},
 title = {Special Report: Why Facebook is losing the war on hate speech in Myanmar},
 journal = {Reuters},
 url = {https://www.reuters.com/article/us-myanmar-facebook-hate-specialreport/special-report-why-facebook-is-losing-the-war-on-hate-speech-in-myanmar-idUSKBN1L01JY},
 urldate = {2022-11-14}
}

@article{Quaranto2022-QUADWC,
	title = {Dog Whistles, Covertly Coded Speech, and the Practices That Enable Them},
	volume = {200},
	pages = {1--34},
	journal = {Synthese},
	author = {Anne Quaranto},
	number = {4},
	publisher = {Springer Verlag},
	year = {2022},
	doi = {10.1007/s11229-022-03791-y}
}

@book{persily_tucker_2020,
    place={Cambridge},
    editor={Persily, Nathaniel and Tucker, Joshua A.},
    series={SSRC Anxieties of Democracy},
    title={Social Media and Democracy: The State of the Field, Prospects for Reform},
    DOI={10.1017/9781108890960},
    publisher={Cambridge University Press},
    year={2020},
    collection={SSRC Anxieties of Democracy}
}

@book{Fogal2018-FOGNWO,
	title = {New Work on Speech Acts},
	year = {2018},
	publisher = {Oxford University Press},
	author = {Daniel Fogal and Daniel W. Harris and Matt Moss}
}

@article{kantor2000trec,
  title={The TREC-5 Confusion Track},
  author={Kantor, Paul and Voorhees, Ellen},
  journal={Information Retrieval},
  volume={2},
  number={2-3},
  pages={165--176},
  year={2000}
}

@article{jarvelin2002,
  author = {J{\"a}rvelin, Kalervo and Kek{\"a}l{\"a}inen, Jaana},
  title = {Cumulated Gain-based Evaluation of IR Techniques},
  journal = {ACM Trans. Inf. Syst.},
  volume = {20},
  number = {4},
  year = {2002},
  pages = {422--446},
  numpages = {25},
  url = {http://doi.acm.org/10.1145/582415.582418},
}
  

 %{AMS}
 @String{AMSTrans = "American Mathematical Society Translations" }
 @String{AMSTrans = "Amer. Math. Soc. Transl." }
 @String{BullAMS = "Bulletin of the American Mathematical Society" }
 @String{BullAMS = "Bull. Amer. Math. Soc." }
 @String{ProcAMS = "Proceedings of the American Mathematical Society" }
 @String{ProcAMS = "Proc. Amer. Math. Soc." }
 @String{TransAMS = "Transactions of the American Mathematical Society" }
 @String{TransAMS = "Trans. Amer. Math. Soc." }

 %ACM
 @String{CACM = "Communications of the {ACM}" }
 @String{CACM = "Commun. {ACM}" }
 @String{CompServ = "Comput. Surveys" }
 @String{JACM = "J. ACM" }
 @String{ACMMathSoft = "{ACM} Transactions on Mathematical Software" }
 @String{ACMMathSoft = "{ACM} Trans. Math. Software" }
 @String{SIGNUM = "{ACM} {SIGNUM} Newsletter" }
 @String{SIGNUM = "{ACM} {SIGNUM} Newslett." }

 @String{AmerSocio = "American Journal of Sociology" }
 @String{AmerStatAssoc = "Journal of the American Statistical Association" }
 @String{AmerStatAssoc = "J. Amer. Statist. Assoc." }
 @String{ApplMathComp = "Applied Mathematics and Computation" }
 @String{ApplMathComp = "Appl. Math. Comput." }
 @String{AmerMathMonthly = "American Mathematical Monthly" }
 @String{AmerMathMonthly = "Amer. Math. Monthly" }
 @String{BIT = "{BIT}" }
 @String{BritStatPsych = "British Journal of Mathematical and Statistical
          Psychology" }
 @String{BritStatPsych = "Brit. J. Math. Statist. Psych." }
 @String{CanMathBull = "Canadian Mathematical Bulletin" }
 @String{CanMathBull = "Canad. Math. Bull." }
 @String{CompApplMath = "Journal of Computational and Applied Mathematics" }
 @String{CompApplMath = "J. Comput. Appl. Math." }
 @String{CompPhys = "Journal of Computational Physics" }
 @String{CompPhys = "J. Comput. Phys." }
 @String{CompStruct = "Computers and Structures" }
 @String{CompStruct = "Comput. \& Structures" }
 @String{CompJour = "The Computer Journal" }
 @String{CompJour = "Comput. J." }
 @String{CompSysSci = "Journal of Computer and System Sciences" }
 @String{CompSysSci = "J. Comput. System Sci." }
 @String{Computing = "Computing" }
 @String{ContempMath = "Contemporary Mathematics" }
 @String{ContempMath = "Contemp. Math." }
 @String{Crelle = "Crelle's Journal" }
 @String{GiornaleMath = "Giornale di Mathematiche" }
 @String{GiornaleMath = "Giorn. Mat." } % didn't find in AMS MR., ibid.

 %IEEE
 @String{Computer = "{IEEE} Computer" }
 @String{IEEETransComp = "{IEEE} Transactions on Computers" }
 @String{IEEETransComp = "{IEEE} Trans. Comput." }
 @String{IEEETransAC = "{IEEE} Transactions on Automatic Control" }
 @String{IEEETransAC = "{IEEE} Trans. Automat. Control" }
 @String{IEEESpec = "{IEEE} Spectrum" } % didn't find in AMS MR
 @String{ProcIEEE = "Proceedings of the {IEEE}" }
 @String{ProcIEEE = "Proc. {IEEE}" } % didn't find in AMS MR
 @String{IEEETransAeroElec = "{IEEE} Transactions on Aerospace and Electronic
     Systems" }
 @String{IEEETransAeroElec = "{IEEE} Trans. Aerospace Electron. Systems" }

 @String{IMANumerAna = "{IMA} Journal of Numerical Analysis" }
 @String{IMANumerAna = "{IMA} J. Numer. Anal." }
 @String{InfProcLet = "Information Processing Letters" }
 @String{InfProcLet = "Inform. Process. Lett." }
 @String{InstMathApp = "Journal of the Institute of Mathematics and
     its Applications" }
 @String{InstMathApp = "J. Inst. Math. Appl." }
 @String{IntControl = "International Journal of Control" }
 @String{IntControl = "Internat. J. Control" }
 @String{IntNumerEng = "International Journal for Numerical Methods in
     Engineering" }
 @String{IntNumerEng = "Internat. J. Numer. Methods Engrg." }
 @String{IntSuper = "International Journal of Supercomputing Applications" }
 @String{IntSuper = "Internat. J. Supercomputing Applic." } % didn't find
%% in AMS MR
 @String{Kibernetika = "Kibernetika" }
 @String{JResNatBurStand = "Journal of Research of the National Bureau
     of Standards" }
 @String{JResNatBurStand = "J. Res. Nat. Bur. Standards" }
 @String{LinAlgApp = "Linear Algebra and its Applications" }
 @String{LinAlgApp = "Linear Algebra Appl." }
 @String{MathAnaAppl = "Journal of Mathematical Analysis and Applications" }
 @String{MathAnaAppl = "J. Math. Anal. Appl." }
 @String{MathAnnalen = "Mathematische Annalen" }
 @String{MathAnnalen = "Math. Ann." }
 @String{MathPhys = "Journal of Mathematical Physics" }
 @String{MathPhys = "J. Math. Phys." }
 @String{MathComp = "Mathematics of Computation" }
 @String{MathComp = "Math. Comp." }
 @String{MathScand = "Mathematica Scandinavica" }
 @String{MathScand = "Math. Scand." }
 @String{TablesAidsComp = "Mathematical Tables and Other Aids to Computation" }
 @String{TablesAidsComp = "Math. Tables Aids Comput." }
 @String{NumerMath = "Numerische Mathematik" }
 @String{NumerMath = "Numer. Math." }
 @String{PacificMath = "Pacific Journal of Mathematics" }
 @String{PacificMath = "Pacific J. Math." }
 @String{ParDistComp = "Journal of Parallel and Distributed Computing" }
 @String{ParDistComp = "J. Parallel and Distrib. Comput." } % didn't find
%% in AMS MR
 @String{ParComputing = "Parallel Computing" }
 @String{ParComputing = "Parallel Comput." }
 @String{PhilMag = "Philosophical Magazine" }
 @String{PhilMag = "Philos. Mag." }
 @String{ProcNAS = "Proceedings of the National Academy of Sciences
                    of the USA" }
 @String{ProcNAS = "Proc. Nat. Acad. Sci. U. S. A." }
 @String{Psychometrika = "Psychometrika" }
 @String{QuartMath = "Quarterly Journal of Mathematics, Oxford, Series (2)" }
 @String{QuartMath = "Quart. J. Math. Oxford Ser. (2)" }
 @String{QuartApplMath = "Quarterly of Applied Mathematics" }
 @String{QuartApplMath = "Quart. Appl. Math." }
 @String{RevueInstStat = "Review of the International Statisical Institute" }
 @String{RevueInstStat = "Rev. Inst. Internat. Statist." }

 %SIAM
 @String{JSIAM = "Journal of the Society for Industrial and Applied
     Mathematics" }
 @String{JSIAM = "J. Soc. Indust. Appl. Math." }
 @String{JSIAMB = "Journal of the Society for Industrial and Applied
     Mathematics, Series B, Numerical Analysis" }
 @String{JSIAMB = "J. Soc. Indust. Appl. Math. Ser. B Numer. Anal." }
 @String{SIAMAlgMeth = "{SIAM} Journal on Algebraic and Discrete Methods" }
 @String{SIAMAlgMeth = "{SIAM} J. Algebraic Discrete Methods" }
 @String{SIAMAppMath = "{SIAM} Journal on Applied Mathematics" }
 @String{SIAMAppMath = "{SIAM} J. Appl. Math." }
 @String{SIAMComp = "{SIAM} Journal on Computing" }
 @String{SIAMComp = "{SIAM} J. Comput." }
 @String{SIAMMatrix = "{SIAM} Journal on Matrix Analysis and Applications" }
 @String{SIAMMatrix = "{SIAM} J. Matrix Anal. Appl." }
 @String{SIAMNumAnal = "{SIAM} Journal on Numerical Analysis" }
 @String{SIAMNumAnal = "{SIAM} J. Numer. Anal." }
 @String{SIAMReview = "{SIAM} Review" }
 @String{SIAMReview = "{SIAM} Rev." }
 @String{SIAMSciStat = "{SIAM} Journal on Scientific and Statistical
     Computing" }
 @String{SIAMSciStat = "{SIAM} J. Sci. Statist. Comput." }

 @String{SoftPracExp = "Software Practice and Experience" }
 @String{SoftPracExp = "Software Prac. Experience" } % didn't find in AMS MR
 @String{StatScience = "Statistical Science" }
 @String{StatScience = "Statist. Sci." }
 @String{Techno = "Technometrics" }
 @String{USSRCompMathPhys = "{USSR} Computational Mathematics and Mathematical
     Physics" }
 @String{USSRCompMathPhys = "{U. S. S. R.} Comput. Math. and Math. Phys." }
 @String{VLSICompSys = "Journal of {VLSI} and Computer Systems" }
 @String{VLSICompSys = "J. {VLSI} Comput. Syst." }
 @String{ZAngewMathMech = "Zeitschrift fur Angewandte Mathematik und
     Mechanik" }
 @String{ZAngewMathMech = "Z. Angew. Math. Mech." }
 @String{ZAngewMathPhys = "Zeitschrift fur Angewandte Mathematik und Physik" }
 @String{ZAngewMathPhys = "Z. Angew. Math. Phys." }

% Publishers % ================================================= |

 @String{Academic = "Academic Press" }
 @String{ACMPress = "{ACM} Press" }
 @String{AdamHilger = "Adam Hilger" }
 @String{AddisonWesley = "Addison-Wesley" }
 @String{AllynBacon = "Allyn and Bacon" }
 @String{AMS = "American Mathematical Society" }
 @String{Birkhauser = "Birkha{\"u}ser" }
 @String{CambridgePress = "Cambridge University Press" }
 @String{Chelsea = "Chelsea" }
 @String{ClaredonPress = "Claredon Press" }
 @String{DoverPub = "Dover Publications" }
 @String{Eyolles = "Eyolles" }
 @String{HoltRinehartWinston = "Holt, Rinehart and Winston" }
 @String{Interscience = "Interscience" }
 @String{JohnsHopkinsPress = "The Johns Hopkins University Press" }
 @String{JohnWileySons = "John Wiley and Sons" }
 @String{Macmillan = "Macmillan" }
 @String{MathWorks = "The Math Works Inc." }
 @String{McGrawHill = "McGraw-Hill" }
 @String{NatBurStd = "National Bureau of Standards" }
 @String{NorthHolland = "North-Holland" }
 @String{OxfordPress = "Oxford University Press" }  %address Oxford or London?
 @String{PergamonPress = "Pergamon Press" }
 @String{PlenumPress = "Plenum Press" }
 @String{PrenticeHall = "Prentice-Hall" }
 @String{SIAMPub = "{SIAM} Publications" }
 @String{Springer = "Springer-Verlag" }
 @String{TexasPress = "University of Texas Press" }
 @String{VanNostrand = "Van Nostrand" }
 @String{WHFreeman = "W. H. Freeman and Co." }

%Entries

@Article{Abril07,
  author        = "Patricia S. Abril and Robert Plant",
  title         = "The patent holder's dilemma: Buy, sell, or troll?",
  journal       = "Communications of the ACM",
  volume        = "50",
  number        = "1",
  month         = jan,
  year          = "2007",
  pages         = "36--44",
  doi           = "10.1145/1188913.1188915",
  url           = "http://doi.acm.org/10.1145/1219092.1219093",
  note          = "",
}

@Article{Cohen07,
  author        = "Sarah Cohen and Werner Nutt and Yehoshua Sagic",
  title         = "Deciding equivalances among conjunctive aggregate queries",
  journal       = JACM,
  articleno     = 5,
  numpages      = 50,
  volume        = 54,
  number        = 2,
  month         = apr,
  year          = 2007,
  doi           = "10.1145/1219092.1219093",
  url           = "http://doi.acm.org/10.1145/1219092.1219093",
  acmid         = 1219093,
}


@periodical{JCohen96,
  key =          "Cohen",
  editor =       "Jacques Cohen",
  title =        "Special issue: Digital Libraries",
  journal =      CACM,
  volume =       "39",
  number =       "11",
  month =        nov,
  year =         "1996",
}


@Book{Kosiur01,
  author =       "David Kosiur",
  title =        "Understanding Policy-Based Networking",
  publisher =    "Wiley",
  year =         "2001",
  address =      "New York, NY",
  edition =      "2nd.",
  editor =       "",
  volume =       "",
  number =       "",
  series =       "",
  month =        "",
  note =         "",
}


@Book{Harel79,
  author =       "David Harel",
  year =         "1979",
  title =        "First-Order Dynamic Logic",
  series =       "Lecture Notes in Computer Science",
  volume =       "68",
  address =      "New York, NY",
  publisher =    "Springer-Verlag",
  doi =          "10.1007/3-540-09237-4",
  url =          "http://dx.doi.org/10.1007/3-540-09237-4",
  editor =       "",
  number =       "",
  month =        "",
  note =         "",
}


@Inbook{Editor00,
  author =       "",
  editor =       "Ian Editor",
  title =        "The title of book one",
  subtitle =     "The book subtitle",
  series =       "The name of the series one",
  year =         "2007",
  volume =       "9",
  address =      "Chicago",
  edition =      "1st.",
  publisher =    "University of Chicago Press",
  doi =          "10.1007/3-540-09237-4",
  url =          "http://dx.doi.org/10.1007/3-540-09456-9",
  chapter =      "",
  pages =        "",
  number =       "",
  type =         "",
  month =        "",
  note =         "",
}

%
@InBook{Editor00a,
  author =       "",
  editor =       "Ian Editor",
  title =        "The title of book two",
  subtitle =     "The book subtitle",
  series =       "The name of the series two",
  year =         "2008",
  address =      "Chicago",
  edition =      "2nd.",
  publisher =    "University of Chicago Press",
  doi =          "10.1007/3-540-09237-4",
  url =          "http://dx.doi.org/10.1007/3-540-09456-9",
  volume =       "",
  chapter =      "100",
  pages =        "",
  number =       "",
  type =         "",
  month =        "",
  note =         "",
}


% incollection (has an editor, title, and possibly a booktitle)
@Incollection{Spector90,
  author =       "Asad Z. Spector",
  title =        "Achieving application requirements",
  booktitle =    "Distributed Systems",
  publisher =    "ACM Press",
  address =      "New York, NY",
  year =         "1990",
  edition =      "2nd.",
  chapter =      "",
  editor =       "Sape Mullender",
  pages =        "19--33",
  doi =          "10.1145/90417.90738",
  url =          "http://doi.acm.org/10.1145/90417.90738",
  volume =       "",
  number =       "",
  series =       "",
  type =         "",
  month =        "",
  note =         "",
}


% incollection (has an editor, title, and possibly a booktitle)
@Incollection{Douglass98,
  author =       "Bruce P. Douglass and David Harel and Mark B. Trakhtenbrot",
  title =        "Statecarts in use: structured analysis and object-orientation",
  series =       "Lecture Notes in Computer Science",
  booktitle =    "Lectures on Embedded Systems",
  publisher =    "Springer-Verlag",
  address =      "London",
  volume =       "1494",
  year =         "1998",
  chapter =      "",
  editor =       "Grzegorz Rozenberg and Frits W. Vaandrager",
  pages =        "368--394",
  doi =          "10.1007/3-540-65193-4_29",
  url =          "http://dx.doi.org/10.1007/3-540-65193-4_29",
  edition =      "",
  number =       "",
  type =         "",
  month =        "",
  note =         "",
}


@Book{Knuth97,
  author =       "Donald E. Knuth",
  title =        "The Art of Computer Programming, Vol. 1: Fundamental Algorithms (3rd. ed.)",
  publisher =    "Addison Wesley Longman Publishing Co., Inc.",
  year =         "1997",
  address =      "",
  edition =      "",
  editor =       "",
  volume =       "",
  number =       "",
  series =       "",
  month =        "",
  note =         "",
}


@Book{Knuth98,
  author =       "Donald E. Knuth",
  year =         "1998",
  title =        "The Art of Computer Programming",
  series =       "Fundamental Algorithms",
  volume =       "1",
  edition =      "3rd",
  address =      "",
  publisher =    "Addison Wesley Longman Publishing Co., Inc.",
  doi =          "",
  url =          "",
  editor =       "",
  number =       "",
  month =        "",
  note =         "(book)",
}

%Inbook{Knuth97,
%  author =       "Donald E. Knuth",
%  title =        "The Art of Computer Programming",
%  booktitle =    "the booktitle",
%  edition =      "3",
%  volume =       "1",
%  year =         "1997",
%  publisher =    "Addison Wesley Longman Publishing Co., Inc.",
%  editor =       "",
%  number =       "",
%  series =       "Fundamental Algorithms",
%  type =         "",
%  chapter =      "",
%  pages =        "",
%  address =      "",
%  month =        "",
%  note =         "(inbook)",
%}

%INBOOK{DK:73-inbook-full,
%   author = "Donald E. Knuth",
%   title = "Fundamental Algorithms (inbook w series)",
%   volume = 1,
%   series = "The Art of Computer Programming",
%   publisher = "Addison-Wesley",
%   address = "Reading, Massachusetts",
%   edition = "Second",
%   month = "10~" # jan,
%   year = "1973",
%   type = "Section",
%   chapter = "1.2",
%   pages = "10--119",
%   note = "Full INBOOK entry (w series)",
%}

%INcollection{DK:74-incoll,
%   author = "Donald E. Knuth",
%   title = "Fundamental Algorithms (incoll)",
%   volume = 1,
%   booktitle = "The Art of Computer Programming",
%   publisher = "Addison-Wesley",
%   address = "Reading, Massachusetts",
%   month = "10~" # jan,
%   year = "1974",
%   pages = "10--119",
%   editor = "Bernard Rous",
%   note = "This is a full incoll entry with an editor",
%}

%INcollection{DK:75-incollws,
%   author = "Donald E. Knuth",
%   title = "Fundamental Algorithms (incoll w series)",
%   volume = 1,
%   booktitle = "The Art of Computer Programming",
%   series = "The Art of Computer Programming",
%   publisher = "Addison-Wesley",
%   address = "Reading, Massachusetts",
%   month = "10~" # jan,
%   year = "1975",
%   pages = "10--119",
%   editor = "Bernard Rous",
%   note = "This is a full incoll entry with an editor and series",
%}


@incollection{GM05,
Author= "Dan Geiger and Christopher Meek",
Title= "Structured Variational Inference Procedures and their Realizations (as incol)",
Year= 2005,
Booktitle="Proceedings of Tenth International Workshop on Artificial Intelligence and Statistics, {\rm The Barbados}",
Publisher="The Society for Artificial Intelligence and Statistics",
Month= jan,
Editors= "Z. Ghahramani and R. Cowell"
}

@Inproceedings{Smith10,
  author =       "Stan W. Smith",
  title =        "An experiment in bibliographic mark-up: Parsing metadata for XML export",
  booktitle =    "Proceedings of the 3rd. annual workshop on Librarians and Computers",
  series =       "LAC '10",
  editor =       "Reginald N. Smythe and Alexander Noble",
  volume =       "3",
  year =         "2010",
  publisher =    "Paparazzi Press",
  address =      "Milan Italy",
  pages =        "422--431",
  doi =          "99.9999/woot07-S422",
  url =          "http://dx.doi.org/99.0000/woot07-S422",
  number =       "",
  month =        "",
  organization = "",
  note =         "",
}

@Inproceedings{VanGundy07,
  author =       "Matthew Van Gundy and Davide Balzarotti and Giovanni Vigna",
  year =         2007,
  title =        "Catch me, if you can: Evading network signatures with web-based polymorphic worms",
  booktitle =    "Proceedings of the first USENIX workshop on Offensive Technologies",
  series =       "WOOT '07",
  publisher =    "USENIX Association",
  address =      "Berkley, CA",
  articleno =    {Paper 7},
  numpages =     9,
}

@Inproceedings{VanGundy08,
  author =       "Matthew Van Gundy and Davide Balzarotti and Giovanni Vigna",
  year =         2008,
  title =        "Catch me, if you can: Evading network signatures with web-based polymorphic worms",
  booktitle =    "Proceedings of the first USENIX workshop on Offensive Technologies",
  series =       "WOOT '08",
  publisher =    "USENIX Association",
  address =      "Berkley, CA",
  articleno =    7,
  numpages =     2,
  pages =        "99-100",
}

@Inproceedings{VanGundy09,
  author =       "Matthew Van Gundy and Davide Balzarotti and Giovanni Vigna",
  year =         2009,
  title =        "Catch me, if you can: Evading network signatures with web-based polymorphic worms",
  booktitle =    "Proceedings of the first USENIX workshop on Offensive Technologies",
  series =       "WOOT '09",
  publisher =    "USENIX Association",
  address =      "Berkley, CA",
  pages =        "90--100",
}

@Inproceedings{Andler79,
  author =       "Sten Andler",
  title =        "Predicate Path expressions",
  booktitle =    "Proceedings of the 6th. ACM SIGACT-SIGPLAN symposium on Principles of Programming Languages",
  series =       "POPL '79",
  year =         "1979",
  publisher =    "ACM Press",
  address =      "New York, NY",
  pages =        "226--236",
  doi =          "10.1145/567752.567774",
  url =          "http://doi.acm.org/10.1145/567752.567774",
  editor =       "",
  volume =       "",
  number =       "",
  month =        "",
  organization = "",
  note =         "",
}

@Techreport{Harel78,
  author =       "David Harel",
  year =         "1978",
  title =        "LOGICS of Programs: AXIOMATICS and DESCRIPTIVE POWER",
  institution =  "Massachusetts Institute of Technology",
  type =         "MIT Research Lab Technical Report",
  number =       "TR-200",
  address =      "Cambridge, MA",
  month =        "",
  note =         "",
}

@MASTERSTHESIS{anisi03,
author = {David A. Anisi},
title = {Optimal Motion Control of a Ground Vehicle},
school = {Royal Institute of Technology (KTH), Stockholm, Sweden},
intitution = {FOI-R-0961-SE, Swedish Defence Research Agency (FOI)},
year = {2003},
}


@Phdthesis{Clarkson85,
  author =       "Kenneth L. Clarkson",
  year =         "1985",
  title =        "Algorithms for Closest-Point Problems (Computational Geometry)",
  school =       "Stanford University",
  address =      "Palo Alto, CA",
  note =         "UMI Order Number: AAT 8506171",
  type =         "",
  month =        "",
}


@online{Thornburg01,
  author =       "Harry Thornburg",
  year =         "2001",
  title =        "Introduction to Bayesian Statistics",
  url =          "http://ccrma.stanford.edu/~jos/bayes/bayes.html",
  month =        mar,
  lastaccessed = "March 2, 2005",
}


@online{Ablamowicz07,
  author =       "Rafal Ablamowicz and Bertfried Fauser",
  year =         "2007",
  title =        "CLIFFORD: a Maple 11 Package for Clifford Algebra Computations, version 11",
  url =          "http://math.tntech.edu/rafal/cliff11/index.html",
  lastaccessed = "February 28, 2008",
}


@misc{Poker06,
  author =       "Poker-Edge.Com",
  year =         "2006",
  month =        mar,
  title =        "Stats and Analysis",
  lastaccessed = "June 7, 2006",
  url =          "http://www.poker-edge.com/stats.php",
}

@misc{Obama08,
  author        = "Barack Obama",
  year          = "2008",
  title         = "A more perfect union",
  howpublished  = "Video",
  day           = "5",
  url           = "http://video.google.com/videoplay?docid=6528042696351994555",
  month         = mar,
  lastaccessed  = "March 21, 2008",
  note          =  "",
}

@misc{JoeScientist001,
  author =       "Joseph Scientist",
  year =         "2009",
  title =        "The fountain of youth",
  note =         "Patent No. 12345, Filed July 1st., 2008, Issued Aug. 9th., 2009",
  url =          "",
  howpublished = "",
  month =        aug,
  lastaccessed = "",
}


@Inproceedings{Novak03,
  author =       "Dave Novak",
  title =        "Solder man",
  booktitle =    "ACM SIGGRAPH 2003 Video Review on Animation theater Program: Part I - Vol. 145 (July 27--27, 2003)",
  year =         "2003",
  publisher =    "ACM Press",
  address =      "New York, NY",
  pages =        "4",
  month =        "March 21, 2008",
  doi =          "99.9999/woot07-S422",
  url =          "http://video.google.com/videoplay?docid=6528042696351994555",
  note =         "",
  howpublished = "Video",
  editor =       "",
  volume =       "",
  number =       "",
  series =       "",
  organization = "",
  distinctURL = 1
}


@article{Lee05,
  author =       "Newton Lee",
  year =         "2005",
  title =        "Interview with Bill Kinder: January 13, 2005",
  journal =      "Comput. Entertain.",
  eid =          "4",
  volume =       "3",
  number =       "1",
  month =        "Jan.-March",
  doi =          "10.1145/1057270.1057278",
  url =          "http://doi.acm.org/10.1145/1057270.1057278",
  howpublished = "Video",
  note =         "",
}

@article{rous08,
  author =       "Bernard Rous",
  year =         "2008",
  title =        "The Enabling of Digital Libraries",
  journal =      "Digital Libraries",
  volume =       "12",
  number =       "3",
  month =        jul,
  articleno =    "Article~5",
  doi =          "",
  url =          "",
  howpublished = "",
  note =         "To appear",
}

@article{384253,
 author = {Werneck,, Renato and Setubal,, Jo\~{a}o and da Conceic\~{a}o,, Arlindo},
 title = {(old) Finding minimum congestion spanning trees},
 journal = {J. Exp. Algorithmics},
 volume = {5},
 year = {2000},
 issn = {1084-6654},
 pages = {11},
 doi = {http://doi.acm.org/10.1145/351827.384253},
 publisher = {ACM},
 address = {New York, NY, USA},
 }


@article{Werneck:2000:FMC:351827.384253,
 author = {Werneck, Renato and Setubal, Jo\~{a}o and da Conceic\~{a}o, Arlindo},
 title = {(new) Finding minimum congestion spanning trees},
 journal = {J. Exp. Algorithmics},
 volume = 5,
 month = dec,
 year = 2000,
 issn = {1084-6654},
 articleno = 11,
 url = {http://portal.acm.org/citation.cfm?id=351827.384253},
 doi = {10.1145/351827.384253},
 acmid = 384253,
 publisher = {ACM},
 address = {New York, NY, USA},
}

@article{1555162,
 author = {Conti, Mauro and Di Pietro, Roberto and Mancini, Luigi V. and Mei, Alessandro},
 title = {(old) Distributed data source verification in wireless sensor networks},
 journal = {Inf. Fusion},
 volume = {10},
 number = {4},
 year = {2009},
 issn = {1566-2535},
 pages = {342--353},
 doi = {http://dx.doi.org/10.1016/j.inffus.2009.01.002},
 publisher = {Elsevier Science Publishers B. V.},
 address = {Amsterdam, The Netherlands, The Netherlands},
 }

@article{Conti:2009:DDS:1555009.1555162,
 author = {Conti, Mauro and Di Pietro, Roberto and Mancini, Luigi V. and Mei, Alessandro},
 title = {(new) Distributed data source verification in wireless sensor networks},
 journal = {Inf. Fusion},
 volume = {10},
 number = {4},
 month = oct,
 year = {2009},
 issn = {1566-2535},
 pages = {342--353},
 numpages = {12},
 url = {http://portal.acm.org/citation.cfm?id=1555009.1555162},
 doi = {10.1016/j.inffus.2009.01.002},
 acmid = {1555162},
 publisher = {Elsevier Science Publishers B. V.},
 address = {Amsterdam, The Netherlands, The Netherlands},
 keywords = {Clone detection, Distributed protocol, Securing data fusion, Wireless sensor networks},
}

@inproceedings{Li:2008:PUC:1358628.1358946,
 author = {Li, Cheng-Lun and Buyuktur, Ayse G. and Hutchful, David K. and Sant, Natasha B. and Nainwal, Satyendra K.},
 title = {Portalis: using competitive online interactions to support aid initiatives for the homeless},
 booktitle = {CHI '08 extended abstracts on Human factors in computing systems},
 year = {2008},
 isbn = {978-1-60558-012-X},
 location = {Florence, Italy},
 pages = {3873--3878},
 numpages = {6},
 url = {http://portal.acm.org/citation.cfm?id=1358628.1358946},
 doi = {10.1145/1358628.1358946},
 acmid = {1358946},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cscw, distributed knowledge acquisition, incentive design, online games, recommender systems, reputation systems, user studies, virtual community},
}

@book{Hollis:1999:VBD:519964,
 author = {Hollis, Billy S.},
 title = {Visual Basic 6: Design, Specification, and Objects with Other},
 year = {1999},
 isbn = {0130850845},
 edition = {1st},
 publisher = {Prentice Hall PTR},
 address = {Upper Saddle River, NJ, USA},
 }


@book{Goossens:1999:LWC:553897,
 author = {Goossens, Michel and Rahtz, S. P. and Moore, Ross and Sutor, Robert S.},
 title = {The  Latex Web Companion: Integrating TEX, HTML, and XML},
 year = {1999},
 isbn = {0201433117},
 edition = {1st},
 publisher = {Addison-Wesley Longman Publishing Co., Inc.},
 address = {Boston, MA, USA},
 }

% need to test genres for errant isbn output

% techreport
@techreport{897367,
 author = {Buss, Jonathan F. and Rosenberg, Arnold L. and Knott, Judson D.},
 title = {Vertex Types in Book-Embeddings},
 year = {1987},
 source = {http://www.ncstrl.org:8900/ncstrl/servlet/search?formname=detail\&id=oai%3Ancstrlh%3Aumass_cs%3Ancstrl.umassa_cs%2F%2FUM-CS-1987-018},
 publisher = {University of Massachusetts},
 address = {Amherst, MA, USA},
 }

@techreport{Buss:1987:VTB:897367,
 author = {Buss, Jonathan F. and Rosenberg, Arnold L. and Knott, Judson D.},
 title = {Vertex Types in Book-Embeddings},
 year = {1987},
 source = {http://www.ncstrl.org:8900/ncstrl/servlet/search?formname=detail\&id=oai%3Ancstrlh%3Aumass_cs%3Ancstrl.umassa_cs%2F%2FUM-CS-1987-018},
 publisher = {University of Massachusetts},
 address = {Amherst, MA, USA},
 }

% whole proceedings

@proceedings{Czerwinski:2008:1358628,
 author = {},
 note = {General Chair-Czerwinski, Mary and General Chair-Lund, Arnie and Program Chair-Tan, Desney},
 title = {CHI '08: CHI '08 extended abstracts on Human factors in computing systems},
 year = {2008},
 isbn = {978-1-60558-012-X},
 location = {Florence, Italy},
 order_no = {608085},
 publisher = {ACM},
 address = {New York, NY, USA},
 }

% phdthesis

@phdthesis{Clarkson:1985:ACP:911891,
 author = {Clarkson, Kenneth Lee},
 advisor = {Yao, Andrew C.},
 title = {Algorithms for Closest-Point Problems (Computational Geometry)},
 year = {1985},
 note = {AAT 8506171},
 school = {Stanford University},
 address = {Stanford, CA, USA},
 }
% school is being picked up -- but not publisher (which is OK)
% Also -- the title is NOT being output in italics !!! Arrrrgh! - I fixed it. :-)


%%% compare with 'old'
%%% atsign-Phdthesis{Clarkson85,
%%%  author =       "Kenneth L. Clarkson",
%%%  year =         "1985",
%%%  title =        "Algorithms for Closest-Point Problems (Computational Geometry)",
%%%  school =       "Stanford University",
%%%  address =      "Palo Alto, CA",
%%%  note =         "UMI Order Number: AAT 8506171",
%%%  type =         "",
%%%  month =        "",
%%%}

% A bibliography
@Article{1984:1040142,
 journal = {SIGCOMM Comput. Commun. Rev.},
 year = {1984},
 issn = {0146-4833},
 volume = {13-14},
 number = {5-1},
 issue_date = {January/April 1984},
 publisher = {ACM},
 address = {New York, NY, USA},
 }


% grinder
@inproceedings{2004:ITE:1009386.1010128,
 key = {IEEE},
 title = {IEEE TCSC Executive Committee},
 booktitle = {Proceedings of the IEEE International Conference on Web Services},
 series = {ICWS '04},
 year = {2004},
 isbn = {0-7695-2167-3},
 pages = {21--22},
 url = {http://dx.doi.org/10.1109/ICWS.2004.64},
 doi = {http://dx.doi.org/10.1109/ICWS.2004.64},
 acmid = {1010128},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
}

% div book
@book{Mullender:1993:DS:302430,
 editor = {Mullender, Sape},
 title = {Distributed systems (2nd Ed.)},
 year = {1993},
 isbn = {0-201-62427-3},
 publisher = {ACM Press/Addison-Wesley Publishing Co.},
 address = {New York, NY, USA},
 }

% master thesis (as techreport and thesis)

@techreport{Petrie:1986:NAD:899644,
 author = {Petrie, Charles J.},
 title = {New Algorithms for Dependency-Directed Backtracking (Master's thesis)},
 year = {1986},
 source = {http://www.ncstrl.org:8900/ncstrl/servlet/search?formname=detail\&id=oai%3Ancstrlh%3Autexas_cs%3AUTEXAS_CS%2F%2FAI86-33},
 publisher = {University of Texas at Austin},
 address = {Austin, TX, USA},
 }

@MASTERSTHESIS{Petrie:1986:NAD:12345,
 author = {Petrie, Charles J.},
 title = {New Algorithms for Dependency-Directed Backtracking (Master's thesis)},
 year = {1986},
 source = {http://www.ncstrl.org:8900/ncstrl/servlet/search?formname=detail\&id=oai%3Ancstrlh%3Autexas_cs%3AUTEXAS_CS%2F%2FAI86-33},
 school = {University of Texas at Austin},
 address = {Austin, TX, USA},
 }




@BOOK{book-minimal,
   author = "Donald E. Knuth",
   title = "Seminumerical Algorithms",
   publisher = "Addison-Wesley",
   year = "1981",
}

% incollection (has an editor, title, and possibly a booktitle)
@INcollection{KA:2001,
 author = {Kong, Wei-Chang},
 Title = {The implementation of electronic commerce in SMEs in Singapore (as Incoll)},
 booktitle = {E-commerce and cultural values},
 year = {2001},
 isbn = {1-59140-056-2},
 pages = {51--74},
 numpages = {24},
 url = {http://portal.acm.org/citation.cfm?id=887006.887010},
 acmid = {887010},
 publisher = {IGI Publishing},
 address = {Hershey, PA, USA},
}


% with bibfield 'type' before chapter (note no editor)
@INBOOK{KAGM:2001,
 author = {Kong, Wei-Chang},
 type = {Name of Chapter:},
 chapter = {The implementation of electronic commerce in SMEs in Singapore (Inbook-w-chap-w-type)},
 title = {E-commerce and cultural values},
 year = {2001},
 isbn = {1-59140-056-2},
 pages = {51--74},
 numpages = {24},
 url = {http://portal.acm.org/citation.cfm?id=887006.887010},
 acmid = {887010},
 publisher = {IGI Publishing},
 address = {Hershey, PA, USA},
}

%%% Notes! This is because the atsign-INBOOK citation type specifies EITHER
%%% editor or author, but not both. In my experiments with the harvard/dcu
%%% bibtex style (and presumably this applies to other styles too), bibtex
%%% ignores the editor information if author information exists in an
%%% atsign-INBOOK entry. atsign-INCOLLECTION is far more commonly used in my references,
%%% and in the absence of an editor I believe most bibtex styles will just
%%% ommit the editor from the reference - the chapter information will not
%%% end up in the in-text citation as you suggest it should be but at least
%%% there is a place to put the editor if necessary.



% was 'Inbook' -- changed to incollection - (editor is different to author) - need to tell Asad to codify as such.
@incollection{Kong:2002:IEC:887006.887010,
  author =      {Kong, Wei-Chang},
  editor =      {Theerasak Thanasankit},
  title =       {Chapter 9},
  booktitle =   {E-commerce and cultural values (Incoll-w-text (chap 9) 'title')},
  year =        {2002},
  address =     {Hershey, PA, USA},
  publisher =   {IGI Publishing},
  url =         {http://portal.acm.org/citation.cfm?id=887006.887010},
  pages =       {51--74},
  numpages =    {24},
  acmid =       {887010},
  isbn =        {1-59140-056-2},
  number =      "",
  type =        "",
  month =       "",
  note =        "",
}

% incol when the chapter is 'text' - due to presence of editor (different to author)
@incollection{Kong:2003:IEC:887006.887011,
 author = {Kong, Wei-Chang},
 title = {The implementation of electronic commerce in SMEs in Singapore (Incoll)},
 booktitle = {E-commerce and cultural values},
 editor = {Thanasankit, Theerasak},
 year = {2003},
 isbn = {1-59140-056-2},
 pages = {51--74},
 numpages = {24},
 url = {http://portal.acm.org/citation.cfm?id=887006.887010},
 acmid = {887010},
 publisher = {IGI Publishing},
 address = {Hershey, PA, USA},
}

% ------ test
%incollection{Kong:2003:IEC:887006.887010,
% author = {Kong, Wei-Chang},
% chapter = {The implementation of electronic commerce in SMEs in Singapore (Incoll-text-in-chap)},
% booktitle = {booktitle E-commerce and cultural values},
% title =   {The title},
% editor = {Thanasankit, Theerasak},
% year = {2003},
% isbn = {1-59140-056-2},
% pages = {51--74},
% numpages = {24},
% url = {http://portal.acm.org/citation.cfm?id=887006.887010},
% acmid = {887010},
% publisher = {IGI Publishing},
% address = {Hershey, PA, USA},
%}


% ---------





% Need inbook with num in chapter

% and inbook with number in chapter
@InBook{Kong:2004:IEC:123456.887010,
  author =      {Kong, Wei-Chang},
  editor =      {Theerasak Thanasankit},
  title =       {E-commerce and cultural values - (InBook-num-in-chap)},
  chapter =     {9},
  year =        {2004},
  address =     {Hershey, PA, USA},
  publisher =   {IGI Publishing},
  url =         {http://portal.acm.org/citation.cfm?id=887006.887010},
  pages =       {51--74},
  numpages =    {24},
  acmid =       {887010},
  isbn =        {1-59140-056-2},
  number =      "",
  type =        "",
  month =       "",
  note =        "",
}


% and inbook with text in chapter
@Inbook{Kong:2005:IEC:887006.887010,
  author =      {Kong, Wei-Chang},
  editor =      {Theerasak Thanasankit},
  title =       {E-commerce and cultural values (Inbook-text-in-chap)},
  chapter =     {The implementation of electronic commerce in SMEs in Singapore},
  year =        {2005},
  address =     {Hershey, PA, USA},
  publisher =   {IGI Publishing},
  url =         {http://portal.acm.org/citation.cfm?id=887006.887010},
  type =        {Chapter:},
  pages =       {51--74},
  numpages =    {24},
  acmid =       {887010},
  isbn =        {1-59140-056-2},
  number =      "",
  month =       "",
  note =        "",
}


% and inbook with a num and type field
@Inbook{Kong:2006:IEC:887006.887010,
  author =      {Kong, Wei-Chang},
  editor =      {Theerasak Thanasankit},
  title =       {E-commerce and cultural values (Inbook-num chap)},
  chapter =     {22},
  year =        {2006},
  address =     {Hershey, PA, USA},
  publisher =   {IGI Publishing},
  url =         {http://portal.acm.org/citation.cfm?id=887006.887010},
  type =        {Chapter (in type field)},
  pages =       {51--74},
  numpages =    {24},
  acmid =       {887010},
  isbn =        {1-59140-056-2},
  number =      "",
  month =       "",
  note =        "",
}


% and incol coz we have a BLANK chapter - due to presence of editor
%atIncollection{Kong:2006:IEC:887006.887011,
%  author =     {Kong, Wei-Chang},
%  editor =     {Theerasak Thanasankit},
%  title =      "The title"
%  booktitle =  {E-commerce and cultural values (Incol-coz-blank-chap)},
%  year =       {2006},
%  address =    {Hershey, PA, USA},
%  publisher =  {IGI Publishing},
%  url =        {http://portal.acm.org/citation.cfm?id=887006.887010},
%  type =       {Type!},
%  chapter =    {},
%  pages =      {51--74},
%  numpages =   {24},
%  acmid =      {887010},
%  isbn =       {1-59140-056-2},
%  number =     "",
%  month =      "",
%  note =       "",
%}

@article{SaeediMEJ10,
            author = {Mehdi Saeedi and Morteza Saheb Zamani and Mehdi Sedighi},
            title = {A library-based synthesis methodology for reversible logic},
            journal = {Microelectron. J.},
            volume = {41},
            number = {4},
            month = apr,
            year = {2010},
            pages = {185--194},
}

@ARTICLE{SaeediJETC10,
            author = {Mehdi Saeedi and Morteza Saheb Zamani and Mehdi Sedighi and Zahra Sasanian},
            title = {Synthesis of Reversible Circuit Using Cycle-Based Approach},
            journal = {J. Emerg. Technol. Comput. Syst.},
            volume = {6},
            number = {4},
            month = dec,
            year = {2010}
            }

% Asad's new version
@article{Kirschmer:2010:AEI:1958016.1958018,
 author = {Kirschmer, Markus and Voight, John},
 title = {Algorithmic Enumeration of Ideal Classes for Quaternion Orders},
 journal = {SIAM J. Comput.},
 issue_date = {January 2010},
 volume = {39},
 number = {5},
 month = jan,
 year = {2010},
 issn = {0097-5397},
 pages = {1714--1747},
 numpages = {34},
 url = {http://dx.doi.org/10.1137/080734467},
 doi = {https://doi.org/10.1137/080734467},
 acmid = {1958018},
 publisher = {Society for Industrial and Applied Mathematics},
 address = {Philadelphia, PA, USA},
 keywords = {ideal classes, maximal orders, number theory, quaternion algebras},
}


% incol due to presence of booktitle
@incollection{Hoare:1972:CIN:1243380.1243382,
 author = {Hoare, C. A. R.},
 title = {Chapter II: Notes on data structuring},
 booktitle = {Structured programming (incoll)},
 editor = {Dahl, O. J. and Dijkstra, E. W. and Hoare, C. A. R.},
 year = {1972},
 isbn = {0-12-200550-3},
 pages = {83--174},
 numpages = {92},
 url = {http://portal.acm.org/citation.cfm?id=1243380.1243382},
 acmid = {1243382},
 publisher = {Academic Press Ltd.},
 address = {London, UK, UK},
}

% incol due to presence of booktitle
@incollection{Lee:1978:TQA:800025.1198348,
 author = {Lee, Jan},
 title = {Transcript of question and answer session},
 booktitle = {History of programming languages I (incoll)},
 editor = {Wexelblat, Richard L.},
 year = {1981},
 isbn = {0-12-745040-8},
 pages = {68--71},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/800025.1198348},
 doi = {http://doi.acm.org/10.1145/800025.1198348},
 acmid = {1198348},
 publisher = {ACM},
 address = {New York, NY, USA},
}

% incol due to booktitle
@incollection{Dijkstra:1979:GSC:1241515.1241518,
 author = {Dijkstra, E.},
 title = {Go to statement considered harmful},
 booktitle = {Classics in software engineering (incoll)},
 year = {1979},
 isbn = {0-917072-14-6},
 pages = {27--33},
 numpages = {7},
 url = {http://portal.acm.org/citation.cfm?id=1241515.1241518},
 acmid = {1241518},
 publisher = {Yourdon Press},
 address = {Upper Saddle River, NJ, USA},
}

% incol due to booktitle
@incollection{Wenzel:1992:TVA:146022.146089,
 author = {Wenzel, Elizabeth M.},
 title = {Three-dimensional virtual acoustic displays},
 booktitle = {Multimedia interface design (incoll)},
 year = {1992},
 isbn = {0-201-54981-6},
 pages = {257--288},
 numpages = {32},
 url = {http://portal.acm.org/citation.cfm?id=146022.146089},
 doi = {10.1145/146022.146089},
 acmid = {146089},
 publisher = {ACM},
 address = {New York, NY, USA},
}

% incol due to booktitle
@incollection{Mumford:1987:MES:54905.54911,
 author = {Mumford, E.},
 title = {Managerial expert systems and organizational change: some critical research issues},
 booktitle = {Critical issues in information systems research (incoll)},
 year = {1987},
 isbn = {0-471-91281-6},
 pages = {135--155},
 numpages = {21},
 url = {http://portal.acm.org/citation.cfm?id=54905.54911},
 acmid = {54911},
 publisher = {John Wiley \& Sons, Inc.},
 address = {New York, NY, USA},
}

@book{McCracken:1990:SSC:575315,
 author = {McCracken, Daniel D. and Golden, Donald G.},
 title = {Simplified Structured COBOL with Microsoft/MicroFocus COBOL},
 year = {1990},
 isbn = {0471514071},
 publisher = {John Wiley \& Sons, Inc.},
 address = {New York, NY, USA},
}

% Let's include Boris / BBeeton entries  (multi-volume works)

@book {MR781537,
    AUTHOR = {H{\"o}rmander, Lars},
     TITLE = {The analysis of linear partial differential operators. {III}},
    SERIES = {Grundlehren der Mathematischen Wissenschaften [Fundamental
              Principles of Mathematical Sciences]},
    VOLUME = {275},
      NOTE = {Pseudodifferential operators},
PUBLISHER = {Springer-Verlag},
   ADDRESS = {Berlin, Germany},
      YEAR = {1985},
     PAGES = {viii+525},
      ISBN = {3-540-13828-5},
   MRCLASS = {35-02 (35Sxx 47G05 58G15)},
  MRNUMBER = {781536 (87d:35002a)},
MRREVIEWER = {Min You Qi},
}

@book {MR781536,
    AUTHOR = {H{\"o}rmander, Lars},
     TITLE = {The analysis of linear partial differential operators. {IV}},
    SERIES = {Grundlehren der Mathematischen Wissenschaften [Fundamental
              Principles of Mathematical Sciences]},
    VOLUME = {275},
      NOTE = {Fourier integral operators},
PUBLISHER = {Springer-Verlag},
   ADDRESS = {Berlin, Germany},
      YEAR = {1985},
     PAGES = {vii+352},
      ISBN = {3-540-13829-3},
   MRCLASS = {35-02 (35Sxx 47G05 58G15)},
  MRNUMBER = {781537 (87d:35002b)},
MRREVIEWER = {Min You Qi},
}

%%%%%%%%%%%%%%%%%%%%%% Start of Aptara sample bib entries

% acmsmall-sam.bib
@InProceedings{Adya-01,
  author        = {A. Adya and P. Bahl and J. Padhye and A.Wolman and L. Zhou},
  title         = {A multi-radio unification protocol for {IEEE} 802.11 wireless networks},
  booktitle     = {Proceedings of the IEEE 1st International Conference on Broadnets Networks (BroadNets'04)},
  publisher     = "IEEE",
  address       = "Los Alamitos, CA",
  year          = {2004},
  pages         = "210--217"
}

@article{Akyildiz-01,
  author        = {I. F. Akyildiz and W. Su and Y. Sankarasubramaniam and E. Cayirci},
  title         = {Wireless Sensor Networks: A Survey},
  journal       = {Comm. ACM},
  volume        = 38,
  number        = "4",
  year          = {2002},
  pages         = "393--422"
}

@article{Akyildiz-02,
  author        = {I. F. Akyildiz and T. Melodia and K. R. Chowdhury},
  title         = {A Survey on Wireless Multimedia Sensor Networks},
  journal       = {Computer Netw.},
  volume        = 51,
  number        = "4",
  year          = {2007},
  pages         = "921--960"
}

@InProceedings{Bahl-02,
  author        = {P. Bahl and R. Chancre and J. Dungeon},
  title         = {{SSCH}: Slotted Seeded Channel Hopping for Capacity Improvement in {IEEE} 802.11 Ad-Hoc Wireless Networks},
  booktitle     = {Proceeding of the 10th International Conference on Mobile Computing and Networking (MobiCom'04)},
  publisher     = "ACM",
  address       = "New York, NY",
  year          = {2004},
  pages         = "112--117"
}

@misc{CROSSBOW,
  key       = {CROSSBOW},
  title     = {{XBOW} Sensor Motes Specifications},
  note      = {http://www.xbow.com},
  year      = 2008
}

@article{Culler-01,
  author        = {D. Culler and D. Estrin and M. Srivastava},
  title         = {Overview of Sensor Networks},
  journal       = {IEEE Comput.},
  volume        = 37,
  number        = "8 (Special Issue on Sensor Networks)",
  publisher     = "IEEE",
  address       = "Los Alamitos, CA",
  year          = {2004},
  pages         = "41--49"
}

@misc{Harvard-01,
    key         = {Harvard CodeBlue},
    title       = {{CodeBlue}: Sensor Networks for Medical Care},
    note        = {http://www.eecs.harvard.edu/mdw/ proj/codeblue/},
    year        = 2008
}

@InProceedings{Natarajan-01,
    author      = {A. Natarajan and M. Motani and B. de Silva and K. Yap and K. C. Chua},
    title       = {Investigating Network Architectures for Body Sensor Networks},
    booktitle   = {Network Architectures},
    editor      = {G. Whitcomb and P. Neece},
    publisher   = "Keleuven Press",
    address     = "Dayton, OH",
    year        = {2007},
    pages       = "322--328",
    eprint      = "960935712",
    primaryclass = "cs",
}

@techreport{Tzamaloukas-01,
  author        = {A. Tzamaloukas and J. J. Garcia-Luna-Aceves},
  title         = {Channel-Hopping Multiple Access},
  number =        {I-CA2301},
  institution =   {Department of Computer Science, University of California},
  address =       {Berkeley, CA},
  year          = {2000}
}

@BOOK{Zhou-06,
  author        = {G. Zhou and J. Lu and C.-Y. Wan and M. D. Yarvis and J. A. Stankovic},
  title         = {Body Sensor Networks},
  publisher     = "MIT Press",
  address       = "Cambridge, MA",
  year          = {2008}
}

@mastersthesis{ko94,
author = "Jacob Kornerup",
title = "Mapping Powerlists onto Hypercubes",
school = "The University of Texas at Austin",
note = "(In preparation)",
year = "1994"}
%month = "dec",}

@PhdThesis{gerndt:89,
  author =       "Michael Gerndt",
  title =        "Automatic Parallelization for Distributed-Memory
                  Multiprocessing Systems",
  school =       "University of Bonn",
  year =         1989,
  address =      "Bonn, Germany",
  month =        dec
}

@article{6:1:1,
author = "J. E. {Archer, Jr.} and R. Conway and F. B. Schneider",
title = "User recovery and reversal in interactive systems",
journal = "ACM Trans. Program. Lang. Syst.",
volume =  "6",
number = "1",
month = jan,
year = 1984,
pages = "1--19"}

@article{7:1:137,
author = "D. D. Dunlop and V. R. Basili",
title = "Generalizing specifications for uniformly implemented loops",
journal = "ACM Trans. Program. Lang. Syst.",
volume =  "7",
number = "1",
month = jan,
year = 1985,
pages = "137--158"}

@article{7:2:183,
author = "J. Heering and P. Klint",
title = "Towards monolingual programming environments",
journal = "ACM Trans. Program. Lang. Syst.",
volume =  "7",
number = "2",
month = apr,
year = 1985,
pages = "183--213"}

@book{knuth:texbook,
author = "Donald E. Knuth",
title = "The {\TeX{}book}",
publisher = "Addison-Wesley",
address = "Reading, MA.",
year = 1984}

@article{6:3:380,
author = "E. Korach and D.  Rotem and N. Santoro",
title = "Distributed algorithms for finding centers and medians in networks",
journal = "ACM Trans. Program. Lang. Syst.",
volume =  "6",
number = "3",
month = jul,
year = 1984,
pages = "380--401"}

@book{Lamport:LaTeX,
author = "Leslie Lamport",
title = "\it {\LaTeX}: A Document Preparation System",
publisher = "Addison-Wesley",
address = "Reading, MA.",
year = 1986}

@article{7:3:359,
author = "F. Nielson",
title = "Program transformations in a denotational setting",
journal = "ACM Trans. Program. Lang. Syst.",
volume =  "7",
number = "3",
month = jul,
year = 1985,
pages = "359--379"}

%testing
@BOOK{test,
   author = "Donald E. Knuth",
   title = "Seminumerical Algorithms",
   volume = 2,
   series = "The Art of Computer Programming",
   publisher = "Addison-Wesley",
   address = "Reading, MA",
   edition = "2nd",
   month = "10~" # jan,
   year = "1981",
}

@inproceedings{reid:scribe,
author = "Brian K. Reid",
title = "A high-level approach to computer document formatting",
booktitle = "Proceedings of the 7th Annual Symposium on Principles of
  Programming Languages",
month = jan,
year = 1980,
publisher = "ACM",
address = "New York",
pages = "24--31"}

@article{Zhou:2010:MMS:1721695.1721705,
 author = {Zhou, Gang and Wu, Yafeng and Yan, Ting and He, Tian and Huang, Chengdu and Stankovic, John A. and Abdelzaher, Tarek F.},
 title = {A multifrequency MAC specially designed for wireless sensor network applications},
 journal = {ACM Trans. Embed. Comput. Syst.},
 issue_date = {March 2010},
 volume = 9,
 number = 4,
 month = {April},
 year = 2010,
 issn = {1539-9087},
 pages = {39:1--39:41},
 articleno = 39,
 numpages = 41,
 url = {http://doi.acm.org/10.1145/1721695.1721705},
 doi = {10.1145/1721695.1721705},
 acmid = 1721705,
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Wireless sensor networks, media access control, multi-channel, radio interference, time synchronization},
}


@online{TUGInstmem,
  key =          {TUG},
  year  =        2017,
  title =        "Institutional members of the {\TeX} Users Group",
  url =          "http://wwtug.org/instmem.html",
  lastaccessed = "May 27, 2017",
}

@online{CTANacmart,
  author =    {Boris Veytsman},
  title =  {acmart---{Class} for typesetting publications of {ACM}},
  year = 2017,
  url =    {http://www.ctan.org/pkg/acmart},
  lastaccessed = {May 27, 2017}
  }

@online{doclicense,
  author =    {Robin Schneider},
  title =  {The \textsl{doclicense} package},
  year = 2022,
  url =    {http://www.ctan.org/pkg/doclicense},
  lastaccessed = {May 27, 2022}
  }

@ARTICLE{bowman:reasoning,
    author = {Bowman, Mic and Debray, Saumya K. and Peterson, Larry L.},
    title = {Reasoning About Naming Systems},
    journal = {ACM Trans. Program. Lang. Syst.},
    volume = {15},
    number = {5},
    pages = {795-825},
    month = {November},
    year = {1993},
    doi = {10.1145/161468.161471},
}

@ARTICLE{braams:babel,
    author = {Braams, Johannes},
    title = {Babel, a Multilingual Style-Option System for Use with LaTeX's Standard Document Styles},
    journal = {TUGboat},
    volume = {12},
    number = {2},
    pages = {291-301},
    month = {June},
    year = {1991},
}

@INPROCEEDINGS{clark:pct,
  AUTHOR = "Malcolm Clark",
  TITLE = "Post Congress Tristesse",
  BOOKTITLE = "TeX90 Conference Proceedings",
  PAGES = "84-89",
  ORGANIZATION = "TeX Users Group",
  MONTH = "March",
  YEAR = {1991}
}

@ARTICLE{herlihy:methodology,
    author = {Herlihy, Maurice},
    title = {A Methodology for Implementing Highly Concurrent Data Objects},
    journal = {ACM Trans. Program. Lang. Syst.},
    volume = {15},
    number = {5},
    pages = {745-770},
    month = {November},
    year = {1993},
    doi = {10.1145/161468.161469},
}

@BOOK{salas:calculus,
  AUTHOR = "S.L. Salas and Einar Hille",
  TITLE = "Calculus: One and Several Variable",
  PUBLISHER = "John Wiley and Sons",
  ADDRESS = "New York",
  YEAR = "1978"
}

@MANUAL{Fear05,
  title =        {Publication quality tables in {\LaTeX}},
  author =       {Simon Fear},
  month =        {April},
  year =         2005,
  note =         {\url{http://www.ctan.org/pkg/booktabs}}
}

@Manual{Amsthm15,
  title =        {Using the amsthm Package},
  organization = {American Mathematical Society},
  month =        {April},
  year =         2015,
  note =         {\url{http://www.ctan.org/pkg/amsthm}}
}

@ArtifactSoftware{R,
    title = {R: A Language and Environment for Statistical Computing},
    author = {{R Core Team}},
    organization = {R Foundation for Statistical Computing},
    address = {Vienna, Austria},
    year = {2019},
    url = {https://www.R-project.org/},
}

@ArtifactDataset{UMassCitations,
 author    =  {Sam Anzaroot and Andrew McCallum},
 title     =  {{UMass} Citation Field Extraction Dataset},
 year      = 2013,
 url       =
    {http://www.iesl.cs.umass.edu/data/data-umasscitationfield},
 lastaccessed = {May 27, 2019}
}

@Eprint{Bornmann2019,
       author = {Bornmann, Lutz and Wray, K. Brad and Haunschild,
                  Robin},
        title = {Citation concept analysis {(CCA)}---A new form of
                  citation analysis revealing the usefulness of
                  concepts for other researchers illustrated by two
                  exemplary case studies including classic books by
                  {Thomas S.~Kuhn} and {Karl R.~Popper}},
     keywords = {Computer Science - Digital Libraries},
         year = 2019,
        month = "May",
          eid = {arXiv:1905.12410},
archivePrefix = {arXiv},
       eprint = {1905.12410},
 primaryClass = {cs.DL},
}

@Eprint{AnzarootPBM14,
  author    = {Sam Anzaroot and
               Alexandre Passos and
               David Belanger and
               Andrew McCallum},
  title     = {Learning Soft Linear Constraints with Application to
                  Citation Field Extraction},
  year      = {2014},
  archivePrefix = {arXiv},
  eprint    = {1403.1349},
}

@inproceedings{Hagerup1993,
title        = {Maintaining Discrete Probability Distributions Optimally},
author       = {Hagerup, Torben and Mehlhorn, Kurt and Munro, J. Ian},
booktitle    = {Proceedings of the 20th International Colloquium on Automata, Languages and Programming},
series       = {Lecture Notes in Computer Science},
volume       = {700},
pages        = {253--264},
year         = {1993},
publisher    = {Springer-Verlag},
address      = {Berlin},
}

@article{luccioni2023counting,
  title={Counting Carbon: A Survey of Factors Influencing the Emissions of Machine Learning},
  author={Alexandra Sasha Luccioni and Alex Hern{\'a}ndez-Garc{\'i}a},
  journal={ArXiv},
  year={2023},
  volume={abs/2302.08476}
}

@article{nyt2020,
  title={Meet GPT-3. It Has Learned to Code (and Blog and Argue).},
  author={NYT},
  year={2020},
  url={https://www.nytimes.com/2020/11/24/science/artificial-intelligence-ai-gpt3.html}
}


@InProceedings{pmlr-v202-kirchenbauer23a,
  title = 	 {A Watermark for Large Language Models},
  author =       {Kirchenbauer, John and Geiping, Jonas and Wen, Yuxin and Katz, Jonathan and Miers, Ian and Goldstein, Tom},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {17061--17084},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/kirchenbauer23a/kirchenbauer23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/kirchenbauer23a.html},
  abstract = 	 {Potential harms of large language models can be mitigated by watermarking model output, i.e., embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. We propose a watermarking framework for proprietary language models. The watermark can be embedded with negligible impact on text quality, and can be detected using an efficient open-source algorithm without access to the language model API or parameters. The watermark works by selecting a randomized set of "green" tokens before a word is generated, and then softly promoting use of green tokens during sampling. We propose a statistical test for detecting the watermark with interpretable p-values, and derive an information-theoretic framework for analyzing the sensitivity of the watermark. We test the watermark using a multi-billion parameter model from the Open Pretrained Transformer (OPT) family, and discuss robustness and security.}
}


@article{nature2021,
  title={The big question},
  author={Nature},
  year={2021},
  url={https://www.nature.com/articles/s42256-021-00395-y}
}

@article{economist2022,
  title={Huge “foundation models” are turbo-charging AI progress},
  author={Economist},
  year={2022},
  url={https://www.economist.com/interactive/briefing/2022/06/11/huge-foundation-models-are-turbo-charging-ai-progress}
}

@article{cnn2023,
  title={Why you’re about to see ChatGPT in more of your apps},
  author={CNN},
  year={2023},
  url={https://www.cnn.com/2023/03/01/tech/chatgpt-api/index.html}
}

@article{kaufmann2022scaleindex,
title={Introducing: the Scale Generative AI Index},
author={Jeremy Kaufmann and Max Abram and Maggie Basta},
year={2022},
url={https://www.scalevp.com/blog/introducing-the-scale-generative-ai-index}
}

@article{hu2023chatgpt,
  title={ChatGPT sets record for fastest-growing user base - analyst note},
  author={Krystal Hu},
  year={2023},
  url={https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/}
}

@article{appenzeller2022stablediffusion,
  title={Art Isn’t Dead, It’s Just Machine-Generated},
  author={Guido Appenzeller and Matt Bornstein and Martin Casado and Yoko Li},
  year={2022},
  url={https://a16z.com/2022/11/16/creativity-as-an-app/}
}

@article{openai2021gpt3apps,
  title={GPT-3 powers the next generation of apps},
  author={OpenAI},
  year={2021},
  url={https://openai.com/blog/gpt-3-apps}
}

@inproceedings{
alayrac2022flamingo,
title={Flamingo: a Visual Language Model for Few-Shot Learning},
author={Jean-Baptiste Alayrac and Jeff Donahue and Pauline Luc and Antoine Miech and Iain Barr and Yana Hasson and Karel Lenc and Arthur Mensch and Katherine Millican and Malcolm Reynolds and Roman Ring and Eliza Rutherford and Serkan Cabi and Tengda Han and Zhitao Gong and Sina Samangooei and Marianne Monteiro and Jacob Menick and Sebastian Borgeaud and Andrew Brock and Aida Nematzadeh and Sahand Sharifzadeh and Mikolaj Binkowski and Ricardo Barreira and Oriol Vinyals and Andrew Zisserman and Karen Simonyan},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=EbMuimAbPbs}
}

@article{perrigo2022kenya,
  title={Exclusive: OpenAI Used Kenyan Workers on Less Than 2 Per Hour to Make ChatGPT Less Toxic},
  author={Billy Perrigo},
  journal={Time},
  year={2022},
  url={https://time.com/6247678/openai-chatgpt-kenya-workers}
}

@article{bommasani2023transparency,
  title={Improving Transparency in AI Language Models: A Holistic Evaluation},
  author={Rishi Bommasani and Daniel Zhang and Tony Lee and Percy Liang},
  journal={Foundation Model Issue Brief Series},
  year={2023},
  url={https://hai.stanford.edu/foundation-model-issue-brief-series}
}

@article{zhao2017devising,
  title={Devising effective policies for bug-bounty platforms and security vulnerability discovery},
  author={Zhao, Mingyi and Laszka, Aron and Grossklags, Jens},
  journal={Journal of Information Policy},
  volume={7},
  pages={372--418},
  year={2017},
  publisher={Duke University Press}
}

@article{chowdhury2021twitter,
  title={Introducing Twitter’s first algorithmic bias bounty challenge},
  author={Rumman Chowdhury and Jutta Williams},
  year={2021},
  url={https://blog.twitter.com/engineering/en_us/topics/insights/2021/algorithmic-bias-bounty-challenge}
}

@InProceedings{ethayarajh2022dataset,
  title = 	 {Understanding Dataset Difficulty with $\mathcal{V}$-Usable Information},
  author =       {Ethayarajh, Kawin and Choi, Yejin and Swayamdipta, Swabha},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {5988--6008},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/ethayarajh22a/ethayarajh22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/ethayarajh22a.html},
  abstract = 	 {Estimating the difficulty of a dataset typically involves comparing state-of-the-art models to humans; the bigger the performance gap, the harder the dataset is said to be. However, this comparison provides little understanding of how difficult each instance in a given distribution is, or what attributes make the dataset difficult for a given model. To address these questions, we frame dataset difficulty—w.r.t. a model $\mathcal{V}$—as the lack of $\mathcal{V}$-usable information (Xu et al., 2019), where a lower value indicates a more difficult dataset for $\mathcal{V}$. We further introduce pointwise $\mathcal{V}$-information (PVI) for measuring the difficulty of individual instances w.r.t. a given distribution. While standard evaluation metrics typically only compare different models for the same dataset, $\mathcal{V}$-usable information and PVI also permit the converse: for a given model $\mathcal{V}$, we can compare different datasets, as well as different instances/slices of the same dataset. Furthermore, our framework allows for the interpretability of different input attributes via transformations of the input, which we use to discover annotation artefacts in widely-used NLP benchmarks.}
}


@article{mitchell2022measuring,
  title={Measuring Data},
  author={Margaret Mitchell and Alexandra Sasha Luccioni and Nathan Lambert and Marissa Gerchick and Angelina McMillan-Major and Ezinwanne Ozoani and Nazneen Rajani and Tristan Thrush and Yacine Jernite and Douwe Kiela},
  journal={ArXiv},
  year={2022},
  volume={abs/2212.05129}
}

@inproceedings{crisan2022interactive,
author = {Crisan, Anamaria and Drouhard, Margaret and Vig, Jesse and Rajani, Nazneen},
title = {Interactive Model Cards: A Human-Centered Approach to Model Documentation},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533108},
doi = {10.1145/3531146.3533108},
abstract = {Deep learning models for natural language processing (NLP) are increasingly adopted and deployed by analysts without formal training in NLP or machine learning (ML). However, the documentation intended to convey the model’s details and appropriate use is tailored primarily to individuals with ML or NLP expertise. To address this gap, we conduct a design inquiry into interactive model cards, which augment traditionally static model cards with affordances for exploring model documentation and interacting with the models themselves. Our investigation consists of an initial conceptual study with experts in ML, NLP, and AI Ethics, followed by a separate evaluative study with non-expert analysts who use ML models in their work. Using a semi-structured interview format coupled with a think-aloud protocol, we collected feedback from a total of 30 participants who engaged with different versions of standard and interactive model cards. Through a thematic analysis of the collected data, we identified several conceptual dimensions that summarize the strengths and limitations of standard and interactive model cards, including: stakeholders; design; guidance; understandability &amp; interpretability; sensemaking &amp; skepticism; and trust &amp; safety. Our findings demonstrate the importance of carefully considered design and interactivity for orienting and supporting non-expert analysts using deep learning models, along with a need for consideration of broader sociotechnical contexts and organizational dynamics. We have also identified design elements, such as language, visual cues, and warnings, among others, that support interactivity and make non-interactive content accessible. We summarize our findings as design guidelines and discuss their implications for a human-centered approach towards AI/ML documentation.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {427–439},
numpages = {13},
keywords = {model cards, interactive data visualization, human centered design},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}

@article{wang2022against,
  title={Against Predictive Optimization: On the Legitimacy of Decision-Making Algorithms that Optimize Predictive Accuracy},
  author={Wang, Angelina and Kapoor, Sayash and Barocas, Solon and Narayanan, Arvind},
  journal={Available at SSRN},
  year={2022}
}

@article{lazar2023algorithmiccity,
  title={Governing the Algorithmic City},
  author={Seth Lazar},
  journal={Tanner Lectures},
  year={2023},
  url={https://write.as/sethlazar/}
}

@inproceedings{
schuhmann2022laion,
title={{LAION}-5B: An open large-scale dataset for training next generation image-text models},
author={Christoph Schuhmann and Romain Beaumont and Richard Vencu and Cade W Gordon and Ross Wightman and Mehdi Cherti and Theo Coombes and Aarush Katta and Clayton Mullis and Mitchell Wortsman and Patrick Schramowski and Srivatsa R Kundurthy and Katherine Crowson and Ludwig Schmidt and Robert Kaczmarczyk and Jenia Jitsev},
booktitle={Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
year={2022},
url={https://openreview.net/forum?id=M3Y74vmsMcY}
}

@inproceedings{joshi1969sag,
    title = {{Properties of Formal Grammars with Mixed Type of Rules and their Linguistic Relevance}},
    author = "Joshi, Aravind K.",
    booktitle = "{I}nternational {C}onference on {C}omputational {L}inguistics {COLING} 1969: Preprint No. 47",
    month = sep,
    year = "1969",
    address = {S{\aa}nga S{\"a}by, Sweden},
    url = "https://aclanthology.org/C69-4701",
}

@article{joshi1972sag,
title = {String adjunct grammars: I. Local and distributed adjunction},
journal = {Information and Control},
volume = {21},
number = {2},
pages = {93-116},
year = {1972},
issn = {0019-9958},
doi = {https://doi.org/10.1016/S0019-9958(72)90051-4},
url = {https://www.sciencedirect.com/science/article/pii/S0019995872900514},
author = {Aravind K. Joshi and S. Rao Kosaraju and H.M. Yamada},
abstract = {In this paper [and in Joshi et al. (1972) which is a sequel to this paper] a new style of formal grammar called String Adjunct Grammars (AG) has been studied. The rules in an AG have a character essentially different from the “rewrite rule” in a Phrase Structure Grammar (PSG). Such a study of formal grammars of different styles is of great interest because each style is well suited for characterizing certain aspects of natural language structure but has inherent difficulty in characterizing certain other aspects. Several subclasses of AG's motivated by strong linguistic considerations have been studied. Linguistic relevance of these grammars and other grammars suggested by this study has also been discussed.}
}

@inproceedings{grosz1983providing,
    title = {{Providing a Unified Account of Definite Noun Phrases in Discourse}},
    author = "Grosz, Barbara J.  and
      Joshi, Aravind K.  and
      Weinstein, Scott",
    booktitle = "21st Annual Meeting of the Association for Computational Linguistics",
    month = jun,
    year = "1983",
    address = "Cambridge, Massachusetts, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P83-1007",
    doi = "10.3115/981311.981320",
    pages = "44--50",
}

@inproceedings{joshi1986research,
    title = "Research in Natural Language Processing",
    author = "Joshi, Aravind  and
      Finin, Tim  and
      Miller, Dale  and
      Shastri, Lokendra  and
      Webber, Bonnie",
    booktitle = "Strategic Computing - Natural Language Workshop: Proceedings of a Workshop Held at Marina del Rey, California, May 1-2, 1986",
    year = "1986",
    url = "https://aclanthology.org/H86-1005",
}

@inproceedings{joshi1989evaluation,
    title = "An Evaluation of Lexicalization in Parsing",
    author = "Joshi, Aravind K.  and
      Schabes, Yves",
    booktitle = "Speech and Natural Language: Proceedings of a Workshop Held at Cape Cod, Massachusetts, October 15-18, 1989",
    year = "1989",
    url = "https://aclanthology.org/H89-2053",
}

@book{jones1995evaluating,
  title={Evaluating Natural Language Processing Systems: An Analysis and Review},
  author={Sp\"arck Jones, Karen and Galliers, Julia R.},
  year={1995},
  address={Berlin},
  publisher={Springer Verlag},
  series={Lecture Notes in Computer Science},
  number={1083},
}

@inproceedings{miltsakaki2004pdtb,
    title = "The {P}enn {D}iscourse {T}reebank",
    author = "Miltsakaki, Eleni  and
      Prasad, Rashmi  and
      Joshi, Aravind  and
      Webber, Bonnie",
    booktitle = "Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04)",
    month = may,
    year = "2004",
    address = "Lisbon, Portugal",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2004/pdf/618.pdf",
}

@inproceedings{prasad2008pdtb2,
    title = "The {P}enn {D}iscourse {T}ree{B}ank 2.0.",
    author = "Prasad, Rashmi  and
      Dinesh, Nikhil  and
      Lee, Alan  and
      Miltsakaki, Eleni  and
      Robaldo, Livio  and
      Joshi, Aravind  and
      Webber, Bonnie",
    booktitle = "Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08)",
    month = may,
    year = "2008",
    address = "Marrakech, Morocco",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2008/pdf/754_paper.pdf",
    abstract = "We present the second version of the Penn Discourse Treebank, PDTB-2.0, describing its lexically-grounded annotations of discourse relations and their two abstract object arguments over the 1 million word Wall Street Journal corpus. We describe all aspects of the annotation, including (a) the argument structure of discourse relations, (b) the sense annotation of the relations, and (c) the attribution of discourse relations and each of their arguments. We list the differences between PDTB-1.0 and PDTB-2.0. We present representative statistics for several aspects of the annotation in the corpus.",
}

@inproceedings{krizhevsky2012alexnet,
 author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {ImageNet Classification with Deep Convolutional Neural Networks},
 url = {https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
 volume = {25},
 year = {2012}
}

@article{prasad2014reflections,
    title = "Reflections on the {P}enn {D}iscourse {T}ree{B}ank, Comparable Corpora, and Complementary Annotation",
    author = "Prasad, Rashmi  and
      Webber, Bonnie  and
      Joshi, Aravind",
    journal = "Computational Linguistics",
    volume = "40",
    number = "4",
    month = dec,
    year = "2014",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/J14-4007",
    doi = "10.1162/COLI_a_00204",
    pages = "921--950",
}

@misc{rogaway2015moral,
      author = {Phillip Rogaway},
      title = {The Moral Character of Cryptographic Work},
      howpublished = {Cryptology ePrint Archive, Paper 2015/1162},
      year = {2015},
      note = {\url{https://eprint.iacr.org/2015/1162}},
      url = {https://eprint.iacr.org/2015/1162}
}

@article{hallinan2016netflix,
author = {Blake Hallinan and Ted Striphas},
title ={Recommended for you: The Netflix Prize and the production of algorithmic culture},
journal = {New Media \& Society},
volume = {18},
number = {1},
pages = {117-137},
year = {2016},
doi = {10.1177/1461444814538646},

URL = { 
        https://doi.org/10.1177/1461444814538646
    
},
eprint = { 
        https://doi.org/10.1177/1461444814538646
    
}
,
    abstract = { How does algorithmic information processing affect the meaning of the word culture, and, by extension, cultural practice? We address this question by focusing on the Netflix Prize (2006–2009), a contest offering US 1m to the first individual or team to boost the accuracy of the company’s existing movie recommendation system by 10\%. Although billed as a technical challenge intended for engineers, we argue that the Netflix Prize was equally an effort to reinterpret the meaning of culture in ways that overlapped with, but also diverged in important respects from, the three dominant senses of the term assayed by Raymond Williams. Thus, this essay explores the conceptual and semantic work required to render algorithmic information processing systems legible as forms of cultural decision making. It also then represents an effort to add depth and dimension to the concept of “algorithmic culture.” }
}

@inproceedings{rajpurkar2016squad,
    title = "{SQ}u{AD}: 100,000+ Questions for Machine Comprehension of Text",
    author = "Rajpurkar, Pranav  and
      Zhang, Jian  and
      Lopyrev, Konstantin  and
      Liang, Percy",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D16-1264",
    doi = "10.18653/v1/D16-1264",
    pages = "2383--2392",
}


@article{liberman2005joshi,
    title = {{Franklin Medal to Aravind Joshi}},
    author = "Mark Liberman",
    journal = "Language Log",
    year = "2005",
    url = "http://itre.cis.upenn.edu/~myl/languagelog/archives/001989.html",
}

@inproceedings{nivre2016universal,
    title = "{U}niversal {D}ependencies v1: A Multilingual Treebank Collection",
    author = "Nivre, Joakim  and
      de Marneffe, Marie-Catherine  and
      Ginter, Filip  and
      Goldberg, Yoav  and
      Haji{\v{c}}, Jan  and
      Manning, Christopher D.  and
      McDonald, Ryan  and
      Petrov, Slav  and
      Pyysalo, Sampo  and
      Silveira, Natalia  and
      Tsarfaty, Reut  and
      Zeman, Daniel",
    booktitle = "Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16)",
    month = may,
    year = "2016",
    address = "Portoro{\v{z}}, Slovenia",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://aclanthology.org/L16-1262",
    pages = "1659--1666",
    abstract = "Cross-linguistically consistent annotation is necessary for sound comparative evaluation and cross-lingual learning experiments. It is also useful for multilingual system development and comparative linguistic studies. Universal Dependencies is an open community effort to create cross-linguistically consistent treebank annotation for many languages within a dependency-based lexicalist framework. In this paper, we describe v1 of the universal guidelines, the underlying design principles, and the currently available treebanks for 33 languages.",
}


@inproceedings{parra2017shared,
    title = "Ethical Considerations in {NLP} Shared Tasks",
    author = "Parra Escart{\'\i}n, Carla  and
      Reijers, Wessel  and
      Lynn, Teresa  and
      Moorkens, Joss  and
      Way, Andy  and
      Liu, Chao-Hong",
    booktitle = "Proceedings of the First {ACL} Workshop on Ethics in Natural Language Processing",
    month = apr,
    year = "2017",
    address = "Valencia, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W17-1608",
    doi = "10.18653/v1/W17-1608",
    pages = "66--73",
    abstract = "Shared tasks are increasingly common in our field, and new challenges are suggested at almost every conference and workshop. However, as this has become an established way of pushing research forward, it is important to discuss how we researchers organise and participate in shared tasks, and make that information available to the community to allow further research improvements. In this paper, we present a number of ethical issues along with other areas of concern that are related to the competitive nature of shared tasks. As such issues could potentially impact on research ethics in the Natural Language Processing community, we also propose the development of a framework for the organisation of and participation in shared tasks that can help mitigate against these issues arising.",
}

@article{nissim2017sharing,
    title = "Sharing Is Caring: The Future of Shared Tasks",
    author = "Nissim, Malvina  and
      Abzianidze, Lasha  and
      Evang, Kilian  and
      van der Goot, Rob  and
      Haagsma, Hessel  and
      Plank, Barbara  and
      Wieling, Martijn",
    journal = "Computational Linguistics",
    volume = "43",
    number = "4",
    month = dec,
    year = "2017",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/J17-4007",
    doi = "10.1162/COLI_a_00304",
    pages = "897--904",
}

@inproceedings{vaswani2017transformers,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@article{webber2018obituary,
    title = "{O}bituary: {A}ravind {K}. {J}oshi",
    author = "Webber, Bonnie",
    journal = "Computational Linguistics",
    volume = "44",
    number = "3",
    month = sep,
    year = "2018",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/J18-3001",
    doi = "10.1162/coli_a_00321",
    pages = "387--392",
}

@inproceedings{gururangan2018annotation,
    title = "Annotation Artifacts in Natural Language Inference Data",
    author = "Gururangan, Suchin  and
      Swayamdipta, Swabha  and
      Levy, Omer  and
      Schwartz, Roy  and
      Bowman, Samuel  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-2017",
    doi = "10.18653/v1/N18-2017",
    pages = "107--112",
    abstract = "Large-scale datasets for natural language inference are created by presenting crowd workers with a sentence (premise), and asking them to generate three new sentences (hypotheses) that it entails, contradicts, or is logically neutral with respect to. We show that, in a significant portion of such data, this protocol leaves clues that make it possible to identify the label by looking only at the hypothesis, without observing the premise. Specifically, we show that a simple text categorization model can correctly classify the hypothesis alone in about 67{\%} of SNLI (Bowman et. al, 2015) and 53{\%} of MultiNLI (Williams et. al, 2017). Our analysis reveals that specific linguistic phenomena such as negation and vagueness are highly correlated with certain inference classes. Our findings suggest that the success of natural language inference models to date has been overestimated, and that the task remains a hard open problem.",
}


@inproceedings{kaushik2018rc,
    title = "How Much Reading Does Reading Comprehension Require? A Critical Investigation of Popular Benchmarks",
    author = "Kaushik, Divyansh  and
      Lipton, Zachary C.",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1546",
    doi = "10.18653/v1/D18-1546",
    pages = "5010--5015",
    abstract = "Many recent papers address reading comprehension, where examples consist of (question, passage, answer) tuples. Presumably, a model must combine information from both questions and passages to predict corresponding answers. However, despite intense interest in the topic, with hundreds of published papers vying for leaderboard dominance, basic questions about the difficulty of many popular benchmarks remain unanswered. In this paper, we establish sensible baselines for the bAbI, SQuAD, CBT, CNN, and Who-did-What datasets, finding that question- and passage-only models often perform surprisingly well. On 14 out of 20 bAbI tasks, passage-only models achieve greater than 50{\%} accuracy, sometimes matching the full model. Interestingly, while CBT provides 20-sentence passages, only the last is needed for accurate prediction. By comparison, SQuAD and CNN appear better-constructed.",
}

@inproceedings{
wang2019glue,
title={{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
author={Alex Wang and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=rJ4km2R5t7},
}


@inproceedings{raji2019actionable,
author = {Raji, Inioluwa Deborah and Buolamwini, Joy},
title = {Actionable Auditing: Investigating the Impact of Publicly Naming Biased Performance Results of Commercial AI Products},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314244},
doi = {10.1145/3306618.3314244},
abstract = {Although algorithmic auditing has emerged as a key strategy to expose systematic biases embedded in software platforms, we struggle to understand the real-world impact of these audits, as scholarship on the impact of algorithmic audits on increasing algorithmic fairness and transparency in commercial systems is nascent. To analyze the impact of publicly naming and disclosing performance results of biased AI systems, we investigate the commercial impact of Gender Shades, the first algorithmic audit of gender and skin type performance disparities in commercial facial analysis models. This paper 1) outlines the audit design and structured disclosure procedure used in the Gender Shades study, 2) presents new performance metrics from targeted companies IBM, Microsoft and Megvii (Face++) on the Pilot Parliaments Benchmark (PPB) as of August 2018, 3) provides performance results on PPB by non-target companies Amazon and Kairos and, 4) explores differences in company responses as shared through corporate communications that contextualize differences in performance on PPB. Within 7 months of the original audit, we find that all three targets released new API versions. All targets reduced accuracy disparities between males and females and darker and lighter-skinned subgroups, with the most significant update occurring for the darker-skinned female subgroup, that underwent a 17.7\% - 30.4\% reduction in error between audit periods. Minimizing these disparities led to a 5.72\% to 8.3\% reduction in overall error on the Pilot Parliaments Benchmark (PPB) for target corporation APIs. The overall performance of non-targets Amazon and Kairos lags significantly behind that of the targets, with error rates of 8.66\% and 6.60\% overall, and error rates of 31.37\% and 22.50\% for the darker female subgroup, respectively.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {429–435},
numpages = {7},
keywords = {ethics, machine learning, commercial applications, computer vision, fairness, facial recognition, artificial intelligence},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{ginart2019right,
 author = {Ginart, Antonio and Guan, Melody and Valiant, Gregory and Zou, James Y},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Making AI Forget You: Data Deletion in Machine Learning},
 url = {https://proceedings.neurips.cc/paper/2019/file/cb79f8fa58b91d3af6c9c991f63962d3-Paper.pdf},
 volume = {32},
 year = {2019}
}

@article{welty2019metrology,
  title={Metrology for AI: From Benchmarks to Instruments},
  author={Chris Welty and Praveen K. Paritosh and Lora Aroyo},
  journal={ArXiv},
  year={2019},
  volume={abs/1911.01875},
  url = {https://arxiv.org/abs/1911.01875}
}

@article{kaplan2020scaling,
  title={Scaling Laws for Neural Language Models},
  author={Jared Kaplan and Sam McCandlish and T. J. Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeff Wu and Dario Amodei},
  journal={ArXiv},
  year={2020},
  volume={abs/2001.08361}
}

@article{dotan2020value,
  title={Value-laden disciplinary shifts in machine learning},
  author={Ravit Dotan and Smitha Milli},
  journal={Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
  year={2020}
}

@inproceedings{raji2020saving,
author = {Raji, Inioluwa Deborah and Gebru, Timnit and Mitchell, Margaret and Buolamwini, Joy and Lee, Joonseok and Denton, Emily},
title = {Saving Face: Investigating the Ethical Concerns of Facial Recognition Auditing},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375820},
doi = {10.1145/3375627.3375820},
abstract = {Although essential to revealing biased performance, well intentioned attempts at algorithmic auditing can have effects that may harm the very populations these measures are meant to protect. This concern is even more salient while auditing biometric systems such as facial recognition, where the data is sensitive and the technology is often used in ethically questionable manners. We demonstrate a set of fiveethical concerns in the particular case of auditing commercial facial processing technology, highlighting additional design considerations and ethical tensions the auditor needs to be aware of so as not exacerbate or complement the harms propagated by the audited system. We go further to provide tangible illustrations of these concerns, and conclude by reflecting on what these concerns mean for the role of the algorithmic audit and the fundamental product limitations they reveal.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {145–151},
numpages = {7},
keywords = {benchmarking, ethics, evaluation, facial recognition, algorithms, auditing},
location = {New York, NY, USA},
series = {AIES '20}
}


@inproceedings{linzen2020accelerate,
    title = "How Can We Accelerate Progress Towards Human-like Linguistic Generalization?",
    author = "Linzen, Tal",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.465",
    doi = "10.18653/v1/2020.acl-main.465",
    pages = "5210--5217",
    abstract = "This position paper describes and critiques the Pretraining-Agnostic Identically Distributed (PAID) evaluation paradigm, which has become a central tool for measuring progress in natural language understanding. This paradigm consists of three stages: (1) pre-training of a word prediction model on a corpus of arbitrary size; (2) fine-tuning (transfer learning) on a training set representing a classification task; (3) evaluation on a test set drawn from the same distribution as that training set. This paradigm favors simple, low-bias architectures, which, first, can be scaled to process vast amounts of data, and second, can capture the fine-grained statistical properties of a particular data set, regardless of whether those properties are likely to generalize to examples of the task outside the data set. This contrasts with humans, who learn language from several orders of magnitude less data than the systems favored by this evaluation paradigm, and generalize to new tasks in a consistent way. We advocate for supplementing or replacing PAID with paradigms that reward architectures that generalize as quickly and robustly as humans.",
}

@article{denton2020bringing,
  title={Bringing the People Back In: Contesting Benchmark Machine Learning Datasets},
  author={Emily L. Denton and A. Hanna and Razvan Amironesei and Andrew Smart and Hilary Nicole and Morgan Klaus Scheuerman},
  journal={ArXiv},
  year={2020},
  volume={abs/2007.07399}
}

@inproceedings{nie2020anli,
    title = "Adversarial {NLI}: A New Benchmark for Natural Language Understanding",
    author = "Nie, Yixin  and
      Williams, Adina  and
      Dinan, Emily  and
      Bansal, Mohit  and
      Weston, Jason  and
      Kiela, Douwe",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.441",
    doi = "10.18653/v1/2020.acl-main.441",
    pages = "4885--4901",
    abstract = "We introduce a new large-scale NLI benchmark dataset, collected via an iterative, adversarial human-and-model-in-the-loop procedure. We show that training models on this new dataset leads to state-of-the-art performance on a variety of popular NLI benchmarks, while posing a more difficult challenge with its new test set. Our analysis sheds light on the shortcomings of current state-of-the-art models, and shows that non-expert annotators are successful at finding their weaknesses. The data collection method can be applied in a never-ending learning scenario, becoming a moving target for NLU, rather than a static benchmark that will quickly saturate.",
}

@inproceedings{wolf2020transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Wolf, Thomas  and
      Debut, Lysandre  and
      Sanh, Victor  and
      Chaumond, Julien  and
      Delangue, Clement  and
      Moi, Anthony  and
      Cistac, Pierric  and
      Rault, Tim  and
      Louf, Remi  and
      Funtowicz, Morgan  and
      Davison, Joe  and
      Shleifer, Sam  and
      von Platen, Patrick  and
      Ma, Clara  and
      Jernite, Yacine  and
      Plu, Julien  and
      Xu, Canwen  and
      Le Scao, Teven  and
      Gugger, Sylvain  and
      Drame, Mariama  and
      Lhoest, Quentin  and
      Rush, Alexander",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-demos.6",
    doi = "10.18653/v1/2020.emnlp-demos.6",
    pages = "38--45",
    abstract = "Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/huggingface/transformers.",
}

@misc{rogers2020resource,
  title = { Peer review in NLP: resource papers},
  journal = {Hacking Semantics},
  url = { https://hackingsemantics.xyz/2020/reviewing-data/ },
  author = {Rogers, Anna},
  day = { 16 },
  month = { Apr },
  year = { 2020 }
}

@inproceedings{ethayarajh2020classifier,
    title = "Is Your Classifier Actually Biased? Measuring Fairness under Uncertainty with Bernstein Bounds",
    author = "Ethayarajh, Kawin",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.262",
    doi = "10.18653/v1/2020.acl-main.262",
    pages = "2914--2919",
    abstract = "Most NLP datasets are not annotated with protected attributes such as gender, making it difficult to measure classification bias using standard measures of fairness (e.g., equal opportunity). However, manually annotating a large dataset with a protected attribute is slow and expensive. Instead of annotating all the examples, can we annotate a subset of them and use that sample to estimate the bias? While it is possible to do so, the smaller this annotated sample is, the less certain we are that the estimate is close to the true bias. In this work, we propose using Bernstein bounds to represent this uncertainty about the bias estimate as a confidence interval. We provide empirical evidence that a 95{\%} confidence interval derived this way consistently bounds the true bias. In quantifying this uncertainty, our method, which we call Bernstein-bounded unfairness, helps prevent classifiers from being deemed biased or unbiased when there is insufficient evidence to make either claim. Our findings suggest that the datasets currently used to measure specific biases are too small to conclusively identify bias except in the most egregious cases. For example, consider a co-reference resolution system that is 5{\%} more accurate on gender-stereotypical sentences {--} to claim it is biased with 95{\%} confidence, we need a bias-specific dataset that is 3.8 times larger than WinoBias, the largest available.",
}

@INPROCEEDINGS{birhane2021pyrrhic,  author={Birhane, Abeba and Prabhu, Vinay Uday},  booktitle={2021 IEEE Winter Conference on Applications of Computer Vision (WACV)},   title={Large image datasets: A pyrrhic win for computer vision?},   year={2021},  volume={},  number={},  pages={1536-1546},  doi={10.1109/WACV48630.2021.00158}}

@misc{liu2021small,
  author    = {Nelson F. Liu  and  Tony Lee  and  Robin Jia  and  Percy Liang},
  title     = {Can Small and Synthetic Benchmarks Drive Modeling Innovation? A Retrospective Study of Question Answering Modeling Approaches},
  note      = {arXiv:2102.01065},
  year      = {2021},
  url = {https://arxiv.org/abs/2102.01065}
}

@inproceedings{blodgett2021stereotyping,
    title = "Stereotyping {N}orwegian Salmon: An Inventory of Pitfalls in Fairness Benchmark Datasets",
    author = "Blodgett, Su Lin  and
      Lopez, Gilsinia  and
      Olteanu, Alexandra  and
      Sim, Robert  and
      Wallach, Hanna",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.81",
    doi = "10.18653/v1/2021.acl-long.81",
    pages = "1004--1015",
    abstract = "Auditing NLP systems for computational harms like surfacing stereotypes is an elusive goal. Several recent efforts have focused on benchmark datasets consisting of pairs of contrastive sentences, which are often accompanied by metrics that aggregate an NLP system{'}s behavior on these pairs into measurements of harms. We examine four such benchmarks constructed for two NLP tasks: language modeling and coreference resolution. We apply a measurement modeling lens{---}originating from the social sciences{---}to inventory a range of pitfalls that threaten these benchmarks{'} validity as measurement models for stereotyping. We find that these benchmarks frequently lack clear articulations of what is being measured, and we highlight a range of ambiguities and unstated assumptions that affect how these benchmarks conceptualize and operationalize stereotyping.",
}

@inproceedings{aribandi2021reliable,
    title = "How Reliable are Model Diagnostics?",
    author = "Aribandi, Vamsi  and
      Tay, Yi  and
      Metzler, Donald",
    booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-acl.155",
    doi = "10.18653/v1/2021.findings-acl.155",
    pages = "1778--1785",
}

@inproceedings{jacobs2021measurement,
author = {Jacobs, Abigail Z. and Wallach, Hanna},
title = {Measurement and Fairness},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445901},
doi = {10.1145/3442188.3445901},
abstract = {We propose measurement modeling from the quantitative social sciences as a framework for understanding fairness in computational systems. Computational systems often involve unobservable theoretical constructs, such as socioeconomic status, teacher effectiveness, and risk of recidivism. Such constructs cannot be measured directly and must instead be inferred from measurements of observable properties (and other unobservable theoretical constructs) thought to be related to them---i.e., operationalized via a measurement model. This process, which necessarily involves making assumptions, introduces the potential for mismatches between the theoretical understanding of the construct purported to be measured and its operationalization. We argue that many of the harms discussed in the literature on fairness in computational systems are direct results of such mismatches. We show how some of these harms could have been anticipated and, in some cases, mitigated if viewed through the lens of measurement modeling. To do this, we contribute fairness-oriented conceptualizations of construct reliability and construct validity that unite traditions from political science, education, and psychology and provide a set of tools for making explicit and testing assumptions about constructs and their operationalizations. We then turn to fairness itself, an essentially contested construct that has different theoretical understandings in different contexts. We argue that this contestedness underlies recent debates about fairness definitions: although these debates appear to be about different operationalizations, they are, in fact, debates about different theoretical understandings of fairness. We show how measurement modeling can provide a framework for getting to the core of these debates.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {375–385},
numpages = {11},
keywords = {fairness, construct reliability, construct validity, measurement},
location = {Virtual Event, Canada},
series = {FAccT '21}
}

@article{dehghani2021lottery,
  title={The Benchmark Lottery},
  author={Mostafa Dehghani and Yi Tay and Alexey A. Gritsenko and Zhe Zhao and Neil Houlsby and Fernando Diaz and Donald Metzler and Oriol Vinyals},
  journal={ArXiv},
  year={2021},
  volume={abs/2107.07002}
}

@article{denton2021genealogy,
author = {Emily Denton and Alex Hanna and Razvan Amironesei and Andrew Smart and Hilary Nicole},
title ={On the genealogy of machine learning datasets: A critical history of ImageNet},
journal = {Big Data \& Society},
volume = {8},
number = {2},
pages = {20539517211035955},
year = {2021},
doi = {10.1177/20539517211035955},

URL = { 
        https://doi.org/10.1177/20539517211035955
    
},
eprint = { 
        https://doi.org/10.1177/20539517211035955
    
}
,
    abstract = { In response to growing concerns of bias, discrimination, and unfairness perpetuated by algorithmic systems, the datasets used to train and evaluate machine learning models have come under increased scrutiny. Many of these examinations have focused on the contents of machine learning datasets, finding glaring underrepresentation of minoritized groups. In contrast, relatively little work has been done to examine the norms, values, and assumptions embedded in these datasets. In this work, we conceptualize machine learning datasets as a type of informational infrastructure, and motivate a genealogy as method in examining the histories and modes of constitution at play in their creation. We present a critical history of ImageNet as an exemplar, utilizing critical discourse analysis of major texts around ImageNet’s creation and impact. We find that assumptions around ImageNet and other large computer vision datasets more generally rely on three themes: the aggregation and accumulation of more data, the computational construction of meaning, and making certain types of data labor invisible. By tracing the discourses that surround this influential benchmark, we contribute to the ongoing development of the standards and norms around data development in machine learning and artificial intelligence research. }
}

@inproceedings{bowman2021fix,
    title = "What Will it Take to Fix Benchmarking in Natural Language Understanding?",
    author = "Bowman, Samuel R.  and
      Dahl, George",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2021.naacl-main.385",
    doi = "10.18653/v1/2021.naacl-main.385",
    pages = "4843--4855",
    abstract = "Evaluation for many natural language understanding (NLU) tasks is broken: Unreliable and biased systems score so highly on standard benchmarks that there is little room for researchers who develop better systems to demonstrate their improvements. The recent trend to abandon IID benchmarks in favor of adversarially-constructed, out-of-distribution test sets ensures that current models will perform poorly, but ultimately only obscures the abilities that we want our benchmarks to measure. In this position paper, we lay out four criteria that we argue NLU benchmarks should meet. We argue most current benchmarks fail at these criteria, and that adversarial data collection does not meaningfully address the causes of these failures. Instead, restoring a healthy evaluation ecosystem will require significant progress in the design of benchmark datasets, the reliability with which they are annotated, their size, and the ways they handle social bias.",
}

@inproceedings{
raji2021ai,
title={{AI} and the Everything in the Whole Wide World Benchmark},
author={Inioluwa Deborah Raji and Emily Denton and Emily M. Bender and Alex Hanna and Amandalynne Paullada},
booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)},
year={2021},
url={https://openreview.net/forum?id=j6NxpQbREA1}
}

@article{scheuerman2021politics,
author = {Scheuerman, Morgan Klaus and Hanna, Alex and Denton, Emily},
title = {Do Datasets Have Politics? Disciplinary Values in Computer Vision Dataset Development},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW2},
url = {https://doi.org/10.1145/3476058},
doi = {10.1145/3476058},
abstract = {Data is a crucial component of machine learning. The field is reliant on data to train, validate, and test models. With increased technical capabilities, machine learning research has boomed in both academic and industry settings, and one major focus has been on computer vision. Computer vision is a popular domain of machine learning increasingly pertinent to real-world applications, from facial recognition in policing to object detection for autonomous vehicles. Given computer vision's propensity to shape machine learning research and impact human life, we seek to understand disciplinary practices around dataset documentation - how data is collected, curated, annotated, and packaged into datasets for computer vision researchers and practitioners to use for model tuning and development. Specifically, we examine what dataset documentation communicates about the underlying values of vision data and the larger practices and goals of computer vision as a field. To conduct this study, we collected a corpus of about 500 computer vision datasets, from which we sampled 114 dataset publications across different vision tasks. Through both a structured and thematic content analysis, we document a number of values around accepted data practices, what makes desirable data, and the treatment of humans in the dataset construction process. We discuss how computer vision datasets authors value efficiency at the expense of care; universality at the expense of contextuality; impartiality at the expense of positionality; and model work at the expense of data work. Many of the silenced values we identify sit in opposition with social computing practices. We conclude with suggestions on how to better incorporate silenced values into the dataset creation and curation process.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {oct},
articleno = {317},
numpages = {37},
keywords = {values in design, machine learning, work practice, datasets, computer vision}
}


@inproceedings{
liao2021are,
title={Are We Learning Yet? A Meta Review of Evaluation Failures Across Machine Learning},
author={Thomas Liao and Rohan Taori and Inioluwa Deborah Raji and Ludwig Schmidt},
booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)},
year={2021},
url={https://openreview.net/forum?id=mPducS1MsEK}
}

@inproceedings{kiela2021dynabench,
    title = "Dynabench: Rethinking Benchmarking in {NLP}",
    author = "Kiela, Douwe  and
      Bartolo, Max  and
      Nie, Yixin  and
      Kaushik, Divyansh  and
      Geiger, Atticus  and
      Wu, Zhengxuan  and
      Vidgen, Bertie  and
      Prasad, Grusha  and
      Singh, Amanpreet  and
      Ringshia, Pratik  and
      Ma, Zhiyi  and
      Thrush, Tristan  and
      Riedel, Sebastian  and
      Waseem, Zeerak  and
      Stenetorp, Pontus  and
      Jia, Robin  and
      Bansal, Mohit  and
      Potts, Christopher  and
      Williams, Adina",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.324",
    doi = "10.18653/v1/2021.naacl-main.324",
    pages = "4110--4124",
    abstract = "We introduce Dynabench, an open-source platform for dynamic dataset creation and model benchmarking. Dynabench runs in a web browser and supports human-and-model-in-the-loop dataset creation: annotators seek to create examples that a target model will misclassify, but that another person will not. In this paper, we argue that Dynabench addresses a critical need in our community: contemporary models quickly achieve outstanding performance on benchmark tasks but nonetheless fail on simple challenge examples and falter in real-world scenarios. With Dynabench, dataset creation, model development, and model assessment can directly inform each other, leading to more robust and informative benchmarks. We report on four initial NLP tasks, illustrating these concepts and highlighting the promise of the platform, and address potential objections to dynamic benchmarking as a new standard for the field.",
}

@inproceedings{
peng2021mitigating,
title={Mitigating dataset harms requires stewardship: Lessons from 1000 papers},
author={Kenneth L Peng and Arunesh Mathur and Arvind Narayanan},
booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)},
year={2021},
url={https://openreview.net/forum?id=KGeAHDH4njY}
}

@inproceedings{
koch2021reduced,
title={Reduced, Reused and Recycled: The Life of a  Dataset in Machine Learning Research},
author={Bernard Koch and Emily Denton and Alex Hanna and Jacob Gates Foster},
booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)},
year={2021},
url={https://openreview.net/forum?id=zNQBIBKJRkd}
}

@inproceedings{
bragg2021flex,
title={{FLEX}: Unifying Evaluation for Few-Shot {NLP}},
author={Jonathan Bragg and Arman Cohan and Kyle Lo and Iz Beltagy},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=_WnGcwXLYOE}
}


@inproceedings{gehrmann2021gem,
    title = "The {GEM} Benchmark: Natural Language Generation, its Evaluation and Metrics",
    author = "Gehrmann, Sebastian  and
      Adewumi, Tosin  and
      Aggarwal, Karmanya  and
      Ammanamanchi, Pawan Sasanka  and
      Aremu, Anuoluwapo  and
      Bosselut, Antoine  and
      Chandu, Khyathi Raghavi  and
      Clinciu, Miruna-Adriana  and
      Das, Dipanjan  and
      Dhole, Kaustubh  and
      Du, Wanyu  and
      Durmus, Esin  and
      Du{\v{s}}ek, Ond{\v{r}}ej  and
      Emezue, Chris Chinenye  and
      Gangal, Varun  and
      Garbacea, Cristina  and
      Hashimoto, Tatsunori  and
      Hou, Yufang  and
      Jernite, Yacine  and
      Jhamtani, Harsh  and
      Ji, Yangfeng  and
      Jolly, Shailza  and
      Kale, Mihir  and
      Kumar, Dhruv  and
      Ladhak, Faisal  and
      Madaan, Aman  and
      Maddela, Mounica  and
      Mahajan, Khyati  and
      Mahamood, Saad  and
      Majumder, Bodhisattwa Prasad  and
      Martins, Pedro Henrique  and
      McMillan-Major, Angelina  and
      Mille, Simon  and
      van Miltenburg, Emiel  and
      Nadeem, Moin  and
      Narayan, Shashi  and
      Nikolaev, Vitaly  and
      Niyongabo Rubungo, Andre  and
      Osei, Salomey  and
      Parikh, Ankur  and
      Perez-Beltrachini, Laura  and
      Rao, Niranjan Ramesh  and
      Raunak, Vikas  and
      Rodriguez, Juan Diego  and
      Santhanam, Sashank  and
      Sedoc, Jo{\~a}o  and
      Sellam, Thibault  and
      Shaikh, Samira  and
      Shimorina, Anastasia  and
      Sobrevilla Cabezudo, Marco Antonio  and
      Strobelt, Hendrik  and
      Subramani, Nishant  and
      Xu, Wei  and
      Yang, Diyi  and
      Yerukola, Akhila  and
      Zhou, Jiawei",
    booktitle = "Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.gem-1.10",
    doi = "10.18653/v1/2021.gem-1.10",
    pages = "96--120",
    abstract = "We introduce GEM, a living benchmark for natural language Generation (NLG), its Evaluation, and Metrics. Measuring progress in NLG relies on a constantly evolving ecosystem of automated metrics, datasets, and human evaluation standards. Due to this moving target, new models often still evaluate on divergent anglo-centric corpora with well-established, but flawed, metrics. This disconnect makes it challenging to identify the limitations of current models and opportunities for progress. Addressing this limitation, GEM provides an environment in which models can easily be applied to a wide set of tasks and in which evaluation strategies can be tested. Regular updates to the benchmark will help NLG research become more multilingual and evolve the challenge alongside models. This paper serves as the description of the data for the 2021 shared task at the associated GEM Workshop.",
}

@article{demarneffe2021universal,
    title = "{U}niversal {D}ependencies",
    author = "de Marneffe, Marie-Catherine  and
      Manning, Christopher D.  and
      Nivre, Joakim  and
      Zeman, Daniel",
    journal = "Computational Linguistics",
    volume = "47",
    number = "2",
    month = jun,
    year = "2021",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2021.cl-2.11",
    doi = "10.1162/coli_a_00402",
    pages = "255--308",
    abstract = "Abstract Universal dependencies (UD) is a framework for morphosyntactic annotation of human language, which to date has been used to create treebanks for more than 100 languages. In this article, we outline the linguistic theory of the UD framework, which draws on a long tradition of typologically oriented grammatical theories. Grammatical relations between words are centrally used to explain how predicate{--}argument structures are encoded morphosyntactically in different languages while morphological features and part-of-speech classes give the properties of words. We argue that this theory is a good basis for crosslinguistically consistent annotation of typologically diverse languages in a way that supports computational natural language understanding as well as broader linguistic studies.",
}

@inproceedings{welm2021welm,
author = {{WELM}},
year = 2021, 
url = {https://welmworkshop.github.io/},
title = {{Workshop on Enormous Language Models (WELM)}},
location = {Workshop at the International Conference on Learning Representations '21}
}

@inproceedings{birhane2022values,
author = {Birhane, Abeba and Kalluri, Pratyusha and Card, Dallas and Agnew, William and Dotan, Ravit and Bao, Michelle},
title = {The Values Encoded in Machine Learning Research},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533083},
doi = {10.1145/3531146.3533083},
abstract = {Machine learning currently exerts an outsized influence on the world, increasingly affecting institutional practices and impacted communities. It is therefore critical that we question vague conceptions of the field as value-neutral or universally beneficial, and investigate what specific values the field is advancing. In this paper, we first introduce a method and annotation scheme for studying the values encoded in documents such as research papers. Applying the scheme, we analyze 100 highly cited machine learning papers published at premier machine learning conferences, ICML and NeurIPS. We annotate key features of papers which reveal their values: their justification for their choice of project, which attributes of their project they uplift, their consideration of potential negative consequences, and their institutional affiliations and funding sources. We find that few of the papers justify how their project connects to a societal need (15) and far fewer discuss negative potential (1). Through line-by-line content analysis, we identify 59 values that are uplifted in ML research, and, of these, we find that the papers most frequently justify and assess themselves based on Performance, Generalization, Quantitative evidence, Efficiency, Building on past work, and Novelty. We present extensive textual evidence and identify key themes in the definitions and operationalization of these values. Notably, we find systematic textual evidence that these top values are being defined and applied with assumptions and implications generally supporting the centralization of power. Finally, we find increasingly close ties between these highly cited papers and tech companies and elite universities.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {173–184},
numpages = {12},
keywords = {Power asymmetries, ICML, NeurIPS, Corporate ties, Encoded values of ML},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}

@inproceedings{ganguli2022predictability,
author = {Ganguli, Deep and Hernandez, Danny and Lovitt, Liane and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and Dassarma, Nova and Drain, Dawn and Elhage, Nelson and El Showk, Sheer and Fort, Stanislav and Hatfield-Dodds, Zac and Henighan, Tom and Johnston, Scott and Jones, Andy and Joseph, Nicholas and Kernian, Jackson and Kravec, Shauna and Mann, Ben and Nanda, Neel and Ndousse, Kamal and Olsson, Catherine and Amodei, Daniela and Brown, Tom and Kaplan, Jared and McCandlish, Sam and Olah, Christopher and Amodei, Dario and Clark, Jack},
title = {Predictability and Surprise in Large Generative Models},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533229},
doi = {10.1145/3531146.3533229},
abstract = {Large-scale pre-training has recently emerged as a technique for creating capable, general-purpose, generative models such as GPT-3, Megatron-Turing NLG, Gopher, and many others. In this paper, we highlight a counterintuitive property of such models and discuss the policy implications of this property. Namely, these generative models have a paradoxical combination of predictable loss on a broad training distribution (as embodied in their ”scaling laws”), and unpredictable specific capabilities, inputs, and outputs. We believe that the high-level predictability and appearance of useful capabilities drives rapid development of such models, while the unpredictable qualities make it difficult to anticipate the consequences of model deployment. We go through examples of how this combination can lead to socially harmful behavior with examples from the literature and real world observations, and we also perform two novel experiments to illustrate our point about harms from unpredictability. Furthermore, we analyze how these conflicting properties combine to give model developers various motivations for deploying these models, and challenges that can hinder deployment. We conclude with a list of possible interventions the AI community may take to increase the chance of these models having a beneficial impact. We intend for this paper to be useful to policymakers who want to understand and regulate AI systems, technologists who care about the potential policy impact of their work, funders who want to support work addressing these challenges, and academics who want to analyze, critique, and potentially develop large generative models.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1747–1764},
numpages = {18},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}

@inproceedings{bommasani2022evaluation,
    title = "Evaluation for Change",
    author = "Bommasani, Rishi",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.522",
    doi = "10.18653/v1/2023.findings-acl.522",
    pages = "8227--8239",
    abstract = "Evaluation is the central means for assessing, understanding, and communicating about NLP models. In this position paper, we argue evaluation should be more than that: it is a force for driving change, carrying a sociological and political character beyond its technical dimensions. As a force, evaluation{'}s power arises from its adoption: under our view, evaluation succeeds when it achieves the desired change in the field. Further, by framing evaluation as a force, we consider how it competes with other forces. Under our analysis, we conjecture that the current trajectory of NLP suggests evaluation{'}s power is waning, in spite of its potential for realizing more pluralistic ambitions in the field. We conclude by discussing the legitimacy of this power, who acquires this power and how it distributes. Ultimately, we hope the research community will more aggressively harness evaluation to drive change.",
}


@misc{liang2022norms,
    author = {Liang, Percy and Bommasani, Rishi and Creel, Kathleen A. and Reich, Rob},
    title  = {The Time Is Now to Develop Community Norms for the Release of Foundation Models},
    url    = {https://crfm.stanford.edu/2022/05/17/community-norms.html},
    year   = {2022}
}

@article{gehrmann2022gemv2,
  title={GEMv2: Multilingual NLG Benchmarking in a Single Line of Code},
  author={Sebastian Gehrmann and Abhik Bhattacharjee and Abinaya Mahendiran and Alex Wang and Alexandros Papangelis and Aman Madaan and Angelina McMillan-Major and Anna V. Shvets and Ashish Upadhyay and Bingsheng Yao and Bryan Wilie and Chandra Bhagavatula and Chaobin You and Craig Thomson and Cristina Garbacea and Dakuo Wang and Daniel Deutsch and Deyi Xiong and Di Jin and Dimitra Gkatzia and Dragomir Radev and Elizabeth Clark and Esin Durmus and Faisal Ladhak and Filip Ginter and Genta Indra Winata and Hendrik Strobelt and Hiroaki Hayashi and Jekaterina Novikova and Jenna Kanerva and Jenny Chim and Jiawei Zhou and Jordan Clive and Joshua Maynez and Jo{\~a}o Sedoc and Juraj Juraska and Kaustubh D. Dhole and Khyathi Raghavi Chandu and Leonardo F. R. Ribeiro and Lewis Tunstall and Li Zhang and Mahima Pushkarna and Mathias Creutz and Michael White and Mihir Kale and Moussa Kamal Eddine and Nico Daheim and Nishant Subramani and Ondrej Dusek and Paul Pu Liang and Pawan Sasanka Ammanamanchi and Qinqin Zhu and Ratish Puduppully and Reno Kriz and Rifat Shahriyar and Ronald Cardenas and Saad Mahamood and Salomey Osei and Samuel Cahyawijaya and Sanja vStajner and S{\'e}bastien Montella and Shailza and Shailza Jolly and Simon Mille and Tahmid Hasan and Tianhao Shen and Tosin P. Adewumi and Vikas Raunak and Vipul Raheja and Vitaly Nikolaev and Vivian Tsai and Yacine Jernite and Yi Xu and Yisi Sang and Yixin Liu and Yufang Hou},
  journal={ArXiv},
  year={2022},
  volume={abs/2206.11249}
}

@inproceedings{werra2022evaluate,
  title={Evaluate\&Evaluation on the Hub: Better Best Practices for Data and Model Measurements},
  author={Leandro von Werra and Lewis Tunstall and Abhishek Thakur and Alexandra Sasha Luccioni and Tristan Thrush and Aleksandra Piktus and Felix Marty and Nazneen Rajani and Victor Mustar and Helen Ngo and Omar Sanseviero and Mario vSavsko and Albert Villanova and Quentin Lhoest and Julien Chaumond and Margaret Mitchell and Alexander M. Rush and Thomas Wolf and Douwe Kiela},
  year={2022}
}

@inproceedings{
yuan2022decentralized,
title={Decentralized Training of Foundation Models in Heterogeneous Environments},
author={Binhang Yuan and Yongjun He and Jared Quincy Davis and Tianyi Zhang and Tri Dao and Beidi Chen and Percy Liang and Christopher Re and Ce Zhang},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=UHoGOaGjEq}
}


@article{bender-friedman-2018-data,
    title = "Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science",
    author = "Bender, Emily M.  and
      Friedman, Batya",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "6",
    year = "2018",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q18-1041",
    doi = "10.1162/tacl_a_00041",
    pages = "587--604",
    abstract = "In this paper, we propose data statements as a design solution and professional practice for natural language processing technologists, in both research and development. Through the adoption and widespread use of data statements, the field can begin to address critical scientific and ethical issues that result from the use of data from certain populations in the development of technology for other populations. We present a form that data statements can take and explore the implications of adopting them as part of regular practice. We argue that data statements will help alleviate issues related to exclusion and bias in language technology, lead to better precision in claims about how natural language processing research can generalize and thus better engineering results, protect companies from public embarrassment, and ultimately lead to language technology that meets its users in their own preferred linguistic style and furthermore does not misrepresent them to others.",
}

@misc{vipra2023, title={Computational power and ai}, url={https://ainowinstitute.org/publication/policy/compute-and-ai}, journal={AI Now Institute}, author={Vipra, Jai and Myers West, Sarah}, year={2023}, month={Sep}} 

@article{winograd2023privacy,
  title={Loose-lipped large language modells spill your secrets: The privacy implications of large language models},
  author={Amy Winograd},
  journal={Harvard Journal of Law and Technology},
  year={2023},
  volume={36},
  number={2}
}

@article{eu2016,
  author={EU},
  title={Official Journal of the European Union 2016},
  volume={L 119/1}, 
  journal={Official Journal of the European Union}, 
  year={2016}, 
  month={Apr},
  url="https://eur-lex.europa.eu/legal-content/EN/TXT/?qid=1552662547490&uri=CELEX%3A32016R0679"} 

@misc{cohere2022, 
  title={Best practices for deploying Language Models}, 
  url={https://txt.cohere.com/best-practices-for-deploying-language-models/}, journal={Context by Cohere}, 
  publisher={Context by Cohere}, 
  author={Cohere}, 
  year={2022}, 
  month={Jul}} 

@inproceedings{qiaosi2023ux,
author = {Wang, Qiaosi and Madaio, Michael and Kane, Shaun and Kapania, Shivani and Terry, Michael and Wilcox, Lauren},
title = {Designing Responsible AI: Adaptations of UX Practice to Meet Responsible AI Challenges},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581278},
doi = {10.1145/3544548.3581278},
abstract = {Technology companies continue to invest in efforts to incorporate responsibility in their Artificial Intelligence (AI) advancements, while efforts to audit and regulate AI systems expand. This shift towards Responsible AI (RAI) in the tech industry necessitates new practices and adaptations to roles—undertaken by a variety of practitioners in more or less formal positions, many of whom focus on the user-centered aspects of AI. To better understand practices at the intersection of user experience (UX) and RAI, we conducted an interview study with industrial UX practitioners and RAI subject matter experts, both of whom are actively involved in addressing RAI concerns throughout the early design and development of new AI-based prototypes, demos, and products, at a large technology company. Many of the specific practices and their associated challenges have yet to be surfaced in the literature, and distilling them offers a critical view into how practitioners’ roles are adapting to meet present-day RAI challenges. We present and discuss three emerging practices in which RAI is being enacted and reified in UX practitioners’ everyday work. We conclude by arguing that the emerging practices, goals, and types of expertise that surfaced in our study point to an evolution in praxis, with associated challenges that suggest important areas for further research in HCI.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {249},
numpages = {16},
keywords = {interview, responsible AI, industry practice, UX},
location = {Hamburg, Germany},
series = {CHI '23}
}

@misc{nakao2022responsible,
      title={Towards Responsible AI: A Design Space Exploration of Human-Centered Artificial Intelligence User Interfaces to Investigate Fairness}, 
      author={Yuri Nakao and Lorenzo Strappelli and Simone Stumpf and Aisha Naseer and Daniele Regoli and Giulia Del Gamba},
      year={2022},
      eprint={2206.00474},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@inproceedings{raji2022audit,
author = {Raji, Inioluwa Deborah and Xu, Peggy and Honigsberg, Colleen and Ho, Daniel},
title = {Outsider Oversight: Designing a Third Party Audit Ecosystem for AI Governance},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534181},
doi = {10.1145/3514094.3534181},
abstract = {Much attention has focused on algorithmic audits and impact assessments to hold developers and users of algorithmic systems accountable. But existing algorithmic accountability policy approaches have neglected the lessons from non-algorithmic domains: notably, the importance of third parties. Our paper synthesizes lessons from other fields on how to craft effective systems of external oversight for algorithmic deployments. First, we discuss the challenges of third party oversight in the current AI landscape. Second, we survey audit systems across domains - e.g., financial, environmental, and health regulation - and show that the institutional design of such audits are far from monolithic. Finally, we survey the evidence base around these design components and spell out the implications for algorithmic auditing. We conclude that the turn toward audits alone is unlikely to achieve actual algorithmic accountability, and sustained focus on institutional design will be required for meaningful third party involvement.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {557–571},
numpages = {15},
keywords = {policy, society, accountability, algorithms, auditing},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@misc{widder2023thinking,
      title={Thinking Upstream: Ethics and Policy Opportunities in AI Supply Chains}, 
      author={David Gray Widder and Richmond Wong},
      year={2023},
      eprint={2303.07529},
      archivePrefix={arXiv},
      primaryClass={cs.CY}
}

@inproceedings{cobbe2023supply,
	doi = {10.1145/3593013.3594073},
	url = {https://doi.org/10.1145%2F3593013.3594073},
	year = 2023,
	month = {jun},
	publisher = {{ACM}
},
	author = {Jennifer Cobbe and Michael Veale and Jatinder Singh},
	title = {Understanding accountability in algorithmic supply chains},
	booktitle = {2023 {ACM} Conference on Fairness, Accountability, and Transparency}
}

@book{crawford2021atlas,
  title={The atlas of AI: Power, politics, and the planetary costs of artificial intelligence},
  author={Crawford, Kate},
  year={2021},
  publisher={Yale University Press}
}

@book{gray2019ghost,
  title={Ghost work: How to stop Silicon Valley from building a new global underclass},
  author={Gray, Mary L and Suri, Siddharth},
  year={2019},
  publisher={Eamon Dolan Books}
}

@article{liu2023competitive,
title = {Building a competitive advantage based on transparency: When and why does transparency matter for corporate social responsibility?},
journal = {Business Horizons},
volume = {66},
number = {4},
pages = {517-527},
year = {2023},
issn = {0007-6813},
doi = {https://doi.org/10.1016/j.bushor.2022.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S0007681322001306},
author = {Yeyi Liu and Martin Heinberg and Xuan Huang and Andreas B. Eisingerich},
keywords = {Corporate transparency, Corporate social responsibility, Consumer skepticism, Customer involvement, Brand signaling},
abstract = {Customers today are increasingly demanding transparency from firms. This article discusses the concept of performance transparency and explores when and why transparency plays a key role for a firm’s corporate social responsibility (CSR) effectiveness. In doing so, it addresses consumer skepticism as the logic of the transparency-CSR interaction and presents empirical evidence of the effect. It also discusses key principles for managing performance transparency effectively. Companies should monitor and track performance transparency regularly, initiate consistent transparency practices along with CSR activities, pay attention to the types of information made available, and adapt the transparency practices to the involvement level.}
}

@article{bloomfield1999market,
 ISSN = {08939454, 14657368},
 URL = {http://www.jstor.org/stable/2645985},
 abstract = {This study uses laboratory experiments to determine the effects of trade and quote disclosure on market efficiency, bid-ask spreads, and trader welfare. We show that trade disclosure increases the informational efficiency of transaction prices, but also increases opening bid-ask spreads, apparently by reducing market-makers' incentives to compete for order flow. As a result, trade disclosure benefits market makers at the expense of liquidity traders and informed traders. We find that quote disclosure has no discernible effects on market performance. Overall our results demonstrate that the degree of market transparency has important effects on market equilibria and on trader and market-maker welfare.},
 author = {Robert Bloomfield and Maureen O'Hara},
 journal = {The Review of Financial Studies},
 number = {1},
 pages = {5--35},
 publisher = {[Oxford University Press, Society for Financial Studies]},
 title = {Market Transparency: Who Wins and Who Loses?},
 urldate = {2023-10-18},
 volume = {12},
 year = {1999}
}



@article{granados2013transparency,
 ISSN = {02767783},
 URL = {http://www.jstor.org/stable/43825928},
 abstract = {We contend that in order to compete effectively in a digital business environment, firms should develop a transparency strategy by selectively disclosing information outside the boundaries of the firm. We make the case for transparency strategy by showing why it is relevant in the digital business world, and the consequences of not having such a strategy. We then provide some foundations to develop the strategy and make a call for research.},
 author = {Nelson Granados and Alok Gupta},
 journal = {MIS Quarterly},
 number = {2},
 pages = {637--641},
 publisher = {Management Information Systems Research Center, University of Minnesota},
 title = {Transparency Strategy: Competing with Information in a Digital World},
 urldate = {2023-10-18},
 volume = {37},
 year = {2013}
}



@article{hao2023cleaning,
  title={Cleaning Up ChatGPT Takes Heavy Toll on Human Workers},
  author={Hao, Karen and Seetharaman, Deepa},
  journal={The Wall Street Journal},
  year={2023},
  month={July},
  day={24},
  url={https://www.wsj.com/articles/chatgpt-openai-content-abusive-sexually-explicit-harassment-kenya-workers-on-human-workers-cf191483},
  note={Photographs by Natalia Jidovanu},
}

@inproceedings{kittur2013future,
  title={The future of crowd work},
  author={Kittur, Aniket and Nickerson, Jeffrey V and Bernstein, Michael and Gerber, Elizabeth and Shaw, Aaron and Zimmerman, John and Lease, Matt and Horton, John},
  booktitle={Proceedings of the 2013 conference on Computer supported cooperative work},
  pages={1301--1318},
  year={2013}
}


@article{dzieza2023ai,
  title={AI Is a Lot of Work: As the technology becomes ubiquitous, a vast tasker underclass is emerging — and not going anywhere.},
  author={Dzieza, Josh},
  journal={The Verge},
  year={2023},
  month={Jun},
  day={20},
  url={https://www.theverge.com/features/23764584/ai-artificial-intelligence-data-notation-labor-scale-surge-remotasks-openai-chatbots},
  note={Illustrations by Richard Parry},
}

@article{west2019data,
  title={Data capitalism: Redefining the logics of surveillance and privacy},
  author={West, Sarah Myers},
  journal={Business \& society},
  volume={58},
  number={1},
  pages={20--41},
  year={2019},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}

@techreport{vipra2023comments,
  title={Comments on the Business Practices of Cloud Computing Providers},
  author={Vipra, Jai and West, Sarah Myers},
  institution={AI Now Institute},
  year={2023},
  month={June},
  day={21},
  url={https://ainowinstitute.org/wp-content/uploads/2023/06/Cloud-RFI-Submission_06222023.pdf},
  note={Filed before the Federal Trade Commission, Docket ID FTC-2023-0028},
}

@misc{tremblay2023openai,
    title = {Paul Tremblay, Mona Awad vs. OpenAI, Inc., et al.},
    author = {Saveri, Joseph R. and Zirpoli, Cadio and Young, Christopher K.L. and McMahon, Kathleen J.},
    year = {2023},
    note = {Case 3:23-cv-03223-AMO Document 1 Filed 06/28/23, UNITED STATES DISTRICT COURT, NORTHERN DISTRICT OF CALIFORNIA, SAN FRANCISCO DIVISION},
    url = {https://storage.courtlistener.com/recap/gov.uscourts.cand.414822/gov.uscourts.cand.414822.1.0_1.pdf}
}

@techreport{crs2021netneutrality,
  title={The Federal Net Neutrality Debate: Access to Broadband Networks},
  institution={Congressional Research Service},
  author={Congressional Research Service},
  year={2021},
  month={Feb},
  day={24},
  url={https://crsreports.congress.gov/product/pdf/R/R40616},
  note={Report Number: R40616},
}

@inproceedings{englehardt2015cookies,
  title={Cookies that give you away: The surveillance implications of web tracking},
  author={Englehardt, Steven and Reisman, Dillon and Eubank, Christian and Zimmerman, Peter and Mayer, Jonathan and Narayanan, Arvind and Felten, Edward W},
  booktitle={Proceedings of the 24th International Conference on World Wide Web},
  pages={289--299},
  year={2015}
}

@inproceedings{englehardt2016online,
  title={Online tracking: A 1-million-site measurement and analysis},
  author={Englehardt, Steven and Narayanan, Arvind},
  booktitle={Proceedings of the 2016 ACM SIGSAC conference on computer and communications security},
  pages={1388--1401},
  year={2016}
}

@article{narayanan2017princeton,
  title={The Princeton web transparency and accountability project},
  author={Narayanan, Arvind and Reisman, Dillon},
  journal={Transparent data mining for big and small data},
  pages={45--67},
  year={2017},
  publisher={Springer}
}

@online{ftc2023epicgames,
  title={FTC Finalizes Order Requiring Fortnite maker Epic Games to Pay 245 Million for Tricking Users into Making Unwanted Charges},
  organization={Federal Trade Commission},
  author={Federal Trade Commission},
  year={2023},
  month={Mar},
  day={14},
  url={https://www.ftc.gov/news-events/news/press-releases/2023/03/ftc-finalizes-order-requiring-fortnite-maker-epic-games-pay-245-million-tricking-users-making},
  note={FTC will use the money to provide refunds to consumers},
}

@article{rosenblat2016algorithmic,
  title={Algorithmic Labor and Information Asymmetries: A Case Study of Uber’s Drivers},
  author={Rosenblat, Alex and Stark, Luke},
  journal={International Journal Of Communication},
  volume={10},
  pages={27},
  year={2016},
  month={Jul},
  day={30},
  url={https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2686227},
  institution={Data & Society Research Institute, Western University, Faculty of Information and Media Studies},
  note={Posted: 5 Nov 2015; Last revised: 25 Jul 2017},
}

@techreport{keller2022platform,
  title={Hearing on Platform Transparency: Understanding the Impact of Social Media},
  author={Keller, Daphne},
  institution={United States Senate Committee on the Judiciary, Subcommittee on Privacy, Technology and the Law},
  year={2022},
  month={May},
  day={5},
  url={https://www.judiciary.senate.gov/imo/media/doc/Keller%20Testimony1.pdf},
  note={Statement of Daphne Keller, Stanford University Cyber Policy Center},
}

@article{doi:10.1177/20539517231164119,
author = {Ritwick Ghosh and Hilary Oliva Faxon},
title ={Smart corruption: Satirical strategies for gaming accountability},

journal = {Big Data \& Society},
volume = {10},
number = {1},
pages = {20539517231164119},
year = {2023},
doi = {10.1177/20539517231164119},

URL = { 
        https://doi.org/10.1177/20539517231164119
},
eprint = { 
        https://doi.org/10.1177/20539517231164119
}
,
    abstract = { Although new forms of data can be used to hold power to account, they also grant the powerful new resources to game accountability. We dub the latter behavior “smart corruption.” The concept highlights the possibility of appropriating algorithms, infrastructures, and data publics to accumulate benefits and obscure responsibility while leaning into the positive associations of transparency. Unlike conventional forms of corruption, smart corruption is disguised as progressive, and is thus difficult to spot or analyze through existing legal or ethical frameworks. To illustrate, we outline a satirical strategy for gaming accountability. Identifying the particular mechanisms and outcomes of transgressive activities carried out under the veneer of data-driven transparency, as well as the key actors and organizations most active in gaming accountability, is an important research and political project. }
}

@misc{boyd2016algorithmic,
  title={Algorithmic Accountability and Transparency},
  author={Boyd, Danah},
  howpublished={Open Transcripts},
  year={2016},
  month={Nov},
  day={7},
  url={http://opentranscripts.org/transcript/danah-boyd-algorithmic-accountability-transparency/},
  note={Presented by danah boyd in Algorithmic Accountability and Transparency in the Digital Economy},
}


@article{confessore2018cambridge,
  title={Cambridge Analytica and Facebook: The Scandal and the Fallout So Far},
  author={Confessore, Nicholas},
  journal={The New York Times},
  year={2018},
  month={Apr},
  day={4},
  url={https://www.nytimes.com/2018/04/04/us/politics/cambridge-analytica-scandal-fallout.html},
  note={Revelations that digital consultants to the Trump campaign misused the data of millions of Facebook users set off a furor on both sides of the Atlantic. This is how The Times covered it.},
}

@article{diresta2022openblackbox,
  title={It’s Time to Open the Black Box of Social Media},
  author={DiResta, Renée and Edelson, Laura and Nyhan, Brendan and Zuckerman, Ethan},
  journal={Scientific American},
  year={2022},
  month={Apr},
  day={28},
  url={https://www.scientificamerican.com/article/its-time-to-open-the-black-box-of-social-media/},
  note={Social media companies need to give their data to independent researchers to better understand how to keep users safe},
}


@online{skowron2023euaiact,
  title={EleutherAI's Thoughts on the EU AI Act},
  author={Skowron, Aviya and Biderman, Stella},
  year={2023},
  month={Jul},
  day={26},
  url={https://blog.eleuther.ai/eu-aia/},
  note={How we are supporting open source and open science in the EU AI Act},
}

@misc{phang2022eleutherai,
      title={EleutherAI: Going Beyond "Open Science" to "Science in the Open"}, 
      author={Jason Phang and Herbie Bradley and Leo Gao and Louis Castricato and Stella Biderman},
      year={2022},
      eprint={2210.06413},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@online{laion2023transparentai,
  title={TOWARDS A TRANSPARENT AI FUTURE: THE CALL FOR LESS REGULATORY HURDLES ON OPEN-SOURCE AI IN EUROPE},
  organization={LAION},
  author={LAION},
  year={2023},
  month={Sep},
  day={21},
  url={https://laion.ai/blog/transparent-ai/},
  note={Following our previous open letter to the European Parliament on the significance of open-source AI, LAION, backed by European Laboratory for Learning and Intelligent Systems (ELLIS) and a long list of very impactful AI researchers, we submit this new open letter to the European Parliament},
}

@misc{piktus2023roots,
      title={The ROOTS Search Tool: Data Transparency for LLMs}, 
      author={Aleksandra Piktus and Christopher Akiki and Paulo Villegas and Hugo Laurençon and Gérard Dupont and Alexandra Sasha Luccioni and Yacine Jernite and Anna Rogers},
      year={2023},
      eprint={2302.14035},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{shahbaz2023repressive,
    author ={Shahbaz, Funk, Vesteinsson},
    title = {The Repressive Power of Artificial Intelligence},
    journal = {Freedom on the Net 2023},
    year = {2023},
    url  = {https://freedomonthenet.org}
}

@inbook{kaminski_2020, place={Cambridge}, series={Cambridge Law Handbooks}, title={Understanding Transparency in Algorithmic Accountability}, DOI={10.1017/9781108680844.006}, booktitle={The Cambridge Handbook of the Law of Algorithms}, publisher={Cambridge University Press}, author={Kaminski, Margot E.}, editor={Barfield, WoodrowEditor}, year={2020}, pages={121–138}, collection={Cambridge Law Handbooks}}

@book{gray_ghost_2019,
	title = {Ghost {Work}: {How} to {Stop} {Silicon} {Valley} from {Building} a {New} {Global} {Underclass}},
	isbn = {978-1-328-56624-9},
	url = {https://books.google.com/books?id=u10-uQEACAAJ},
	publisher = {Houghton Mifflin Harcourt},
	author = {Gray, M.L. and Suri, S.},
	year = {2019},
	lccn = {2018042557},
}

@article{maslej2023ai,
  title={The AI index 2023 annual report},
  author={Maslej, Nestor and Fattorini, Loredana and Brynjolfsson, Erik and Etchemendy, John and Ligett, Katrina and Lyons, Terah and Manyika, James and Ngo, Helen and Niebles, Juan Carlos and Parli, Vanessa and others},
  journal={AI Index Steering Committee, Institute for Human-Centered AI, Stanford University, Stanford, CA},
  year={2023}
}

@article{zhang2022ai,
  title={The AI index 2022 annual report. AI index steering committee},
  author={Zhang, Daniel and Maslej, Nestor and Brynjolfsson, Erik and Etchemendy, John and Lyons, Terah and Manyika, James and Ngo, Helen and Niebles, Juan Carlos and Sellitto, Michael and Sakhaee, Ellie and others},
  journal={Stanford Institute for Human-Centered AI, Stanford University},
  pages={123},
  year={2022}
}

@article{hopkins1991human,
  title={Human development revisited: A new UNDP report},
  author={Hopkins, Michael},
  journal={World Development},
  volume={19},
  number={10},
  pages={1469--1473},
  year={1991},
  publisher={Elsevier}
}

@article{whitford2009political,
  title={Political and social foundations for environmental sustainability},
  author={Whitford, Andrew B and Wong, Karen},
  journal={Political Research Quarterly},
  volume={62},
  number={1},
  pages={190--204},
  year={2009},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}

@article{index2018corruption,
  title={Corruption perception index},
  author={Index, Corruption Perceptions},
  journal={Transparancy International},
  year={2018}
}

@book{joint2008handbook,
  title={Handbook on constructing composite indicators: methodology and user guide},
  author={Joint Research Centre-European Commission and others},
  year={2008},
  publisher={OECD publishing}
}

@book{saisana2002state,
  title={State-of-the-art report on current methodologies and practices for composite indicator development},
  year={2002},
  author={Saisana, Michaela and Tarantola, Stefano},
  volume={214}
}

@misc{reuter2023im,
      title={I'm Afraid I Can't Do That: Predicting Prompt Refusal in Black-Box Generative Language Models}, 
      author={Max Reuter and William Schulze},
      year={2023},
      eprint={2306.03423},
      archhttps://arxiv.org/help/api/indexivePrefix={arXiv},
      primaryClass={cs.AI}
}

@inproceedings{nissenbaum2004contextual,
    author={Helen Nissenbaum},
    title={Privacy as Contextual Integrity},
    booktitle={Washington Law Review},
    year={2024},
    url={https://digitalcommons.law.uw.edu/wlr/vol79/iss1/10}
}

@techreport{king2020privacy,
    author={Jen King},
    title={Redesigning Data Privacy: Reimagining Notice and Consent for human technology interaction},
    institution={The Center for Internet and Society, Stanford Law School},
    year={2020}
}

@article{mulligan2017privacy,
    author={Deirdre K. Mulligan and Colin Koopman and Nick Doty},
    title={Privacy is an essentially contested concept: a multi-dimensional analytic for mapping privacy},
    journal={Philosophical Transactions Royal Society A},
    volume={374},
    year={2016}
}

@misc{chen2023chatgpts,
      title={How is ChatGPT's behavior changing over time?}, 
      author={Lingjiao Chen and Matei Zaharia and James Zou},
      year={2023},
      eprint={2307.09009},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{sathyavageesran2022privacy,
      title={Privacy Leakage in Discrete Time Updating Systems}, 
      author={Nitya Sathyavageesran and Roy D. Yates and Anand D. Sarwate and Narayan Mandayam},
      year={2022},
      eprint={2205.15630},
      archivePrefix={arXiv},
      primaryClass={eess.SY}
}

@online{hashesh2023version,
    title={Version Control for ML Models: Why You Need It, What It Is, How To Implement It}, 
    author={Ahmed Hashesh},
    year={2023},
    url={https://neptune.ai/blog/version-control-for-ml-models}
}

@inproceedings{biderman2023pythia,
  title={Pythia: A suite for analyzing large language models across training and scaling},
  author={Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin Gregory and Bradley, Herbie and O’Brien, Kyle and Hallahan, Eric and Khan, Mohammad Aflah and Purohit, Shivanshu and Prashanth, USVSN Sai and Raff, Edward and others},
  booktitle={International Conference on Machine Learning},
  pages={2397--2430},
  year={2023},
  organization={PMLR}
}

@inproceedings{corbett2023interrogating,
  title={Interrogating the T in FAccT},
  author={Corbett, Eric and Denton, Emily},
  booktitle={Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency},
  pages={1624--1634},
  year={2023}
}

@book{birchall2021radical,
  title={Radical secrecy: The ends of transparency in datafied America},
  author={Birchall, Clare},
  volume={60},
  year={2021},
  publisher={U of Minnesota Press}
}

@book{han2015transparency,
  title={The transparency society},
  author={Han, Byung-Chul},
  year={2015},
  publisher={Stanford University Press}
}

@article{castelvecchi2016openblackbox,
  title={Can we open the black box of AI?},
  author={Castelvecchi, Davide},
  journal={Nature},
  year={2016},
  month={10},
  day={05},
  url={https://www.nature.com/news/can-we-open-the-black-box-of-ai-1.20731},
  note={Artificial intelligence is everywhere. But before scientists trust it, they first need to understand how machines learn.}
}


@book{florini2007right,
  title={The right to know: transparency for an open world},
  author={Florini, Ann},
  year={2007},
  publisher={Columbia University Press}
}

@book{robinson2012nations,
  title={Why nations fail: The origins of power, prosperity and poverty},
  author={Robinson, James A and Acemoglu, Daron},
  year={2012},
  publisher={Profile London}
}

@book{lathrop2010open,
  title={Open government: Collaboration, transparency, and participation in practice},
  author={Lathrop, Daniel and Ruma, Laurel},
  year={2010},
  publisher={" O'Reilly Media, Inc."}
}

@article{johnston2006good,
  title={Good governance: Rule of law, transparency, and accountability},
  author={Johnston, Michael},
  journal={New York: United Nations Public Administration Network},
  pages={1--32},
  year={2006}
}

@misc{solaiman2023evaluating,
      title={Evaluating the Social Impact of Generative AI Systems in Systems and Society}, 
      author={Irene Solaiman and Zeerak Talat and William Agnew and Lama Ahmad and Dylan Baker and Su Lin Blodgett and Hal Daumé III au2 and Jesse Dodge and Ellie Evans and Sara Hooker and Yacine Jernite and Alexandra Sasha Luccioni and Alberto Lusoli and Margaret Mitchell and Jessica Newman and Marie-Therese Png and Andrew Strait and Apostol Vassilev},
      year={2023},
      eprint={2306.05949},
      archivePrefix={arXiv},
      primaryClass={cs.CY}
}

@book{hardin2002trust,
  title={Trust and trustworthiness},
  author={Hardin, Russell},
  year={2002},
  publisher={Russell Sage Foundation}
}

@article{ng_conceptualizing_2021,
	title = {Conceptualizing {AI} literacy: {An} exploratory review},
	volume = {2},
	issn = {2666-920X},
	url = {https://www.sciencedirect.com/science/article/pii/S2666920X21000357},
	doi = {https://doi.org/10.1016/j.caeai.2021.100041},
	journal = {Computers and Education: Artificial Intelligence},
	author = {Ng, Davy Tsz Kit and Leung, Jac Ka Lok and Chu, Samuel Kai Wah and Qiao, Maggie Shen},
	year = {2021},
	keywords = {AI ethics, AI in education, AI learning and teaching, AI literacy, AI literacy questionnaire},
	pages = {100041}
}

@book{pasquale2015black,
  title={The black box society: The secret algorithms that control money and information},
  author={Pasquale, Frank},
  year={2015},
  publisher={Harvard University Press}
}

@book{schudson2015rise,
  title={The rise of the right to know: Politics and the culture of transparency, 1945-1975},
  author={Schudson, Michael},
  year={2015},
  publisher={Harvard University Press}
}

@misc{huang2023catastrophic,
      title={Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation}, 
      author={Yangsibo Huang and Samyak Gupta and Mengzhou Xia and Kai Li and Danqi Chen},
      year={2023},
      eprint={2310.06987},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{wang2023decodingtrust,
      title={DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models}, 
      author={Boxin Wang and Weixin Chen and Hengzhi Pei and Chulin Xie and Mintong Kang and Chenhui Zhang and Chejian Xu and Zidi Xiong and Ritik Dutta and Rylan Schaeffer and Sang T. Truong and Simran Arora and Mantas Mazeika and Dan Hendrycks and Zinan Lin and Yu Cheng and Sanmi Koyejo and Dawn Song and Bo Li},
      year={2023},
      eprint={2306.11698},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{qi2023finetuning,
      title={Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!}, 
      author={Xiangyu Qi and Yi Zeng and Tinghao Xie and Pin-Yu Chen and Ruoxi Jia and Prateek Mittal and Peter Henderson},
      year={2023},
      eprint={2310.03693},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{meta2023,
    author={Meta},
    title={Meta Platform Terms},
    year={2023},
    url={https://developers.facebook.com/terms/#datause}
}

@inproceedings{Lam2020,
	doi = {10.1145/3426428.3426922},
  
	url = {https://doi.org/10.1145%2F3426428.3426922},
  
	year = 2020,
	month = {nov},
  
	publisher = {{ACM}
},
  
	author = {Patrick Lam and Jens Dietrich and David J. Pearce},
  
	title = {Putting the semantics into semantic versioning},
  
	booktitle = {Proceedings of the 2020 {ACM} {SIGPLAN} International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software}
}

@misc{li2016watch,
      title={Watch out for This Commit! A Study of Influential Software Changes}, 
      author={Daoyuan Li and Li Li and Dongsun Kim and Tegawendé F. Bissyandé and David Lo and Yves Le Traon},
      year={2016},
      eprint={1606.03266},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}

@misc{haryono2020automatic,
      title={Automatic Android Deprecated-API Usage Update by Learning from Single Updated Example}, 
      author={Stefanus Agus Haryono and Ferdian Thung and Hong Jin Kang and Lucas Serrano and Gilles Muller and Julia Lawall and David Lo and Lingxiao Jiang},
      year={2020},
      eprint={2005.13220},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}

@article{Adelani2020,
	doi = {10.1140/epjds/s13688-020-00243-w},
  
	url = {https://doi.org/10.1140%2Fepjds%2Fs13688-020-00243-w},
  
	year = 2020,
	month = {aug},
  
	publisher = {Springer Science and Business Media {LLC}
},
  
	volume = {9},
  
	number = {1},
  
	author = {David Ifeoluwa Adelani and Ryota Kobayashi and Ingmar Weber and Przemyslaw A. Grabowicz},
  
	title = {Estimating community feedback effect on topic choice in social media with predictive modeling},
  
	journal = {{EPJ} Data Science}
}

@misc{springer2018progressive,
      title={Progressive Disclosure: Designing for Effective Transparency}, 
      author={Aaron Springer and Steve Whittaker},
      year={2018},
      eprint={1811.02164},
      archivePrefix={arXiv},
      primaryClass={cs.HC}
}

@misc{sevilla2022compute,
      title={Compute Trends Across Three Eras of Machine Learning}, 
      author={Jaime Sevilla and Lennart Heim and Anson Ho and Tamay Besiroglu and Marius Hobbhahn and Pablo Villalobos},
      year={2022},
      eprint={2202.05924},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{lukas2023analyzing,
      title={Analyzing Leakage of Personally Identifiable Information in Language Models}, 
      author={Nils Lukas and Ahmed Salem and Robert Sim and Shruti Tople and Lukas Wutschitz and Santiago Zanella-Béguelin},
      year={2023},
      eprint={2302.00539},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{kim2023propile,
      title={ProPILE: Probing Privacy Leakage in Large Language Models}, 
      author={Siwon Kim and Sangdoo Yun and Hwaran Lee and Martin Gubri and Sungroh Yoon and Seong Joon Oh},
      year={2023},
      eprint={2307.01881},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}

@inproceedings{Pistilli2023,
	doi = {10.1145/3593013.3594002},
  
	url = {https://doi.org/10.1145%2F3593013.3594002},
  
	year = 2023,
	month = {jun},
  
	publisher = {{ACM}
},
  
	author = {Giada Pistilli and Carlos Mu{\~{n}}oz Ferrandis and Yacine Jernite and Margaret Mitchell},
  
	title = {Stronger Together: on the Articulation of Ethical Charters, Legal Tools, and Technical Documentation in {ML}},
  
	booktitle = {2023 {ACM} Conference on Fairness, Accountability, and Transparency}
}

@misc{rakova2022termsweservewith,
      title={Terms-we-Serve-with: a feminist-inspired social imaginary for improved transparency and engagement in AI}, 
      author={Bogdana Rakova and Megan Ma and Renee Shelby},
      year={2022},
      eprint={2206.02492},
      archivePrefix={arXiv},
      primaryClass={cs.CY}
}

@misc{chen2023investigation,
      title={An investigation of licensing of datasets for machine learning based on the GQM model}, 
      author={Junyu Chen and Norihiro Yoshida and Hiroaki Takada},
      year={2023},
      eprint={2303.13735},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}

@misc{liu2021identifying,
      title={Identifying Terms and Conditions Important to Consumers using Crowdsourcing}, 
      author={Xingyu Liu and Annabel Sun and Jason I. Hong},
      year={2021},
      eprint={2111.12182},
      archivePrefix={arXiv},
      primaryClass={cs.HC}
}

@misc{chou2012government,
    title={Transparency Report: Government requests on the rise},
    author={Dorothy Chou},
    url={https://blog.google/technology/safety-security/transparency-report-government-requests/}
}

@misc{chen2021achieving,
      title={Achieving Transparency Report Privacy in Linear Time}, 
      author={Chien-Lun Chen and Leana Golubchik and Ranjan Pal},
      year={2021},
      eprint={2104.00137},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{piorkowski2022evaluating,
      title={Evaluating a Methodology for Increasing AI Transparency: A Case Study}, 
      author={David Piorkowski and John Richards and Michael Hind},
      year={2022},
      eprint={2201.13224},
      archivePrefix={arXiv},
      primaryClass={cs.CY}
}

@misc{lapowsky2018cambridge,
    title={How Cambridge Analytica Sparked the Great Privacy Awakening},
    author={Issie Lapowsky},
    url={https://www.wired.com/story/cambridge-analytica-facebook-privacy-awakening/},
    year={2018}
}

@misc{hoffmann2022training,
      title={Training Compute-Optimal Large Language Models}, 
      author={Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de Las Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and Tom Hennigan and Eric Noland and Katie Millican and George van den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and Karen Simonyan and Erich Elsen and Jack W. Rae and Oriol Vinyals and Laurent Sifre},
      year={2022},
      eprint={2203.15556},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{lee2023talkin,
  title={Talkin''Bout AI Generation: Copyright and the Generative-AI Supply Chain},
  author={Lee, Katherine and Cooper, A Feder and Grimmelmann, James},
  journal={arXiv preprint arXiv:2309.08133},
  year={2023}
}