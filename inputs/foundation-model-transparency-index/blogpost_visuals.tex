% \pdfoutput=1
% \documentclass[11pt]{article}

% %\usepackage[review]{acl}
% \usepackage{acl}
% \usepackage{times}
% \usepackage{latexsym}
% \usepackage{amsmath}
% \usepackage{amssymb}
% \usepackage{ifthen}
% \usepackage{graphicx}
% \usepackage{booktabs}
% \usepackage{multirow}
% \usepackage{subcaption}
% \usepackage{xspace}
% \usepackage[T1]{fontenc}
% \usepackage[utf8]{inputenc}
% \usepackage{microtype}
% % \usepackage{abstract}

% % use the same count for tables and figures 
% \makeatletter
% \let\c@table\c@figure % for (1)
% \let\ftype@table\ftype@figure % for (2)
% \makeatother

% % For URL wrapping.
% \expandafter\def\expandafter\UrlBreaks\expandafter{\UrlBreaks%  save the current one
%   \do\a\do\b\do\c\do\d\do\e\do\f\do\g\do\h\do\i\do\j%
%   \do\k\do\l\do\m\do\n\do\o\do\p\do\q\do\r\do\s\do\t%
%   \do\u\do\v\do\w\do\x\do\y\do\z\do\A\do\B\do\C\do\D%
%   \do\E\do\F\do\G\do\H\do\I\do\J\do\K\do\L\do\M\do\N%
%   \do\O\do\P\do\Q\do\R\do\S\do\T\do\U\do\V\do\W\do\X%
%   \do\Y\do\Z}

% \input std-macros
% \input{macros}

% \title{Holistic Evaluation of Language Models}
% % \title{\frameworkName: A Modular Framework for Language Model Evaluation}

% \author{
% Center for Research on Foundation Models (CRFM) \\
% Stanford University \\
% Version: 0.1 (last updated Oct 6)
% }


\documentclass[screen, authorversion, acmsmall]{acmart}
\citestyle{acmauthoryear}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{natbib}
\usepackage{xcolor}
\usepackage{ifthen}
\usepackage[normalem]{ulem}
\usepackage{svg}
\usepackage{subfiles}
\usepackage{xspace}
\usepackage{cleveref}
\usepackage{wrapfig}
\usepackage{dialogue}
\usepackage[most]{tcolorbox}
\usepackage{subcaption}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}


\input std-macros
\input macros

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{authblk}
\renewcommand\Authsep{\hskip1em\relax}  % separation between authors
\renewcommand\Authands{\hskip1em\relax} % separation before last author

\makeatletter         
\renewcommand\maketitle{
{\raggedright 
\begin{center}
{\Huge \bfseries \sffamily \@title }\\[2ex]
{\@author}\\[2ex] 
\end{center}}}
\makeatother

\renewenvironment{abstract}{%
    \newline
    \itshape
    }
{}
% end hack
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\title{Holistic Evaluation of Language Models}
\input authors
\affil{Center for Research on Foundation Models (CRFM) \\ Stanford Institute for Human-Centered Artificial Intelligence (HAI) \\ Stanford University \\
DRAFT - PLEASE DO NOT DISTRIBUTE 
[Version 0.5 (last updated Nov 8)]
}

\renewcommand{\shortauthors}{Center for Research on Foundation Models (CRFM)}

\FigTop{figures/fig_taxonomy_v1}{0.35}{taxonomy}
{\textbf{Importance of taxonomy in \benchmarkname approach.}
In comparison to prior bottom-up approaches in evaluation (\textit{left}), in \benchmarkname we take a top-down approach of first explicitly stating what we want to evaluate (\ie scenarios and metrics) as a taxonomy. Then, given this stated taxonomy, we make deliberate decisions subject to it on what we implement and evaluate. This makes explicit what we currently lack.}

\FigTop{figures/fig_multi_metric}{0.42}{multi-metric}
{\textbf{Many metrics for every use case.}
In comparison to most prior benchmarks of language technologies, which primarily center accuracy and often relegate other desiderata to their own bespoke datasets (if at all), in \benchmarkname we take a multi-metric approach.
This reflects our position that the desiderata beyond accuracy are \textit{not} second-class, and that the nature and relevance of these desiderata depend on the use case.
}

\FigTop{figures/fig_model_comparison}{0.25}{evaluation-status-quo}
{\textbf{Standardizing language model evaluation.} Prior to our work (\textit{left}), the evaluation of language models, including the \numtextmodels (natural) language models we consider, was uneven.
In fact, many of our \numcorescenarios \core scenarios had no models evaluated on them.
After our evaluation (\textit{right}), models are now evaluated under the same conditions on many scenarios.
}

\FigTop{figures/evaluation-primitives}{0.5}{primitives}
{\textbf{Evaluation primitives.} Each evaluation run requires the specification of 
a \textit{scenario} (what we want), 
a \textit{model} with an \textit{adaptation} process (how we get it), 
and one or more \textit{metrics} (how good are the results).
}


\newpage
\end{document}
