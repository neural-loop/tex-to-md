\hypertarget{introduction}{\section{Introduction}}
\label{sec:introduction}
Foundation models (FMs) like LLaMA and DALL-E 3 are an emerging class of digital technology that has transformed artificial intelligence \citep{bommasani2021opportunities}. 
These resource-intensive models are often built by processing trillions of bytes of data, with some of the most capable systems, like \openai's \gptfour, costing hundreds of millions of dollars to build.\footnote{\url{https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/}} 
Foundation models power some of the fastest-growing consumer technologies in history,\footnote{\url{https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/}}
including myriad generative AI applications,\footnote{\url{https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-economic-potential-of-generative-ai-the-next-productivity-frontier}} bringing immense commercial investment and public awareness to AI. 
Simultaneously, these models have captured the interest of policymakers around the world: the United States,\footnote{\url{https://www.whitehouse.gov/wp-content/uploads/2023/07/Ensuring-Safe-Secure-and-Trustworthy-AI.pdf}                                                   } China,\footnote{\url{http://www.cac.gov.cn/2023-07/13/c_1690898327029107.htm}} Canada,\footnote{\url{https://ised-isde.canada.ca/site/ised/en/voluntary-code-conduct-responsible-development-and-management-advanced-generative-ai-systems}} the European Union,\footnote{\url{https://www.europarl.europa.eu/news/en/press-room/20230609IPR96212/meps-ready-to-negotiate-first-ever-rules-for-safe-and-transparent-ai}} the United Kingdom,\footnote{\url{https://www.gov.uk/cma-cases/ai-foundation-models-initial-review}} India,\footnote{\url{https://indiaai.s3.ap-south-1.amazonaws.com/docs/generative-ai-report.pdf}} Japan,\footnote{\url{https://english.kyodonews.net/news/2023/10/3b83adf1e28d-japans-ai-draft-guidelines-ask-for-measures-to-address-overreliance.html}} the G7,\footnote{\url{https://www.politico.eu/wp-content/uploads/2023/09/07/3e39b82d-464d-403a-b6cb-dc0e1bdec642-230906_Ministerial-clean-Draft-Hiroshima-Ministers-Statement68.pdf}} and a wide range of other governments have already taken action on foundation models and generative AI.
Foundation models are positioned to be the defining digital technology of the decade ahead.

Transparency is an essential precondition for public accountability, scientific innovation, and effective governance of digital technologies.
Without adequate transparency, stakeholders cannot understand foundation models, who they affect, and the impact they have on society.
Historically, digital technologies often follow a familiar pattern: a new technology provides opportunities and benefits, but companies are not transparent in how they develop and deploy the technology, and this opacity eventually leads to harm. 
In the case of social media, companies have not been transparent about the ways in which they moderate content and share user data, contributing to massacres like the Rohingya genocide in Myanmar\footnote{\url{/https://about.fb.com/wp-content/uploads/2018/11/bsr-facebook-myanmar-hria_final.pdf}} and gross violations of privacy like the Cambridge Analytica scandal.\footnote{\url{https://www.nytimes.com/2018/04/04/us/politics/cambridge-analytica-scandal-fallout.html}}
Consequently, a chorus of academics, civil society organizations, firms, and governments have called for foundation model developers to improve transparency.\footnote{See \refapp{transparency} for further discussion.} 
Groups such as the Partnership on AI, Mozilla, and Freedom House have noted that increased transparency is a crucial intervention.\footnote{\url{http://partnershiponai.org/wp-content/uploads/2021/08/PAI-Responsible-Sourcing-of-Data-Enrichment-Services.pdf}}
UN Secretary-General António Guterres has proposed that the international community should “make transparency, fairness and accountability the core of AI governance ... [and] Consider the adoption of a declaration on data rights that enshrines transparency.”\footnote{\url{https://indonesia.un.org/sites/default/files/2023-07/our-common-agenda-policy-brief-gobal-digi-compact-en.pdf}} 

Foundation models appear to be on track to replicate the opacity of social media. 
Consider \openai's \gptfour, one of the most influential foundation models today. 
\openai states plainly its intention to be nontransparent in the \gptfour technical report, which “contains no further details about the architecture (including model size), hardware, training compute, dataset construction, training method, or similar” \citep{openai2023gpt4}. 
Companies often claim that such information is proprietary or that sharing it would undermine their market position and pose a danger to society as a whole, but this does not negate the enormous risks stemming from foundation models these same companies openly acknowledge, as well as the value of greater transparency. 

While the downsides of opacity are clear, transparency in the foundation model ecosystem today remains minimal.
Little to no evidence exists about which foundation model developers are transparent about which matters, and where there are blind spots in the industry.
How best to improve transparency remains an open question despite rising concerns.


To determine the status quo and track how it evolves over time, we introduce the \projectname (\projectabbreviation).\footnote{See \indexUrl.}
A composite \textit{index} measures a complex construct (\eg transparency) as the basis for scoring/ranking entities (\eg foundation model developers) by aggregating  many low-level quantifiable \textit{indicators} of transparency.
Indexes are not common in AI\footnote{We note that the AI Index from the Stanford Institute for Human-Centered AI \citep{zhang2022ai, maslej2023ai} is a related effort, but the AI Index tracks broader trends in AI, rather than scoring specific entities or aggregating to a single value.} but are a standard methodology in the social sciences: iconic examples include the United Nations Development Programme's Human Development Index \citep{undp2022hdr}, which ranks countries, and Ranking Digital Rights' Corporate Accountability Index, which ranks companies \citep{rdr2020index}.
We score the transparency of foundation model developers in an effort to promote responsible business practices and greater public accountability. 
We deconstruct the concept of transparency into \numdomains high-level domains: the \textit{upstream} (\eg the data, labor, and compute resources used to build a foundation model), \textit{model}-level (\eg the capabilities, risks, and evaluations of the foundation model), and \textit{downstream} (\eg the distribution channels, usage policies, and affected geographies) practices of the foundation model developer.

\paragraph{The \projectversionedname.}
For the 2023 index, each domain is further broken down into 32--35 indicators: these are concrete, specific, and decidable aspects of transparency (\eg does the foundation model developer disclose the size of its model?). 
Ultimately, the index consists of \numindicators indicators (see \refapp{indicators}) that comprehensively codify what it means for a foundation model developer to be transparent, building upon formative works on transparency for AI and other digital technologies \citep{gebru2021datasheets, bender2018data, mitchell2018modelcards, raji2019actionable, gray2019ghost, crawford2021atlas, cdt2021, keller2022platform}.\footnote{See \url{https://transparency.dsa.ec.europa.eu/} and \url{https://www.tspa.org/curriculum/ts-fundamentals/transparency-report/}} 

We score \numcompanies major foundation model developers on each of the \numindicators indicators to determine how transparent each company is in the development and deployment of its models. 
In particular, we score developers based on their practices in relation to their flagship foundation models:
we assess \openai (\gptfour), \anthropic (\claude), \google (\palm), \meta (\llama), \inflection (\inflectionone), \amazon (\titan), \cohere (\command), \aitwentyone (\jurassic), \huggingface (\bloomz; as host of BigScience),\footnote{Our objective is to assess \huggingface as a company that can be tracked over time.
\bloomz however was not built unilaterally by \huggingface, but instead through the BigScience open collaboration. 
As a result, we refer to \huggingface in the prose but include the BigScience logo in visuals; we provide further discussion in \refsec{model-selection}.} and \stability (\stablediffusion). 
In addition, for downstream indicators we consider the flagship or in-house distribution channel: \openai (OpenAI API), \anthropic (Claude API), \google (PaLM API), \meta (Microsoft Azure),\footnote{\meta announced Microsoft as the "preferred partner" for \llama via Azure: \url{https://about.fb.com/news/2023/07/llama-2/}} \inflection (Pi), \amazon (Bedrock), \cohere (Cohere API), \aitwentyone (AI21 Studio), \huggingface (Hugging Face Model Hub), and \stability (Stability API).
We assess developers on the basis of publicly-available information to make our findings reproducible and encourage transparency vis-à-vis the public on the whole.
To ensure our scoring is consistent, we identify information using a rigorous search protocol (see \refapp{search-protocol}).
To ensure our scoring is accurate, we notified developers and provided them the opportunity to contest any scores prior to the release of this work (all 10 responded and 8 of the 10 explicitly contested some scores).
We summarize our core findings, recommendations, and contributions below and make all core materials (\eg indicators, scores, justifications, visuals) publicly available.\footnote{\materialsUrl}