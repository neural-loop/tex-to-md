\section{DISCUSSION AND CONCLUSION}
\label{sec:discussion}

\subsection{Discussion of Results}
We know surprisingly little about how people perceive pervasive technologies in the workplace. Yet we need to know more to inform the design of such technologies. This appears of crucial importance, not least because of the two polar opposite views animating today's debate over technology. On one hand, informed by a widespread algorithmic aversion, we risk rejecting technologies that could make our lives better. On the other hand, informed by technological optimism, we may adopt technologies that could have detrimental impacts. To avoid rejecting good technologies and designing bad ones, we should unpack AI ethics in Pervasive Computing.

The heuristics we have found offer a guide on how technologies are likely to be morally judged. Having a technology that is easy to implement and does not interfere with work is not necessarily a technology that should be deployed. \emph{Tracking facial expressions} (even beyond the nefarious uses---of dubious effectiveness---of inferring political orientation or sexual preferences from faces~\cite{wang2018deep}) is possible and could be done in seamless ways (e.g., with existing off-the-shelf cameras), yet it would be still considered harmful and unethical. \emph{Tracking eye movements}, \emph{task completion}, or \emph{typing behavior} was considered a proxy for focus (harmless) yet intrusive as it would ``get in the way.'' \emph{Tracking social media use in remote work} was considered not only intrusive but also harmful, as it infringes on privacy rights.

Finally, regardless of the work context (on-site \emph{vs.} remote work), most scenarios are either harmless (e.g., \emph{tracking application usage} was considered to be a proxy for focus on work) or harmful (e.g., \emph{tracking physical movements, body posture, or facial expressions} was not). Yet, other scenarios were context dependent (Figure~\ref{fig:harmful_quadrant}). \emph{Tracking text messages during meetings} was considered less harmful (more fit-for-purpose) onsite than remotely. Text messages in onsite settings were considered ``fair game'' as they could reflect a meeting's productivity, while those in a remote setting were usually used beyond the meeting's purpose (e.g., used to catch up with colleagues), making them a poorer proxy for meeting productivity. Again, the heuristic used remained the same: whether a technology (e.g., inferring productivity from text messages) was fit for purpose. The only difference was that the same technology was fit for purpose in one context (e.g., in the constrained setting of a meeting) but not in the other (e.g., in the wider context of remote work).

\subsection{Limitations and Future Work}
This work has three main limitations that call for future research efforts. The first limitation concerns the generalizability of our findings. While our sample demographics were fairly distributed across industry sectors and ethnic backgrounds, most of the crowd-workers were based in the US. Our findings hold for this specific cohort. The second limitation concerns the negative connotation of the three questions being asked in the crowd-sourcing experiment, which might have biased the responses. Even in such a case, the results would still make sense in a comparative way as the responses would be systematically biased across all scenarios. The third limitation concerns the pervasive technologies under study. Given the rapid technological advancements, at the time of writing, the 16 technologies in Table~\ref{tab:technologies} were considered of likely adoption. Future studies could replicate our methodology to larger and diverse cohorts, in specific corporate contexts or geographical units, and to
emerging technologies such as AR headsets and EMG body-tracking devices.

\subsection{Theoretical and Practical Implications}
From a theoretical standpoint, it contributes to the ongoing debate of ethical and fair use of AI~\cite{arrieta2020explainable}---the emergent field of Responsible AI\footnote{\url{https://www.bell-labs.com/research-innovation/responsible-ai}}. As we showed, one needs to consider whether a technology is irresponsible in the first place well before its design. While tracking facial expressions is supported from current technology (viable) and can be done in seamless ways (unobtrusive), it was yet considered to be irresponsible (causing harm) in the office context. This translates into saying that companies should be more thoughtful about the ways they manage their workforce, and not deploying tools just because the technology allows them to. To determine whether a technology is irresponsible is a complex matter though, not least because it entails ethical concepts that are hard to define. That is why new approaches helping AI developers and industry leaders think about multi-faceted Responsible AI concepts should be developed in the near future.
From a practical standpoint, the Ubicomp community currently focuses on how to design better technologies by blending them into the background: monitoring movements in a building through wifi signals~\cite{adib2013see}, for example, remain hidden. The problem is that, by blending technologies into the background, individuals are unaware of them and, as a result, their ethical concerns are often overlooked. That is why the Ubicomp community's aspiration of blending technologies needs to go hand-in-hand with the need of unpacking AI ethics.