\section{RELATED WORK}
\label{sec:related_work}

On a pragmatic level, organizations adopted ``surveillance'' tools mainly to ensure security and boost productivity~\cite{ball2010workplace}. In a fully remote work setting, organizations had to adopt new security protocols~\cite{mckinsey} due to the increased volume of online attacks,\footnote{\url{https://www.dbxuk.com/statistics/cyber-security-risks-wfh}} and they
ensured productivity by tracking the efficient use of resources~\cite{ball2010workplace}.

However, well-meaning technologies could inadvertently be turned into surveillance tools. For example, a technology that produces an aggregated productivity score\footnote{\url{https://www.theguardian.com/technology/2020/nov/26/microsoft-productivity-score-feature-criticised-workplace-surveillance}} based on diverse inputs (e.g., email, network connectivity, and exchanged content) can be a double-edged sword. On the one hand, it may provide managers and senior leadership visibility into how well an organization is doing. On the other hand, it may well be turned into an evil tool that puts employees under constant surveillance and unnecessary psychological pressure.\footnote{\url{https://twitter.com/dhh/status/1331266225675137024}} More worryingly, one in two employees in the UK thinks that it is likely that they are being monitored at work~\cite{tucsurvey}, while more than two-thirds are concerned that workplace surveillance could be used in a discriminatory way, if left unregulated. Previous studies also found that employees are willing to be `monitored' but only when a company's motivations for doing so are transparently communicated~\cite{marchant2019best}. Technologies focused on workplace safety typically receive the highest acceptance rates~\cite{jacobs2019employee}, while technologies for unobtrusive and continuous stress detection receive the lowest,  with employees mainly raising concerns about tracking privacy-sensitive information~\cite{kallio2021unobtrusive}.

To take a more responsible approach in designing new technologies, researchers have recently explored which factors affect people's judgments of these technologies. In his book \emph{``How humans judge machines''}~\cite{hidalgo2021humans}, Cesar Hidalgo showed that people do not judge humans and machines equally, and that differences were the result of two principles. First, people judge humans by their intentions and machines by their outcomes (e.g., \emph{``in natural disasters like the tsunami, fire, or hurricane scenarios, there is evidence that humans are judged more positively when they try to save everyone and fail---a privilege that machines do not enjoy''~\cite{hidalgo2021humans}-p. 157)}. Second, people assign extreme intentions to humans and narrow intentions to machines, and, surprisingly, they may excuse human actions more than machine actions in accidental scenarios
(e.g., \emph{``when a car accident is caused by either a falling tree or a person jumping in front of a car, people assign more intention to the machine than to the human behind the wheel''~\cite{hidalgo2021humans}-p. 130)}.

Previous work has mostly focused on scenarios typically involving aggression, physical, or psychological harm. Here, in the workplace context, we explore scenarios reflecting aspects tailored to the pervasive computing research agenda that transcend harm such as ease of adoption and technological intrusiveness. 