\section{Existing and Emerging Regulations}
%On AI Transparency
\label{sec:laws}

In recent years, countries around the world have increasingly been drafting strategies, action plans, and policy directives to govern the use of AI systems. To some extent, regulatory approaches vary by country and region. For example, policy strategies in the US and the EU reflect their respective strengths: free-market ideas for the former, and citizen voice for the latter~\cite{gill2020policy}. Yet, despite country-level variation, many AI policies contain similar themes and ideas. A meta-analysis of over 80 AI ethics guidelines and soft-laws found that 87\% mention transparency, and include an effort to increase the explainability of AI systems~\cite{DBLP:journals/corr/abs-1906-11668}. Unfortunately, all documents to date have one major limitation: they are filled with uncertainty on \textit{how} transparency and explainability should actually be implemented in a way that is compliant with the evolving regulatory landscape ~\cite{DBLP:journals/corr/abs-1906-11668, DBLP:journals/corr/abs-1906-11668, DBLP:journals/internet/GasserA17, loi2021towards}. This limitation has 3 main causes: (1) it is difficult to design transparency regulations that can easily be standardized across different fields of AI, such as self-driving cars, robotics, and predictive modeling ~\cite{wachter2017transparent}; (2) when it comes to transparency, there is a strong information asymmetry between technologists and policymakers, and, ultimately, the individuals who are impacted by AI systems~\cite{KUZIEMSKI2020101976}; (3) there is no normative consensus around AI transparency, and most policy debates are focused on the risks of AI rather than the opportunities~\cite{DBLP:journals/internet/GasserA17}. For the purposes of scope, we will focus on regulations in the United States and Europe. However, its important noting that there is meaningful AI regulation emerging in Latin and South America, Asia, Africa, and beyond, and summarizing those regulations is an avenue for future work. For example, in 2021, Chile presented it's first national action plan on AI policy~\footnote{\url{https://www.gob.cl/en/news/chile-presents-first-national-policy-artificial-intelligence/}}.

\subsection{United States}

In 2019 the US took two major steps in the direction of AI regulation. First, Executive Order 13859 was issued with the purpose of establishing federal principles for AI systems, and to promote AI research, economic competitiveness, and national security. Importantly, the order mandates that AI algorithms implemented for use by public bodies must be ``understandable'', ``transparent'', ``responsible'', and ``accountable.'' Second, the Algorithmic Accountability Act of 2019 was introduced to the House of Representatives, and more recently reintroduced under the name Algorithmic Accountability Act of 2022. If passed into law, the Algorithmic Accountability Act would be a landmark legalisation for AI regulation in the US. The purpose of the bill is to create transparency and prevent disparate outcomes for AI systems, and it would require companies to assess the impacts of the AI systems they use and sell. The bill describes the impact assessment in detail --- which must be submitted to an oversight committee--- and states that the assessment must address ``the transparency and explainability of [an AI system] and the degree to which a consumer may contest, correct, or appeal a decision or opt out of such system or process'', % including.'' The bill emphasizes ``mechanisms by which a consumer may contest, correct, or appeal a decision or opt out of such system or process'', 
which speaks directly to what AI practitioners refer to as ``recourse'', or the ability of an individual to understand the outcome of an AI system and what they could do to change that outcome~\cite{wachter2017counterfactual, ustun2019actionable}.

In 2019 the OPEN Government Data Act was passed into law, requiring that federal agencies maintain and publish their information online as open data. The data also must be cataloged on Data.gov, a public data repository created by the the US government. While this law only applies to public data, it demonstrates how policy can address transparency within the whole pipeline of an AI system, from the data to the algorithm to the system outcome.

There are also some industry-specific standards for transparency that could act as a model for future cross-industry regulations. Under the Equal Credit Opportunity Act, creditors who deny loan applicants must provide a specific reason for the denial. This includes denials made by AI systems. The explanations for a denial come from a standardized list of numeric reason codes, such as: ``U4: Too many recently opened accounts with balances\footnote{\url{https://www.fico.com/en/latest-thinking/solution-sheet/us-fico-score-reason-codes}}.''

\subsection{European Union}

In 2019 the EU published a white paper titled ``Ethics Guidelines for Trustworthy AI,'' containing a legal framework that outlines ethical principles and legal obligations for EU member states to follow when deploying AI\footnote{\url{https://ec.europa.eu/digital-single-market/en/news/ethics-guidelines-trustworthy-ai}}. While the white paper is non-binding, it lays out expectations on how member-states should regulate the transparency of AI systems: ``... data, system and AI business models should be transparent. Traceability mechanisms can help achieving this. Moreover, AI systems and their decisions should be explained in a manner adapted to the stakeholder concerned. Humans need to be aware that they are interacting with an AI system, and must be informed of the systemâ€™s capabilities and limitations.''

Currently, the European Commission is reviewing the Artificial Intelligence Act\footnote{\url{https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX\%3A52021PC0206}}, which would create a common legal framework for governing all types of AI used in all non-military sectors in Europe. The directive takes the position that AI systems pose a significant risk to the health, safety and fundamental rights of persons, and governs from that perspective. With respect to transparency, the directive delineates between non-high-risk and high-risk AI systems (neither of which are rigorously defined at this time). It states that for ``non-high-risk AI systems, only very limited transparency obligations are imposed, for example in terms of the provision of information to flag the use of an AI system when interacting with humans.'' Yet, for high-risk systems, ``the requirements of high quality data, documentation and traceability, transparency, human oversight, accuracy and robustness, are strictly necessary to mitigate the risks to fundamental rights and safety posed by AI and that are not covered by other existing legal frameworks.'' Notably, as in the Algorithmic Accountability Act in the United States, the document contains explicit text mentioning recourse (referred to as ``redress'') for persons affected by AI systems.

The EU has also passed Regulation (EU) 2019/1150 that sets guidelines for the transparency of rankings for online search.\footnote{\url{https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX\%3A32019R1150}} In practice, this means that online stores and search engines should be required to disclose the algorithmic parameters used to rank goods and services on their site. The regulation also states that explanations about rankings should contain redress mechanisms for individuals and businesses affected by the rankings.

\subsubsection{Right to Explanation.}

The Right to Explanation is a proposed fundamental human right that would guarantee individuals access to an explanation for any AI system decision that affects them. The Right to Explanation was written into the EU's 2016 GDPR regulations, and reads as follows: ``[the data subject should have] the right ... to obtain an explanation of the decision reached.''\footnote{\url{https://www.privacy-regulation.eu/en/r71.htm}} The legal meaning and obligation of the text has been debated heavily by legal scholars, who are unsure under which circumstances it applies, what constitutes an explanation~\cite{DBLP:conf/fat/SelbstP18}, and %the flexibility of 
how the right is applicable to different AI systems~\cite{doshi2017accountability}. The Right to Explanation is an example of how emerging AI technologies may ``reveal'' additional rights that need to be considered by lawmakers and legal experts~\cite{10.1145/3306618.3314274}.

The EU's recently proposed Artificial Intelligence Act simultaneously reinforces the idea that explanations about AI systems are a human right, while slightly rolling back the Right to Explanation by acknowledging that there are both non-high-risk and high-risk AI systems. Discussions about the Right are likely to continue, and will be a central part of debates on regulating AI transparency. In fact, some local governing bodies have already taken steps to adopt the Right to Explanation. France passed the Digital Republic Act in 2016, which gives the Right to Explanation for individuals affected by an AI system in the public sector~\cite{edwards2018enslaving}. Hungary also has a similar law~\cite{malgieri2019automated}.

\subsection{Local}

There has been significant movement on the regulation of specific forms of AI systems at local levels of government. In response to the well-documented biases of facial recognition software when identifying people of different races and ethnicities~\cite{DBLP:conf/fat/BuolamwiniG18}, Washington State signed Senate Bill 6820 into law in 2020, which prohibits the use of facial recognition software in surveillance and limits its use in criminal investigation.\footnote{\url{https://app.leg.wa.gov/billsummary?BillNumber=6280&Initiative=false&Year=2019}} Detroit has also reacted to concerns about facial recognition, and its City Council approved legislation that mandates transparency and accountability for the procurement process of video and camera surveillance contracts used in the city.\footnote{\url{https://www.detroitnews.com/story/news/local/detroit-city/2021/05/25/detroit-council-approves-ordinance-boost-transparency-surveillance-camera-contracts/7433185002/}} The New York City Council recently regulated the use of AI systems in relation to employment decisions (Local Law 144 of 2021).\footnote{\url{https://legistar.council.nyc.gov/LegislationDetail.aspx?ID=4344524&GUID=B051915D-A9AC-451E-81F8-6596032FA3F9&Options=Advanced&Search}} The bill requires that AI tools for hiring employees be subject to yearly bias audits. An additional requirement is to notify job seekers that they were screened by a tool, and to disclose to them what ``qualifications or characteristics'' were used by the tool as basis of decisions. Finally, in the Netherlands, the municipality of Rotterdam has created a Data-Driven Working program which has been critical of transparency surrounding the algorithms used for fraud detection.\footnote{\url{https://nos.nl/artikel/2376810-rekenkamer-rotterdam-risico-op-vooringenomen-uitkomsten-door-gebruik-algoritmes}}


