\section{Introduction}
\label{sec:intro}

In the past decade, there has been widespread proliferation of artificial intelligence (AI) systems into the private and public sectors. These systems have been implemented in a broad range of contexts, including employment, healthcare, lending, criminal justice, and more. The rapid development and implementation of AI technologies has greatly outpaced public oversight, creating a ``wild-west''-style regulatory environment. As policy makers struggle to catch up, the issues of unregulated AI have become glaringly obvious, especially for underprivileged and marginalized communities. Famously, ProPublica revealed that the AI-driven system COMPAS used to assess the likelihood of a prisoner recidivating was highly discriminatory against black individuals~\cite{angwin2016machine}. In another example, Amazon  built and implemented an automated resume screening and hiring AI system--only to later find out that the system was biased against hiring women ~\cite{DBLP:journals/corr/abs-1909-03567}. In an effort to address these issues, countries around the world have begun regulating the use of AI systems. Over 50 nations and intergovernmental organizations have published AI strategies, actions plans, policy papers or directives ~\cite{unicri}. A survey of existing and proposed regulation around AI transparency is given in Section~\ref{sec:laws}.

Unfortunately, most strategies, directives and laws to date lack specificity on how AI regulation should be carried out \emph{in practice} by technologists. Where there is specificity, there is a lack of mechanisms for enforcing laws and holding institutions using AI accountable.  Documents on AI governance have focused on \emph{what} to do (or what not to do) with respect to AI, but leave the brunt of the work to practitioners to figure out \emph{how} things should be done~\cite{DBLP:journals/corr/abs-1906-11668}. This tension plays out heavily in regulations governing the transparency of AI systems (called ``explainability'' by AI practitioners). The most prominent example of this is the ``right to explanation'' of data use that is included in the EUâ€™s General Data Protection Regulation (GDPR). Despite being passed into law in 2016, the meaning and scope of the right is still being debated by legal scholars, with little of the discussion resulting in concrete benefits for citizens~\cite{DBLP:conf/fat/SelbstP18}.

While regulation can help weigh the benefits of new technology against the risks, developing  effective regulation is difficult, as is establishing effective mechanisms to comply with existing regulation. This paper aims to fill a gap in the existing literature by writing to technologists and AI practitioners about the existing AI regulatory landscape, and speaks to their role in designing complaint systems. We make a case for why AI practitioners should be leading efforts to ensure the transparency of AI systems, and to this end, we propose a novel framework for implementing regulatory-compliant explanations for stakeholders. We also consider an instantiation of our stakeholder-first approach in the context of a real-world example using work done by a national employment agency.

We make the following three contributions: (1) provide a survey of existing and proposed regulations on the transparency and explainability of AI systems; (2) propose a novel framework for a stakeholder-first approach to designing transparent AI systems; and (3) present a case-study that illustrates how this stakeholder-first approach could be used in practice.



