
% Journals

% First the Full Name is given, then the abbreviation used in the AMS Math
% Reviews, with an indication if it could not be found there.
% Note the 2nd overwrites the 1st, so swap them if you want the full name.

 %{AMS}
 @String{AMSTrans = "American Mathematical Society Translations" }
 @String{AMSTrans = "Amer. Math. Soc. Transl." }
 @String{BullAMS = "Bulletin of the American Mathematical Society" }
 @String{BullAMS = "Bull. Amer. Math. Soc." }
 @String{ProcAMS = "Proceedings of the American Mathematical Society" }
 @String{ProcAMS = "Proc. Amer. Math. Soc." }
 @String{TransAMS = "Transactions of the American Mathematical Society" }
 @String{TransAMS = "Trans. Amer. Math. Soc." }

 %ACM
 @String{CACM = "Communications of the {ACM}" }
 @String{CACM = "Commun. {ACM}" }
 @String{CompServ = "Comput. Surveys" }
 @String{JACM = "J. ACM" }
 @String{ACMMathSoft = "{ACM} Transactions on Mathematical Software" }
 @String{ACMMathSoft = "{ACM} Trans. Math. Software" }
 @String{SIGNUM = "{ACM} {SIGNUM} Newsletter" }
 @String{SIGNUM = "{ACM} {SIGNUM} Newslett." }

 @String{AmerSocio = "American Journal of Sociology" }
 @String{AmerStatAssoc = "Journal of the American Statistical Association" }
 @String{AmerStatAssoc = "J. Amer. Statist. Assoc." }
 @String{ApplMathComp = "Applied Mathematics and Computation" }
 @String{ApplMathComp = "Appl. Math. Comput." }
 @String{AmerMathMonthly = "American Mathematical Monthly" }
 @String{AmerMathMonthly = "Amer. Math. Monthly" }
 @String{BIT = "{BIT}" }
 @String{BritStatPsych = "British Journal of Mathematical and Statistical
          Psychology" }
 @String{BritStatPsych = "Brit. J. Math. Statist. Psych." }
 @String{CanMathBull = "Canadian Mathematical Bulletin" }
 @String{CanMathBull = "Canad. Math. Bull." }
 @String{CompApplMath = "Journal of Computational and Applied Mathematics" }
 @String{CompApplMath = "J. Comput. Appl. Math." }
 @String{CompPhys = "Journal of Computational Physics" }
 @String{CompPhys = "J. Comput. Phys." }
 @String{CompStruct = "Computers and Structures" }
 @String{CompStruct = "Comput. \& Structures" }
 @String{CompJour = "The Computer Journal" }
 @String{CompJour = "Comput. J." }
 @String{CompSysSci = "Journal of Computer and System Sciences" }
 @String{CompSysSci = "J. Comput. System Sci." }
 @String{Computing = "Computing" }
 @String{ContempMath = "Contemporary Mathematics" }
 @String{ContempMath = "Contemp. Math." }
 @String{Crelle = "Crelle's Journal" }
 @String{GiornaleMath = "Giornale di Mathematiche" }
 @String{GiornaleMath = "Giorn. Mat." } % didn't find in AMS MR., ibid.

 %IEEE
 @String{Computer = "{IEEE} Computer" }
 @String{IEEETransComp = "{IEEE} Transactions on Computers" }
 @String{IEEETransComp = "{IEEE} Trans. Comput." }
 @String{IEEETransAC = "{IEEE} Transactions on Automatic Control" }
 @String{IEEETransAC = "{IEEE} Trans. Automat. Control" }
 @String{IEEESpec = "{IEEE} Spectrum" } % didn't find in AMS MR
 @String{ProcIEEE = "Proceedings of the {IEEE}" }
 @String{ProcIEEE = "Proc. {IEEE}" } % didn't find in AMS MR
 @String{IEEETransAeroElec = "{IEEE} Transactions on Aerospace and Electronic
     Systems" }
 @String{IEEETransAeroElec = "{IEEE} Trans. Aerospace Electron. Systems" }

 @String{IMANumerAna = "{IMA} Journal of Numerical Analysis" }
 @String{IMANumerAna = "{IMA} J. Numer. Anal." }
 @String{InfProcLet = "Information Processing Letters" }
 @String{InfProcLet = "Inform. Process. Lett." }
 @String{InstMathApp = "Journal of the Institute of Mathematics and
     its Applications" }
 @String{InstMathApp = "J. Inst. Math. Appl." }
 @String{IntControl = "International Journal of Control" }
 @String{IntControl = "Internat. J. Control" }
 @String{IntNumerEng = "International Journal for Numerical Methods in
     Engineering" }
 @String{IntNumerEng = "Internat. J. Numer. Methods Engrg." }
 @String{IntSuper = "International Journal of Supercomputing Applications" }
 @String{IntSuper = "Internat. J. Supercomputing Applic." } % didn't find
%% in AMS MR
 @String{Kibernetika = "Kibernetika" }
 @String{JResNatBurStand = "Journal of Research of the National Bureau
     of Standards" }
 @String{JResNatBurStand = "J. Res. Nat. Bur. Standards" }
 @String{LinAlgApp = "Linear Algebra and its Applications" }
 @String{LinAlgApp = "Linear Algebra Appl." }
 @String{MathAnaAppl = "Journal of Mathematical Analysis and Applications" }
 @String{MathAnaAppl = "J. Math. Anal. Appl." }
 @String{MathAnnalen = "Mathematische Annalen" }
 @String{MathAnnalen = "Math. Ann." }
 @String{MathPhys = "Journal of Mathematical Physics" }
 @String{MathPhys = "J. Math. Phys." }
 @String{MathComp = "Mathematics of Computation" }
 @String{MathComp = "Math. Comp." }
 @String{MathScand = "Mathematica Scandinavica" }
 @String{MathScand = "Math. Scand." }
 @String{TablesAidsComp = "Mathematical Tables and Other Aids to Computation" }
 @String{TablesAidsComp = "Math. Tables Aids Comput." }
 @String{NumerMath = "Numerische Mathematik" }
 @String{NumerMath = "Numer. Math." }
 @String{PacificMath = "Pacific Journal of Mathematics" }
 @String{PacificMath = "Pacific J. Math." }
 @String{ParDistComp = "Journal of Parallel and Distributed Computing" }
 @String{ParDistComp = "J. Parallel and Distrib. Comput." } % didn't find
%% in AMS MR
 @String{ParComputing = "Parallel Computing" }
 @String{ParComputing = "Parallel Comput." }
 @String{PhilMag = "Philosophical Magazine" }
 @String{PhilMag = "Philos. Mag." }
 @String{ProcNAS = "Proceedings of the National Academy of Sciences
                    of the USA" }
 @String{ProcNAS = "Proc. Nat. Acad. Sci. U. S. A." }
 @String{Psychometrika = "Psychometrika" }
 @String{QuartMath = "Quarterly Journal of Mathematics, Oxford, Series (2)" }
 @String{QuartMath = "Quart. J. Math. Oxford Ser. (2)" }
 @String{QuartApplMath = "Quarterly of Applied Mathematics" }
 @String{QuartApplMath = "Quart. Appl. Math." }
 @String{RevueInstStat = "Review of the International Statisical Institute" }
 @String{RevueInstStat = "Rev. Inst. Internat. Statist." }

 %SIAM
 @String{JSIAM = "Journal of the Society for Industrial and Applied
     Mathematics" }
 @String{JSIAM = "J. Soc. Indust. Appl. Math." }
 @String{JSIAMB = "Journal of the Society for Industrial and Applied
     Mathematics, Series B, Numerical Analysis" }
 @String{JSIAMB = "J. Soc. Indust. Appl. Math. Ser. B Numer. Anal." }
 @String{SIAMAlgMeth = "{SIAM} Journal on Algebraic and Discrete Methods" }
 @String{SIAMAlgMeth = "{SIAM} J. Algebraic Discrete Methods" }
 @String{SIAMAppMath = "{SIAM} Journal on Applied Mathematics" }
 @String{SIAMAppMath = "{SIAM} J. Appl. Math." }
 @String{SIAMComp = "{SIAM} Journal on Computing" }
 @String{SIAMComp = "{SIAM} J. Comput." }
 @String{SIAMMatrix = "{SIAM} Journal on Matrix Analysis and Applications" }
 @String{SIAMMatrix = "{SIAM} J. Matrix Anal. Appl." }
 @String{SIAMNumAnal = "{SIAM} Journal on Numerical Analysis" }
 @String{SIAMNumAnal = "{SIAM} J. Numer. Anal." }
 @String{SIAMReview = "{SIAM} Review" }
 @String{SIAMReview = "{SIAM} Rev." }
 @String{SIAMSciStat = "{SIAM} Journal on Scientific and Statistical
     Computing" }
 @String{SIAMSciStat = "{SIAM} J. Sci. Statist. Comput." }

 @String{SoftPracExp = "Software Practice and Experience" }
 @String{SoftPracExp = "Software Prac. Experience" } % didn't find in AMS MR
 @String{StatScience = "Statistical Science" }
 @String{StatScience = "Statist. Sci." }
 @String{Techno = "Technometrics" }
 @String{USSRCompMathPhys = "{USSR} Computational Mathematics and Mathematical
     Physics" }
 @String{USSRCompMathPhys = "{U. S. S. R.} Comput. Math. and Math. Phys." }
 @String{VLSICompSys = "Journal of {VLSI} and Computer Systems" }
 @String{VLSICompSys = "J. {VLSI} Comput. Syst." }
 @String{ZAngewMathMech = "Zeitschrift fur Angewandte Mathematik und
     Mechanik" }
 @String{ZAngewMathMech = "Z. Angew. Math. Mech." }
 @String{ZAngewMathPhys = "Zeitschrift fur Angewandte Mathematik und Physik" }
 @String{ZAngewMathPhys = "Z. Angew. Math. Phys." }

% Publishers % ================================================= |

 @String{Academic = "Academic Press" }
 @String{ACMPress = "{ACM} Press" }
 @String{AdamHilger = "Adam Hilger" }
 @String{AddisonWesley = "Addison-Wesley" }
 @String{AllynBacon = "Allyn and Bacon" }
 @String{AMS = "American Mathematical Society" }
 @String{Birkhauser = "Birkha{\"u}ser" }
 @String{CambridgePress = "Cambridge University Press" }
 @String{Chelsea = "Chelsea" }
 @String{ClaredonPress = "Claredon Press" }
 @String{DoverPub = "Dover Publications" }
 @String{Eyolles = "Eyolles" }
 @String{HoltRinehartWinston = "Holt, Rinehart and Winston" }
 @String{Interscience = "Interscience" }
 @String{JohnsHopkinsPress = "The Johns Hopkins University Press" }
 @String{JohnWileySons = "John Wiley and Sons" }
 @String{Macmillan = "Macmillan" }
 @String{MathWorks = "The Math Works Inc." }
 @String{McGrawHill = "McGraw-Hill" }
 @String{NatBurStd = "National Bureau of Standards" }
 @String{NorthHolland = "North-Holland" }
 @String{OxfordPress = "Oxford University Press" }  %address Oxford or London?
 @String{PergamonPress = "Pergamon Press" }
 @String{PlenumPress = "Plenum Press" }
 @String{PrenticeHall = "Prentice-Hall" }
 @String{SIAMPub = "{SIAM} Publications" }
 @String{Springer = "Springer-Verlag" }
 @String{TexasPress = "University of Texas Press" }
 @String{VanNostrand = "Van Nostrand" }
 @String{WHFreeman = "W. H. Freeman and Co." }
 
@article{rakova2020responsible,
  title={Where Responsible AI meets Reality: Practitioner Perspectives on Enablers for shifting Organizational Practices},
  author={Rakova, Bogdana and Yang, Jingying and Cramer, Henriette and Chowdhury, Rumman},
  journal={arXiv preprint arXiv:2006.12358},
  year={2020}
}

@article{DBLP:journals/corr/abs-1906-11668,
  author    = {Anna Jobin and
               Marcello Ienca and
               Effy Vayena},
  title     = {Artificial Intelligence: the global landscape of ethics guidelines},
  journal   = {CoRR},
  volume    = {abs/1906.11668},
  year      = {2019},
  url       = {http://arxiv.org/abs/1906.11668},
  archivePrefix = {arXiv},
  eprint    = {1906.11668},
  timestamp = {Mon, 01 Jul 2019 13:00:07 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1906-11668.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/fat/BuolamwiniG18,
  author    = {Joy Buolamwini and
               Timnit Gebru},
  editor    = {Sorelle A. Friedler and
               Christo Wilson},
  title     = {Gender Shades: Intersectional Accuracy Disparities in Commercial Gender
               Classification},
  booktitle = {Conference on Fairness, Accountability and Transparency, {FAT} 2018,
               23-24 February 2018, New York, NY, {USA}},
  series    = {Proceedings of Machine Learning Research},
  volume    = {81},
  pages     = {77--91},
  publisher = {{PMLR}},
  year      = {2018},
  url       = {http://proceedings.mlr.press/v81/buolamwini18a.html},
  timestamp = {Wed, 03 Apr 2019 18:17:20 +0200},
  biburl    = {https://dblp.org/rec/conf/fat/BuolamwiniG18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{angwin2016machine,
  title={Machine bias. ProPublica},
  author={Angwin, Julia and Larson, Jeff and Mattu, Surya and Kirchner, Lauren},
  journal={See https://www. propublica. org/article/machine-bias-risk-assessments-in-criminal-sentencing},
  year={2016}
} 

@article{KUZIEMSKI2020101976,
title = "AI governance in the public sector: Three tales from the frontiers of automated decision-making in democratic settings",
journal = "Telecommunications Policy",
volume = "44",
number = "6",
pages = "101976",
year = "2020",
note = "Artificial intelligence, economy and society",
issn = "0308-5961",
doi = "https://doi.org/10.1016/j.telpol.2020.101976",
url = "http://www.sciencedirect.com/science/article/pii/S0308596120300689",
author = "Maciej Kuziemski and Gianluca Misuraca",
keywords = "Artificial intelligence, Public sector innovation, Automated decision making, Algorithmic accountability",
abstract = "The rush to understand new socio-economic contexts created by the wide adoption of AI is justified by its far-ranging consequences, spanning almost every walk of life. Yet, the public sector's predicament is a tragic double bind: its obligations to protect citizens from potential algorithmic harms are at odds with the temptation to increase its own efficiency - or in other words - to govern algorithms, while governing by algorithms. Whether such dual role is even possible, has been a matter of debate, the challenge stemming from algorithms' intrinsic properties, that make them distinct from other digital solutions, long embraced by the governments, create externalities that rule-based programming lacks. As the pressures to deploy automated decision making systems in the public sector become prevalent, this paper aims to examine how the use of AI in the public sector in relation to existing data governance regimes and national regulatory practices can be intensifying existing power asymmetries. To this end, investigating the legal and policy instruments associated with the use of AI for strenghtening the immigration process control system in Canada; “optimising” the employment services” in Poland, and personalising the digital service experience in Finland, the paper advocates for the need of a common framework to evaluate the potential impact of the use of AI in the public sector. In this regard, it discusses the specific effects of automated decision support systems on public services and the growing expectations for governments to play a more prevalent role in the digital society and to ensure that the potential of technology is harnessed, while negative effects are controlled and possibly avoided. This is of particular importance in light of the current COVID-19 emergency crisis where AI and the underpinning regulatory framework of data ecosystems, have become crucial policy issues as more and more innovations are based on large scale data collections from digital devices, and the real-time accessibility of information and services, contact and relationships between institutions and citizens could strengthen – or undermine - trust in governance systems and democracy."
}

@article{DBLP:journals/corr/abs-2001-09734,
  author    = {Kacper Sokol and
               Peter A. Flach},
  title     = {One Explanation Does Not Fit All: The Promise of Interactive Explanations
               for Machine Learning Transparency},
  journal   = {CoRR},
  volume    = {abs/2001.09734},
  year      = {2020},
  url       = {https://arxiv.org/abs/2001.09734},
  archivePrefix = {arXiv},
  eprint    = {2001.09734},
  timestamp = {Thu, 30 Jan 2020 18:46:36 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2001-09734.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/fat/SelbstP18,
  author    = {Andrew Selbst and
               Julia Powles},
  editor    = {Sorelle A. Friedler and
               Christo Wilson},
  title     = {"Meaningful Information" and the Right to Explanation},
  booktitle = {Conference on Fairness, Accountability and Transparency, {FAT} 2018,
               23-24 February 2018, New York, NY, {USA}},
  series    = {Proceedings of Machine Learning Research},
  volume    = {81},
  pages     = {48},
  publisher = {{PMLR}},
  year      = {2018},
  url       = {http://proceedings.mlr.press/v81/selbst18a.html},
  timestamp = {Wed, 03 Apr 2019 18:17:20 +0200},
  biburl    = {https://dblp.org/rec/conf/fat/SelbstP18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{reed2018,
  author = {C. Reed},
  title = {How should we regulate artificial intelligence?},
  journal = {Trans. R. Soc. A},
  volume = {376},
  year = {2018},
  url = {http://dx.doi.org/10.1098/rsta.2017.0360}
}

% Cite this article: Reed C. 2018 How should we regulate artificial intelligence?Phil. Trans. R. Soc. A 376: 20170360. http://dx.doi.org/10.1098/rsta.2017.0360

@article{1894_showreel,
    author = {Julia Stoyanovich and Steven Kuyan and Meghan McDermott and Maria Grillo and Mona Sloane},
    title = {Public Engagement Showreel, Int 1894},
    journal = {NYU Center for Responsible AI},
    url = {https://dataresponsibly.github.io/documents/Bill1894Showreel.pdf},
    year = {2020},
    month = {11},
    day = {12},
    keywords ={public},
    author+an={1=self}
}

@article{DBLP:journals/pvldb/StoyanovichHJ20,
  author    = {Julia Stoyanovich and Bill Howe and H.V. Jagadish},
  title     = {Responsible Data Management},
  journal   = {PVLDB},
  year      = {2020},
  volume    = {13},
  number    = {12},
  pages     = {3474-3489},
  doi       = {10.14778/3415478.3415570},
  keywords  = {invited,selected},
  author+an = {1=self},
  addendum  = {paper accompanying keynote presentation at the 46th International Conference on Very Large Data Bases, {VLDB}}
}


@inproceedings{DBLP:conf/aies/KrafftYKHB20,
  author    = {P. M. Krafft and
               Meg Young and
               Michael A. Katell and
               Karen Huang and
               Ghislain Bugingo},
  editor    = {Annette N. Markham and
               Julia Powles and
               Toby Walsh and
               Anne L. Washington},
  title     = {Defining {AI} in Policy versus Practice},
  booktitle = {{AIES} '20: {AAAI/ACM} Conference on AI, Ethics, and Society, New
               York, NY, USA, February 7-8, 2020},
  pages     = {72--78},
  publisher = {{ACM}},
  year      = {2020},
  url       = {https://doi.org/10.1145/3375627.3375835},
  doi       = {10.1145/3375627.3375835},
  timestamp = {Fri, 08 Jan 2021 08:52:12 +0100},
  biburl    = {https://dblp.org/rec/conf/aies/KrafftYKHB20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{molnar2019,
  title      = {Interpretable Machine Learning},
  author     = {Christoph Molnar},
  note       = {\url{https://christophm.github.io/interpretable-ml-book/}},
  year       = {2019},
  subtitle   = {A Guide for Making Black Box Models Explainable}
}

@article{DBLP:journals/corr/abs-2010-14374,
  author    = {Kasun Amarasinghe and
               Kit T. Rodolfa and
               Hemank Lamba and
               Rayid Ghani},
  title     = {Explainable Machine Learning for Public Policy: Use Cases, Gaps, and
               Research Directions},
  journal   = {CoRR},
  volume    = {abs/2010.14374},
  year      = {2020},
  url       = {https://arxiv.org/abs/2010.14374},
  archivePrefix = {arXiv},
  eprint    = {2010.14374},
  timestamp = {Mon, 02 Nov 2020 18:17:09 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2010-14374.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{meske,
author = {Meske, Christian and Bunde, Enrico and Schneider, Johannes and Gersch, Martin},
year = {2020},
month = {12},
pages = {},
title = {Explainable Artificial Intelligence: Objectives, Stakeholders and Future Research Opportunities},
journal = {Information Systems Management},
doi = {10.1080/10580530.2020.1849465}
}

@inproceedings{DBLP:conf/chi/LiaoGM20,
  author    = {Q. Vera Liao and
               Daniel M. Gruen and
               Sarah Miller},
  editor    = {Regina Bernhaupt and
               Florian 'Floyd' Mueller and
               David Verweij and
               Josh Andres and
               Joanna McGrenere and
               Andy Cockburn and
               Ignacio Avellino and
               Alix Goguey and
               Pernille Bj{\o}n and
               Shengdong Zhao and
               Briane Paul Samson and
               Rafal Kocielnik},
  title     = {Questioning the {AI:} Informing Design Practices for Explainable {AI}
               User Experiences},
  booktitle = {{CHI} '20: {CHI} Conference on Human Factors in Computing Systems,
               Honolulu, HI, USA, April 25-30, 2020},
  pages     = {1--15},
  publisher = {{ACM}},
  year      = {2020},
  url       = {https://doi.org/10.1145/3313831.3376590},
  doi       = {10.1145/3313831.3376590},
  timestamp = {Fri, 25 Dec 2020 01:14:19 +0100},
  biburl    = {https://dblp.org/rec/conf/chi/LiaoGM20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/Miller17a,
  author    = {Tim Miller},
  title     = {Explanation in Artificial Intelligence: Insights from the Social Sciences},
  journal   = {CoRR},
  volume    = {abs/1706.07269},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.07269},
  archivePrefix = {arXiv},
  eprint    = {1706.07269},
  timestamp = {Mon, 13 Aug 2018 16:47:22 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/Miller17a.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/jmlr/AryaBCDHHHLLMMP20,
  author    = {Vijay Arya and
               Rachel K. E. Bellamy and
               Pin{-}Yu Chen and
               Amit Dhurandhar and
               Michael Hind and
               Samuel C. Hoffman and
               Stephanie Houde and
               Q. Vera Liao and
               Ronny Luss and
               Aleksandra Mojsilovic and
               Sami Mourad and
               Pablo Pedemonte and
               Ramya Raghavendra and
               John T. Richards and
               Prasanna Sattigeri and
               Karthikeyan Shanmugam and
               Moninder Singh and
               Kush R. Varshney and
               Dennis Wei and
               Yunfeng Zhang},
  title     = {{AI} Explainability 360: An Extensible Toolkit for Understanding Data
               and Machine Learning Models},
  journal   = {J. Mach. Learn. Res.},
  volume    = {21},
  pages     = {130:1--130:6},
  year      = {2020},
  url       = {http://jmlr.org/papers/v21/19-1035.html},
  timestamp = {Wed, 18 Nov 2020 15:58:12 +0100},
  biburl    = {https://dblp.org/rec/journals/jmlr/AryaBCDHHHLLMMP20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{yang2020fairness,
  title={Fairness-aware instrumentation of preprocessing pipelines for machine learning},
  author={Yang, Ke and Huang, Biao and Stoyanovich, Julia and Schelter, Sebastian},
  booktitle={HILDA workshop at SIGMOD},
  year={2020}
}

@article{zhang2019should,
  title={" Why Should You Trust My Explanation?" Understanding Uncertainty in LIME Explanations},
  author={Zhang, Yujia and Song, Kuangyan and Sun, Yiming and Tan, Sarah and Udell, Madeleine},
  journal={arXiv preprint arXiv:1904.12991},
  year={2019}
}

@inproceedings{DBLP:conf/aies/SlackHJSL20,
  author    = {Dylan Slack and
               Sophie Hilgard and
               Emily Jia and
               Sameer Singh and
               Himabindu Lakkaraju},
  editor    = {Annette N. Markham and
               Julia Powles and
               Toby Walsh and
               Anne L. Washington},
  title     = {Fooling {LIME} and {SHAP:} Adversarial Attacks on Post hoc Explanation
               Methods},
  booktitle = {{AIES} '20: {AAAI/ACM} Conference on AI, Ethics, and Society, New
               York, NY, USA, February 7-8, 2020},
  pages     = {180--186},
  publisher = {{ACM}},
  year      = {2020},
  url       = {https://doi.org/10.1145/3375627.3375830},
  doi       = {10.1145/3375627.3375830},
  timestamp = {Mon, 24 Feb 2020 12:40:26 +0100},
  biburl    = {https://dblp.org/rec/conf/aies/SlackHJSL20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{platform2018tackling,
  title={Tackling Long-Term Unemployment through Risk Profiling and Outreach},
  author={Anette Scoppetta and Arthur Buckenleib.},
  year={2018}
}

@article{loxha2014profiling,
  title={Profiling the Unemployed: a review of OECD experiences and implications for emerging economies},
  author={Loxha, Artan and Morgandi, Matteo},
  year={2014},
  publisher={World Bank Group, Washington, DC}
}

@inproceedings{riipinen2011risk,
  title={Risk profiling of long-term unemployment in Finland},
  author={Riipinen, T},
  booktitle={Power Point presentation at the European Commission’s “PES to PES Dialogue Dissemination Conference,” Brussels, September},
  pages={8--9},
  year={2011}
}

@article{caswell2010unemployed,
  title={Unemployed citizen or ‘at risk’client? Classification systems and employment services in Denmark and Australia},
  author={Caswell, Dorte and Marston, Greg and Larsen, J{\o}rgen Elm},
  journal={Critical Social Policy},
  volume={30},
  number={3},
  pages={384--404},
  year={2010},
  publisher={Sage Publications Sage UK: London, England}
}

@book{matty2013predicting,
  title={Predicting Likelihood of Long-term Unemployment: The Development of a UK Jobseekers' Classification Instrument},
  author={Matty, Simon},
  year={2013},
  publisher={Corporate Document Services}
}

@article{sztandar2018changing,
  title={Changing social citizenship through information technology},
  author={Sztandar-Sztanderska, Karolina and Zielenska, Marianna},
  journal={Social Work \& Society},
  volume={16},
  number={2},
  year={2018}
}

@article{raso2017displacement,
  title={Displacement as regulation: New regulatory technologies and front-line decision-making in Ontario works},
  author={Raso, Jennifer},
  journal={Canadian Journal of Law and Society},
  volume={32},
  number={1},
  pages={75--95},
  year={2017},
  publisher={Cambridge University Press}
}

@article{wagner2019liable,
  title={Liable, but not in control? Ensuring meaningful human agency in automated decision-making systems},
  author={Wagner, Ben},
  journal={Policy \& Internet},
  volume={11},
  number={1},
  pages={104--122},
  year={2019},
  publisher={Wiley Online Library}
}

@article{gillingham2019can,
  title={Can predictive algorithms assist decision-making in social work with children and families?},
  author={Gillingham, Philip},
  journal={Child abuse review},
  volume={28},
  number={2},
  pages={114--126},
  year={2019},
  publisher={Wiley Online Library}
}

@article{gill2020policy,
  title={Policy Approaches to Artificial Intelligence Based Technologies in China, European Union and the United States},
  author={Gill, Indermit S},
  year={2020},
  publisher={Duke Global Working Paper Series}
}

@article{narayanan2018humans,
  title={How do humans understand explanations from machine learning systems? an evaluation of the human-interpretability of explanation},
  author={Narayanan, Menaka and Chen, Emily and He, Jeffrey and Kim, Been and Gershman, Sam and Doshi-Velez, Finale},
  journal={arXiv preprint arXiv:1802.00682},
  year={2018}
}

@article{unicri,
title = {Towards Responsible Artificial Intelligence Innovation},
author = {UNICRI},
year = {2020},
url = {http://www.unicri.it/index.php/topics/ai_robotics}
}

@article{DBLP:journals/corr/abs-1909-03567,
  author    = {Andi Peng and
               Besmira Nushi and
               Emre Kiciman and
               Kori Inkpen and
               Siddharth Suri and
               Ece Kamar},
  title     = {What You See Is What You Get? The Impact of Representation Criteria
               on Human Bias in Hiring},
  journal   = {CoRR},
  volume    = {abs/1909.03567},
  year      = {2019},
  url       = {http://arxiv.org/abs/1909.03567},
  archivePrefix = {arXiv},
  eprint    = {1909.03567},
  timestamp = {Tue, 17 Sep 2019 11:23:44 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1909-03567.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/softcomp/BekriKH19,
  author    = {Nadia El Bekri and
               Jasmin Kling and
               Marco F. Huber},
  editor    = {Francisco Mart{\'{\i}}nez{-}{\'{A}}lvarez and
               Alicia Troncoso Lora and
               Jos{\'{e}} Ant{\'{o}}nio S{\'{a}}ez Mu{\~{n}}oz and
               H{\'{e}}ctor Quinti{\'{a}}n and
               Emilio Corchado},
  title     = {A Study on Trust in Black Box Models and Post-hoc Explanations},
  booktitle = {14th International Conference on Soft Computing Models in Industrial
               and Environmental Applications {(SOCO} 2019) - Seville, Spain, May
               13-15, 2019, Proceedings},
  series    = {Advances in Intelligent Systems and Computing},
  volume    = {950},
  pages     = {35--46},
  publisher = {Springer},
  year      = {2019},
  url       = {https://doi.org/10.1007/978-3-030-20055-8\_4},
  doi       = {10.1007/978-3-030-20055-8\_4},
  timestamp = {Tue, 29 Dec 2020 18:31:06 +0100},
  biburl    = {https://dblp.org/rec/conf/softcomp/BekriKH19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{lipton2018mythos,
  title={The Mythos of Model Interpretability: In machine learning, the concept of interpretability is both important and slippery.},
  author={Lipton, Zachary C},
  journal={Queue},
  volume={16},
  number={3},
  pages={31--57},
  year={2018},
  publisher={ACM New York, NY, USA}
}

@article{doshi2017towards,
  title={Towards a rigorous science of interpretable machine learning},
  author={Doshi-Velez, Finale and Kim, Been},
  journal={arXiv preprint arXiv:1702.08608},
  year={2017}
}

@inproceedings{ventocilla2018towards,
  title={Towards a taxonomy for interpretable and interactive machine learning},
  author={Ventocilla, Elio and Helldin, Tove and Riveiro, Maria and Bae, Juhee and Boeva, Veselka and Falkman, G{\"o}ran and Lavesson, Niklas},
  booktitle={XAI Workshop on Explainable Artificial Intelligence},
  pages={151--157},
  year={2018}
}

@article{stoyanovich2016revealing,
  title={Revealing algorithmic rankers},
  author={Stoyanovich, Julia and Goodman, Ellen P},
  journal={Freedom to Tinker (August 5 2016)},
  year={2016}
}

@article{DBLP:journals/internet/GasserA17,
  author    = {Urs Gasser and
               Virg{\'{\i}}lio A. F. Almeida},
  title     = {A Layered Model for {AI} Governance},
  journal   = {{IEEE} Internet Comput.},
  volume    = {21},
  number    = {6},
  pages     = {58--62},
  year      = {2017},
  url       = {https://doi.org/10.1109/MIC.2017.4180835},
  doi       = {10.1109/MIC.2017.4180835},
  timestamp = {Mon, 26 Oct 2020 09:03:54 +0100},
  biburl    = {https://dblp.org/rec/journals/internet/GasserA17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{wachter2017transparent,
  title={Transparent, explainable, and accountable AI for robotics},
  author={Wachter, Sandra and Mittelstadt, Brent and Floridi, Luciano},
  year={2017}
}

@article{gosiewska2019not,
  title={Do not trust additive explanations},
  author={Gosiewska, Alicja and Biecek, Przemyslaw},
  journal={arXiv preprint arXiv:1903.11420},
  year={2019}
}

@article{rudin2019stop,
  title={Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead},
  author={Rudin, Cynthia},
  journal={Nature Machine Intelligence},
  volume={1},
  number={5},
  pages={206--215},
  year={2019},
  publisher={Nature Publishing Group}
}

@article{DBLP:journals/corr/abs-2101-09429,
  author    = {Sheikh Rabiul Islam and
               William Eberle and
               Sheikh Khaled Ghafoor and
               Mohiuddin Ahmed},
  title     = {Explainable Artificial Intelligence Approaches: {A} Survey},
  journal   = {CoRR},
  volume    = {abs/2101.09429},
  year      = {2021},
  url       = {https://arxiv.org/abs/2101.09429},
  archivePrefix = {arXiv},
  eprint    = {2101.09429},
  timestamp = {Sat, 30 Jan 2021 18:02:51 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2101-09429.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/scai/AllahyariL11,
  author    = {Hiva Allahyari and
               Niklas Lavesson},
  editor    = {Anders Kofod{-}Petersen and
               Fredrik Heintz and
               Helge Langseth},
  title     = {User-oriented Assessment of Classification Model Understandability},
  booktitle = {Eleventh Scandinavian Conference on Artificial Intelligence, {SCAI}
               2011, Trondheim, Norway, May 24th - 26th, 2011},
  series    = {Frontiers in Artificial Intelligence and Applications},
  volume    = {227},
  pages     = {11--19},
  publisher = {{IOS} Press},
  year      = {2011},
  url       = {https://doi.org/10.3233/978-1-60750-754-3-11},
  doi       = {10.3233/978-1-60750-754-3-11},
  timestamp = {Fri, 19 May 2017 01:25:16 +0200},
  biburl    = {https://dblp.org/rec/conf/scai/AllahyariL11.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{rodolfa2020machine,
      title={Machine learning for public policy: Do we need to sacrifice accuracy to make models fair?}, 
      author={Kit T. Rodolfa and Hemank Lamba and Rayid Ghani},
      year={2020},
      eprint={2012.02972},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{DBLP:journals/debu/StoyanovichH19,
  author    = {Julia Stoyanovich and
               Bill Howe},
  title     = {Nutritional Labels for Data and Models},
  journal   = {{IEEE} Data Eng. Bull.},
  volume    = {42},
  number    = {3},
  pages     = {13--23},
  year      = {2019},
  url       = {http://sites.computer.org/debull/A19sept/p13.pdf},
  timestamp = {Tue, 10 Mar 2020 16:23:50 +0100},
  biburl    = {https://dblp.org/rec/journals/debu/StoyanovichH19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{campos2011nutrition,
  title={Nutrition labels on pre-packaged foods: a systematic review},
  author={Campos, Sarah and Doxey, Juliana and Hammond, David},
  journal={Public health nutrition},
  volume={14},
  number={8},
  pages={1496--1506},
  year={2011},
  publisher={Cambridge University Press}
}

@inproceedings{ribeiro2016should,
  title={" Why should i trust you?" Explaining the predictions of any classifier},
  author={Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  booktitle={Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining},
  pages={1135--1144},
  year={2016}
}

@inproceedings{datta2016algorithmic,
  title={Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems},
  author={Datta, Anupam and Sen, Shayak and Zick, Yair},
  booktitle={2016 IEEE symposium on security and privacy (SP)},
  pages={598--617},
  year={2016},
  organization={IEEE}
}

@inproceedings{DBLP:conf/nips/LundbergL17,
  author    = {Scott M. Lundberg and
               Su{-}In Lee},
  editor    = {Isabelle Guyon and
               Ulrike von Luxburg and
               Samy Bengio and
               Hanna M. Wallach and
               Rob Fergus and
               S. V. N. Vishwanathan and
               Roman Garnett},
  title     = {A Unified Approach to Interpreting Model Predictions},
  booktitle = {Advances in Neural Information Processing Systems 30: Annual Conference
               on Neural Information Processing Systems 2017, December 4-9, 2017,
               Long Beach, CA, {USA}},
  pages     = {4765--4774},
  year      = {2017},
  url       = {https://proceedings.neurips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html},
  timestamp = {Thu, 21 Jan 2021 15:15:21 +0100},
  biburl    = {https://dblp.org/rec/conf/nips/LundbergL17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/fat/SokolF20,
  author    = {Kacper Sokol and
               Peter A. Flach},
  editor    = {Mireille Hildebrandt and
               Carlos Castillo and
               Elisa Celis and
               Salvatore Ruggieri and
               Linnet Taylor and
               Gabriela Zanfir{-}Fortuna},
  title     = {Explainability fact sheets: a framework for systematic assessment
               of explainable approaches},
  booktitle = {FAT* '20: Conference on Fairness, Accountability, and Transparency,
               Barcelona, Spain, January 27-30, 2020},
  pages     = {56--67},
  publisher = {{ACM}},
  year      = {2020},
  url       = {https://doi.org/10.1145/3351095.3372870},
  doi       = {10.1145/3351095.3372870},
  timestamp = {Fri, 24 Jan 2020 19:41:57 +0100},
  biburl    = {https://dblp.org/rec/conf/fat/SokolF20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/chi/HohmanHCDD19,
  author    = {Fred Hohman and
               Andrew Head and
               Rich Caruana and
               Robert DeLine and
               Steven Mark Drucker},
  editor    = {Stephen A. Brewster and
               Geraldine Fitzpatrick and
               Anna L. Cox and
               Vassilis Kostakos},
  title     = {Gamut: {A} Design Probe to Understand How Data Scientists Understand
               Machine Learning Models},
  booktitle = {Proceedings of the 2019 {CHI} Conference on Human Factors in Computing
               Systems, {CHI} 2019, Glasgow, Scotland, UK, May 04-09, 2019},
  pages     = {579},
  publisher = {{ACM}},
  year      = {2019},
  url       = {https://doi.org/10.1145/3290605.3300809},
  doi       = {10.1145/3290605.3300809},
  timestamp = {Fri, 24 Jan 2020 16:59:38 +0100},
  biburl    = {https://dblp.org/rec/conf/chi/HohmanHCDD19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{gunaratne2017using,
  title={Using interactive “Nutrition labels” for financial products to assist decision making under uncertainty},
  author={Gunaratne, Junius and Nov, Oded},
  journal={Journal of the Association for Information Science and Technology},
  volume={68},
  number={8},
  pages={1836--1849},
  year={2017},
  publisher={Wiley Online Library}
}

@article{zejnilovic2020machine,
  title={Machine Learning Informed Decision-Making: A Field Intervention in Public Employment Service},
  author={Zejnilovic, Leid and Lavado, Susana and Soares, Carlos and Rituerto de Troya, {\'I}{\~n}igo and Bell, Andrew and Ghani, Rayid},
  journal={Available at SSRN 3715529},
  year={2020}
}

@article{zejnilovic2020algorithmic,
  title={Algorithmic Long-Term Unemployment Risk Assessment in Use: Counselors’ Perceptions and Use Practices},
  author={Zejnilovi{\'c}, Leid and Lavado, Susana and Mart{\'\i}nez de Rituerto de Troya, {\'I}{\~n}igo and Sim, Samantha and Bell, Andrew},
  journal={Global Perspectives},
  volume={1},
  number={1},
  year={2020},
  publisher={University of California Press}
}

@article{doshi2017accountability,
  title={Accountability of AI under the law: The role of explanation},
  author={Doshi-Velez, Finale and Kortz, Mason and Budish, Ryan and Bavitz, Chris and Gershman, Sam and O'Brien, David and Scott, Kate and Schieber, Stuart and Waldo, James and Weinberger, David and others},
  journal={arXiv preprint arXiv:1711.01134},
  year={2017}
}

@article{DBLP:journals/corr/abs-2012-01805,
  author    = {Ricards Marcinkevics and
               Julia E. Vogt},
  title     = {Interpretability and Explainability: {A} Machine Learning Zoo Mini-tour},
  journal   = {CoRR},
  volume    = {abs/2012.01805},
  year      = {2020},
  url       = {https://arxiv.org/abs/2012.01805},
  archivePrefix = {arXiv},
  eprint    = {2012.01805},
  timestamp = {Fri, 04 Dec 2020 12:07:23 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2012-01805.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{wilson2021building,
  title={Building and Auditing Fair Algorithms: A Case Study in Candidate Screening},
  author={Wilson, Christo and Ghosh, Avijit and Jiang, Shan and Mislove, Alan and Baker, Lewis and Szary, Janelle and Trindel, Kelly and Polli, Frida},
  year={2021}
}

@article{meyers2007street,
  title={Street-level bureaucrats and the implementation of public policy},
  author={Meyers, Marcia K and Vorsanger, Susan and Peters, B Guy and Pierre, Jon},
  journal={The handbook of public administration},
  pages={153--163},
  year={2007},
  publisher={sage Publications London, UK}
}

@article{tal2016blinded,
  title={Blinded with science: Trivial graphs and formulas increase ad persuasiveness and belief in product efficacy},
  author={Tal, Aner and Wansink, Brian},
  journal={Public Understanding of Science},
  volume={25},
  number={1},
  pages={117--125},
  year={2016},
  publisher={SAGE Publications Sage UK: London, England}
}

@inproceedings{pandey2015deceptive,
  title={How deceptive are deceptive visualizations? An empirical analysis of common distortion techniques},
  author={Pandey, Anshul Vikram and Rall, Katharina and Satterthwaite, Margaret L and Nov, Oded and Bertini, Enrico},
  booktitle={Proceedings of the 33rd annual acm conference on human factors in computing systems},
  pages={1469--1478},
  year={2015}
}

@article{DBLP:journals/tvcg/PandeyMNSB14,
  author    = {Anshul Vikram Pandey and
               Anjali Manivannan and
               Oded Nov and
               Margaret Satterthwaite and
               Enrico Bertini},
  title     = {The Persuasive Power of Data Visualization},
  journal   = {{IEEE} Trans. Vis. Comput. Graph.},
  volume    = {20},
  number    = {12},
  pages     = {2211--2220},
  year      = {2014},
  url       = {https://doi.org/10.1109/TVCG.2014.2346419},
  doi       = {10.1109/TVCG.2014.2346419},
  timestamp = {Wed, 14 Nov 2018 10:22:06 +0100},
  biburl    = {https://dblp.org/rec/journals/tvcg/PandeyMNSB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{wachter2017counterfactual,
  title={Counterfactual explanations without opening the black box: Automated decisions and the GDPR},
  author={Wachter, Sandra and Mittelstadt, Brent and Russell, Chris},
  journal={Harv. JL \& Tech.},
  volume={31},
  pages={841},
  year={2017},
  publisher={HeinOnline}
}

@inproceedings{ustun2019actionable,
  title={Actionable recourse in linear classification},
  author={Ustun, Berk and Spangher, Alexander and Liu, Yang},
  booktitle={Proceedings of the conference on fairness, accountability, and transparency},
  pages={10--19},
  year={2019}
}

@article{edwards2018enslaving,
  title={Enslaving the algorithm: From a “Right to an Explanation” to a “Right to Better Decisions”?},
  author={Edwards, Lilian and Veale, Michael},
  journal={IEEE Security \& Privacy},
  volume={16},
  number={3},
  pages={46--54},
  year={2018},
  publisher={IEEE}
}

@article{malgieri2019automated,
  title={Automated decision-making in the EU Member States: The right to explanation and other “suitable safeguards” in the national legislations},
  author={Malgieri, Gianclaudio},
  journal={Computer law \& security review},
  volume={35},
  number={5},
  pages={105327},
  year={2019},
  publisher={Elsevier}
}

@misc{bhatt2020explainable,
      title={Explainable Machine Learning in Deployment}, 
      author={Umang Bhatt and Alice Xiang and Shubham Sharma and Adrian Weller and Ankur Taly and Yunhan Jia and Joydeep Ghosh and Ruchir Puri and José M. F. Moura and Peter Eckersley},
      year={2020},
      eprint={1909.06342},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@misc{slack2020fooling,
      title={Fooling LIME and SHAP: Adversarial Attacks on Post hoc Explanation Methods}, 
      author={Dylan Slack and Sophie Hilgard and Emily Jia and Sameer Singh and Himabindu Lakkaraju},
      year={2020},
      eprint={1911.02508},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{guidotti2018survey,
  author    = {Riccardo Guidotti and
               Anna Monreale and
               Salvatore Ruggieri and
               Franco Turini and
               Fosca Giannotti and
               Dino Pedreschi},
  title     = {A Survey of Methods for Explaining Black Box Models},
  journal   = {{ACM} Comput. Surv.},
  volume    = {51},
  number    = {5},
  pages     = {93:1--93:42},
  year      = {2019},
  url       = {https://doi.org/10.1145/3236009},
  doi       = {10.1145/3236009},
  timestamp = {Sat, 08 Jan 2022 02:23:15 +0100},
  biburl    = {https://dblp.org/rec/journals/csur/GuidottiMRTGP19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2004-00668,
  author    = {Ian Covert and
               Scott M. Lundberg and
               Su{-}In Lee},
  title     = {DBLP:journals/corr/abs-2004-00668 Feature Contributions Through Additive Importance
               Measures},
  journal   = {CoRR},
  volume    = {abs/2004.00668},
  year      = {2020},
  url       = {https://arxiv.org/abs/2004.00668},
  eprinttype = {arXiv},
  eprint    = {2004.00668},
  timestamp = {Fri, 26 Nov 2021 16:33:35 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2004-00668.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Black_2020,
   title={FlipTest},
   ISBN={9781450369367},
   url={http://dx.doi.org/10.1145/3351095.3372845},
   DOI={10.1145/3351095.3372845},
   journal={Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
   publisher={ACM},
   author={Black, Emily and Yeom, Samuel and Fredrikson, Matt},
   year={2020},
   month={Jan}
}

@article{osti_10182459,
place = {Country unknown/Code not available}, title = {Fairness-Aware Instrumentation of Preprocessing~Pipelines for Machine Learning}, url = {https://par.nsf.gov/biblio/10182459}, DOI = {10.1145/3398730.3399194}, abstractNote = {Surfacing and mitigating bias in ML pipelines is a complex topic, with a dire need to provide system-level support to data scientists. Humans should be empowered to debug these pipelines, in order to control for bias and to improve data quality and representativeness. We propose fairDAGs, an open-source library that extracts directed acyclic graph (DAG) representations of the data flow in preprocessing pipelines for ML. The library subsequently instruments the pipelines with tracing and visualization code to capture changes in data distributions and identify distortions with respect to protected group membership as the data travels through the pipeline. We illustrate the utility of fairDAGs, with experiments on publicly available ML pipelines.}, journal = {Workshop on Human-In-the-Loop Data Analytics (HILDA'20)}, author = {Yang, Ke and Huang, Biao and Stoyanovich, Julia and Schelter, Sebastian}, }

@misc{marcinkevics2020interpretability,
      title={Interpretability and Explainability: A Machine Learning Zoo Mini-tour}, 
      author={Ričards Marcinkevičs and Julia E. Vogt},
      year={2020},
      eprint={2012.01805},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{saha2020measuring,
      title={Measuring Non-Expert Comprehension of Machine Learning Fairness Metrics}, 
      author={Debjani Saha and Candice Schumann and Duncan C. McElfresh and John P. Dickerson and Michelle L. Mazurek and Michael Carl Tschantz},
      year={2020},
      eprint={2001.00089},
      archivePrefix={arXiv},
      primaryClass={cs.CY}
}

@article{STUMPF2009639,
title = {Interacting meaningfully with machine learning systems: Three experiments},
journal = {International Journal of Human-Computer Studies},
volume = {67},
number = {8},
pages = {639-662},
year = {2009},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2009.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S1071581909000457},
author = {Simone Stumpf and Vidya Rajaram and Lida Li and Weng-Keen Wong and Margaret Burnett and Thomas Dietterich and Erin Sullivan and Jonathan Herlocker},
keywords = {Intelligent user interfaces, Rich feedback, Explanations, Machine learning},
abstract = {Although machine learning is becoming commonly used in today's software, there has been little research into how end users might interact with machine learning systems, beyond communicating simple “right/wrong” judgments. If the users themselves could work hand-in-hand with machine learning systems, the users’ understanding and trust of the system could improve and the accuracy of learning systems could be improved as well. We conducted three experiments to understand the potential for rich interactions between users and machine learning systems. The first experiment was a think-aloud study that investigated users’ willingness to interact with machine learning reasoning, and what kinds of feedback users might give to machine learning systems. We then investigated the viability of introducing such feedback into machine learning systems, specifically, how to incorporate some of these types of user feedback into machine learning systems, and what their impact was on the accuracy of the system. Taken together, the results of our experiments show that supporting rich interactions between users and machine learning systems is feasible for both user and machine. This shows the potential of rich human–computer collaboration via on-the-spot interactions as a promising direction for machine learning systems and users to collaboratively share intelligence.}
}

@article {Obermeyer447,
	author = {Obermeyer, Ziad and Powers, Brian and Vogeli, Christine and Mullainathan, Sendhil},
	title = {Dissecting racial bias in an algorithm used to manage the health of populations},
	volume = {366},
	number = {6464},
	pages = {447--453},
	year = {2019},
	doi = {10.1126/science.aax2342},
	publisher = {American Association for the Advancement of Science},
	abstract = {The U.S. health care system uses commercial algorithms to guide health decisions. Obermeyer et al. find evidence of racial bias in one widely used algorithm, such that Black patients assigned the same level of risk by the algorithm are sicker than White patients (see the Perspective by Benjamin). The authors estimated that this racial bias reduces the number of Black patients identified for extra care by more than half. Bias occurs because the algorithm uses health costs as a proxy for health needs. Less money is spent on Black patients who have the same level of need, and the algorithm thus falsely concludes that Black patients are healthier than equally sick White patients. Reformulating the algorithm so that it no longer uses costs as a proxy for needs eliminates the racial bias in predicting who needs extra care.Science, this issue p. 447; see also p. 421Health systems rely on commercial prediction algorithms to identify and help patients with complex health needs. We show that a widely used algorithm, typical of this industry-wide approach and affecting millions of patients, exhibits significant racial bias: At a given risk score, Black patients are considerably sicker than White patients, as evidenced by signs of uncontrolled illnesses. Remedying this disparity would increase the percentage of Black patients receiving additional help from 17.7 to 46.5\%. The bias arises because the algorithm predicts health care costs rather than illness, but unequal access to care means that we spend less money caring for Black patients than for White patients. Thus, despite health care cost appearing to be an effective proxy for health by some measures of predictive accuracy, large racial biases arise. We suggest that the choice of convenient, seemingly effective proxies for ground truth can be an important source of algorithmic bias in many contexts.},
	issn = {0036-8075},
	URL = {https://science.sciencemag.org/content/366/6464/447},
	eprint = {https://science.sciencemag.org/content/366/6464/447.full.pdf},
	journal = {Science}
}

@article{BARTLETT2021,
title = {Consumer-lending discrimination in the FinTech Era},
journal = {Journal of Financial Economics},
year = {2021},
issn = {0304-405X},
doi = {https://doi.org/10.1016/j.jfineco.2021.05.047},
url = {https://www.sciencedirect.com/science/article/pii/S0304405X21002403},
author = {Robert Bartlett and Adair Morse and Richard Stanton and Nancy Wallace},
keywords = {Discrimination, FinTech, GSE mortgages, Credit scoring, Algorithmic underwriting, Big-data lending, Platform loans, Statistical discrimination, Legitimate business necessity},
abstract = {U.S. fair-lending law prohibits lenders from making credit determinations that disparately affect minority borrowers if those determinations are based on characteristics unrelated to creditworthiness. Using an identification under this rule, we show risk-equivalent Latinx/Black borrowers pay significantly higher interest rates on GSE-securitized and FHA-insured loans, particularly in high-minority-share neighborhoods. We estimate these rate differences cost minority borrowers over $450 million yearly. FinTech lenders’ rate disparities were similar to those of non-Fintech lenders for GSE mortgages, but lower for FHA mortgages issued in 2009–2015 and for FHA refi mortgages issued in 2018–2019.}
}

@misc{holstein2021equity,
      title={Equity and Artificial Intelligence in Education: Will "AIEd" Amplify or Alleviate Inequities in Education?}, 
      author={Kenneth Holstein and Shayan Doroudi},
      year={2021},
      eprint={2104.12920},
      archivePrefix={arXiv},
      primaryClass={cs.HC}
}

@misc{baker_hawn_2021,
 title={Algorithmic Bias in Education},
 url={edarxiv.org/pbmvz},
 DOI={10.35542/osf.io/pbmvz},
 publisher={EdArXiv},
 author={Baker, Ryan S and Hawn, Aaron},
 year={2021},
 month={Mar}
}

@misc{hu_2020, title={Towards Fair Educational Data Mining: A Case Study on Detecting At-Risk Students.}, url={https://eric.ed.gov/?id=ED608050}, journal={International Educational Data Mining Society}, publisher={International Educational Data Mining Society. e-mail: admin@educationaldatamining.org; Web site: http://www.educationaldatamining.org}, author={Hu, Qian|Rangwala}, year={2020}, month={Jun}}

@inproceedings{Sapiezynski2017AcademicPP,
  title={Academic performance prediction in a gender-imbalanced environment},
  author={Piotr Sapiezynski and Valentin Kassarnig and Christo Wilson},
  year={2017}
}

@misc{lundberg2017unified,
      title={A Unified Approach to Interpreting Model Predictions}, 
      author={Scott Lundberg and Su-In Lee},
      year={2017},
      eprint={1705.07874},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{ribeiro2016why,
      title={"Why Should I Trust You?": Explaining the Predictions of Any Classifier}, 
      author={Marco Tulio Ribeiro and Sameer Singh and Carlos Guestrin},
      year={2016},
      eprint={1602.04938},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{ECOA1994,
  author    = {FDIC: Federal Deposit Insurance Corporation},
  title     = {IV. Fair Lending — Fair Lending Laws and Regulations},
  year      = {1994},
  url       = {https://www.fdic.gov/resources/supervision-and-examinations/consumer-compliance-examination-manual/documents/4/iv-1-1.pdf}
}

@article{lee2018detecting,
  title={Detecting racial bias in algorithms and machine learning},
  author={Lee, Nicol Turner},
  journal={Journal of Information, Communication and Ethics in Society},
  year={2018},
  publisher={Emerald Publishing Limited}
}

@article{decamp2020latent,
  title={Latent bias and the implementation of artificial intelligence in medicine},
  author={DeCamp, Matthew and Lindvall, Charlotta},
  journal={Journal of the American Medical Informatics Association},
  volume={27},
  number={12},
  pages={2020--2023},
  year={2020},
  publisher={Oxford University Press}
}


@article{DBLP:journals/corr/abs-2102-03054,
  author    = {Sahil Verma and
               Michael D. Ernst and
               Ren{\'{e}} Just},
  title     = {Removing biased data to improve fairness and accuracy},
  journal   = {CoRR},
  volume    = {abs/2102.03054},
  year      = {2021},
  url       = {https://arxiv.org/abs/2102.03054},
  eprinttype = {arXiv},
  eprint    = {2102.03054},
  timestamp = {Wed, 10 Feb 2021 15:24:31 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2102-03054.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{FHA1968,
  author    = {FDIC: Federal Deposit Insurance Corporation},
  title     = {Civil Rights Act of 1968},
  year      = {1968},
  url       = {https://www.fdic.gov/regulations/laws/rules/6000-1400.html}
}

@article{gunning2019xai,
  title={XAI—Explainable artificial intelligence},
  author={Gunning, David and Stefik, Mark and Choi, Jaesik and Miller, Timothy and Stumpf, Simone and Yang, Guang-Zhong},
  journal={Science Robotics},
  volume={4},
  number={37},
  year={2019},
  publisher={Science Robotics}
}

@article{doshivelez2017rigorous,
      title={Towards A Rigorous Science of Interpretable Machine Learning}, 
      author={Finale Doshi-Velez and Been Kim},
      year={2017},
      eprint={1702.08608},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@inproceedings{NIPS2016_5680522b,
 author = {Kim, Been and Khanna, Rajiv and Koyejo, Oluwasanmi O},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Examples are not enough, learn to criticize! Criticism for Interpretability},
 url = {https://proceedings.neurips.cc/paper/2016/file/5680522b8e2bb01943234bce7bf84534-Paper.pdf},
 volume = {29},
 year = {2016}
}

@misc{hara2016making,
      title={Making Tree Ensembles Interpretable}, 
      author={Satoshi Hara and Kohei Hayashi},
      year={2016},
      eprint={1606.05390},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{julia_angwin_2016, title={Machine Bias}, url={https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing}, journal={ProPublica}, author={Julia Angwin, Jeff Larson}, year={2016}, month={May}}

@article{Hong_2020,
   title={Human Factors in Model Interpretability: Industry Practices, Challenges, and Needs},
   volume={4},
   ISSN={2573-0142},
   url={http://dx.doi.org/10.1145/3392878},
   DOI={10.1145/3392878},
   number={CSCW1},
   journal={Proceedings of the ACM on Human-Computer Interaction},
   publisher={Association for Computing Machinery (ACM)},
   author={Hong, Sungsoo Ray and Hullman, Jessica and Bertini, Enrico},
   year={2020},
   month={May},
   pages={1–26}
}

@article{narayanan2018humans,
  title={How do humans understand explanations from machine learning systems? an evaluation of the human-interpretability of explanation},
  author={Narayanan, Menaka and Chen, Emily and He, Jeffrey and Kim, Been and Gershman, Sam and Doshi-Velez, Finale},
  journal={arXiv preprint arXiv:1802.00682},
  year={2018}
}

@inproceedings{zejnilovic2021machine,
  title={Machine Learning Informed Decision-Making with Interpreted Model’s Outputs: A Field Intervention},
  author={Zejnilovic, Leid and Lavado, Susana and Soares, Carlos and Mart{\'\i}nez De Rituerto De Troya, {\'I}{\~n}igo and Bell, Andrew and Ghani, Rayid},
  booktitle={Academy of Management Proceedings},
  volume={2021},
  number={1},
  pages={15424},
  year={2021},
  organization={Academy of Management Briarcliff Manor, NY 10510}
}

@article{10.1525/gp.2020.12908,
    author = {Zejnilović, Leid and Lavado, Susana and Martínez de Rituerto de Troya, Íñigo and Sim, Samantha and Bell, Andrew},
    title = "{Algorithmic Long-Term Unemployment Risk Assessment in Use: Counselors’ Perceptions and Use Practices}",
    journal = {Global Perspectives},
    volume = {1},
    number = {1},
    year = {2020},
    month = {06},
    abstract = "{The recent surge of interest in algorithmic decision-making among scholars across disciplines is associated with its potential to resolve the challenges common to administrative decision-making in the public sector, such as greater fairness and equal treatment of each individual, among others. However, algorithmic decision-making combined with human judgment may introduce new complexities with unclear consequences. This article offers evidence that contributes to the ongoing discussion about algorithmic decision-making and governance, contextualizing it within a public employment service. In particular, we discuss the use of a decision support system that employs an algorithm to assess individual risk of becoming long-term unemployed and that informs counselors to assign interventions accordingly. We study the human interaction with algorithms in this context using the lenses of human detachment from and attachment to decision-making. Employing a mixed-method research approach, we show the complexity of enacting the potentials of the data-driven decision-making in the context of a public agency.}",
    issn = {2575-7350},
    doi = {10.1525/gp.2020.12908},
    url = {https://doi.org/10.1525/gp.2020.12908},
    note = {12908},
    eprint = {https://online.ucpress.edu/gp/article-pdf/1/1/12908/462946/12908.pdf},
}

@article{huysmans2006using,
  title={Using rule extraction to improve the comprehensibility of predictive models},
  author={Huysmans, Johan and Baesens, Bart and Vanthienen, Jan},
  year={2006},
  publisher={KU Leuven KBI Working Paper}
}

@inproceedings{bell2019proactive,
  title={Proactive advising: a machine learning driven approach to vaccine hesitancy},
  author={Bell, Andrew and Rich, Alexander and Teng, Melisande and Ore{\v{s}}kovi{\'c}, Tin and Bras, Nuno B and Mestrinho, L{\'e}nia and Golubovic, Srdan and Pristas, Ivan and Zejnilovic, Leid},
  booktitle={2019 IEEE International Conference on Healthcare Informatics (ICHI)},
  pages={1--6},
  year={2019},
  organization={IEEE}
}

@article{stiglic2015comprehensible,
  title={Comprehensible predictive modeling using regularized logistic regression and comorbidity based features},
  author={Stiglic, Gregor and Povalej Brzan, Petra and Fijacko, Nino and Wang, Fei and Delibasic, Boris and Kalousis, Alexandros and Obradovic, Zoran},
  journal={PloS one},
  volume={10},
  number={12},
  pages={e0144439},
  year={2015},
  publisher={Public Library of Science San Francisco, CA USA}
}

@inproceedings{de2018predicting,
  title={Predicting, explaining, and understanding risk of long-term unemployment},
  author={de Troya, Inigo Martinez and Chen, Ruqian and Moraes, Laura O and Bajaj, Pranjal and Kupersmith, Jordan and Ghani, Rayid and Br{\'a}s, Nuno B and Zejnilovic, Leid},
  booktitle={NeurIPS Workshop on AI for Social Good},
  year={2018}
}

@article{lamba2021empirical,
  title={An Empirical Comparison of Bias Reduction Methods on Real-World Problems in High-Stakes Policy Settings},
  author={Lamba, Hemank and Rodolfa, Kit T and Ghani, Rayid},
  journal={ACM SIGKDD Explorations Newsletter},
  volume={23},
  number={1},
  pages={69--85},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@article{gleicher2016framework,
  title={A framework for considering comprehensibility in modeling},
  author={Gleicher, Michael},
  journal={Big data},
  volume={4},
  number={2},
  pages={75--88},
  year={2016},
  publisher={Mary Ann Liebert, Inc. 140 Huguenot Street, 3rd Floor New Rochelle, NY 10801 USA}
}

@article{wilde_2021, title={A recommendation and risk classification system for connecting rough sleepers to essential outreach services}, volume={3}, DOI={10.1017/dap.2020.23}, journal={Data and Policy}, publisher={Cambridge University Press}, author={Wilde, Harrison and Chen, Lucia L. and Nguyen, Austin and Kimpel, Zoe and Sidgwick, Joshua and De Unanue, Adolfo and Veronese, Davide and Mateen, Bilal and Ghani, Rayid and Vollmer, Sebastian and et al.}, year={2021}, pages={e2}}

@inproceedings{carton2016identifying,
  title={Identifying police officers at risk of adverse events},
  author={Carton, Samuel and Helsby, Jennifer and Joseph, Kenneth and Mahmud, Ayesha and Park, Youngsoo and Walsh, Joe and Cody, Crystal and Patterson, CPT Estella and Haynes, Lauren and Ghani, Rayid},
  booktitle={Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining},
  pages={67--76},
  year={2016}
}

@inproceedings{aguiar2015and,
  title={Who, when, and why: A machine learning approach to prioritizing students at risk of not graduating high school on time},
  author={Aguiar, Everaldo and Lakkaraju, Himabindu and Bhanpuri, Nasir and Miller, David and Yuhas, Ben and Addison, Kecia L},
  booktitle={Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
  pages={93--102},
  year={2015}
}

@article{holzinger2020measuring,
  title={Measuring the quality of explanations: the system causability scale (SCS)},
  author={Holzinger, Andreas and Carrington, Andr{\'e} and M{\"u}ller, Heimo},
  journal={KI-K{\"u}nstliche Intelligenz},
  pages={1--6},
  year={2020},
  publisher={Springer}
}

@article{aha1988instance,
  title={Instance-based prediction of heart-disease presence with the Cleveland database},
  author={Aha, D and Kibler, Dennis},
  journal={University of California},
  volume={3},
  number={1},
  pages={3--2},
  year={1988}
}

@misc{das2020opportunities,
      title={Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey}, 
      author={Arun Das and Paul Rad},
      year={2020},
      eprint={2006.11371},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{10.1093/jamia/ocz229,
    author = {Diprose, William K and Buist, Nicholas and Hua, Ning and Thurier, Quentin and Shand, George and Robinson, Reece},
    title = "{Physician understanding, explainability, and trust in a hypothetical machine learning risk calculator}",
    journal = {Journal of the American Medical Informatics Association},
    volume = {27},
    number = {4},
    pages = {592-600},
    year = {2020},
    month = {02},
    abstract = "{Implementation of machine learning (ML) may be limited by patients’ right to “meaningful information about the logic involved” when ML influences healthcare decisions. Given the complexity of healthcare decisions, it is likely that ML outputs will need to be understood and trusted by physicians, and then explained to patients. We therefore investigated the association between physician understanding of ML outputs, their ability to explain these to patients, and their willingness to trust the ML outputs, using various ML explainability methods.We designed a survey for physicians with a diagnostic dilemma that could be resolved by an ML risk calculator. Physicians were asked to rate their understanding, explainability, and trust in response to 3 different ML outputs. One ML output had no explanation of its logic (the control) and 2 ML outputs used different model-agnostic explainability methods. The relationships among understanding, explainability, and trust were assessed using Cochran-Mantel-Haenszel tests of association.The survey was sent to 1315 physicians, and 170 (13\\%) provided completed surveys. There were significant associations between physician understanding and explainability (P \\&lt; .001), between physician understanding and trust (P \\&lt; .001), and between explainability and trust (P \\&lt; .001). ML outputs that used model-agnostic explainability methods were preferred by 88\\% of physicians when compared with the control condition; however, no particular ML explainability method had a greater influence on intended physician behavior.Physician understanding, explainability, and trust in ML risk calculators are related. Physicians preferred ML outputs accompanied by model-agnostic explanations but the explainability method did not alter intended physician behavior.}",
    issn = {1527-974X},
    doi = {10.1093/jamia/ocz229},
    url = {https://doi.org/10.1093/jamia/ocz229},
    eprint = {https://academic.oup.com/jamia/article-pdf/27/4/592/34153285/ocz229.pdf},
}

@article{10.1002/isaf.1422,
author = {Preece, Alun},
title = {Asking ‘Why’ in AI: Explainability of intelligent systems – perspectives and challenges},
journal = {Intelligent Systems in Accounting, Finance and Management},
volume = {25},
number = {2},
pages = {63-72},
keywords = {artificial intelligence, explainability, interpretability, machine learning},
doi = {https://doi.org/10.1002/isaf.1422},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/isaf.1422},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/isaf.1422},
abstract = {Summary Recent rapid progress in machine learning (ML), particularly so-called ‘deep learning’, has led to a resurgence in interest in explainability of artificial intelligence (AI) systems, reviving an area of research dating back to the 1970s. The aim of this article is to view current issues concerning ML-based AI systems from the perspective of classical AI, showing that the fundamental problems are far from new, and arguing that elements of that earlier work offer routes to making progress towards explainable AI today.},
year = {2018}
}

@article{Ehsan_2021,
   title={Expanding Explainability: Towards Social Transparency in AI systems},
   ISBN={9781450380966},
   url={http://dx.doi.org/10.1145/3411764.3445188},
   DOI={10.1145/3411764.3445188},
   journal={Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
   publisher={ACM},
   author={Ehsan, Upol and Liao, Q. Vera and Muller, Michael and Riedl, Mark O. and Weisz, Justin D.},
   year={2021},
   month={May}
}

@inproceedings{abdul2020cogam,
  title={COGAM: Measuring and Moderating Cognitive Load in Machine Learning Model Explanations},
  author={Abdul, Ashraf and von der Weth, Christian and Kankanhalli, Mohan and Lim, Brian Y},
  booktitle={Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
  pages={1--14},
  year={2020}
}

@inproceedings{10.1145/1620545.1620576,
author = {Lim, Brian Y. and Dey, Anind K.},
title = {Assessing Demand for Intelligibility in Context-Aware Applications},
year = {2009},
isbn = {9781605584317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1620545.1620576},
doi = {10.1145/1620545.1620576},
abstract = {Intelligibility can help expose the inner workings and inputs of context-aware applications
that tend to be opaque to users due to their implicit sensing and actions. However,
users may not be interested in all the information that the applications can produce.
Using scenarios of four real-world applications that span the design space of context-aware
computing, we conducted two experiments to discover what information users are interested
in. In the first experiment, we elicit types of information demands that users have
and under what moderating circumstances they have them. In the second experiment,
we verify the findings by soliciting users about which types they would want to know
and establish whether receiving such information would satisfy them. We discuss why
users demand certain types of information, and provide design implications on how
to provide different intelligibility types to make context-aware applications intelligible
and acceptable to users.},
booktitle = {Proceedings of the 11th International Conference on Ubiquitous Computing},
pages = {195–204},
numpages = {10},
keywords = {explanations, context-aware, satisfaction, intelligibility},
location = {Orlando, Florida, USA},
series = {UbiComp '09}
}

@inbook{10.1145/1518701.1519023,
author = {Lim, Brian Y. and Dey, Anind K. and Avrahami, Daniel},
title = {Why and Why Not Explanations Improve the Intelligibility of Context-Aware Intelligent Systems},
year = {2009},
isbn = {9781605582467},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1518701.1519023},
abstract = {Context-aware intelligent systems employ implicit inputs, and make decisions based
on complex rules and machine learning models that are rarely clear to users. Such
lack of system intelligibility can lead to loss of user trust, satisfaction and acceptance
of these systems. However, automatically providing explanations about a system's decision
process can help mitigate this problem. In this paper we present results from a controlled
study with over 200 participants in which the effectiveness of different types of
explanations was examined. Participants were shown examples of a system's operation
along with various automatically generated explanations, and then tested on their
understanding of the system. We show, for example, that explanations describing why
the system behaved a certain way resulted in better understanding and stronger feelings
of trust. Explanations describing why the system did not behave a certain way, resulted
in lower understanding yet adequate performance. We discuss implications for the use
of our findings in real-world context-aware applications.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {2119–2128},
numpages = {10}
}

@book{10.5555/3208509,
author = {Eubanks, Virginia},
title = {Automating Inequality: How High-Tech Tools Profile, Police, and Punish the Poor},
year = {2018},
isbn = {1250074312},
publisher = {St. Martin's Press, Inc.},
address = {USA},
abstract = {Naomi Klein: "This book is downright scary."Ethan Zuckerman, MIT: "Should be required
reading."Dorothy Roberts, author of Killing the Black Body: "A must-read."Astra Taylor,
author of The People's Platform: "The single most important book about technology
you will read this year."Cory Doctorow: "Indispensable."A powerful investigative look
at data-based discriminationand how technology affects civil and human rights and
economic equity The State of Indiana denies one million applications for healthcare,
foodstamps and cash benefits in three yearsbecause a new computer system interprets
any mistake as failure to cooperate. In Los Angeles, an algorithm calculates the comparative
vulnerability of tens of thousands of homeless people in order to prioritize them
for an inadequate pool of housing resources. In Pittsburgh, a child welfare agency
uses a statistical model to try to predict which children might be future victims
of abuse or neglect. Since the dawn of the digital age, decision-making in finance,
employment, politics, health and human services has undergone revolutionary change.
Today, automated systemsrather than humanscontrol which neighborhoods get policed,
which families attain needed resources, and who is investigated for fraud. While we
all live under this new regime of data, the most invasive and punitive systems are
aimed at the poor. In Automating Inequality, Virginia Eubanks systematically investigates
the impacts of data mining, policy algorithms, and predictive risk models on poor
and working-class people in America. The book is full of heart-wrenching and eye-opening
stories, from a woman in Indiana whose benefits are literally cut off as she lays
dying to a family in Pennsylvania in daily fear of losing their daughter because they
fit a certain statistical profile. The U.S. has always used its most cutting-edge
science and technology to contain, investigate, discipline and punish the destitute.
Like the county poorhouse and scientific charity before them, digital tracking and
automated decision-making hide poverty from the middle-class public and give the nation
the ethical distance it needs to make inhumane choices: which families get food and
which starve, who has housing and who remains homeless, and which families are broken
up by the state. In the process, they weaken democracy and betray our most cherished
national values. This deeply researched and passionate book could not be more timely.}
}

@book{10.5555/3002861,
author = {O'Neil, Cathy},
title = {Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy},
year = {2016},
isbn = {0553418815},
publisher = {Crown Publishing Group},
address = {USA},
abstract = {A former Wall Street quant sounds an alarm on the mathematical models that pervade
modern life and threaten to rip apart our social fabricWe live in the age of the algorithm.
Increasingly, the decisions that affect our liveswhere we go to school, whether we
get a car loan, how much we pay for health insuranceare being made not by humans,
but by mathematical models. In theory, this should lead to greater fairness: Everyone
is judged according to the same rules, and bias is eliminated. But as Cathy ONeil
reveals in this urgent and necessary book, the opposite is true. The models being
used today are opaque, unregulated, and uncontestable, even when theyre wrong. Most
troubling, they reinforce discrimination: If a poor student cant get a loan because
a lending model deems him too risky (by virtue of his zip code), hes then cut off
from the kind of education that could pull him out of poverty, and a vicious spiral
ensues. Models are propping up the lucky and punishing the downtrodden, creating a
toxic cocktail for democracy. Welcome to the dark side of Big Data. Tracing the arc
of a persons life, ONeil exposes the black box models that shape our future, both
as individuals and as a society. These weapons of math destruction score teachers
and students, sort rsums, grant (or deny) loans, evaluate workers, target voters,
set parole, and monitor our health. ONeil calls on modelers to take more responsibility
for their algorithms and on policy makers to regulate their use. But in the end, its
up to us to become more savvy about the models that govern our lives. This important
book empowers us to ask the tough questions, uncover the truth, and demand change.}
}

@misc{chouldechova2016fair,
      title={Fair prediction with disparate impact: A study of bias in recidivism prediction instruments}, 
      author={Alexandra Chouldechova},
      year={2016},
      eprint={1610.07524},
      archivePrefix={arXiv},
      primaryClass={stat.AP}
}

@article{Goodman_2017,
   title={European Union Regulations on Algorithmic Decision-Making and a “Right to Explanation”},
   volume={38},
   ISSN={0738-4602},
   url={http://dx.doi.org/10.1609/aimag.v38i3.2741},
   DOI={10.1609/aimag.v38i3.2741},
   number={3},
   journal={AI Magazine},
   publisher={Association for the Advancement of Artificial Intelligence (AAAI)},
   author={Goodman, Bryce and Flaxman, Seth},
   year={2017},
   month={Oct},
   pages={50–57}
}

@inproceedings{Cortez2008UsingDM,
  title={Using data mining to predict secondary school student performance},
  author={P. Cortez and A. M. G. Silva},
  year={2008}
}

@article{OpenML2013,
author = {Vanschoren, Joaquin and van Rijn, Jan N. and Bischl, Bernd and Torgo, Luis},
title = {OpenML: Networked Science in Machine Learning},
journal = {SIGKDD Explorations},
volume = {15},
number = {2},
year = {2013},
pages = {49--60},
url = {http://doi.acm.org/10.1145/2641190.2641198},
doi = {10.1145/2641190.2641198},
publisher = {ACM},
address = {New York, NY, USA},
}

@article{miller2019explanation,
  title={Explanation in artificial intelligence: Insights from the social sciences},
  author={Miller, Tim},
  journal={Artificial intelligence},
  volume={267},
  pages={1--38},
  year={2019},
  publisher={Elsevier}
}

@book{molnar2020interpretable,
  title={Interpretable machine learning},
  author={Molnar, Christoph},
  year={2020},
  publisher={Lulu. com}
}

@inproceedings{yang2019study,
  title={A study on interaction in human-in-the-loop machine learning for text analytics},
  author={Yang, Yiwei and Kandogan, Eser and Li, Yunyao and Sen, Prithviraj and Lasecki, Walter S},
  booktitle={IUI Workshops},
  year={2019}
}

@article{ross2017impact,
  title={The impact of property tax appeals on vertical equity in Cook County, IL},
  author={Ross, Robert},
  journal={Univerity of Chicago, Harris School of Public Policy Working Paper},
  year={2017}
}

@article{amarasinghe2020explainable,
  title={Explainable machine learning for public policy: Use cases, gaps, and research directions},
  author={Amarasinghe, Kasun and Rodolfa, Kit and Lamba, Hemank and Ghani, Rayid},
  journal={arXiv preprint arXiv:2010.14374},
  year={2020}
}

@inproceedings{jesus2021can,
  title={How can I choose an explainer? An Application-grounded Evaluation of Post-hoc Explanations},
  author={Jesus, S{\'e}rgio and Bel{\'e}m, Catarina and Balayan, Vladimir and Bento, Jo{\~a}o and Saleiro, Pedro and Bizarro, Pedro and Gama, Jo{\~a}o},
  booktitle={Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
  pages={805--815},
  year={2021}
}

@article{fuster2020predictably,
  title={Predictably unequal? the effects of machine learning on credit markets},
  author={Fuster, Andreas and Goldsmith-Pinkham, Paul and Ramadorai, Tarun and Walther, Ansgar},
  journal={The Effects of Machine Learning on Credit Markets (October 1, 2020)},
  year={2020}
}

@article{dziugaite2020enforcing,
  title={Enforcing Interpretability and its Statistical Impacts: Trade-offs between Accuracy and Interpretability},
  author={Dziugaite, Gintare Karolina and Ben-David, Shai and Roy, Daniel M},
  journal={arXiv preprint arXiv:2010.13764},
  year={2020}
}

@inproceedings{barocas2020hidden,
  title={The hidden assumptions behind counterfactual explanations and principal reasons},
  author={Barocas, Solon and Selbst, Andrew D and Raghavan, Manish},
  booktitle={Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
  pages={80--89},
  year={2020}
}

@inproceedings{dai2021fair,
  title={Fair machine learning under partial compliance},
  author={Dai, Jessica and Fazelpour, Sina and Lipton, Zachary},
  booktitle={Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
  pages={55--65},
  year={2021}
}

@inproceedings{loi2021towards,
  title={Towards accountability in the use of artificial intelligence for public administrations},
  author={Loi, Michele and Spielkamp, Matthias},
  booktitle={Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
  pages={757--766},
  year={2021}
}

@article{lu2019good,
  title={Good explanation for algorithmic transparency},
  author={Lu, Joy and Lee, Dokyun and Kim, Tae Wan and Danks, David},
  journal={Available at SSRN 3503603},
  year={2019}
}

@article{jung2017simple,
  title={Simple rules for complex decisions},
  author={Jung, Jongbin and Concannon, Connor and Shroff, Ravi and Goel, Sharad and Goldstein, Daniel G},
  journal={arXiv preprint arXiv:1702.04690},
  year={2017}
}

@inproceedings{10.1145/3306618.3314274,
author = {Parker, Jack and Danks, David},
title = {How Technological Advances Can Reveal Rights},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314274},
doi = {10.1145/3306618.3314274},
abstract = {Over recent decades, technological development has been accompanied by the proposal of new rights by various groups and individuals: the right to public anonymity, the right to be forgotten, and the right to disconnect, for example. Although there is widespread acknowledgment of the motivation behind these proposed rights, there is little agreement about their actual normative status. One potential challenge is that the claims only arise in contingent social-technical contexts, which may affect how we conceive of them ethically (albeit, not necessarily in terms of policy). What sort of morally legitimate rights claims depend on such contingencies? Our paper investigates the grounds on which such proposals might be considered "actual" rights. The full paper can be found at http://www.andrew.cmu.edu/user/cgparker/Parker_Danks_RevealedRights.pdf. We propose the notion of a revealed right, a right that only imposes duties -- and thus is only meaningfully revealed -- in certain technological contexts. Our framework is based on an interest theory approach to rights, which understands rights in terms of a justificatory role: morally important aspects of a person's well-being (interests) ground rights, which then justify holding someone to a duty that promotes or protects that interest. Our framework uses this approach to interpret the conflicts that lead to revealed rights in terms of how technological developments cause shifts in the balance of power to promote particular interests. Different parties can have competing or conflicting interests. It is also generally accepted that some interests are more normatively important than others (even if only within a particular framework). We can refer to this difference in importance by saying that the former interest has less "moral weight" than the latter interest (in that context). The moral weight of an interest is connected to its contribution to the interest-holder's overall well-being, and thereby determines the strength of the reason that a corresponding right provides to justify a duty. Improved technology can offer resources that grant one party increased causal power to realize its interests to the detriment of another's capacity to do so, even while the relative moral weight of their interests remain the same. Such changes in circumstance can make the importance of protecting a particular interest newly salient. If that interest's moral weight justifies establishing a duty to protect it, thereby limiting the threat posed by the new socio-technical context, then a right is revealed. Revealed rights justify realignment between the moral weight and causal power orderings so that people with weightier interests have greater power to protect those interests. In the extended paper, we show how this account can be applied to the interpretation of two recently proposed "rights": the right to be forgotten, and the right to disconnect. Since we are focused on making sense of revealed rights, not any particular substantive theory of interests or well-being, the characterization of 'weights' is a free parameter in this account. Our framework alone cannot provide means to resolve the question of whether specific rights exist, but it can be used to identify empirical questions that need to be answered to decide the existence or non-existence of such rights. The emergence of a revealed right depends on a number of factors, including: whether the plausible uses of the technology could potentially impede another's well-being or interests; whether the technology is sufficiently common to have a wider, social impact; and whether the technology has actually changed the balance of power sufficiently to yield a frequent possibility for misalignment between causal power and moral weight. This approach confronts the question of how, in principle, such rights could be justified, without requiring specific commitments on the ontology of rights. Our account explains why the rhetoric of "new rights" is both accurate (since the rights were not previously recognized) and inaccurate (since the rights were present all along, but without corresponding duties). Further, it explains the rights without grounding their normative status in considerations related to right-holders' capacities to rationally waive or assert claims. This is especially important given that many of the relevant disruptive technological developments pose challenges to understanding by affected parties for the same reasons they pose threats to those parties' well-being. In the course of our discussion, we confront a number of potential objections to the account. We argue that our framework's ability to accommodate highly specific or derivative-seeming rights is un-problematic. We also head off worries that our use of interest theory makes the account likely to recognize absurd rights claims.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {201},
numpages = {1},
keywords = {rights, interest theory, right to be forgotten, right to disconnect},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@article{schmidt2020transparency,
  title={Transparency and trust in artificial intelligence systems},
  author={Schmidt, Philipp and Biessmann, Felix and Teubner, Timm},
  journal={Journal of Decision Systems},
  volume={29},
  number={4},
  pages={260--278},
  year={2020},
  publisher={Taylor \& Francis}
}

@article{nichols2013consequences,
  title={Consequences of long-term unemployment},
  author={Nichols, Austin and Mitchell, Josh and Lindner, Stephan},
  journal={Washington, DC: The Urban Institute},
  year={2013}
}

@inproceedings{cech2021tackling,
  title={Tackling Algorithmic Transparency in Communal Energy Accounting through Participatory Design},
  author={Cech, Florian},
  booktitle={C\&T'21: Proceedings of the 10th International Conference on Communities \& Technologies-Wicked Problems in the Age of Tech},
  pages={258--268},
  year={2021}
}

@inproceedings{eiband2018bringing,
  title={Bringing transparency design into practice},
  author={Eiband, Malin and Schneider, Hanna and Bilandzic, Mark and Fazekas-Con, Julian and Haug, Mareike and Hussmann, Heinrich},
  booktitle={23rd international conference on intelligent user interfaces},
  pages={211--223},
  year={2018}
}

@article{aizenberg2020designing,
  title={Designing for human rights in AI},
  author={Aizenberg, Evgeni and Van Den Hoven, Jeroen},
  journal={Big Data \& Society},
  volume={7},
  number={2},
  pages={2053951720949566},
  year={2020},
  publisher={SAGE Publications Sage UK: London, England}
}

@article{gupta2020participatory,
  title={Participatory Design to build better contact-and proximity-tracing apps},
  author={Gupta, Abhishek and De Gasperis, Tania},
  journal={arXiv preprint arXiv:2006.00432},
  year={2020}
}

@article{preece2018stakeholders,
  title={Stakeholders in explainable AI},
  author={Preece, Alun and Harborne, Dan and Braines, Dave and Tomsett, Richard and Chakraborty, Supriyo},
  journal={arXiv preprint arXiv:1810.00184},
  year={2018}
}

@article{richards2021human,
  title={A Human-Centered Methodology for Creating AI FactSheets.},
  author={Richards, John T and Piorkowski, David and Hind, Michael and Houde, Stephanie and Mojsilovic, Aleksandra and Varshney, Kush R},
  journal={IEEE Data Eng. Bull.},
  volume={44},
  number={4},
  pages={47--58},
  year={2021}
}

@article{hind2019explaining,
  title={Explaining explainable AI},
  author={Hind, Michael},
  journal={XRDS: Crossroads, The ACM Magazine for Students},
  volume={25},
  number={3},
  pages={16--19},
  year={2019},
  publisher={ACM New York, NY, USA}
}