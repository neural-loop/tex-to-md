\section{Conclusion and Future Work}
\label{sec:conclusion}
Machine unlearning emerges with the need to implement the \textit{``right to be forgotten''} (RTBF) efficiently while existing studies overlook its impact on fairness. To the best of our knowledge, we are the first to perform an empirical study on the impacts of machine unlearning methods on fairness. We designed and conducted experiments on two typical machine unlearning methods (SISA and AmnesiacML) along with a retraining method (ORTR) using three fairness datasets under three different deletion strategies. We found that SISA leads to better fairness compared with AmnesiacML and ORTR, while initial training and uniform data deletion do not necessarily affect the fairness of all three methods. Our research has shed light on fairness implications of machine unlearning and provided knowledge for software engineers about the trade-offs when considering machine unlearning methods as a solution for RTBF. In the future, more research efforts are needed to broaden the understanding of fairness implications into other machine unlearning methods as well as investigate the underlying causes of their impact on fairness.
