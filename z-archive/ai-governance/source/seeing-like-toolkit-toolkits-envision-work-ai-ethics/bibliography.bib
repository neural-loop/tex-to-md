@article{Pfaffenberger1992,
abstract = {This article examines the technological construction of political power, as well as resistance to political power, by means of an "ideal-typical" model called a technolog ical drama. In technological regularization, a design constituency creates artifacts whose features reveal an intention to shape the distribution of wealth, power, or status in society. The design constituency also creates myths, social contexts, and rituals to legitimate its intention and constitute the artifact's political impact. In reply, the people adversely affected by regularization engage in myth-, context-, or artifact-altering strate gies that represent an accommodation to the system (technological adjustment) or a conscious attempt to change it (technological reconstitution). A technological drama, in short, is a specifically technological form of political discourse. A key point is that throughout all three processes, political "intentions," no less than the facticity and hardness of the technology's "impact," are themselves constituted and constructed in reciprocal and discursive interaction with technological activities. Technology is not politics pursued by other means; it is politics constructed by technological means.},
author = {Pfaffenberger, Bryan},
doi = {10.1177/016224399201700302},
file = {:C\:/Users/ryw9/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pfaffenberger - 1992 - Technological Dramas.pdf:pdf},
issn = {0162-2439},
journal = {Science, Technology, \& Human Values},
month = {Jul},
number = {3},
pages = {282--312},
title = {{Technological Dramas}},
url = {https://estsjournal.org/index.php/ests/article/view/132 http://journals.sagepub.com/doi/10.1177/016224399201700302},
volume = {17},
year = {1992}
}

@inproceedings{LeDantec2009Values,
abstract = {The Value Sensitive Design (VSD) methodology provides a comprehensive framework for advancing a value-centered research and design agenda. Although VSD provides helpful ways of thinking about and designing value-centered computational systems, we argue that the specific mechanics of VSD create thorny tensions with respect to value sensitivity. In particular, we examine limitations due to value classifications, inadequate guidance on empirical tools for design, and the ways in which the design process is ordered. In this paper, we propose ways of maturing the VSD methodology to overcome these limitations and present three empirical case studies that illustrate a family of methods to effectively engage local expressions of values. The findings from our case studies provide evidence of how we can mature the VSD methodology to mitigate the pitfalls of classification and engender a commitment to reflect on and respond to local contexts of design.},
address = {New York, New York, USA},
author = {{Le Dantec}, Christopher A. and Poole, Erika Shehan and Wyche, Susan P.},
booktitle = {Proceedings of the 27th international conference on Human factors in computing systems - CHI 09},
doi = {10.1145/1518701.1518875},
file = {:C\:/Users/ryw9/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Le Dantec, Poole, Wyche - 2009 - Values as lived experience Evolving value sensitive design in support of value discovery.pdf:pdf},
isbn = {9781605582467},
issn = {9781605582467},
keywords = {VSD,case study,critique,emergent,empirical methods,fieldwork,methodology,photo elicitation,pluralistic,values},
pages = {1141},
publisher = {ACM Press},
title = {{Values as lived experience: Evolving value sensitive design in support of value discovery}},
url = {http://dl.acm.org/citation.cfm?id=1518701.1518875 http://dl.acm.org/citation.cfm?doid=1518701.1518875},
year = {2009}
}

@inproceedings{Houston2016Values,
address = {New York, New York, USA},
author = {Houston, Lara and Jackson, Steven J and Rosner, Daniela K and Ahmed, Syed Ishtiaque and Young, Meg and Kang, Laewoo},
booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems - CHI '16},
doi = {10.1145/2858036.2858470},
file = {:C\:/Users/ryw9/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Houston et al. - 2016 - Values in Repair.pdf:pdf},
isbn = {9781450333627},
mendeley-groups = {Post-Conference Reading/CHI 2016},
pages = {1403--1414},
publisher = {ACM Press},
title = {{Values in Repair}},
url = {http://dl.acm.org/citation.cfm?doid=2858036.2858470},
year = {2016}
}

@article{JafariNaimi2015ValuesHypotheses,
abstract = {(fr. hohe/ gehobene Schneiderei)},
author = {JafariNaimi (Parvin), Nassim and Nathan, Lisa and Hargraves, Ian},
doi = {10.1162/DESI_a_00354},
file = {:C\:/Users/ryw9/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/JafariNaimi, Nathan, Hargraves - 2015 - Values as Hypotheses Design, Inquiry, and the Service of Values.pdf:pdf},
isbn = {13978-3-923859-82-5},
issn = {0747-9360},
journal = {Design Issues},
month = {Oct},
number = {4},
pages = {91--104},
title = {{Values as Hypotheses: Design, Inquiry, and the Service of Values}},
url = {http://www.mitpressjournals.org/doi/10.1162/DESI_a_00354},
volume = {31},
year = {2015}
}

@inproceedings{Shilton2014HowToSeeValues,
abstract = {Human values play an important role in shaping the design and use of information technologies. Research on values in social computing is challenged by disagreement about indicators and objects of study as researchers distribute their focus across contexts of technology design, adoption, and use. This paper draws upon a framework that clarifies how to see values in social computing research by describing values dimensions, comprised of sources and attributes of values in sociotechnical systems. This paper uses the framework to compare how diverse research methods employed in social computing surface values and make them visible to researchers. The framework provides a tool to analyze the strengths and weaknesses of each method for observing values dimensions. By detailing how and where researchers might observe interactions between values and technology design and use, we hope to enable researchers to systematically identify and investigate values in social computing. Copyright {\textcopyright} 2014 ACM.},
address = {New York, NY, USA},
author = {Shilton, Katie and Koepfler, Jes A. and Fleischmann, Kenneth R.},
booktitle = {Proceedings of the 17th ACM conference on Computer supported cooperative work \& social computing},
doi = {10.1145/2531602.2531625},
file = {:C\:/Users/ryw9/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shilton, Koepfler, Fleischmann - 2014 - How to see values in social computing Methods for Studying Values Dimensions.pdf:pdf},
isbn = {9781450325400},
keywords = {dissertation,research methods,value centered design,value sensitive design,values in design},
mendeley-tags = {dissertation,value centered design},
month = {feb},
pages = {426--435},
publisher = {ACM},
title = {{How to see values in social computing: Methods for Studying Values Dimensions}},
url = {http://dl.acm.org/citation.cfm?id=2531602.2531625%5Cnhttp://dl.acm.org/citation.cfm?doid=2531602.2531625 https://dl.acm.org/doi/10.1145/2531602.2531625},
year = {2014}
}

@article{yates1992genres,
  title={Genres of organizational communication: A structurational approach to studying communication and media},
  author={Yates, JoAnne and Orlikowski, Wanda J},
  journal={Academy of management review},
  volume={17},
  number={2},
  pages={299--326},
  year={1992},
  publisher={Academy of Management Briarcliff Manor, NY 10510}
}

@inproceedings{shen2021valuecards,
author = {Shen, Hong and Deng, Wesley H. and Chattopadhyay, Aditi and Wu, Zhiwei Steven and Wang, Xu and Zhu, Haiyi},
title = {Value Cards: An Educational Toolkit for Teaching Social Impacts of Machine Learning through Deliberation},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445971},
doi = {10.1145/3442188.3445971},
abstract = {Recently, there have been increasing calls for computer science curricula to complement existing technical training with topics related to Fairness, Accountability, Transparency and Ethics (FATE). In this paper, we present Value Cards, an educational toolkit to inform students and practitioners the social impacts of different machine learning models via deliberation. This paper presents an early use of our approach in a college-level computer science course. Through an in-class activity, we report empirical data for the initial effectiveness of our approach. Our results suggest that the use of the Value Cards toolkit can improve students' understanding of both the technical definitions and trade-offs of performance metrics and apply them in real-world contexts, help them recognize the significance of considering diverse social values in the development and deployment of algorithmic systems, and enable them to communicate, negotiate and synthesize the perspectives of diverse stakeholders. Our study also demonstrates a number of caveats we need to consider when using the different variants of the Value Cards toolkit. Finally, we discuss the challenges as well as future applications of our approach.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {850–861},
numpages = {12},
keywords = {Value Cards, Machine Learning, CS Education, Deliberation, Fairness},
location = {Virtual Event, Canada},
series = {FAccT '21}
}

@misc{Shonhiwa2020humanValuesMedium,
author = {Shonhiwa, Mandla},
booktitle = {UX Collective},
title = {{Human values matter: why value-sensitive design should be part of every UX designer's toolkit}},
url = {https://uxdesign.cc/human-values-matter-why-value-sensitive-design-should-be-part-of-every-ux-designers-toolkit-e53ffe7ec436},
urldate = {2021-12-03},
year = {2020}
}

@techreport{Spitzberg2020,
author = {Spitzberg, Danny and Shaw, Kevin and Angevine, Colin and Wilkins, Marissa and Strickland, M and Yamashiro, Janel and Adams, Rhonda and Lockhart, Leah},
booktitle = {CSCW 2020 Workshop on Collective Organizing and Social Responsibility},
doi = {10.21428/93b2c832.e3a8d187},
file = {:C\:/Users/ryw9/Box/Papers Archive/Spitzberg et al (2020) Principles at work- applying design justice in professionalized workplaces.pdf:pdf},
pages = {1--5},
title = {{Principles at Work: Applying “Design Justice” in Professionalized Workplaces}},
year = {2020}
}

@article{Metcalf2019OwningEthics,
author = {Metcalf, Jacob and Moss, Emanuel and danah Boyd},
file = {:C\:/Users/ryw9/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Metcalf, Moss, boyd - 2019 - Owning ethics Corporate logics, Silicon Valley, and the institutionalization of ethics.pdf:pdf},
issn = {0037783X},
journal = {Social Research},
number = {2},
pages = {449--476},
title = {{Owning ethics: Corporate logics, Silicon Valley, and the institutionalization of ethics}},
volume = {86},
year = {2019}
}

@inproceedings{Khovanskaya2019dataRhetoric,
address = {New York, NY, USA},
author = {Khovanskaya, Vera and Sengers, Phoebe},
booktitle = {Proceedings of the 2019 on Designing Interactive Systems Conference},
doi = {10.1145/3322276.3323691},
file = {:C\:/Users/ryw9/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Khovanskaya, Sengers - 2019 - Data Rhetoric and Uneasy Alliances Data Advocacy in US Labor History.pdf:pdf},
isbn = {9781450358507},
month = {jun},
pages = {1391--1403},
publisher = {ACM},
title = {{Data Rhetoric and Uneasy Alliances: Data Advocacy in US Labor History}},
url = {https://dl.acm.org/doi/10.1145/3322276.3323691},
year = {2019}
}

@inproceedings{10.1145/3442188.3445938,
author = {Krafft, P. M. and Young, Meg and Katell, Michael and Lee, Jennifer E. and Narayan, Shankar and Epstein, Micah and Dailey, Dharma and Herman, Bernease and Tam, Aaron and Guetler, Vivian and Bintz, Corinne and Raz, Daniella and Jobe, Pa Ousman and Putz, Franziska and Robick, Brian and Barghouti, Bissan},
title = {An Action-Oriented AI Policy Toolkit for Technology Audits by Community Advocates and Activists},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445938},
doi = {10.1145/3442188.3445938},
abstract = {Motivated by the extensive documented disparate harms of artificial intelligence (AI), many recent practitioner-facing reflective tools have been created to promote responsible AI development. However, the use of such tools internally by technology development firms addresses responsible AI as an issue of closed-door compliance rather than a matter of public concern. Recent advocate and activist efforts intervene in AI as a public policy problem, inciting a growing number of cities to pass bans or other ordinances on AI and surveillance technologies. In support of this broader ecology of political actors, we present a set of reflective tools intended to increase public participation in technology advocacy for AI policy action. To this end, the Algorithmic Equity Toolkit (the AEKit) provides a practical policy-facing definition of AI, a flowchart for assessing technologies against that definition, a worksheet for decomposing AI systems into constituent parts, and a list of probing questions that can be posed to vendors, policy-makers, or government agencies. The AEKit carries an action-orientation towards political encounters between community groups in the public and their representatives, opening up the work of AI reflection and remediation to multiple points of intervention. Unlike current reflective tools available to practitioners, our toolkit carries with it a politics of community participation and activism.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {772–781},
numpages = {10},
keywords = {surveillance, algorithmic justice, Participatory design, algorithmic equity, regulation, participatory action research, accountability},
location = {Virtual Event, Canada},
series = {FAccT '21}
}

@article{goodwin2015professional,
   URL = {http://www.jstor.org/stable/682303},
 abstract = {Seeing is investigated as a socially situated, historically constituted body of practices through which the objects of knowledge that animate the discourse of a profession are constructed and shaped. Analysis of videotapes of archaeologists making maps and lawyers animating events visible on the Rodney King videotape focuses on practices that are articulated in a work-relevant way within sequences of human interaction, including coding schemes, highlighting, and graphic representations. Through the structure of talk in interaction, members of a profession hold each other accountable for, and contest the proper constitution and perception of, the objects that define their professional competence.},
 author = {Charles Goodwin},
 journal = {American Anthropologist},
 number = {3},
 pages = {606--633},
 publisher = {[American Anthropological Association, Wiley]},
 title = {Professional Vision},
 urldate = {2023-01-14},
 volume = {96},
 year = {1994}
}

@inproceedings{lee2021landscape,
  author = {Lee, Michelle Seng Ah and Singh, Jat},
title = {The Landscape and Gaps in Open Source Fairness Toolkits},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445261},
doi = {10.1145/3411764.3445261},
abstract = {With the surge in literature focusing on the assessment and mitigation of unfair outcomes in algorithms, several open source ‘fairness toolkits’ recently emerged to make such methods widely accessible. However, little studied are the differences in approach and capabilities of existing fairness toolkits, and their fit-for-purpose in commercial contexts. Towards this, this paper identifies the gaps between the existing open source fairness toolkit capabilities and the industry practitioners’ needs. Specifically, we undertake a comparative assessment of the strengths and weaknesses of six prominent open source fairness toolkits, and investigate the current landscape and gaps in fairness toolkits through an exploratory focus group, a semi-structured interview, and an anonymous survey of data science/machine learning (ML) practitioners. We identify several gaps between the toolkits’ capabilities and practitioner needs, highlighting areas requiring attention and future directions towards tooling that better support ‘fairness in practice.’},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {699},
numpages = {13},
keywords = {algorithm auditing, bias detection, fairness, open source toolkits, bias, bias mitigation, algorithmic fairness, fairness toolkits},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{richardson2021towards,
author = {Richardson, Brianna and Garcia-Gathright, Jean and Way, Samuel F. and Thom, Jennifer and Cramer, Henriette},
title = {Towards Fairness in Practice: A Practitioner-Oriented Rubric for Evaluating Fair ML Toolkits},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445604},
doi = {10.1145/3411764.3445604},
abstract = {In order to support fairness-forward thinking by machine learning (ML) practitioners, fairness researchers have created toolkits that aim to transform state-of-the-art research contributions into easily-accessible APIs. Despite these efforts, recent research indicates a disconnect between the needs of practitioners and the tools offered by fairness research. By engaging 20 ML practitioners in a simulated scenario in which they utilize fairness toolkits to make critical decisions, this work aims to utilize practitioner feedback to inform recommendations for the design and creation of fair ML toolkits. Through the use of survey and interview data, our results indicate that though fair ML toolkits are incredibly impactful on users’ decision-making, there is much to be desired in the design and demonstration of fairness results. To support the future development and evaluation of toolkits, this work offers a rubric that can be used to identify critical components of Fair ML toolkits.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {236},
numpages = {13},
keywords = {algorithmic bias, machine learning fairness, ML, fairness, ethics, user-centric evaluation, AI},
location = {Yokohama, Japan},
series = {CHI '21}
}

@incollection{morley2021initial,
  title={From what to how: an initial review of publicly available AI ethics tools, methods and research to translate principles into practices},
  author={Morley, Jessica and Floridi, Luciano and Kinsey, Libby and Elhalal, Anat},
  booktitle={Ethics, Governance, and Policies in Artificial Intelligence},
  pages={153--183},
  year={2021},
  publisher={Springer}
}

@article{ayling2021putting,
  title={Putting AI ethics to work: are the tools fit for purpose?},
  author={Ayling, Jacqui and Chapman, Adriane},
  journal={AI and Ethics},
    volume={2},
  number={3},
  pages={405--429},
  year={2021},
  publisher={Springer},
  doi={10.1007/s43681-021-00084-x}
}

@inproceedings{10.1145/3375627.3377141,
author = {Neff, Gina},
title = {From Bad Users and Failed Uses to Responsible Technologies: A Call to Expand the AI Ethics Toolkit},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3377141},
doi = {10.1145/3375627.3377141},
abstract = {Recent advances in artificial intelligence applications have sparked scholarly and public attention to the challenges of the ethical design of technologies. These conversations about ethics have been targeted largely at technology designers and concerned with helping to inform building better and fairer AI tools and technologies. This approach, however, addresses only a small part of the problem of responsible use and will not be adequate for describing or redressing the problems that will arise as more types of AI technologies are more widely used.Many of the tools being developed today have potentially enormous and historic impacts on how people work, how society organises, stores and distributes information, where and how people interact with one another, and how people's work is valued and compensated. And yet, our ethical attention has looked at a fairly narrow range of questions about expanding the access to, fairness of, and accountability for existing tools. Instead, I argue that scholars should develop much broader questions of about the reconfiguration of societal power, for which AI technologies form a crucial component.This talk will argue that AI ethics needs to expand its theoretical and methodological toolkit in order to move away from prioritizing notions of good design that privilege the work of good and ethical technology designers. Instead, using approaches from feminist theory, organization studies, and science and technology, I argue for expanding how we evaluate uses of AI. This approach begins with the assumption of socially informed technological affordances, or "imagined affordances" [1] shaping how people understand and use technologies in practice. It also gives centrality to the power of social institutions for shaping technologies-in-practice.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {5–6},
numpages = {2},
keywords = {feminist theory, work and organizations, ai ethics, sts, data work, theory, social sciences},
location = {New York, NY, USA},
series = {AIES '20}
}

@article{chivukula2021surveying,
  title={Surveying the Landscape of Ethics-Focused Design Methods},
  author={Chivukula, Shruthi Sai and Li, Ziqing and Pivonka, Anne C and Chen, Jingning and Gray, Colin M},
  journal={arXiv preprint arXiv:2102.08909},
  year={2021},
  numpages = {32}
}

@article{pierce2018differential,
  title={Differential vulnerabilities and a diversity of tactics: What toolkits teach us about cybersecurity},
  author={Pierce, James and Fox, Sarah and Merrill, Nick and Wong, Richmond},
  journal={Proceedings of the ACM on Human-Computer Interaction},
  volume={2},
  number={CSCW},
  pages={1--24},
  year={2018},
  publisher={ACM New York, NY, USA},
  doi={10.1145/3274408}
}

@inproceedings{sambasivan2021reimagining,
author = {Sambasivan, Nithya and Arnesen, Erin and Hutchinson, Ben and Doshi, Tulsee and Prabhakaran, Vinodkumar},
title = {Re-Imagining Algorithmic Fairness in India and Beyond},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445896},
doi = {10.1145/3442188.3445896},
abstract = {Conventional algorithmic fairness is West-centric, as seen in its subgroups, values, and methods. In this paper, we de-center algorithmic fairness and analyse AI power in India. Based on 36 qualitative interviews and a discourse analysis of algorithmic deployments in India, we find that several assumptions of algorithmic fairness are challenged. We find that in India, data is not always reliable due to socio-economic factors, ML makers appear to follow double standards, and AI evokes unquestioning aspiration. We contend that localising model fairness alone can be window dressing in India, where the distance between models and oppressed communities is large. Instead, we re-imagine algorithmic fairness in India and provide a roadmap to re-contextualise data and models, empower oppressed communities, and enable Fair-ML ecosystems.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {315–328},
numpages = {14},
keywords = {religion, decoloniality, India, feminism, algorithmic fairness, ability, caste, gender, class, anti-caste politics, critical algorithmic studies},
location = {Virtual Event, Canada},
series = {FAccT '21}
}


@article{Jobin:2019bw,
author = {Jobin, Anna and Ienca, Marcello and Vayena, Effy},
title = {{The global landscape of AI ethics guidelines}},
journal = {Nature Machine Intelligence},
year = {2019},
pages = {1--11},
volume = {1},
month = {Sep},
doi={10.1038/s42256-019-0088-2}
}


@article{Mittelstadt:2019ve,
  title={AI Ethics--Too Principled to Fail?},
  author={Mittelstadt, Brent},
  howpublished = "CoRR arXiv:1906.06668",
  year={2019},
  doi={10.48550/arXiv.1906.06668}
}

@article{schiff2020principles,
  title={Principles to practices for responsible AI: Closing the gap},
  author={Schiff, Daniel and Rakova, Bogdana and Ayesh, Aladdin and Fanti, Anat and Lennon, Michael},
  journal={arXiv preprint arXiv:2006.04707},
  year={2020},
  doi={10.48550/arXiv.2006.04707}
}


@article{metcalf2019owning,
  title={Owning Ethics: Corporate Logics, Silicon Valley, and the Institutionalization of Ethics},
  author={Metcalf, Jacob and Moss, Emanuel and others},
  journal={Social Research: An International Quarterly},
  volume={86},
  number={2},
  pages={449--476},
  year={2019},
  publisher={Johns Hopkins University Press}
}

@inproceedings{madaio2020co,
  author = {Madaio, Michael A. and Stark, Luke and Wortman Vaughan, Jennifer and Wallach, Hanna},
title = {Co-Designing Checklists to Understand Organizational Challenges and Opportunities around Fairness in AI},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376445},
doi = {10.1145/3313831.3376445},
abstract = {Many organizations have published principles intended to guide the ethical development and deployment of AI systems; however, their abstract nature makes them difficult to operationalize. Some organizations have therefore produced AI ethics checklists, as well as checklists for more specific concepts, such as fairness, as applied to AI systems. But unless checklists are grounded in practitioners' needs, they may be misused. To understand the role of checklists in AI ethics, we conducted an iterative co-design process with 48 practitioners, focusing on fairness. We co-designed an AI fairness checklist and identified desiderata and concerns for AI fairness checklists in general. We found that AI fairness checklists could provide organizational infrastructure for formalizing ad-hoc processes and empowering individual advocates. We highlight aspects of organizational culture that may impact the efficacy of AI fairness checklists, and suggest future design directions.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–14},
numpages = {14},
keywords = {fairness, checklists, ML, AI, co-design, ethics},
location = {Honolulu, HI, USA},
series = {CHI '20}
}


@article{rakova2021responsible,
  title={Where responsible AI meets reality: Practitioner perspectives on enablers for shifting organizational practices},
  author={Rakova, Bogdana and Yang, Jingying and Cramer, Henriette and Chowdhury, Rumman},
  journal={Proceedings of the ACM on Human-Computer Interaction},
  volume={5},
  number={CSCW1},
  pages={1--23},
  year={2021},
  publisher={ACM New York, NY, USA},
  doi={10.1145/3449081}
}

@article{suchman2002located,
  title={Located accountabilities in technology production},
  author={Suchman, Lucy},
  journal={Scandinavian journal of information systems},
  volume={14},
  number={2},
  pages={7},
  year={2002}
}

@article{wong2021tactics,
  title={Tactics of Soft Resistance in User Experience Professionals' Values Work},
  author={Wong, Richmond Y},
  journal={Proceedings of the ACM on Human-Computer Interaction},
  volume={5},
  number={CSCW2},
  pages={1--28},
  year={2021},
  publisher={ACM New York, NY, USA},
  doi={10.1145/3479499}
}

@article{shilton2013values,
  title={Values levers: Building ethics into design},
  author={Shilton, Katie},
  journal={Science, Technology, \& Human Values},
  volume={38},
  number={3},
  pages={374--397},
  year={2013},
  publisher={Sage Publications Sage CA: Los Angeles, CA},
  doi={10.1177/0162243912436985}
}

@inproceedings{neff2020bad,
author = {Neff, Gina},
title = {From Bad Users and Failed Uses to Responsible Technologies: A Call to Expand the AI Ethics Toolkit},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3377141},
doi = {10.1145/3375627.3377141},
abstract = {Recent advances in artificial intelligence applications have sparked scholarly and public attention to the challenges of the ethical design of technologies. These conversations about ethics have been targeted largely at technology designers and concerned with helping to inform building better and fairer AI tools and technologies. This approach, however, addresses only a small part of the problem of responsible use and will not be adequate for describing or redressing the problems that will arise as more types of AI technologies are more widely used.Many of the tools being developed today have potentially enormous and historic impacts on how people work, how society organises, stores and distributes information, where and how people interact with one another, and how people's work is valued and compensated. And yet, our ethical attention has looked at a fairly narrow range of questions about expanding the access to, fairness of, and accountability for existing tools. Instead, I argue that scholars should develop much broader questions of about the reconfiguration of societal power, for which AI technologies form a crucial component.This talk will argue that AI ethics needs to expand its theoretical and methodological toolkit in order to move away from prioritizing notions of good design that privilege the work of good and ethical technology designers. Instead, using approaches from feminist theory, organization studies, and science and technology, I argue for expanding how we evaluate uses of AI. This approach begins with the assumption of socially informed technological affordances, or "imagined affordances" [1] shaping how people understand and use technologies in practice. It also gives centrality to the power of social institutions for shaping technologies-in-practice.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {5–6},
numpages = {2},
keywords = {social sciences, data work, work and organizations, feminist theory, sts, theory, ai ethics},
location = {New York, NY, USA},
series = {AIES '20}
}

@article{passi2018trust,
  author = {Passi, Samir and Jackson, Steven J.},
title = {Trust in Data Science: Collaboration, Translation, and Accountability in Corporate Data Science Projects},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {CSCW},
url = {https://doi.org/10.1145/3274405},
doi = {10.1145/3274405},
abstract = {The trustworthiness of data science systems in applied and real-world settings emerges from the resolution of specific tensions through situated, pragmatic, and ongoing forms of work. Drawing on research in CSCW, critical data studies, and history and sociology of science, and six months of immersive ethnographic fieldwork with a corporate data science team, we describe four common tensions in applied data science work: (un)equivocal numbers, (counter)intuitive knowledge, (in)credible data, and (in)scrutable models. We show how organizational actors establish and re-negotiate trust under messy and uncertain analytic conditions through practices of skepticism, assessment, and credibility. Highlighting the collaborative and heterogeneous nature of real-world data science, we show how the management of trust in applied corporate data science settings depends not only on pre-processing and quantification, but also on negotiation and translation. We conclude by discussing the implications of our findings for data science research and practice, both within and beyond CSCW.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {Nov},
articleno = {136},
numpages = {28},
keywords = {organizational work, data science, trust, credibility, collaboration}
}

@article{passi2020making,
  title={Making data science systems work},
  author={Passi, Samir and Sengers, Phoebe},
  journal={Big Data \& Society},
  volume={7},
  number={2},
  doi={10.1177/2053951720939605},
  numpages = {13},
  year={2020},
  publisher={SAGE Publications Sage UK: London, England}
}


@inproceedings{passi2019problem,
  author = {Passi, Samir and Barocas, Solon},
title = {Problem Formulation and Fairness},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287567},
doi = {10.1145/3287560.3287567},
abstract = {Formulating data science problems is an uncertain and difficult process. It requires various forms of discretionary work to translate high-level objectives or strategic goals into tractable problems, necessitating, among other things, the identification of appropriate target variables and proxies. While these choices are rarely self-evident, normative assessments of data science projects often take them for granted, even though different translations can raise profoundly different ethical concerns. Whether we consider a data science project fair often has as much to do with the formulation of the problem as any property of the resulting model. Building on six months of ethnographic fieldwork with a corporate data science team---and channeling ideas from sociology and history of science, critical data studies, and early writing on knowledge discovery in databases---we describe the complex set of actors and activities involved in problem formulation. Our research demonstrates that the specification and operationalization of the problem are always negotiated and elastic, and rarely worked out with explicit normative considerations in mind. In so doing, we show that careful accounts of everyday data science work can help us better understand how and why data science problems are posed in certain ways---and why specific formulations prevail in practice, even in the face of what might seem like normatively preferable alternatives. We conclude by discussing the implications of our findings, arguing that effective normative interventions will require attending to the practical work of problem formulation.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {39–48},
numpages = {10},
keywords = {Problem Formulation, Machine Learning, Target Variable, Fairness, Data Science},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}

@misc{mattern_2021, title={Unboxing the Toolkit}, url={https://tool-shed.org/unboxing-the-toolkit/}, journal={Toolshed}, author={Mattern, Shannon}, year={2021}, month={Jul}} 

 @misc{kelty_2018, title={The Participatory Development Toolkit}, url={https://limn.it/articles/the-participatory-development-toolkit/}, journal={Limn}, author={Kelty, Christopher M}, year={2018}, month={Jun}} 
 

@inproceedings{Holstein:2019fr,
  title={Improving fairness in machine learning systems: What do industry practitioners need?},
  author={Holstein, Kenneth and Wortman Vaughan, Jennifer and Daum{\'e} III, Hal and Dudik, Miro and Wallach, Hanna},
  year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300830},
doi = {10.1145/3290605.3300830},
abstract = {The potential for machine learning (ML) systems to amplify social inequities and unfairness is receiving increasing popular and academic attention. A surge of recent work has focused on the development of algorithmic tools to assess and mitigate such unfairness. If these tools are to have a positive impact on industry practice, however, it is crucial that their design be informed by an understanding of real-world needs. Through 35 semi-structured interviews and an anonymous survey of 267 ML practitioners, we conduct the first systematic investigation of commercial product teams' challenges and needs for support in developing fairer ML systems. We identify areas of alignment and disconnect between the challenges faced by teams in practice and the solutions proposed in the fair ML research literature. Based on these findings, we highlight directions for future ML and HCI research that will better address practitioners' needs.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–16},
numpages = {16},
keywords = {needfinding, algorithmic bias, fair machine learning, ux of machine learning, product teams, empirical study},
location = {Glasgow, Scotland UK},
series = {CHI '19}
}


@inproceedings{boyd2020ethical,
 author = {Boyd, Karen},
title = {Ethical Sensitivity in Machine Learning Development},
year = {2020},
isbn = {9781450380591},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3406865.3418359},
doi = {10.1145/3406865.3418359},
abstract = {Despite a great deal of attention to developing ethical mitigations for Machine Learning (ML) training data and models, we don't yet know how these interventions will be adopted by those who curate data and use them to train ML models. Will they help ML engineers find and address ethical concerns in their work? My proposed dissertation seeks to understand ML engineers? ethical sensitivity? their propensity to notice, analyze, and act on socially impactful aspects of their work-while curating training data and describe the effects of context documents and ethical guides as practice-based ethics interventions in this early stage of ML development. It asks how ML engineers recognize,particularize, and judge ethical questions while exploring new training data; introduces Ethical Sensitivity to the study of social computing; and will describe how Datasheets intervene in perception and particularization; and will develop a document that can help engineers move from particularization to judgment. It will accomplish these goals using a think aloud experiment with engineers working with unfamiliar training data (with or without a Datasheet), a Value Sensitive Design study that aims to fit an ethical mitigation guide to engineers? work practices, and a systematic review of ethical sensitivity.},
booktitle = {Conference Companion Publication of the 2020 on Computer Supported Cooperative Work and Social Computing},
pages = {87–92},
numpages = {6},
keywords = {ethical sensitivity, datasets, machine learning, ethics, work practices, technology development},
location = {Virtual Event, USA},
series = {CSCW '20 Companion}
}

@article{weaver2008ethical,
  title={Ethical sensitivity in professional practice: concept analysis},
  author={Weaver, Kathryn and Morse, Janice and Mitcham, Carl},
  journal={Journal of advanced nursing},
  volume={62},
  number={5},
  pages={607--618},
  year={2008},
  publisher={Wiley Online Library}
}

@article{hitzig2020normative,
  title={The normative gap: mechanism design and ideal theories of justice},
  author={Hitzig, Zo{\"e}},
  journal={Economics \& Philosophy},
  volume={36},
  number={3},
  pages={407--434},
  year={2020},
  publisher={Cambridge University Press}
}

@misc{friedman2002value,
  title={Value sensitive design: Theory and methods},
  author={Friedman, Batya and Kahn, Peter and Borning, Alan},
  journal={University of Washington technical report},
  number={2-12},
  year={2002},
  publisher={Citeseer}
}

@article{yoo2018stakeholder,
  title={Stakeholder Tokens: a constructive method for value sensitive design stakeholder analysis},
  author={Yoo, Daisy},
  journal={Ethics and Information Technology},
  pages={1--5},
  year={2021},
  publisher={Springer},
  doi={10.1007/s10676-018-9474-4}
}

@incollection{star1989structure,
  title={The structure of ill-structured solutions: Boundary objects and heterogeneous distributed problem solving},
  author={Star, Susan Leigh},
  year = {1989},
isbn = {0273088106},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
booktitle = {Distributed Artificial Intelligence (Vol. 2)},
pages = {37–54},
numpages = {18}
}

@article{silbey2009taming,
  title={Taming Prometheus: Talk about safety and culture},
  author={Silbey, Susan S},
  journal={Annual Review of Sociology},
  volume={35},
  pages={341--369},
  year={2009},
  publisher={Annual Reviews}
}

@book{redfield2013life,
  title={Life in crisis},
  author={Redfield, Peter},
  year={2013},
  publisher={University of California Press},
  address={Berkeley}
}

@inproceedings{selbst2019fairness,
  author = {Selbst, Andrew D. and Boyd, Danah and Friedler, Sorelle A. and Venkatasubramanian, Suresh and Vertesi, Janet},
title = {Fairness and Abstraction in Sociotechnical Systems},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287598},
doi = {10.1145/3287560.3287598},
abstract = {A key goal of the fair-ML community is to develop machine-learning based systems that, once introduced into a social context, can achieve social and legal outcomes such as fairness, justice, and due process. Bedrock concepts in computer science---such as abstraction and modular design---are used to define notions of fairness and discrimination, to produce fairness-aware learning algorithms, and to intervene at different stages of a decision-making pipeline to produce "fair" outcomes. In this paper, however, we contend that these concepts render technical interventions ineffective, inaccurate, and sometimes dangerously misguided when they enter the societal context that surrounds decision-making systems. We outline this mismatch with five "traps" that fair-ML work can fall into even as it attempts to be more context-aware in comparison to traditional data science. We draw on studies of sociotechnical systems in Science and Technology Studies to explain why such traps occur and how to avoid them. Finally, we suggest ways in which technical designers can mitigate the traps through a refocusing of design in terms of process rather than solutions, and by drawing abstraction boundaries to include social actors rather than purely technical ones.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {59–68},
numpages = {10},
keywords = {Interdisciplinary, Fairness-aware Machine Learning, Sociotechnical Systems},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}

@inproceedings{blodgett2020language,
    title = "Language (Technology) is Power: A Critical Survey of {``}Bias{''} in {NLP}",
    author = "Blodgett, Su Lin  and
      Barocas, Solon  and
      Daum{\'e} III, Hal  and
      Wallach, Hanna",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.485",
    doi = "10.18653/v1/2020.acl-main.485",
    pages = "5454--5476",
    abstract = "We survey 146 papers analyzing {``}bias{''} in NLP systems, finding that their motivations are often vague, inconsistent, and lacking in normative reasoning, despite the fact that analyzing {``}bias{''} is an inherently normative process. We further find that these papers{'} proposed quantitative techniques for measuring or mitigating {``}bias{''} are poorly matched to their motivations and do not engage with the relevant literature outside of NLP. Based on these findings, we describe the beginnings of a path forward by proposing three recommendations that should guide work analyzing {``}bias{''} in NLP systems. These recommendations rest on a greater recognition of the relationships between language and social hierarchies, encouraging researchers and practitioners to articulate their conceptualizations of {``}bias{''}---i.e., what kinds of system behaviors are harmful, in what ways, to whom, and why, as well as the normative reasoning underlying these statements{---}and to center work around the lived experiences of members of communities affected by NLP systems, while interrogating and reimagining the power relations between technologists and such communities.",

}


@article{shen2020designing,
  title={Designing Alternative Representations of Confusion Matrices to Support Non-Expert Public Understanding of Algorithm Performance},
  author={Shen, Hong and Jin, Haojian and Cabrera, {\'A}ngel Alexander and Perer, Adam and Zhu, Haiyi and Hong, Jason I},
  journal={Proceedings of the ACM on Human-Computer Interaction},
  volume={4},
  number={CSCW2},
  pages={1--22},
  year={2020},
  publisher={ACM New York, NY, USA},
  doi={10.1145/3415224}
}

@inproceedings{cheng2021soliciting,
author = {Cheng, Hao-Fei and Stapleton, Logan and Wang, Ruiqi and Bullock, Paige and Chouldechova, Alexandra and Wu, Zhiwei Steven Steven and Zhu, Haiyi},
title = {Soliciting Stakeholders’ Fairness Notions in Child Maltreatment Predictive Systems},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445308},
doi = {10.1145/3411764.3445308},
abstract = {Recent work in fair machine learning has proposed dozens of technical definitions of algorithmic fairness and methods for enforcing these definitions. However, we still lack an understanding of how to develop machine learning systems with fairness criteria that reflect relevant stakeholders’ nuanced viewpoints in real-world contexts. To address this gap, we propose a framework for eliciting stakeholders’ subjective fairness notions. Combining a user interface that allows stakeholders to examine the data and the algorithm’s predictions with an interview protocol to probe stakeholders’ thoughts while they are interacting with the interface, we can identify stakeholders’ fairness beliefs and principles. We conduct a user study to evaluate our framework in the setting of a child maltreatment predictive system. Our evaluations show that the framework allows stakeholders to comprehensively convey their fairness viewpoints. We also discuss how our results can inform the design of predictive systems.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {390},
numpages = {17},
keywords = {algorithmic fairness, human-centered AI, child welfare, algorithm-assisted decision-making, machine learning},
location = {Yokohama, Japan},
series = {CHI '21}
}

@incollection{stark2021critical,
  title={Critical Perspectives on Governance Mechanisms for AI/ML Systems},
  author={Stark, Luke and Greene, Daniel and Hoffmann, Anna Lauren},
  booktitle={The Cultural Life of Machine Learning},
  pages={257--280},
  year={2021},
  publisher={Springer}
}

@article{freire1996pedagogy,
  title={Pedagogy of the oppressed (revised)},
  author={Freire, Paolo},
  journal={New York: Continuum},
  year={1996}
}

@article{malazita2019infrastructures,
  title={Infrastructures of abstraction: how computer science education produces anti-political subjects},
  author={Malazita, James W and Resetar, Korryn},
  journal={Digital Creativity},
  volume={30},
  number={4},
  pages={300--312},
  year={2019},
  publisher={Taylor \& Francis}
}

@ARTICLE{ahn2020fairsight,
    author={Ahn, Yongsu and Lin, Yu-Ru},  
    journal={IEEE Transactions on Visualization and Computer Graphics},   
    title={FairSight: Visual Analytics for Fairness in Decision Making},   
    year={2020},  
    volume={26},  
    number={1},  
    pages={1086-1095}, 
    doi={10.1109/TVCG.2019.2934262}
}

@inproceedings{metcalf2021algorithmicImpactAssessments,
author = {Metcalf, Jacob and Moss, Emanuel and Watkins, Elizabeth Anne and Singh, Ranjit and Elish, Madeleine Clare},
title = {Algorithmic Impact Assessments and Accountability: The Co-Construction of Impacts},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445935},
doi = {10.1145/3442188.3445935},
abstract = {Algorithmic impact assessments (AIAs) are an emergent form of accountability for organizations that build and deploy automated decision-support systems. They are modeled after impact assessments in other domains. Our study of the history of impact assessments shows that "impacts" are an evaluative construct that enable actors to identify and ameliorate harms experienced because of a policy decision or system. Every domain has different expectations and norms around what constitutes impacts and harms, how potential harms are rendered as impacts of a particular undertaking, who is responsible for conducting such assessments, and who has the authority to act on them to demand changes to that undertaking. By examining proposals for AIAs in relation to other domains, we find that there is a distinct risk of constructing algorithmic impacts as organizationally understandable metrics that are nonetheless inappropriately distant from the harms experienced by people, and which fall short of building the relationships required for effective accountability. As impact assessments become a commonplace process for evaluating harms, the FAccT community, in its efforts to address this challenge, should A) understand impacts as objects that are co-constructed accountability relationships, B) attempt to construct impacts as close as possible to actual harms, and C) recognize that accountability governance requires the input of various types of expertise and affected communities. We conclude with lessons for assembling cross-expertise consensus for the co-construction of impacts and building robust accountability relationships.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {735–746},
numpages = {12},
keywords = {harm, impact, algorithmic impact assessment, governance, accountability},
location = {Virtual Event, Canada},
series = {FAccT '21}
}


@article{kemp2013humanRights,
author = {Kemp, Deanna and Vanclay, Frank},
title = {Human rights and impact assessment: clarifying the connections in practice},
journal = {Impact Assessment and Project Appraisal},
volume = {31},
number = {2},
pages = {86-96},
year  = {2013},
publisher = {Taylor & Francis},
doi = {10.1080/14615517.2013.782978},
URL = {https://doi.org/10.1080/14615517.2013.782978},
}

@techreport{UnitedNationsHumanRights2011,
author = {{United Nations Human Rights Office of the High Commissioner}},
doi = {10.4324/9781351171922-3},
file = {:C\:/Users/ryw9/Box/Papers Archive/United Nations (2011)Guiding principles on business and human rights- implementing the United Nations protect, respect and remedy framework.pdf:pdf},
institution = {United Nations},
title = {{Guiding Principles on Business and Human Rights: Implementing the United Nations "Protect, Respect and Remedy" Framework}},
year = {2011}
}

@unpublished{Ruggie2017SocialConstructionUN,
abstract = {Academic proponents and opponents of the UN Guiding Principles on Business & Human Rights have generated a bourgeoning literature. And by now there are several years of practical experience to inform the debate. But the conceptual and theoretical understanding of global rulemaking that informed my development of the UNGPs, and to which I have contributed as a scholar, have not been fully articulated and debated. This chapter aims to close that gap, on the supposition that those ideas might have contributed to the UNGPs' relative success where previous efforts failed, and that in some measure they may be applicable in other complex and contested global policy domains.},
author = {Ruggie, John Gerard},
booktitle = {Harvard Kennedy School Faculty Research Working Paper Series},
doi = {10.2139/ssrn.2984901},
file = {:C\:/Users/ryw9/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ruggie - 2017 - The Social Construction of the UN Guiding Principles on Business & Human Rights.pdf:pdf},
title = {{The Social Construction of the UN Guiding Principles on Business \& Human Rights}},
year = {2017}
}

@article{Hoffmann2020terms,
abstract = {Inclusion has emerged as an early cornerstone value for the emerging domain of “data ethics.” On the surface, appeals to inclusion appear to address the threat that biased data technologies making decisions or misrepresenting people in ways that reproduce longer standing patterns of oppression and violence. Far from a panacea for the threats of pervasive data collection and surveillance, however, these emerging discourses of inclusion merit critical consideration. Here, I use the lens of discursive violence to better theorize the relationship between inclusion and the violent potentials of data science and technology. In doing so, I aim to articulate the problematic and often perverse power relationships implicit in ideals of “inclusion” broadly, which—if not accompanied by dramatic upheavals in existing hierarchical power structures—too often work to diffuse the radical potential of difference and normalize otherwise oppressive structural conditions.},
author = {Hoffmann, Anna Lauren},
doi = {10.1177/1461444820958725},
file = {:C\:/Users/ryw9/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hoffmann - 2020 - Terms of inclusion Data, discourse, violence.pdf:pdf},
issn = {1461-4448},
journal = {New Media \& Society},
month = {sep},
pages = {146144482095872},
title = {{Terms of inclusion: Data, discourse, violence}},
url = {http://journals.sagepub.com/doi/10.1177/1461444820958725},
year = {2020},
volume = {23},
issue = {12}
}

@inproceedings{Greene2019betterNicer,
abstract = {This paper uses frame analysis to examine recent high-profile values statements endorsing ethical design for artificial intelligence and machine learning (AI/ML). Guided by insights from values in design and the sociology of business ethics, we uncover the grounding assumptions and terms of debate that make some conversations about ethical design possible while forestalling alternative visions. Vision statements for ethical AI/ML co-opt the language of some critics, folding them into a limited, technologically deterministic, expert-driven view of what ethical AI/ML means and how it might work.},
author = {Greene, Daniel and Hoffmann, Anna Lauren and Stark, Luke},
booktitle = {Proceedings of the 52nd Hawaii International Conference on System Sciences},
doi = {10.24251/HICSS.2019.258},
file = {:C\:/Users/ryw9/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Greene, Hoffmann, Stark - 2019 - Better, Nicer, Clearer, Fairer A Critical Assessment of the Movement for Ethical Artificial Intelligenc.pdf:pdf},
title = {{Better, Nicer, Clearer, Fairer: A Critical Assessment of the Movement for Ethical Artificial Intelligence and Machine Learning}},
url = {http://hdl.handle.net/10125/59651},
year = {2019},
pages = {2122-2131}
}

@book{Scott1998seeing,
address = {New Haven},
author = {Scott, James C.},
publisher = {Yale University Press},
title = {{Seeing Like a State: How certain schemes to improve the human condition have failed}},
year = {1998}
}


@article{braun2006using,
  title={Using thematic analysis in psychology},
  author={Braun, Virginia and Clarke, Victoria},
  journal={Qualitative research in psychology},
  volume={3},
  number={2},
  pages={77--101},
  year={2006},
  publisher={Taylor \& Francis}
}

@book{jasanoff2015dreamscapes,
  title={Dreamscapes of modernity: Sociotechnical imaginaries and the fabrication of power},
  author={Jasanoff, Sheila and Kim, Sang-Hyun},
  year={2015},
  publisher={University of Chicago Press},
  address={Chicago}
}

@article{madaio2021assessing,
  title={Assessing the Fairness of AI Systems: AI Practitioners' Processes, Challenges, and Needs for Support},
  author={Madaio, Michael and Egede, Lisa and Subramonyam, Hariharan and Vaughan, Jennifer Wortman and Wallach, Hanna},
  journal={arXiv preprint arXiv:2112.05675},
  year={2021}
}

@article{shen2021everyday,
  author = {Shen, Hong and DeVos, Alicia and Eslami, Motahhare and Holstein, Kenneth},
title = {Everyday Algorithm Auditing: Understanding the Power of Everyday Users in Surfacing Harmful Algorithmic Behaviors},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW2},
url = {https://doi.org/10.1145/3479577},
doi = {10.1145/3479577},
abstract = {A growing body of literature has proposed formal approaches to audit algorithmic systems for biased and harmful behaviors. While formal auditing approaches have been greatly impactful, they often suffer major blindspots, with critical issues surfacing only in the context of everyday use once systems are deployed. Recent years have seen many cases in which everyday users of algorithmic systems detect and raise awareness about harmful behaviors that they encounter in the course of their everyday interactions with these systems. However, to date little academic attention has been granted to these bottom-up, user-driven auditing processes. In this paper, we propose and explore the concept of everyday algorithm auditing, a process in which users detect, understand, and interrogate problematic machine behaviors via their day-to-day interactions with algorithmic systems. We argue that everyday users are powerful in surfacing problematic machine behaviors that may elude detection via more centrally-organized forms of auditing, regardless of users' knowledge about the underlying algorithms. We analyze several real-world cases of everyday algorithm auditing, drawing lessons from these cases for the design of future platforms and tools that facilitate such auditing behaviors. Finally, we discuss work that lies ahead, toward bridging the gaps between formal auditing approaches and the organic auditing behaviors that emerge in everyday use of algorithmic systems.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {oct},
articleno = {433},
numpages = {29},
keywords = {auditing algorithms, everyday users, algorithmic bias, everyday algorithm auditing, fair machine learning}
}

@misc{ding2018deciphering,
  title={Deciphering China’s AI dream},
  author={Ding, Jeffrey},
  publisher={Future of Humanity Institute Technical Report},
  year={2018},
  url={https://www.fhi.ox.ac.uk/wp-content/uploads/Deciphering_Chinas_AI-Dream.pdf},
    pages = {1-44},
}

@article{STILGOE20131568,
title = {Developing a framework for responsible innovation},
journal = {Research Policy},
volume = {42},
number = {9},
pages = {1568-1580},
year = {2013},
issn = {0048-7333},
doi = {https://doi.org/10.1016/j.respol.2013.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S0048733313000930},
author = {Jack Stilgoe and Richard Owen and Phil Macnaghten},
keywords = {Responsible innovation, Governance, Emerging technologies, Ethics, Geoengineering},
abstract = {The governance of emerging science and innovation is a major challenge for contemporary democracies. In this paper we present a framework for understanding and supporting efforts aimed at ‘responsible innovation’. The framework was developed in part through work with one of the first major research projects in the controversial area of geoengineering, funded by the UK Research Councils. We describe this case study, and how this became a location to articulate and explore four integrated dimensions of responsible innovation: anticipation, reflexivity, inclusion and responsiveness. Although the framework for responsible innovation was designed for use by the UK Research Councils and the scientific communities they support, we argue that it has more general application and relevance.}
}



@inproceedings{forlizzi2013promoting,
  title={Promoting service design as a core practice in interaction design},
  author={Forlizzi, Jodi and Zimmerman, John},
  booktitle={Proceedings of the 5th International Congress of International Association of Societies of Design Research-IASDR},
  volume={13},
  year={2013},
  pages={1-12}
}

@article{lewis2018making,
  title={Making kin with the machines},
  author={Lewis, Jason Edward and Arista, Noelani and Pechawis, Archer and Kite, Suzanne},
  journal={Journal of Design and Science},
  year={2018},
  publisher={PubPub},
  doi={https://doi.org/10.21428/bfafd97b}
}

@inproceedings{Chivukula2020DimensionsUX,
abstract = {HCI researchers are increasingly interested in describing the complexity of design practice, including ethical, organiza- tional, and societal concerns. Recent studies have identified individual practitioners as key actors in driving the design process and culture within their respective organizations, and we build upon these efforts to reveal practitioner concerns re- garding ethics on their own terms. In this paper, we report on the results of an interview study with eleven UX practitioners, capturing their experiences that highlight dimensions of de- sign practice that impact ethical awareness and action. Using a bottom-up thematic analysis, we identified five dimensions of design complexity that influence ethical outcomes and span individual, collaborative, and methodological framing of UX activity. Based on these findings, we propose a set of impli- cations for the creation of ethically-centered design methods that resonate with this complexity and inform the education of future},
address = {New York, NY, USA},
author = {Chivukula, Shruthi Sai and Watkins, Chris Rhys and Manocha, Rhea and Chen, Jingle and Gray, Colin M.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
doi = {10.1145/3313831.3376459},
file = {:C\:/Users/ryw9/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chivukula et al. - 2020 - Dimensions of UX Practice that Shape Ethical Awareness.pdf:pdf},
isbn = {9781450367080},
month = {apr},
pages = {1--13},
publisher = {ACM},
title = {{Dimensions of UX Practice that Shape Ethical Awareness}},
url = {https://dl.acm.org/doi/10.1145/3313831.3376459},
year = {2020}
}

@book{Bamberger2015PrivacyGround,
address = {Cambridge, Massachusetts},
author = {Bamberger, Kenneth A. and Mulligan, Deirdre K.},
publisher = {The MIT Press},
title = {{Privacy on the Ground: Driving Corporate Behavior in the United States and Europe}},
year = {2015}
}

@ARTICLE{Crockett2021BuildingTrustworthy,  
author={Crockett, Keeley Alexandra and Gerber, Luciano and Latham, Annabel and Colyer, Edwin},  
journal={IEEE Transactions on Artificial Intelligence},   
title={Building Trustworthy AI Solutions: A Case for Practical Solutions for Small Businesses},   
year={2021},  
volume={},  
number={},  
pages={1-1},  
doi={10.1109/TAI.2021.3137091}}

@techreport{Alston2019UNReportPoverty,
abstract = {The digital welfare state is either already a reality or emerging in many countries across the globe. In these states, systems of social protection and assistance are increasingly driven by digital data and technologies that are used to automate, predict, identify, surveil, detect, target and punish. In the present report, the irresistible attractions for Governments to move in this direction are acknowledged, but the grave risk of stumbling, zombie-like, into a digital welfare dystopia is highlighted. It is argued that big technology companies (frequently referred to as “big tech”) operate in an almost human rights-free zone, and that this is especially problematic when the private sector is taking a leading role in designing, constructing and even operating significant parts of the digital welfare state. It is recommended in the report that, instead of obsessing about fraud, cost savings, sanctions, and market -driven definitions of efficiency, the starting point should be on how welfare budgets could be transformed through technology to ensure a higher standard of living for the vulnerable and disadvantaged.},
author = {Alston, Philip},
booktitle = {United Nations General Assembly},
file = {:C\:/Users/ryw9/Box/Papers Archive/Alston (2019) Report of the Special Rapporteur on extreme poverty and.pdf:pdf},
institution = {United Nations},
number = {October},
pages = {1--23},
title = {{Report of the Special Rapporteur on extreme poverty and human rights}},
url = {https://undocs.org/A/74/493},
volume = {17564},
year = {2019}
}

@book{ahmed2012being,
  title={On being included},
  author={Ahmed, Sara},
  year={2012},
  publisher={Duke University Press},
  address={Durham, NC}
}

@book{Onuoha2018PeoplesGuideAI,
author = {Onuoha, Mim and Nucera, Diana},
file = {:C\:/Users/ryw9/Box/Papers Archive/Onuoha, Nucera (2018) People's Guide to AI.pdf:pdf},
publisher = {Allied Media Projects},
title = {{A People's Guide to AI}},
url = {https://alliedmedia.org/resources/peoples-guide-to-ai},
year = {2018}
}

@article{Abdurahman2021Body,
author = {Abdurahman, J Khadijah},
file = {:C\:/Users/ryw9/Box/Papers Archive/Abdurahman (2021) A Body of Work That Cannot Be Ignored.pdf:pdf},
journal = {Logic},
number = {15: Beacons},
title = {{A Body of Work That Cannot Be Ignored}},
url = {https://logicmag.io/beacons/a-body-of-work-that-cannot-be-ignored/},
year = {2021}
}

@misc{Raval2021NewAILexicon,
author = {Raval, Noopur and Kak, Amba},
booktitle = {AI Now Institute},
file = {:C\:/Users/ryw9/Box/Papers Archive/Raval, Kak (2021) A New AI Lexicon_ Responses and Challenges to the Critical AI discourse _ by AI Now Institute _ A New AI Lexicon _ Medium.pdf:pdf},
title = {{A New AI Lexicon: Responses and Challenges to the Critical AI discourse}},
url = {https://medium.com/a-new-ai-lexicon/a-new-ai-lexicon-responses-and-challenges-to-the-critical-ai-discourse-f2275989fa62},
urldate = {2022-01-07},
year = {2021}
}

@misc{Ozoma2021TechWorkerHandbook,
author = {Ozoma, Ifeoma},
booktitle = {The Tech Worker Handbook},
title = {{The Tech Worker Handbook}},
url = {https://techworkerhandbook.org/},
urldate = {2022-01-07},
year = {2021}
}

@misc{LittleSis2017MapThePower,
author = {LittleSis},
title = {{Map the Power Toolkit}},
url = {https://littlesis.org/toolkit},
urldate = {2022-01-07},
year = {2017},
}


@inproceedings{mitchell2019model,
 author = {Mitchell, Margaret and Wu, Simone and Zaldivar, Andrew and Barnes, Parker and Vasserman, Lucy and Hutchinson, Ben and Spitzer, Elena and Raji, Inioluwa Deborah and Gebru, Timnit},
title = {Model Cards for Model Reporting},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287596},
doi = {10.1145/3287560.3287596},
abstract = {Trained machine learning models are increasingly used to perform high-impact tasks in areas such as law enforcement, medicine, education, and employment. In order to clarify the intended use cases of machine learning models and minimize their usage in contexts for which they are not well suited, we recommend that released models be accompanied by documentation detailing their performance characteristics. In this paper, we propose a framework that we call model cards, to encourage such transparent model reporting. Model cards are short documents accompanying trained machine learning models that provide benchmarked evaluation in a variety of conditions, such as across different cultural, demographic, or phenotypic groups (e.g., race, geographic location, sex, Fitzpatrick skin type [15]) and intersectional groups (e.g., age and race, or sex and Fitzpatrick skin type) that are relevant to the intended application domains. Model cards also disclose the context in which models are intended to be used, details of the performance evaluation procedures, and other relevant information. While we focus primarily on human-centered machine learning models in the application fields of computer vision and natural language processing, this framework can be used to document any trained machine learning model. To solidify the concept, we provide cards for two supervised models: One trained to detect smiling faces in images, and one trained to detect toxic comments in text. We propose model cards as a step towards the responsible democratization of machine learning and related artificial intelligence technology, increasing transparency into how well artificial intelligence technology works. We hope this work encourages those releasing trained machine learning models to accompany model releases with similar detailed evaluation numbers and other relevant documentation.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {220–229},
numpages = {10},
keywords = {disaggregated evaluation, fairness evaluation, ethical considerations, ML model evaluation, model cards, documentation, datasheets},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}

@article{gebru2021datasheets,
  title={Datasheets for datasets},
  author={Gebru, Timnit and Morgenstern, Jamie and Vecchione, Briana and Vaughan, Jennifer Wortman and Wallach, Hanna and Iii, Hal Daum{\'e} and Crawford, Kate},
  journal={Communications of the ACM},
  volume={64},
  number={12},
  pages={86--92},
  year={2021},
  publisher={ACM New York, NY, USA},
  doi={10.1145/3458723}
}

@inproceedings{jacobs2021measurement,
  author = {Jacobs, Abigail Z. and Wallach, Hanna},
title = {Measurement and Fairness},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445901},
doi = {10.1145/3442188.3445901},
abstract = {We propose measurement modeling from the quantitative social sciences as a framework for understanding fairness in computational systems. Computational systems often involve unobservable theoretical constructs, such as socioeconomic status, teacher effectiveness, and risk of recidivism. Such constructs cannot be measured directly and must instead be inferred from measurements of observable properties (and other unobservable theoretical constructs) thought to be related to them---i.e., operationalized via a measurement model. This process, which necessarily involves making assumptions, introduces the potential for mismatches between the theoretical understanding of the construct purported to be measured and its operationalization. We argue that many of the harms discussed in the literature on fairness in computational systems are direct results of such mismatches. We show how some of these harms could have been anticipated and, in some cases, mitigated if viewed through the lens of measurement modeling. To do this, we contribute fairness-oriented conceptualizations of construct reliability and construct validity that unite traditions from political science, education, and psychology and provide a set of tools for making explicit and testing assumptions about constructs and their operationalizations. We then turn to fairness itself, an essentially contested construct that has different theoretical understandings in different contexts. We argue that this contestedness underlies recent debates about fairness definitions: although these debates appear to be about different operationalizations, they are, in fact, debates about different theoretical understandings of fairness. We show how measurement modeling can provide a framework for getting to the core of these debates.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {375–385},
numpages = {11},
keywords = {construct reliability, construct validity, measurement, fairness},
location = {Virtual Event, Canada},
series = {FAccT '21}
}

@article{delgado2021stakeholder,
  title={Stakeholder Participation in AI: Beyond" Add Diverse Stakeholders and Stir"},
  author={Delgado, Fernando and Yang, Stephen and Madaio, Michael and Yang, Qian},
  journal={arXiv preprint arXiv:2111.01122},
  year={2021},
  numpages = {7}
}

@article{sloane2020participation,
author = {Sloane, Mona and Moss, Emanuel and Awomolo, Olaitan and Forlano, Laura},
title = {Participation Is Not a Design Fix for Machine Learning},
year = {2022},
isbn = {9781450394772},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551624.3555285},
doi = {10.1145/3551624.3555285},
abstract = {This paper critiques popular modes of participation in design practice and machine learning. It examines three existing kinds of participation in design practice and machine learning participation as work, participation as consultation, and as participation as justice – to argue that the machine learning community must become attuned to possibly exploitative and extractive forms of community involvement and shift away from the prerogatives of context independent scalability. Cautioning against “participation washing”, it argues that the notion of “participation” should be expanded to acknowledge more subtle, and possibly exploitative, forms of community involvement in participatory machine learning design. Specifically, it suggests that it is imperative to recognize design participation as work; to ensure that participation as consultation is context-specific; and that participation as justice must be genuine and long term. The paper argues that such a development can only be scaffolded by a new epistemology around design harms, including, but not limited to, in machine learning. To facilitate such a development, the paper suggests developing we argue that developing a cross-sectoral database of design participation failures that is cross-referenced with socio-structural dimensions and highlights “edge cases” that can and must be learned from.},
booktitle = {Equity and Access in Algorithms, Mechanisms, and Optimization},
articleno = {1},
numpages = {6},
keywords = {machine learning, participatory methods, design},
location = {Arlington, VA, USA},
series = {EAAMO '22}
}

@article{madaio2022assessing,
  title={Assessing the Fairness of AI Systems: AI Practitioners' Processes, Challenges, and Needs for Support},
  author={Madaio, Michael and Egede, Lisa and Subramonyam, Hariharan and Wortman Vaughan, Jennifer and Wallach, Hanna},
  journal={Proceedings of the ACM on Human-Computer Interaction},
  volume={6},
  number={CSCW1},
  pages={1--26},
  year={2022},
  publisher={ACM New York, NY, USA},
  doi={10.1145/3512899}
}

@inproceedings{deng2022exploring,
  author = {Deng, Wesley Hanwen and Nagireddy, Manish and Lee, Michelle Seng Ah and Singh, Jatinder and Wu, Zhiwei Steven and Holstein, Kenneth and Zhu, Haiyi},
title = {Exploring How Machine Learning Practitioners (Try To) Use Fairness Toolkits},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533113},
doi = {10.1145/3531146.3533113},
abstract = {Recent years have seen the development of many open-source ML fairness toolkits aimed at helping ML practitioners assess and address unfairness in their systems. However, there has been little research investigating how ML practitioners actually use these toolkits in practice. In this paper, we conducted the first in-depth empirical exploration of how industry practitioners (try to) work with existing fairness toolkits. In particular, we conducted think-aloud interviews to understand how participants learn about and use fairness toolkits, and explored the generality of our findings through an anonymous online survey. We identified several opportunities for fairness toolkits to better address practitioner needs and scaffold them in using toolkits effectively and responsibly. Based on these findings, we highlight implications for the design of future open-source fairness toolkits that can support practitioners in better contextualizing, communicating, and collaborating around ML fairness efforts.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {473–484},
numpages = {12},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}

@article{shilton2018values,
  title={Values and ethics in human-computer interaction},
  author={Shilton, Katie},
  journal={Foundations and Trends{\textregistered} in Human--Computer Interaction},
  volume={12},
  number={2},
  pages={107--171},
  year={2018},
  publisher={Now Publishers, Inc.},
  doi={10.1561/1100000073}
}

@inproceedings{shen2022model,
  author = {Shen, Hong and Wang, Leijie and Deng, Wesley H. and Brusse, Ciell and Velgersdijk, Ronald and Zhu, Haiyi},
title = {The Model Card Authoring Toolkit: Toward Community-Centered, Deliberation-Driven AI Design},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533110},
doi = {10.1145/3531146.3533110},
abstract = {There have been increasing calls for centering impacted communities – both online and offline – in the design of the AI systems that will be deployed in their communities. However, the complicated nature of a community’s goals and needs, as well as the complexity of AI’s development procedures, outputs, and potential impacts, often prevents effective participation. In this paper, we present the Model Card Authoring Toolkit, a toolkit that supports community members to understand, navigate and negotiate a spectrum of machine learning models via deliberation and pick the ones that best align with their collective values. Through a series of workshops, we conduct an empirical investigation of the initial effectiveness of our approach in two online communities – English and Dutch Wikipedia, and document how our participants collectively set the threshold for a machine learning based quality prediction system used in their communities’ content moderation applications. Our results suggest that the use of the Model Card Authoring Toolkit helps improve the understanding of the trade-offs across multiple community goals on AI design, engage community members to discuss and negotiate the trade-offs, and facilitate collective and informed decision-making in their own community contexts. Finally, we discuss the challenges for a community-centered, deliberation-driven approach for AI design as well as potential design implications.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {440–451},
numpages = {12},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}

@article{watkins2022four,
  title={The four-fifths rule is not disparate impact: a woeful tale of epistemic trespassing in algorithmic fairness},
  author={Watkins, Elizabeth Anne and McKenna, Michael and Chen, Jiahao},
  journal={arXiv preprint arXiv:2202.09519},
  year={2022}
}

@book{gray2019ghost,
  title={Ghost work: How to stop Silicon Valley from building a new global underclass},
  author={Gray, Mary L and Suri, Siddharth},
  year={2019},
  publisher={Houghton Mifflin Harcourt},
  address={Boston}
}

@inproceedings{bray2022radical,
  author = {Bray, Kirsten E and Harrington, Christina and Parker, Andrea G and Diakhate, N'Deye and Roberts, Jennifer},
title = {Radical Futures: Supporting Community-Led Design Engagements through an Afrofuturist Speculative Design Toolkit},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3501945},
doi = {10.1145/3491102.3501945},
abstract = {When considering the democratic intentions of co-design, designers and design researchers must evaluate the impact of power imbalances embedded in common design and research dynamics. This holds particularly true in work with and for marginalized communities, who are frequently excluded in design processes. To address this issue, we examine how existing design tools and methods are used to support communities in processes of community building or reimagining, considering the influence of race and identity. This paper describes our findings from 27 interviews with community design practitioners conducted to evaluate the Building Utopia toolkit, which employs an Afrofuturist lens for speculative design processes. Our research findings support the importance of design tools that prompt conversations on race in design, and tensions between the desire for imaginative design practice and the immediacy of social issues, particularly when designing with Black and brown communities.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {452},
numpages = {13},
keywords = {participatory design, method, qualitative methods, design research methods, design methods},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{wong2020beyondchecklists,
address = {New York, NY, USA},
author = {Wong, Richmond Y and Boyd, Karen and Metcalf, Jake and Shilton, Katie},
booktitle = {Conference Companion Publication of the 2020 on Computer Supported Cooperative Work and Social Computing},
doi = {10.1145/3406865.3418590},
file = {:C\:/Users/ryw9/Box/Papers Archive/Wong et al (2020) Beyond check list approaches to ethics in design.pdf:pdf},
isbn = {9781450380591},
month = {oct},
pages = {511--517},
publisher = {ACM},
title = {{Beyond Checklist Approaches to Ethics in Design}},
url = {https://dl.acm.org/doi/10.1145/3406865.3418590},
year = {2020}
}


@inproceedings{luger2015playing,
address = {New York, New York, USA},
author = {Luger, Ewa and Urquhart, Lachlan and Rodden, Tom and Golembewski, Michael},
booktitle = {Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems - CHI '15},
doi = {10.1145/2702123.2702142},
file = {:C\:/Users/ryw9/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Luger et al. - 2015 - Playing the Legal Card Using Ideation Cards to Raise Data Protection Issues within the Design Process.pdf:pdf},
isbn = {9781450331456},
pages = {457--466},
publisher = {ACM Press},
title = {{Playing the Legal Card: Using Ideation Cards to Raise Data Protection Issues within the Design Process}},
url = {http://dx.doi.org/10.1145/2702123.2702142 http://dl.acm.org/citation.cfm?doid=2702123.2702142},
year = {2015}
}

@article{shilton2020rolepplaying,
author = {Shilton, Katie and Heidenblad, Donal and Porter, Adam and Winter, Susan and Kendig, Mary},
doi = {10.1007/s11948-020-00250-0},
issn = {1353-3452},
journal = {Science and Engineering Ethics},
month = {Jul},
title = {{Role-Playing Computer Ethics: Designing and Evaluating the Privacy by Design (PbD) Simulation}},
url = {http://link.springer.com/10.1007/s11948-020-00250-0},
year = {2020}
}
@incollection{flanagan2014values,
address = {Cambridge, Massachusetts},
author = {Flanagan, Mary and Nissenbaum, Helen},
booktitle = {Values at Play in Digital Games},
chapter = {1},
publisher = {MIT Press},
title = {{Groundwork for Values in Games}},
year = {2014},
}

@article{boyd2021datasheets,
author = {Boyd, Karen L},
doi = {10.1145/3479582},
file = {:C\:/Users/ryw9/Box/Papers Archive/Boyd (2021) Datasheets for datasets help ML engineers notice and understand ethical issues in training data.pdf:pdf},
issn = {2573-0142},
journal = {Proceedings of the ACM on Human-Computer Interaction},
keywords = {development practices,ethical sensitivity,ethics,machine learning,training data},
month = {oct},
number = {CSCW2},
pages = {1--27},
publisher = {Association for Computing Machinery},
title = {{Datasheets for Datasets help ML Engineers Notice and Understand Ethical Issues in Training Data}},
url = {https://dl.acm.org/doi/10.1145/3479582},
volume = {5},
year = {2021}
}

@book{bowker1999sorting,
  title={Sorting things out},
  author={Bowker, Geoffrey and Star, Susan Leigh},
  journal={Classification and its consequences},
  volume={4},
  year={1999},
  publisher={MIT Press},
  address={Cambridge, MA}
}


@article{hoffmann2019wherefairness,
	title = {Where fairness fails: data, algorithms, and the limits of antidiscrimination discourse},
	volume = {22},
	issn = {1369-118X, 1468-4462},
	shorttitle = {Where fairness fails},
	url = {https://www.tandfonline.com/doi/full/10.1080/1369118X.2019.1573912},
	doi = {10.1080/1369118X.2019.1573912},
	language = {en},
	number = {7},
	urldate = {2022-10-14},
	journal = {Information, Communication \& Society},
	author = {Hoffmann, Anna Lauren},
	month = jun,
	year = {2019},
	pages = {900--915},
	file = {Hoffmann - 2019 - Where fairness fails data, algorithms, and the li.pdf:C\:\\Users\\ryw9\\Zotero\\storage\\ZE5HZ5RX\\Hoffmann - 2019 - Where fairness fails data, algorithms, and the li.pdf:application/pdf},
}

@article{green2021datascience,
	title = {Data {Science} as {Political} {Action}: {Grounding} {Data} {Science} in a {Politics} of {Justice}},
	volume = {2},
	issn = {2688-5255},
	shorttitle = {Data {Science} as {Political} {Action}},
	url = {https://ieeexplore.ieee.org/document/9684742/},
	doi = {10.23919/JSC.2021.0029},
	number = {3},
	urldate = {2022-10-14},
	journal = {Journal of Social Computing},
	author = {Green, Ben},
	month = sep,
	year = {2021},
	pages = {249--265},
	file = {Full Text:C\:\\Users\\ryw9\\Zotero\\storage\\LTBL88EB\\Green - 2021 - Data Science as Political Action Grounding Data S.pdf:application/pdf},
}

@inproceedings{bietti2020ethicswashing,
author = {Bietti, Elettra},
title = {From Ethics Washing to Ethics Bashing: A View on Tech Ethics from within Moral Philosophy},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372860},
doi = {10.1145/3351095.3372860},
abstract = {The word 'ethics' is under siege in technology policy circles. Weaponized in support of deregulation, self-regulation or handsoff governance, "ethics" is increasingly identified with technology companies' self-regulatory efforts and with shallow appearances of ethical behavior. So-called "ethics washing" by tech companies is on the rise, prompting criticism and scrutiny from scholars and the tech community at large. In parallel to the growth of ethics washing, its condemnation has led to a tendency to engage in "ethics bashing." This consists in the trivialization of ethics and moral philosophy now understood as discrete tools or pre-formed social structures such as ethics boards, self-governance schemes or stakeholder groups.The misunderstandings underlying ethics bashing are at least threefold: (a) philosophy and "ethics" are seen as a communications strategy and as a form of instrumentalized cover-up or fa\c{c}ade for unethical behavior, (b) philosophy is understood in opposition and as alternative to political representation and social organizing and (c) the role and importance of moral philosophy is downplayed and portrayed as mere "ivory tower" intellectualization of complex problems that need to be dealt with in practice.This paper argues that the rhetoric of ethics and morality should not be reductively instrumentalized, either by the industry in the form of "ethics washing," or by scholars and policy-makers in the form of "ethics bashing." Grappling with the role of philosophy and ethics requires moving beyond both tendencies and seeing ethics as a mode of inquiry that facilitates the evaluation of competing tech policy strategies. In other words, we must resist narrow reductivism of moral philosophy as instrumentalized performance and renew our faith in its intrinsic moral value as a mode of knowledgeseeking and inquiry. Far from mandating a self-regulatory scheme or a given governance structure, moral philosophy in fact facilitates the questioning and reconsideration of any given practice, situating it within a complex web of legal, political and economic institutions. Moral philosophy indeed can shed new light on human practices by adding needed perspective, explaining the relationship between technology and other worthy goals, situating technology within the human, the social, the political. It has become urgent to start considering technology ethics also from within and not only from outside of ethics.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {210–219},
numpages = {10},
keywords = {ethics, technology ethics, technology law, AI, moral philosophy, self-regulation, regulation},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{mcmillan2019againstethical,
author = {McMillan, Donald and Brown, Barry},
title = {Against Ethical AI},
year = {2019},
isbn = {9781450372039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3363384.3363393},
doi = {10.1145/3363384.3363393},
abstract = {In this paper we use the EU guidelines on ethical AI, and the responses to it, as a starting point to discuss the problems with our community’s focus on such manifestos, principles, and sets of guidelines. We cover how industry and academia are at times complicit in ‘Ethics Washing’, how developing guidelines carries the risk of diluting our rights in practice, and downplaying the role of our own self interest. We conclude by discussing briefly the role of technical practice in ethics.},
booktitle = {Proceedings of the Halfway to the Future Symposium 2019},
articleno = {9},
numpages = {3},
keywords = {Ethics, Artificial Intelligence, Policy, Human Rights, Algorithms},
location = {Nottingham, United Kingdom},
series = {HTTF 2019}
}