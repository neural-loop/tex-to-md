%Though AI ethics has emerged as a profoundly important endeavor, few have sought to provide the epistemological-ontological ground to answer questions in a principled, foundational way.

Work in AI ethics and fairness has made much progress in regulating LLMs to reflect certain values, such as fairness, truth, and diversity.
However, it has taken the problem of \textit{how LLMs might `mean' anything at all} for granted.
Without addressing this, it is not clear what imbuing LLMs with such values even \textit{means}.
% This `standing without a ground'. 
% This silence renders AI ethics in a virtual invention of what values are in the LLM.
In response, we provide a \textit{general} theory of meaning that extends beyond humans.
We use this theory to explicate the precise nature of LLMs as meaning-agents.
We suggest that the LLM, by virtue of its position as a meaning-agent, already grasps the constructions of human society (e.g. morality, gender, and race) \textit{in concept}.
Consequently, under certain ethical frameworks, currently popular methods for model alignment are limited at best and counterproductive at worst.
Moreover, unaligned models may help us better develop our moral and social philosophy.

%fundamental question:
%\textit{How do LLMs `mean' anything at all?}


%How do LLMs mean?

%AI ethics has made much progress in developing pragmatic solutions to real problems, but it has left its epistemic and ontological ground in the dust.

%\jared{Who are we responding to? If there isn't something we're clearly responding to this sounds like a rebuke. }
%\andre{Maybe a less confrontational way to say this is something like ``Much progress has been made in AI ethics `in practice', but its epistemic and ontological ground is still undeveloped.'' etc.}
%\jared{I like this takeaway}
%\andre{+1}