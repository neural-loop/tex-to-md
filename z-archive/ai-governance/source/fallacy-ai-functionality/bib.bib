@article{shankar2021towards,
  title={Towards Observability for Machine Learning Pipelines},
  author={Shankar, Shreya and Parameswaran, Aditya},
  journal={arXiv preprint arXiv:2108.13557},
  year={2021}
}

@article{mulligan2019procurement,
  title={Procurement as policy: Administrative process for machine learning},
  author={Mulligan, Deirdre K and Bamberger, Kenneth A},
  journal={Berkeley Tech. LJ},
  volume={34},
  pages={773},
  year={2019},
  publisher={HeinOnline}
}

@article{richardson2021best,
  title={Best Practices for Government Procurement of Data-Driven Technologies},
  author={Richardson, Rashida},
  journal={Available at SSRN 3855637},
  year={2021}
}

@article{rubenstein2021acquiring,
  title={Acquiring ethical AI},
  author={Rubenstein, David S},
  journal={Florida Law Review},
  volume={73},
  year={2021}
}

@inproceedings{bender2020climbing,
  title={Climbing towards NLU: On meaning, form, and understanding in the age of data},
  author={Bender, Emily M and Koller, Alexander},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={5185--5198},
  year={2020}
}

@misc{menegus2019defense,
  title={Defense of amazon’s face recognition tool undermined by its only known police client},
  author={Menegus, Brian},
  year={2019},
  publisher={Gizmodo}
}

@article{garvie2019garbage,
  title={Garbage in, Garbage out. Face recognition on flawed data},
  author={Garvie, Clare},
  journal={Georgetown Law Center on Privacy \& Technology},
  year={2019}
}

@misc{uber-crash,
  AUTHOR =        {National Transportation Safety Board},
  TITLE =         {Collision Between Vehicle Controlled by Developmental Automated Driving System and Pedestrian},
  NUMBER =        {HWY18MH010},
  INSTITUTION =   {National Transportation Safety Board},
  YEAR  =         {2018},
  FILE  =         {https://ntsb.gov/investigations/Pages/HWY18FH010.aspx},
  URL   =         {https://ntsb.gov/investigations/Pages/HWY18FH010.aspx},
}

@misc{uber-crash-2,
  AUTHOR =        {Eric Weiss}, 
  TITLE =         {‘Inadequate Safety Culture’ Contributed to Uber Automated Test Vehicle Crash - NTSB Calls for Federal Review Process for Automated Vehicle Testing on Public Roads},
  NUMBER =        {HWY18MH010},
  INSTITUTION =   {National Transportation Safety Board},
  YEAR  =         {2019},
  FILE  =         {https://www.ntsb.gov/news/press-releases/Pages/NR20191119c.aspx},
  URL   =         {https://www.ntsb.gov/news/press-releases/Pages/NR20191119c.aspx},
}

@misc{tesla-crash,
  AUTHOR =        {National Transportation Safety Board},
  TITLE =         {Collision Between a Car Operating With Automated Vehicle Control Systems
and a Tractor-Semitrailer Truck},
  NUMBER =        {HWY16FH018},
  INSTITUTION =   {National Transportation Safety Board},
  YEAR  =         {2017},
  FILE  =         {https://ntsb.gov/investigations/Pages/HWY18FH010.aspx},
  URL   =         {https://ntsb.gov/investigations/Pages/HWY18FH010.aspx},
}

@misc{tesla-crash-2,
  AUTHOR =        {National Transportation Safety Board},
  TITLE =         {Driver Errors, Overreliance on Automation, Lack of Safeguards, Led to Fatal Tesla Crash},
  NUMBER =        {HWY16FH018},
  INSTITUTION =   {National Transportation Safety Board},
  YEAR  =         {2017},
  FILE  =         {https://www.ntsb.gov/news/press-releases/pages/pr20170912.aspx},
  URL   =         {https://www.ntsb.gov/news/press-releases/pages/pr20170912.aspx},
}

@article{ratner2019mlsys,
  title={MLSys: The new frontier of machine learning systems},
  author={Ratner, Alexander and Alistarh, Dan and Alonso, Gustavo and Andersen, David G and Bailis, Peter and Bird, Sarah and Carlini, Nicholas and Catanzaro, Bryan and Chayes, Jennifer and Chung, Eric and others},
  journal={arXiv preprint arXiv:1904.03257},
  year={2019}
}

@inproceedings{bender2020climbing,
  title={Climbing towards NLU: On meaning, form, and understanding in the age of data},
  author={Bender, Emily M and Koller, Alexander},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={5185--5198},
  year={2020}
}

@article{sloane2022silicon,
  title={A Silicon Valley love triangle: Hiring algorithms, pseudo-science, and the quest for auditability},
  author={Sloane, Mona and Moss, Emanuel and Chowdhury, Rumman},
  journal={Patterns},
  volume={3},
  number={2},
  pages={100425},
  year={2022},
  publisher={Elsevier}
}

@article{nayak2019understanding,
  title={Understanding searches better than ever before},
  author={Nayak, Pandu},
  journal={The Keyword},
  volume={295},
  year={2019}
}

@misc{covid_us,
  title={White House seeks Silicon Valley help battling coronavirus},
  author={Overly, Steven},
  year={2020},
  publisher={POLITICO}
}

@misc{covid_china,
  title={China's Use of AI in its COVID-19 Response},
  author={Weinstein, Emily },
  year={2020},
  publisher={Center for Security and Emerging Technology}
}

@ARTICLE{covid_greece,
  title    = "Greece used {AI} to curb {COVID}: what other nations can learn",
  journal  = "Nature",
  volume   =  597,
  number   =  7877,
  pages    = "447--448",
  year     =  2021,
  language = "en",
  author = {Nature Editorial}
}

@article{covid_africa,
  title={Leveraging Artificial Intelligence and Big Data to optimize COVID-19 clinical public health and vaccination roll-out strategies in Africa},
  author={Mellado, Bruce and Wu, Jianhong and Kong, Jude Dzevela and Bragazzi, Nicola Luigi and Asgary, Ali and Kawonga, Mary and Choma, Nalomotse and Hayasi, Kentaro and Lieberman, Benjamin and Mathaha, Thuso and others},
  journal={Available at SSRN 3787748},
  year={2021}
}

@article{covid_intl,
  title={Artificial intelligence (AI) provided early detection of the coronavirus (COVID-19) in China and will influence future Urban health policy internationally},
  author={Allam, Zaheer and Dey, Gourav and Jones, David S},
  journal={AI},
  volume={1},
  number={2},
  pages={156--165},
  year={2020},
  publisher={Multidisciplinary Digital Publishing Institute}
}



%Emily Weinstein, "China's Use of AI in its COVID-19 Response" (Center for Security and Emerging Technology, August 2020), https://cset.georgetown.edu/research/china's-use-of-ai-in-its-covid-19-response/. https://doi.org/10.51593/20200053

@article{havens2019principles,
  title={From Principles and Standards to Certification},
  author={Havens, John C and Hessami, Ali},
  journal={Computer},
  volume={52},
  number={4},
  pages={69--72},
  year={2019},
  publisher={IEEE}
}

@inproceedings{ieee_cert,
  title={Ai2: Safety and robustness certification of neural networks with abstract interpretation},
  author={Gehr, Timon and Mirman, Matthew and Drachsler-Cohen, Dana and Tsankov, Petar and Chaudhuri, Swarat and Vechev, Martin},
  booktitle={2018 IEEE Symposium on Security and Privacy (SP)},
  pages={3--18},
  year={2018},
  organization={IEEE}
}

@article{raji2019ml,
  title={About ml: Annotation and benchmarking on understanding and transparency of machine learning lifecycles},
  author={Raji, Inioluwa Deborah and Yang, Jingying},
  journal={arXiv preprint arXiv:1912.06166},
  year={2019}
}

@article{sloane2021ai,
  title={AI and Procurement-A Primer},
  author={Sloane, Mona and Chowdhury, Rumman and Havens, John C and Lazovich, Tomo and Rincon Alba, Luis},
  year={2021}
}

@article{UNESCO,
    author = {Raji, Inioluwa Deborah and Costanza-Chock, Sasha and Buolamwini, Joy},
    title = {Change From the Outside: Towards Credible Third-Party Audits of AI Systems},
    journal = {Missing Links in AI Policy}, 
    year = {2022},
    publisher = {UNESCO},
    address = {Paris, France},
}

@inproceedings{raji2020closing,
  title={Closing the AI accountability gap: Defining an end-to-end framework for internal algorithmic auditing},
  author={Raji, Inioluwa Deborah and Smart, Andrew and White, Rebecca N and Mitchell, Margaret and Gebru, Timnit and Hutchinson, Ben and Smith-Loud, Jamila and Theron, Daniel and Barnes, Parker},
  booktitle={Proceedings of the 2020 conference on fairness, accountability, and transparency},
  pages={33--44},
  year={2020}
}

@book{roland1991system,
  title={System safety engineering and management},
  author={Roland, Harold E and Moriarty, Brian},
  year={1991},
  publisher={John Wiley \& Sons}
}

@book{smith2004functional,
  title={Functional safety},
  author={Smith, David and Simpson, Kenneth},
  year={2004},
  publisher={Routledge}
}

@article{harris2019ntsb,
  title={NTSB investigation into deadly Uber self-driving car crash reveals lax attitude toward safety},
  author={Harris, M},
  journal={IEEE Spectrum},
  year={2019}
}

@article{benjamens2020state,
  title={The state of artificial intelligence-based FDA-approved medical devices and algorithms: an online database},
  author={Benjamens, Stan and Dhunnoo, Pranavsingh and Mesk{\'o}, Bertalan},
  journal={NPJ digital medicine},
  volume={3},
  number={1},
  pages={1--8},
  year={2020},
  publisher={Nature Publishing Group}
}

@article{rivera2020guidelines,
  title={Guidelines for clinical trial protocols for interventions involving artificial intelligence: the SPIRIT-AI extension},
  author={Rivera, Samantha Cruz and Liu, Xiaoxuan and Chan, An-Wen and Denniston, Alastair K and Calvert, Melanie J},
  journal={bmj},
  volume={370},
  year={2020},
  publisher={British Medical Journal Publishing Group}
}

@article{liu2020reporting,
  title={Reporting guidelines for clinical trial reports for interventions involving artificial intelligence: the CONSORT-AI extension},
  author={Liu, Xiaoxuan and Rivera, Samantha Cruz and Moher, David and Calvert, Melanie J and Denniston, Alastair K},
  journal={bmj},
  volume={370},
  year={2020},
  publisher={British Medical Journal Publishing Group}
}


@article{wu2021medical,
  title={How medical AI devices are evaluated: limitations and recommendations from an analysis of FDA approvals},
  author={Wu, Eric and Wu, Kevin and Daneshjou, Roxana and Ouyang, David and Ho, Daniel E and Zou, James},
  journal={Nature Medicine},
  volume={27},
  number={4},
  pages={582--584},
  year={2021},
  publisher={Nature Publishing Group}
}

@book{heppenheimer1995turbulent,
  title={Turbulent skies: the history of commercial aviation},
  author={Heppenheimer, Thomas A and Heppenheimer, Ta},
  year={1995},
  publisher={Wiley New York}
}

@book{silver2012signal,
  title={The signal and the noise: why so many predictions fail--but some don't},
  author={Silver, Nate},
  year={2012},
  publisher={Penguin}
}

@book{vinsel2019moving,
  title={Moving Violations: Automobiles, Experts, and Regulations in the United States},
  author={Vinsel, Lee},
  year={2019},
  publisher={JHU Press}
}

@article{nader1965unsafe,
  title={Unsafe at any speed. The designed-in dangers of the American automobile},
  author={Nader, Ralph},
  year={1965}
}

@book{bausell2009snake,
  title={Snake oil science: The truth about complementary and alternative medicine},
  author={Bausell, R Barker},
  year={2009},
  publisher={Oxford University Press}
}

@book{anderson2015snake,
  title={Snake oil, hustlers and hambones: the American medicine show},
  author={Anderson, Ann},
  year={2015},
  publisher={McFarland}
}

@book{blum2018poison,
  title={The Poison Squad: One Chemist's Single-minded Crusade for Food Safety at the Turn of the Twentieth Century},
  author={Blum, Deborah},
  year={2018},
  publisher={Penguin}
}

@article{moradi2021evaluating,
  title={Evaluating the robustness of neural language models to input perturbations},
  author={Moradi, Milad and Samwald, Matthias},
  journal={arXiv preprint arXiv:2108.12237},
  year={2021}
}

@article{pruthi2019combating,
  title={Combating adversarial misspellings with robust word recognition},
  author={Pruthi, Danish and Dhingra, Bhuwan and Lipton, Zachary C},
  journal={arXiv preprint arXiv:1905.11268},
  year={2019}
}

@article{berger2019mta,
  title={MTA’s Initial Foray Into Facial Recognition at High Speed Is a Bust},
  author={Berger, Paul},
  journal={The Wall Street Journal},
  year={2019}
}

@article{adadi2018peeking,
  title={Peeking inside the black-box: a survey on explainable artificial intelligence (XAI)},
  author={Adadi, Amina and Berrada, Mohammed},
  journal={IEEE access},
  volume={6},
  pages={52138--52160},
  year={2018},
  publisher={IEEE}
}

@inproceedings{bhatt2020explainable,
  title={Explainable machine learning in deployment},
  author={Bhatt, Umang and Xiang, Alice and Sharma, Shubham and Weller, Adrian and Taly, Ankur and Jia, Yunhan and Ghosh, Joydeep and Puri, Ruchir and Moura, Jos{\'e} MF and Eckersley, Peter},
  booktitle={Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
  pages={648--657},
  year={2020}
}

@misc {OED,
    key = {OED Online},
    publisher = {Oxford University Press},
    year = {2021},
    howpublished = "\url{https://www.oed.com/view/Entry/54950742}"
}

@MISC{AIAAIC,
  title        = "AI, Algorithmic and Automation Incident and Controversy Repository (AIAAIC)",
  author       = "{Charlie Pownall}",
  year         = "{2021}",
  howpublished = "\url{https://www.aiaaic.org/}"
}
 
@MISC{digiwatch,
  title        = "The COVID-19 crisis: A digital policy overview",
  publisher    = "{Digwatch}",
  author =       {Digwatch},
  year         = "{2021}",
  howpublished = "\url{https://dig.watch/trends/covid-19-crisis-digital-policy-overview/}"
}


@article{krass2021us,
  title={How US law will evaluate artificial intelligence for covid-19},
  author={Krass, Mark and Henderson, Peter and Mello, Michelle M and Studdert, David M and Ho, Daniel E},
  journal={bmj},
  volume={372},
  year={2021},
  publisher={British Medical Journal Publishing Group}
}

@inproceedings{raghavan2020mitigating,
  title={Mitigating bias in algorithmic hiring: Evaluating claims and practices},
  author={Raghavan, Manish and Barocas, Solon and Kleinberg, Jon and Levy, Karen},
  booktitle={Proceedings of the 2020 conference on fairness, accountability, and transparency},
  pages={469--481},
  year={2020}
}

@inproceedings{wilson2021building,
  title={Building and auditing fair algorithms: A case study in candidate screening},
  author={Wilson, Christo and Ghosh, Avijit and Jiang, Shan and Mislove, Alan and Baker, Lewis and Szary, Janelle and Trindel, Kelly and Polli, Frida},
  booktitle={Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
  pages={666--677},
  year={2021}
}

@misc{engler2021independent,
  title={Independent auditors are struggling to hold AI companies accountable. FastCompany},
  author={Engler, Alex C},
  year={2021}
}


@incollection{shaban2021explainability,
  title={Explainability and Interpretability: Keys to Deep Medicine},
  author={Shaban-Nejad, Arash and Michalowski, Martin and Buckeridge, David L},
  booktitle={Explainable AI in Healthcare and Medicine},
  pages={1--10},
  year={2021},
  publisher={Springer}
}







@MISC{missing_link_xai,
  title        = "Interpretability: The missing link between machine learning, healthcare, and the FDA?",
  publisher    = {H2O.ai Blog},
  author      = {Andrew Langsner and Patrick Hall},
  year         = {2018},
  howpublished = "\url{https://www.h2o.ai/blog/interpretability-the-missing-link-between-machine-learning-healthcare-and-the-fda/}"
}


@MISC{FDA,
  title        = "Good Machine Learning Practice for Medical Device Development: Guiding Principles",
  publisher    = "{U.S. Food and Drug Administration}",
  author = {U.S. Food and Drug Administration},
  year         = "{2021}",
  howpublished = "\url{https://www.fda.gov/medical-devices/software-medical-device-samd/good-machine-learning-practice-medical-device-development-guiding-principles}"
}

@MISC{AAA,
  key        = {Algorithmic Accountability Act of 2019}
  %year         = {2019},
  howpublished = "\url{https://www.congress.gov/bill/116th-congress/senate-bill/1108/text}"
}

@misc{GDPR,
  key = {General Data Protection Regulation},
  title        = "Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC (General Data Protection Regulation)", 
  year         = {2016}
  }


@misc{AI_Act,
 key = {AI Act},
 title        = "Proposal for a Regulation of the European Parliament and of the Council laying down harmonised rules on artificial intelligence (Artificial Intelligence Act) and amending certain Union legislative acts (COM(2021) 206 final)"%,
%howpublished = "\url{https://eur-lex.europa.eu/resource.html?uri=cellar:e0649735-a372-11eb-9585-01aa75ed71a1.0001.02/DOC_1&format=PDF}",
  year         = {2021}
}

@article{maccarthy2019examination,
  title={An Examination of the Algorithmic Accountability Act of 2019},
  author={MacCarthy, Mark},
  journal={Available at SSRN 3615731},
  year={2019}
}

@inproceedings{feldman2015certifying,
  title={Certifying and removing disparate impact},
  author={Feldman, Michael and Friedler, Sorelle A and Moeller, John and Scheidegger, Carlos and Venkatasubramanian, Suresh},
  booktitle={proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining},
  pages={259--268},
  year={2015}
}

@article{veale_euact,
  title={Demystifying the Draft EU Artificial Intelligence Act—Analysing the good, the bad, and the unclear elements of the proposed approach},
  author={Veale, Michael and Borgesius, Frederik Zuiderveen},
  journal={Computer Law Review International},
  volume={22},
  number={4},
  pages={97--112},
  year={2021},
  publisher={Verlag Dr. Otto Schmidt}
}




@MISC{goog_search_fail,
  title        = "How a Google search could end up endangering a life",
  publisher    = "{ITWire}",
  author       = "Sam Varghese",
  year         = "{2021}",
  howpublished = "\url{https://itwire.com/home-it/how-a-google-search-could-end-up-endangering-a-life.html}"
}

@article{ettinger2020bert,
  title={What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models},
  author={Ettinger, Allyson},
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={34--48},
  year={2020},
  publisher={MIT Press}
}

@misc{llmopenai,
      title={Release Strategies and the Social Impacts of Language Models}, 
      author={Irene Solaiman and Miles Brundage and Jack Clark and Amanda Askell and Ariel Herbert-Voss and Jeff Wu and Alec Radford and Gretchen Krueger and Jong Wook Kim and Sarah Kreps and Miles McCain and Alex Newhouse and Jason Blazakis and Kris McGuffie and Jasmine Wang},
      year={2019},
      eprint={1908.09203},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{llmdeepmind,
  title={Ethical and social risks of harm from Language Models},
  author={Weidinger, Laura and Mellor, John and Rauh, Maribeth and Griffin, Conor and Uesato, Jonathan and Huang, Po-Sen and Cheng, Myra and Glaese, Mia and Balle, Borja and Kasirzadeh, Atoosa and others},
  journal={arXiv preprint arXiv:2112.04359},
  year={2021}
}


@article{brundage2018malicious,
  title={The malicious use of artificial intelligence: Forecasting, prevention, and mitigation},
  author={Brundage, Miles and Avin, Shahar and Clark, Jack and Toner, Helen and Eckersley, Peter and Garfinkel, Ben and Dafoe, Allan and Scharre, Paul and Zeitzoff, Thomas and Filar, Bobby and others},
  journal={arXiv preprint arXiv:1802.07228},
  year={2018}
}

@article{jaques2019moral,
  title={Why the moral machine is a monster},
  author={Jaques, Abby Everett},
  journal={University of Miami School of Law},
  volume={10},
  year={2019}
}

@article{talat2021word,
  title={A Word on Machine Ethics: A Response to Jiang et al.(2021)},
  author={Talat, Zeerak and Blix, Hagen and Valvoda, Josef and Ganesh, Maya Indira and Cotterell, Ryan and Williams, Adina},
  journal={arXiv preprint arXiv:2111.04158},
  year={2021}
}

@article{jiang2021delphi,
  title={Delphi: Towards machine ethics and norms},
  author={Jiang, Liwei and Hwang, Jena D and Bhagavatula, Chandra and Bras, Ronan Le and Forbes, Maxwell and Borchardt, Jon and Liang, Jenny and Etzioni, Oren and Sap, Maarten and Choi, Yejin},
  journal={arXiv preprint arXiv:2110.07574},
  year={2021}
}

@article{awad2018moral,
  title={The moral machine experiment},
  author={Awad, Edmond and Dsouza, Sohan and Kim, Richard and Schulz, Jonathan and Henrich, Joseph and Shariff, Azim and Bonnefon, Jean-Fran{\c{c}}ois and Rahwan, Iyad},
  journal={Nature},
  volume={563},
  number={7729},
  pages={59--64},
  year={2018},
  publisher={Nature Publishing Group}
}

@misc{shane2019janelle,
  title={Janelle Shane: The danger of AI is weirder than you think TED Talk, 10: 20. Katsottu 8.8, 2020},
  author={Shane, J},
  year={2019}
}

@article{brundage2015taking,
  title={Taking superintelligence seriously: Superintelligence: Paths, dangers, strategies by Nick Bostrom (Oxford University Press, 2014)},
  author={Brundage, Miles},
  journal={Futures},
  volume={72},
  pages={32--35},
  year={2015},
  publisher={Elsevier}
}

@book{bostrom2014superintelligence,
  title={Superintelligence: Paths, Dangers, Strategies},
  author={Bostrom, N.},
  isbn={9780199678112},
  lccn={2013955152},
  url={https://books.google.com/books?id=7\_H8AwAAQBAJ},
  year={2014},
  publisher={Oxford University Press}
}

@inproceedings{prunkl2020beyond,
  title={Beyond near-and long-term: Towards a clearer account of research priorities in AI ethics and society},
  author={Prunkl, Carina and Whittlestone, Jess},
  booktitle={Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
  pages={138--143},
  year={2020}
}

%an alt cite if we don't want to use crawford 
@article{atkinson2018going,
  title={" It Is Going to Kill Us!" and Other Myths About the Future of Artificial Intelligence},
  author={Atkinson, Robert D},
  journal={IUP Journal of Computer Sciences},
  volume={12},
  number={4},
  pages={7--56},
  year={2018},
  publisher={IUP Publications}
}

@article{crawford2016artificial,
  title={Artificial intelligence’s white guy problem},
  author={Crawford, Kate},
  journal={The New York Times},
  volume={25},
  number={06},
  year={2016}
}

@misc{covidfail_summ,
  title={Hundreds of AI tools have been built to catch covid. None of them helped},
  author={Heaven, Will Douglas},
  year={2021},
  publisher={MIT Technology Review}
}


@MISC{covidfail3,
  title        = "Data science and AI in the age of COVID-19",
  publisher    = "{The Alan Turing Institute}",
  author       = "Inken von Borzyskowski, Anjali Mazumder, Bilal Mateen, Michael Wooldridge",
  year         = "{2021}",
  howpublished = "\url{https://www.turing.ac.uk/sites/default/files/2021-06/data-science-and-ai-in-the-age-of-covid_full-report_2.pdf}"
}



@article{covidfail2,
  title={Prediction models for diagnosis and prognosis of covid-19: systematic review and critical appraisal},
  author={Wynants, Laure and Van Calster, Ben and Collins, Gary S and Riley, Richard D and Heinze, Georg and Schuit, Ewoud and Bonten, Marc MJ and Dahly, Darren L and Damen, Johanna A and Debray, Thomas PA and others},
  journal={bmj},
  volume={369},
  year={2020},
  publisher={British Medical Journal Publishing Group}
}


@article{covidfail1,
  title={Common pitfalls and recommendations for using machine learning to detect and prognosticate for COVID-19 using chest radiographs and CT scans},
  author={Roberts, Michael and Driggs, Derek and Thorpe, Matthew and Gilbey, Julian and Yeung, Michael and Ursprung, Stephan and Aviles-Rivero, Angelica I and Etmann, Christian and McCague, Cathal and Beer, Lucian and others},
  journal={Nature Machine Intelligence},
  volume={3},
  number={3},
  pages={199--217},
  year={2021},
  publisher={Nature Publishing Group}
}


@MISC{NAIR,
  title        = "Building a National AI Research Resource:  A Blueprint for the National Research Cloud",
  publisher    = "{Stanford University Human-centered Artificial Intelligence}",
  author       = "Daniel E. Ho, Jennifer King, Russell C. Wald, Christopher Wan",
  year         = "{2021}",
  howpublished = "\url{https://hai.stanford.edu/sites/default/files/2022-01/HAI_NRCR_v17.pdf}"
}


@MISC{NMIP,
  title        = "National Medical Imaging Platform (NMIP)",
  publisher    = "{NHSx}",
  author       = "NHS AI Lab",
  year         = "{2021}",
  howpublished = "\url{https://www.nhsx.nhs.uk/ai-lab/ai-lab-programmes/ai-in-imaging/national-medical-imaging-platform-nmip/}"
}



@article{democratization2,
  title={A case for Data Democratization},
  author={Awasthi, Pranjal and George, Jordana J},
  year={2020}
}

@inproceedings{democratization3,
  title={On the Democratization of AI},
  author={Garvey, Colin K},
  booktitle={Datapower Conference Proceedings},
  pages={5--3},
  year={2017}
}

@incollection{democratization,
  title={The Democratization of Artificial Intelligence},
  author={Sudmann, Andreas},
  booktitle={The Democratization of Artificial Intelligence},
  pages={9--32},
  year={2020},
  publisher={transcript-Verlag}
}

@article{ahmed2020democratization,
  title={The de-democratization of ai: Deep learning and the compute divide in artificial intelligence research},
  author={Ahmed, Nur and Wahed, Muntasir},
  journal={arXiv preprint arXiv:2010.15581},
  year={2020}
}

@article{yeung2020recommendation,
  title={Recommendation of the council on artificial intelligence (oecd)},
  author={Yeung, Karen},
  journal={International Legal Materials},
  volume={59},
  number={1},
  pages={27--34},
  year={2020},
  publisher={Cambridge University Press}
}

@article{slota2020good,
  title={Good systems, bad data?: Interpretations of AI hype and failures},
  author={Slota, Stephen C and Fleischmann, Kenneth R and Greenberg, Sherri and Verma, Nitin and Cummings, Brenna and Li, Lan and Shenefiel, Chris},
  journal={Proceedings of the Association for Information Science and Technology},
  volume={57},
  number={1},
  pages={e275},
  year={2020},
  publisher={Wiley Online Library}
}

@inproceedings{raji2020saving,
  title={Saving face: Investigating the ethical concerns of facial recognition auditing},
  author={Raji, Inioluwa Deborah and Gebru, Timnit and Mitchell, Margaret and Buolamwini, Joy and Lee, Joonseok and Denton, Emily},
  booktitle={Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
  pages={145--151},
  year={2020}
}

@article{barocas2021designing,
  title={Designing Disaggregated Evaluations of AI Systems: Choices, Considerations, and Tradeoffs},
  author={Barocas, Solon and Guo, Anhong and Kamar, Ece and Krones, Jacquelyn and Morris, Meredith Ringel and Vaughan, Jennifer Wortman and Wadsworth, Duncan and Wallach, Hanna},
  journal={arXiv preprint arXiv:2103.06076},
  year={2021}
}

@inproceedings{raji2019actionable,
  title={Actionable auditing: Investigating the impact of publicly naming biased performance results of commercial ai products},
  author={Raji, Inioluwa Deborah and Buolamwini, Joy},
  booktitle={Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
  pages={429--435},
  year={2019}
}

@article{richardson2021defining,
  title={Defining and Demystifying Automated Decision Systems},
  author={Richardson, Rashida},
  journal={Maryland Law Review, Forthcoming},
  year={2021}
}

@article{narayanan2019recognize,
  title={How to recognize AI snake oil},
  author={Narayanan, Arvind},
  journal={Arthur Miller Lecture on Science and Ethics},
  year={2019},
  publisher={Massachusetts Institute of Technology}
}

@inproceedings{bender2021dangers,
  title={On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
  author={Bender, Emily M and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
  booktitle={Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
  pages={610--623},
  year={2021}
}

@article{tennant2021attachments,
  title={The attachments of ‘autonomous’ vehicles},
  author={Tennant, Chris and Stilgoe, Jack},
  journal={Social Studies of Science},
  volume={51},
  number={6},
  pages={846--870},
  year={2021},
  publisher={SAGE Publications Sage UK: London, England}
}

@book{fake_ai,
  title={Fake AI},
  author={Kaltheuner, Frederike and Birhane, Abeba and Raji, Inioluwa Deborah and Amironesei, Razvan and Denton, Emily and Hanna, Alex and Nicole, Hilary and Smart, Andrew and Oduro, Serena Dokuaa and Vincent, James and Reben, Alexander and Milne, Gemma and Black, Crofton and Harvey, Adam and Strait, Andrew and Parida, Tulsi and Ashok, Aparna and Jansen, Fieke and Cath, Corinne and Peppin, Aidan},
  year={2021},
  publisher={Meatspace Press}
}


@book{broussard2018artificial,
  title={Artificial unintelligence: How computers misunderstand the world},
  author={Broussard, Meredith},
  year={2018},
  publisher={mit Press}
}

@article{sculley2015hidden,
  title={Hidden technical debt in machine learning systems},
  author={Sculley, David and Holt, Gary and Golovin, Daniel and Davydov, Eugene and Phillips, Todd and Ebner, Dietmar and Chaudhary, Vinay and Young, Michael and Crespo, Jean-Francois and Dennison, Dan},
  journal={Advances in neural information processing systems},
  volume={28},
  pages={2503--2511},
  year={2015}
}

@inproceedings{liao2021we,
  title={Are We Learning Yet? A Meta Review of Evaluation Failures Across Machine Learning},
  author={Liao, Thomas and Taori, Rohan and Raji, Inioluwa Deborah and Schmidt, Ludwig},
  booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)},
  year={2021}
}

@misc{kapoor_irreproducible_2021,
	title = {({Ir}){Reproducible} {Machine} {Learning}: {A} {Case} {Study}},
	abstract = {The use of Machine Learning (ML) methods for prediction and forecasting has become widespread across the quantitative sciences. However, there are many known methodological pitfalls in ML-based research. As a case study of these pitfalls, we examine the subﬁeld of civil war onset prediction in Political Science. Our main ﬁnding is that several recent studies published in top Political Science journals claiming superior performance of ML models over Logistic Regression models fail to reproduce. Our results provide two reasons to be skeptical of the use of ML methods in this research area, by both questioning their usefulness and highlighting the pitfalls of applying them correctly. Results identifying errors in studies that use ML methods have appeared in at least seven quantitative science ﬁelds. However, we go farther than most previous research to investigate whether the claims made in the reviewed studies survive once the errors are corrected. We argue that there is a reproducibility crisis brewing in research ﬁelds that use ML methods and discuss a few systemic interventions that could help resolve it.},
	language = {en},
	author = {Kapoor, Sayash and Narayanan, Arvind},
	pages = {6},
	year = {2021},
	howpublished = {\url{https://reproducible.cs.princeton.edu/}},
	url = {https://reproducible.cs.princeton.edu/},
	urldate = {2021-07-28}
}

@article{robertson2021engagement,
  title={Engagement Outweighs Exposure to Partisan and Unreliable News within Google Search},
  author={Robertson, Ronald E and Green, Jon and Ruck, Damian and Ognyanova, Katya and Wilson, Christo and Lazer, David},
  journal={arXiv preprint arXiv:2201.00074},
  year={2021}
}


@article{badnews,
  title={Bad News},
  author={Bernstein, Joseph},
  journal={Harper's Magazine},
  year={2021}, 
   howpublished = "\url{https://harpers.org/archive/2021/09/bad-news-selling-the-story-of-disinformation/}"
}

@article{hern2018cambridge,
  title={Cambridge Analytica: how did it turn clicks into votes},
  author={Hern, Alex},
  journal={The Guardian},
  volume={6},
  year={2018}
}

@article{gibney2018scant,
  title={The scant science behind Cambridge Analytica’s controversial marketing techniques},
  author={Gibney, Elizabeth},
  journal={Nature},
  year={2018}
}

@article{matz2017psychological,
  title={Psychological targeting as an effective approach to digital mass persuasion},
  author={Matz, Sandra C and Kosinski, Michal and Nave, Gideon and Stillwell, David J},
  journal={Proceedings of the national academy of sciences},
  volume={114},
  number={48},
  pages={12714--12719},
  year={2017},
  publisher={National Acad Sciences}
}

@book{hwang2020subprime,
  title={Subprime attention crisis: advertising and the time bomb at the heart of the Internet},
  author={Hwang, Tim},
  year={2020},
  publisher={FSG originals}
}



@article{pineau2021improving,
  title={Improving reproducibility in machine learning research: a report from the NeurIPS 2019 reproducibility program},
  author={Pineau, Joelle and Vincent-Lamarre, Philippe and Sinha, Koustuv and Larivi{\`e}re, Vincent and Beygelzimer, Alina and d’Alch{\'e}-Buc, Florence and Fox, Emily and Larochelle, Hugo},
  journal={Journal of Machine Learning Research},
  volume={22},
  year={2021},
  publisher={Microtome Publishing}
}


@article{firestone2020performance,
  title={Performance vs. competence in human--machine comparisons},
  author={Firestone, Chaz},
  journal={Proceedings of the National Academy of Sciences},
  volume={117},
  number={43},
  pages={26562--26571},
  year={2020},
  publisher={National Acad Sciences}
}

@article{grover,
  title={AI and the everything in the whole wide world benchmark},
  author={Raji, Inioluwa Deborah and Bender, Emily M and Paullada, Amandalynne and Denton, Emily and Hanna, Alex},
  journal={arXiv preprint arXiv:2111.15366},
  year={2021}
}

@article{diaz2021double,
  title={Double Standards in Social Media Content Moderation},
  author={D{\'\i}az, {\'A}ngel and Hecht, Laura},
  journal={New York: Brennan Center for Justice},
  year={2021}, 
   howpublished = "\url{https://www. brennancenter. org/sites/default/files/2021-08/Double\_Standards\_Content\_Moderation. pdf}"
}

@inproceedings{oakden2020hidden,
  title={Hidden stratification causes clinically meaningful failures in machine learning for medical imaging},
  author={Oakden-Rayner, Luke and Dunnmon, Jared and Carneiro, Gustavo and R{\'e}, Christopher},
  booktitle={Proceedings of the ACM conference on health, inference, and learning},
  pages={151--159},
  year={2020}
}

@article{freeman2021use,
  title={Use of artificial intelligence for image analysis in breast cancer screening programmes: systematic review of test accuracy},
  author={Freeman, Karoline and Geppert, Julia and Stinton, Chris and Todkill, Daniel and Johnson, Samantha and Clarke, Aileen and Taylor-Phillips, Sian},
  journal={bmj},
  volume={374},
  year={2021},
  publisher={British Medical Journal Publishing Group}
}

@MISC{pain_wired,
  title        = "The Pain Was Unbearable. So Why Did Doctors Turn Her Away?",
  publisher    = "{Wired}",
  author       = "Szalavitz, Maia",
  year         = "{2021}",
  howpublished = "\url{https://www.wired.com/story/opioid-drug-addiction-algorithm-chronic-pain/}"
}

@article{obermeyer2019dissecting,
  title={Dissecting racial bias in an algorithm used to manage the health of populations},
  author={Obermeyer, Ziad and Powers, Brian and Vogeli, Christine and Mullainathan, Sendhil},
  journal={Science},
  volume={366},
  number={6464},
  pages={447--453},
  year={2019},
  publisher={American Association for the Advancement of Science}
}

@article{paige2020houston,
  title={“Houston, We Have a Lawsuit”: A Cautionary Tale for the Implementation of Value-Added Models for High-Stakes Employment Decisions},
  author={Paige, Mark A and Amrein-Beardsley, Audrey},
  journal={Educational Researcher},
  volume={49},
  number={5},
  pages={350--359},
  year={2020},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}

@article{richardson2019litigating,
  title={Litigating Algorithms: 2019 US Report},
  author={Richardson, Rashida and Schultz, Jason M and Southerland, Vincent M},
  journal={AI Now Institute, September},
  year={2019}
}

@MISC{fb_nudity,
  title        = "Facebook's nudity-spotting AI mistook a photo of some onions for 'sexually suggestive' content",
  publisher    = "{Business Insider}",
  author       = "Hamilton, Isobel Asher",
  year         = "{2020}",
  howpublished = "\url{https://www.businessinsider.com/facebook-mistakes-onions-for-sexualised-content-2020-10}"
}

@MISC{fb_hoes,
  title        = "Facebook cracks down on discussing ‘hoes’ in gardening group",
  publisher    = "{New York Post}",
  author       = "O’Neill, Jesse",
  year         = "{2021}",
  howpublished = "\url{https://nypost.com/2021/07/20/facebook-cracks-down-on-discussing-hoes-in-gardening-group/}"
}



@article{tiktokerror,
  title={Tiktok’s algorithm reportedly bans creators using terms 'Black' and 'BLM'},
  author={Kpakima, Kumba},
  journal={The Verge},
  year={2021},
  howpublished = "\url{https://i-d.vice.com/en_uk/article/m7epya/tiktoks-algorithm-reportedly-bans-creators-using-terms-black-and-blm}"
}

@article{lecher2018happens,
  title={What happens when an algorithm cuts your health care},
  author={Lecher, Colin},
  journal={The Verge},
  year={2018}
}


@misc{alevels,
  title={The student and the algorithm: how the exam results fiasco threatened one pupil’s future},
  author={Lamont, Tom},
  year={2021},
  publisher={The Guardian}
}


@article{kippin2021covid,
  title={The COVID-19 exams fiasco across the UK: four nations and two windows of opportunity},
  author={Kippin, Sean and Cairney, Paul},
  journal={British Politics},
  pages={1--23},
  year={2021},
  publisher={Springer}
}

@article{hill2020wrongfully,
  title={Wrongfully accused by an algorithm},
  author={Hill, Kashmir},
  journal={The New York Times},
  volume={24},
  year={2020}
}

@article{kirchner2020access,
  title={Access Denied: Faulty Automated Background Checks Freeze Out Renters},
  author={Kirchner, Lauren and Goldstein, Matthew},
  year={2020},
  journal={The Markup}
}


@article{kirchner2020automated,
  title={How Automated Background Checks Freeze Out Renters},
  author={Kirchner, Lauren and Goldstein, Matthew},
  journal={The New York Times},
  month = may,
  volume={28},
  year={2020}
}

@MISC{bankrupt_MIDAS,
  title        = "State of Michigan's mistake led to man filing bankruptcy",
  booktitle    = "{Detroit Free Press}",
  author       = "Egan, Paul",
  year         = "{2019}",
  howpublished = "\url{https://www.freep.com/story/news/local/michigan/2019/12/22/government-artificial-intelligence-midas-computer-fraud-fiasco/4407901002/}"
}




@article{charette2018michigan,
  title={Michigan’s MiDAS Unemployment System: Algorithm Alchemy Created Lead, Not Gold-IEEE Spectrum},
  author={Charette, Robert},
  journal={IEEE Spectrum},
  volume={18},
  number={3},
  pages={6},
  year={2018}
}

@article{barocas2016big,
  title={Big data's disparate impact},
  author={Barocas, Solon and Selbst, Andrew D},
  journal={Calif. L. Rev.},
  volume={104},
  pages={671},
  year={2016},
  publisher={HeinOnline}
}

@article{kaminski2019right,
  title={The Right to Explanation, Explained},
  author={Kaminski, Margot E},
  journal={Berkeley Technology Law Journal},
  volume={34},
  pages={189},
  year={2019}
}

@article{kaminski2021right,
  title={The right to contest AI},
  author={Kaminski, Margot E and Urban, Jennifer M},
  journal={Columbia Law Review},
  volume={121},
  number={7},
  pages={1957--2048},
  year={2021},
  publisher={JSTOR}
}

@inproceedings{barocas2020hidden,
  title={The hidden assumptions behind counterfactual explanations and principal reasons},
  author={Barocas, Solon and Selbst, Andrew D and Raghavan, Manish},
  booktitle={Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
  pages={80--89},
  year={2020}
}

@article{edwards2017slave,
  title={Slave to the algorithm: Why a right to an explanation is probably not the remedy you are looking for},
  author={Edwards, Lilian and Veale, Michael},
  journal={Duke L. \& Tech. Rev.},
  volume={16},
  pages={18},
  year={2017},
  publisher={HeinOnline}
}

@article{selbst2017meaningful,
  title={Meaningful information and the right to explanation},
  author={Selbst, Andrew D and Powles, Julia},
  journal={International Data Privacy Law},
  volume={7},
  number={4},
  pages={233--242},
  year={2017},
  publisher={Oxford University Press}
}

@article{wachter2017right,
  title={Why a right to explanation of automated decision-making does not exist in the general data protection regulation},
  author={Wachter, Sandra and Mittelstadt, Brent and Floridi, Luciano},
  journal={International Data Privacy Law},
  volume={7},
  number={2},
  pages={76--99},
  year={2017},
  publisher={Oxford University Press}
}

@book{hartzog2018privacy,
  title={Privacy’s blueprint},
  author={Hartzog, Woodrow},
  year={2018},
  publisher={Harvard University Press}
}

@article{selbst2020negligence,
  title={Negligence and AI's human users},
  author={Selbst, Andrew D},
  journal={BUL Rev.},
  volume={100},
  pages={1315},
  year={2020},
  publisher={HeinOnline}
}

@book{hoofnagle2016federal,
  title={Federal Trade Commission: Privacy Law and Policy},
  author={Hoofnagle, Chris Jay},
  year={2016},
  publisher={Cambridge University Press}
}

@article{calo2015robotics,
  title={Robotics and the Lessons of Cyberlaw},
  author={Calo, Ryan},
  journal={Calif. L. Rev.},
  volume={103},
  pages={513},
  year={2015},
  publisher={HeinOnline}
}

@article{engstrom20133D,
  title={3-D printing and product liability: identifying the obstacles},
  author={Engstrom, Nora Freeman},
  journal={U. Pa. L. Rev. Online},
  volume={162},
  pages={35},
  year={2013},
  publisher={HeinOnline}
}

@article{zollers2004no,
  title={No more soft landings for software: Liability for defects in an industry that has come of age},
  author={Zollers, Frances E and McMullin, Andrew and Hurd, Sandra N and Shears, Peter},
  journal={Santa Clara Computer \& High Tech. LJ},
  volume={21},
  pages={745},
  year={2004},
  publisher={HeinOnline}
}

@misc{Winter,
    key = {Winter v. G.P. Putnam's Sons, 938 F.2d 1033 (9th Cir. 1991)},
    year = {1991}
}

@article{hubbard2014sophisticated,
  title={Sophisticated robots: balancing liability, regulation, and innovation},
  author={Hubbard, F Patrick},
  journal={Fla. L. Rev.},
  volume={66},
  pages={1803},
  year={2014},
  publisher={HeinOnline}
}

@article{owen2001manufacturing,
  title={Manufacturing Defects},
  author={Owen, David G},
  journal={SCL Rev.},
  volume={53},
  pages={851},
  year={2001},
  publisher={HeinOnline}
}

@article{geistfeld2017roadmap,
  title={A roadmap for autonomous vehicles: State tort liability, automobile insurance, and federal safety regulation},
  author={Geistfeld, Mark A},
  journal={Calif. L. Rev.},
  volume={105},
  pages={1611},
  year={2017},
  publisher={HeinOnline}
}

@article{choi2019crashworthy,
  title={Crashworthy code},
  author={Choi, Bryan H},
  journal={Wash. L. Rev.},
  volume={94},
  pages={39},
  year={2019},
  publisher={HeinOnline}
}

@misc{ThirdRestatement_S2,

    key = {Restatement (Third) of Torts: Products Liability § 2},
    year = {}
}

@misc{ThirdRestatement_S3,

    key = {Restatement (Third) of Torts: Products Liability § 3},
    year = {}
}

@misc{UCC_2-314,

    key = {Uniform Commercial Code § 2-314},
    year = {}
}

@misc{UCC_2-315,
    key = {Uniform Commercial Code § 2-315},
    year = {}
}
@misc{Section_5,
    key = {Federal Trade Commission Act, 15 U.S.C. § 45},
    year = {}
}


@misc{CFPB_jx,
    key = {12 U.S.C. § 5511},
    year = {}
}

@misc{CPSC_about,
  author = {Consumer Product Safety Commission},
    howpublished = {\url{https://www.cpsc.gov/About-CPSC}},
    title = {About Us},
}

@article{citron2007technological,
  title={Technological due process},
  author={Citron, Danielle Keats},
  journal={Wash. UL Rev.},
  volume={85},
  pages={1249},
  year={2007},
  publisher={HeinOnline}
}

@misc{Zhang,
    author = {},
    title = {\textup{Zhang v. Superior Ct., 304 P.3d 163 (2013)}},
    year = {2013}
}

@misc{AMG_FTC,
    author = {},
    title = {\textup{AMG Capital Management v. Federal Trade Commission, 141 S.Ct. 1341}},
    year = {2021}
}

@article{mcgeveran2018duty,
  title={The Duty of Data Security},
  author={McGeveran, William},
  journal={Minn. L. Rev.},
  volume={103},
  pages={1135},
  year={2018},
  publisher={HeinOnline}
}

@misc{Snapchat_consent_decree,
    author = {Federal Trade Commission},
    title = {\textup{In re Snapchat, Inc., File No. 132-3078, Docket No. C-4501 (consent decree)}},
    year = {2014}
}

@misc{FB_consent_decree,
    author = {},
    title = {\textup{Stipulated Order for Civil Penalty, Monetary Judgment, and Injunctive Relief, No. 1:19-cv-2184, Docket 2-1 (D.D.C. July 24, 2019) (fining Facebook \$5 billion for violating a prior consent decree)}},
    year = {2019}
}

@misc{FTC_Mag_Moss,
  author={Federal Trade Commission},
  title = {FTC Votes to Update Rulemaking Procedures, Sets Stage for Stronger Deterrence of Corporate Misconduct},
  howpublished = {\url{https://www.ftc.gov/news-events/press-releases/2021/07/ftc-votes-update-rulemaking-procedures-sets-stage-stronger}},
  year={2021},
  month={July 1}
}
@article{citron2016privacy,
  title={The Privacy Policymaking of State Attorneys General},
  author={Citron, Danielle Keats},
  journal={Notre Dame L. Rev.},
  volume={92},
  pages={747},
  year={2016},
  publisher={HeinOnline}
}

@techreport{NCLC_Report,
     title = {Consumer Protection in the States},
     institution = {National Consumer Law Center},
     author = {Carter, Carolyn L.},
     year = {2009},
     month = {02},
}


@article{methods_in_the_magic,
author = {M. C. Elish and danah boyd},
title = {Situating methods in the magic of Big Data and AI},
journal = {Communication Monographs},
volume = {85},
number = {1},
pages = {57-80},
year  = {2018},
publisher = {Routledge},
doi = {10.1080/03637751.2017.1375130},
URL = { 
        https://doi.org/10.1080/03637751.2017.1375130
    
}
}

@ARTICLE{ml_software_practices,
  author={Wan, Zhiyuan and Xia, Xin and Lo, David and Murphy, Gail C.},
  journal={IEEE Transactions on Software Engineering}, 
  title={How does Machine Learning Change Software Development Practices?}, 
  year={2021},
  volume={47},
  number={9},
  pages={1857-1871},
  doi={10.1109/TSE.2019.2937083}}
  
@misc{nao_ets, 
        title={Investigation into the response to cheating in English language tests - national audit office (NAO) press release},
        url={https://www.nao.org.uk/press-release/investigation-into-the-response-to-cheating-in-english-language-tests/}, 
        journal={National Audit Office}, 
        year={2020}, 
        month={Jul},
        author = {National Audit Office}
        }


@ARTICLE{Mitchell2021-pk,
  title     = "Algorithmic Fairness: Choices, Assumptions, and Definitions",
  author    = "Mitchell, Shira and Potash, Eric and Barocas, Solon and D'Amour,
               Alexander and Lum, Kristian",
  abstract  = "A recent wave of research has attempted to define fairness
               quantitatively. In particular, this work has explored what
               fairness might mean in the context of decisions based on the
               predictions of statistical and machine learning models. The
               rapid growth of this new field has led to wildly inconsistent
               motivations, terminology, and notation, presenting a serious
               challenge for cataloging and comparing definitions. This article
               attempts to bring much-needed order. First, we explicate the
               various choices and assumptions made?often implicitly?to justify
               the use of prediction-based decision-making. Next, we show how
               such choices and assumptions can raise fairness concerns and we
               present a notationally consistent catalog of fairness
               definitions from the literature. In doing so, we offer a concise
               reference for thinking through the choices, assumptions, and
               fairness considerations of prediction-based decision-making.",
  journal   = "Annu. Rev. Stat. Appl.",
  publisher = "Annual Reviews",
  volume    =  8,
  number    =  1,
  pages     = "141--163",
  month     =  mar,
  year      =  2021
}


@ARTICLE{Raji2019-od,
  title         = "{ABOUT} {ML}: Annotation and Benchmarking on Understanding
                   and Transparency of Machine Learning Lifecycles",
  author        = "Raji, Inioluwa Deborah and Yang, Jingying",
  abstract      = "We present the ``Annotation and Benchmarking on
                   Understanding and Transparency of Machine Learning
                   Lifecycles'' (ABOUT ML) project as an initiative to
                   operationalize ML transparency and work towards a standard
                   ML documentation practice. We make the case for the
                   project's relevance and effectiveness in consolidating
                   disparate efforts across a variety of stakeholders, as well
                   as bringing in the perspectives of currently missing voices
                   that will be valuable in shaping future conversations. We
                   describe the details of the initiative and the gaps we hope
                   this project will help address.",
  month         =  dec,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CY",
  eprint        = "1912.06166"
}


@INPROCEEDINGS{Karimi2021-jo,
  title     = "Algorithmic Recourse: from Counterfactual Explanations to
               Interventions",
  booktitle = "Proceedings of the 2021 {ACM} Conference on Fairness,
               Accountability, and Transparency",
  author    = "Karimi, Amir-Hossein and Sch{\"o}lkopf, Bernhard and Valera,
               Isabel",
  abstract  = "As machine learning is increasingly used to inform consequential
               decision-making (e.g., pre-trial bail and loan approval), it
               becomes important to explain how the system arrived at its
               decision, and also suggest actions to achieve a favorable
               decision. Counterfactual explanations -``how the world would
               have (had) to be different for a desirable outcome to occur''-
               aim to satisfy these criteria. Existing works have primarily
               focused on designing algorithms to obtain counterfactual
               explanations for a wide range of settings. However, it has
               largely been overlooked that ultimately, one of the main
               objectives is to allow people to act rather than just
               understand. In layman's terms, counterfactual explanations
               inform an individual where they need to get to, but not how to
               get there. In this work, we rely on causal reasoning to caution
               against the use of counterfactual explanations as a
               recommendable set of actions for recourse. Instead, we propose a
               shift of paradigm from recourse via nearest counterfactual
               explanations to recourse through minimal interventions, shifting
               the focus from explanations to interventions.",
  publisher = "Association for Computing Machinery",
  pages     = "353--362",
  series    = "FAccT '21",
  month     =  mar,
  year      =  2021,
  address   = "New York, NY, USA",
  keywords  = "consequential recommendations, algorithmic recourse, explainable
               artificial intelligence, causal inference, counterfactual
               explanations, contrastive explanations, minimal interventions",
  location  = "Virtual Event, Canada"
}


@ARTICLE{Jobin2019-oa,
  title     = "The global landscape of {AI} ethics guidelines",
  author    = "Jobin, Anna and Ienca, Marcello and Vayena, Effy",
  abstract  = "In the past five years, private companies, research institutions
               and public sector organizations have issued principles and
               guidelines for ethical artificial intelligence (AI). However,
               despite an apparent agreement that AI should be `ethical', there
               is debate about both what constitutes `ethical AI' and which
               ethical requirements, technical standards and best practices are
               needed for its realization. To investigate whether a global
               agreement on these questions is emerging, we mapped and analysed
               the current corpus of principles and guidelines on ethical AI.
               Our results reveal a global convergence emerging around five
               ethical principles (transparency, justice and fairness,
               non-maleficence, responsibility and privacy), with substantive
               divergence in relation to how these principles are interpreted,
               why they are deemed important, what issue, domain or actors they
               pertain to, and how they should be implemented. Our findings
               highlight the importance of integrating guideline-development
               efforts with substantive ethical analysis and adequate
               implementation strategies. As AI technology develops rapidly, it
               is widely recognized that ethical guidelines are required for
               safe and fair implementation in society. But is it possible to
               agree on what is `ethical AI'? A detailed analysis of 84 AI
               ethics reports around the world, from national and international
               organizations, companies and institutes, explores this question,
               finding a convergence around core principles but substantial
               divergence on practical implementation.",
  journal   = "Nature Machine Intelligence",
  publisher = "Nature Publishing Group",
  volume    =  1,
  number    =  9,
  pages     = "389--399",
  month     =  sep,
  year      =  2019,
  language  = "en"
}


@MISC{noonetrustai-xo,
    author = "Bryson,Joanna",
  title        = "{AI} \& Global Governance: No One Should Trust {AI} - United
                  Nations University Centre for Policy Research",
  howpublished = "\url{https://cpr.unu.edu/publications/articles/ai-global-governance-no-one-should-trust-ai.html}",
  note         = "Accessed: 2022-1-6"
}


@UNPUBLISHED{Stanton2021-oa,
  title  = "Trust and Artificial Intelligence",
  author = "Stanton, Brian and Jensen, Theodore",
  month  =  mar,
  year   =  2021
}


@MISC{aclu-comment-trust,
  title        = "{ACLU} Comment on {NIST's} Proposal for Managing Bias in {AI}",
  howpublished = "\url{https://www.aclu.org/letter/aclu-comment-nists-proposal-managing-bias-ai}",
  note         = "Accessed: 2022-1-6",
  year = 2021,
  month = sep,
  author = {ACLU}
}


@ARTICLE{ieee_dictionary_dependability,
  title    = "{IEEE} Standard Dictionary of Measures of the Software Aspects of
              Dependability",
  abstract = "A Standard Dictionary of Measures of the Software Aspects of
              Dependability for assessing and predicting the reliability,
              maintainability, and availability of any software system; in
              particular, it applies to mission critical software systems.",
  journal  = "IEEE Std 982. 1-2005 (Revision of IEEE Std 982. 1-1988)",
  author = {IEEE},
  pages    = "1--41",
  month    =  may,
  year     =  2006,
  keywords = "Standards;IEEE Standards;Patents;Software
              measurement;Dictionaries;Warranties;Trademarks;availability;dependability;maintainability;and
              reliability"
}


@INPROCEEDINGS{Passi2019-av,
  title     = "Problem Formulation and Fairness",
  booktitle = "Proceedings of the Conference on Fairness, Accountability, and
               Transparency",
  author    = "Passi, Samir and Barocas, Solon",
  abstract  = "Formulating data science problems is an uncertain and difficult
               process. It requires various forms of discretionary work to
               translate high-level objectives or strategic goals into
               tractable problems, necessitating, among other things, the
               identification of appropriate target variables and proxies.
               While these choices are rarely self-evident, normative
               assessments of data science projects often take them for
               granted, even though different translations can raise profoundly
               different ethical concerns. Whether we consider a data science
               project fair often has as much to do with the formulation of the
               problem as any property of the resulting model. Building on six
               months of ethnographic fieldwork with a corporate data science
               team---and channeling ideas from sociology and history of
               science, critical data studies, and early writing on knowledge
               discovery in databases---we describe the complex set of actors
               and activities involved in problem formulation. Our research
               demonstrates that the specification and operationalization of
               the problem are always negotiated and elastic, and rarely worked
               out with explicit normative considerations in mind. In so doing,
               we show that careful accounts of everyday data science work can
               help us better understand how and why data science problems are
               posed in certain ways---and why specific formulations prevail in
               practice, even in the face of what might seem like normatively
               preferable alternatives. We conclude by discussing the
               implications of our findings, arguing that effective normative
               interventions will require attending to the practical work of
               problem formulation.",
  publisher = "Association for Computing Machinery",
  pages     = "39--48",
  series    = "FAT* '19",
  month     =  jan,
  year      =  2019,
  address   = "New York, NY, USA",
  keywords  = "Problem Formulation, Machine Learning, Fairness, Data Science,
               Target Variable",
  location  = "Atlanta, GA, USA"
}


@ARTICLE{Passi2020-dr,
  title     = "Making data science systems work",
  author    = "Passi, Samir and Sengers, Phoebe",
  abstract  = "How are data science systems made to work? It may seem that
               whether a system works is a function of its technical design,
               but it is also accomplished through ongoing forms of
               discretionary work by many actors. Based on six months of
               ethnographic fieldwork with a corporate data science team, we
               describe how actors involved in a corporate project negotiated
               what work the system should do, how it should work, and how to
               assess whether it works. These negotiations laid the foundation
               for how, why, and to what extent the system ultimately worked.
               We describe three main findings. First, how already-existing
               technologies are essential reference points to determine how and
               whether systems work. Second, how the situated resolution of
               development challenges continually reshapes the understanding of
               how and whether systems work. Third, how business goals, and
               especially their negotiated balance with data science
               imperatives, affect a system?s working. We conclude with
               takeaways for critical data studies, orienting researchers to
               focus on the organizational and cultural aspects of data
               science, the third-party platforms underlying data science
               systems, and ways to engage with practitioners? imagination of
               how systems can and should work.",
  journal   = "Big Data \& Society",
  publisher = "SAGE Publications Ltd",
  volume    =  7,
  number    =  2,
  pages     = "2053951720939605",
  month     =  jul,
  year      =  2020
}


@INPROCEEDINGS{Muller2019-cy,
  title     = "{Human-Centered} Study of Data Science Work Practices",
  booktitle = "Extended Abstracts of the 2019 {CHI} Conference on Human Factors
               in Computing Systems",
  author    = "Muller, Michael and Feinberg, Melanie and George, Timothy and
               Jackson, Steven J and John, Bonnie E and Kery, Mary Beth and
               Passi, Samir",
  abstract  = "With the rise of big data, there has been an increasing need to
               understand who is working in data science and how they are doing
               their work. HCI and CSCW researchers have begun to examine these
               questions. In this workshop, we invite researchers to share
               their observations, experiences, hypotheses, and insights, in
               the hopes of developing a taxonomy of work practices and open
               issues in the behavioral and social study of data science and
               data science workers.",
  publisher = "Association for Computing Machinery",
  number    = "Paper W15",
  pages     = "1--8",
  series    = "CHI EA '19",
  month     =  may,
  year      =  2019,
  address   = "New York, NY, USA",
  keywords  = "work practice, data science",
  location  = "Glasgow, Scotland Uk"
}

@ARTICLE{Passi2018-jt,
  title     = "Trust in Data Science: Collaboration, Translation, and
               Accountability in Corporate Data Science Projects",
  author    = "Passi, Samir and Jackson, Steven J",
  abstract  = "The trustworthiness of data science systems in applied and
               real-world settings emerges from the resolution of specific
               tensions through situated, pragmatic, and ongoing forms of work.
               Drawing on research in CSCW, critical data studies, and history
               and sociology of science, and six months of immersive
               ethnographic fieldwork with a corporate data science team, we
               describe four common tensions in applied data science work:
               (un)equivocal numbers, (counter)intuitive knowledge,
               (in)credible data, and (in)scrutable models. We show how
               organizational actors establish and re-negotiate trust under
               messy and uncertain analytic conditions through practices of
               skepticism, assessment, and credibility. Highlighting the
               collaborative and heterogeneous nature of real-world data
               science, we show how the management of trust in applied
               corporate data science settings depends not only on
               pre-processing and quantification, but also on negotiation and
               translation. We conclude by discussing the implications of our
               findings for data science research and practice, both within and
               beyond CSCW.",
  journal   = "Proc. ACM Hum.-Comput. Interact.",
  publisher = "Association for Computing Machinery",
  volume    =  2,
  number    = "CSCW",
  pages     = "1--28",
  month     =  nov,
  year      =  2018,
  address   = "New York, NY, USA",
  keywords  = "collaboration, organizational work, data science, trust,
               credibility"
}

@MISC{Lehr_undated-aq,
  title        = "Playing with the data: What legal scholars should learn about
                  machine learning",
  author       = "Lehr, David and Ohm, Paul",
  howpublished = "\url{https://lawreview.law.ucdavis.edu/issues/51/2/Symposium/51-2_Lehr_Ohm.pdf}",
  note         = "Accessed: 2021-8-10"
}


@ARTICLE{Henke2018-ua,
  title    = "You Don't Have to Be a Data Scientist to Fill This {Must-Have}
              Analytics Role",
  author   = "Henke, Nicolaus and Levine, Jordan and McInerney, Paul",
  abstract = "It's easier for companies to train existing employees for it than
              to hire new ones.",
  journal  = "Harvard Business Review",
  month    =  feb,
  year     =  2018
}


@ARTICLE{scalefactor,
  title    = "{ScaleFactor} Raised \$100 Million In A Year Then Blamed Covid-19
              For Its Demise. Employees Say It Had Much Bigger Problems",
  author   = "Jeans, David",
  abstract = "Kurt Rathmann told his big-name investors he had developed
              groundbreaking AI to do the books for small businesses. In
              reality, humans did most of the work.",
  journal  = "Forbes Magazine",
  month    =  jul,
  year     =  2020,
  language = "en"
}


@MISC{Translator2018-ki,
  title        = "Neural Machine Translation reaches historic milestone: human
                  parity for Chinese to English translations",
  booktitle    = "Microsoft Translator Blog",
  author       = "Translator, Microsoft",
  abstract     = "Microsoft announced today that its researchers have developed
                  an AI machine translation system that can translate with the
                  same accuracy as a human from Chinese to English. To validate
                  the results, the researchers used an industry standard test
                  set of news stories (newstest2017) to compare human and
                  machine translation results. To further ensure accuracy of
                  the evaluation, the team also....",
  month        =  mar,
  year         =  2018,
  howpublished = "\url{https://www.microsoft.com/en-us/translator/blog/2018/03/14/human-parity-for-chinese-to-english-translations/}",
  note         = "Accessed: 2022-1-12",
  language     = "en"
}

@article{mulligan2019thing,
  title={This thing called fairness: disciplinary confusion realizing a value in technology},
  author={Mulligan, Deirdre K and Kroll, Joshua A and Kohli, Nitin and Wong, Richmond Y},
  journal={Proceedings of the ACM on Human-Computer Interaction},
  volume={3},
  number={CSCW},
  pages={1--36},
  year={2019},
  publisher={ACM New York, NY, USA}
}

@article{CambridgeAnalytica,
 author  = {Halper, Evan},
 date    = {2018-03-21},
 title   = {Was Cambridge Analytica a digital Svengali or snake-oil salesman?},
 journal = {Los Angeles Times},
 url     = {https://www.latimes.com/politics/la-na-pol-cambridge-analytica-20180321-story.html},
}

@ARTICLE{Toral2018-wn,
  title         = "Attaining the Unattainable? Reassessing Claims of Human
                   Parity in Neural Machine Translation",
  author        = "Toral, Antonio and Castilho, Sheila and Hu, Ke and Way, Andy",
  abstract      = "We reassess a recent study (Hassan et al., 2018) that
                   claimed that machine translation (MT) has reached human
                   parity for the translation of news from Chinese into
                   English, using pairwise ranking and considering three
                   variables that were not taken into account in that previous
                   study: the language in which the source side of the test set
                   was originally written, the translation proficiency of the
                   evaluators, and the provision of inter-sentential context.
                   If we consider only original source text (i.e. not
                   translated from another language, or translationese), then
                   we find evidence showing that human parity has not been
                   achieved. We compare the judgments of professional
                   translators against those of non-experts and discover that
                   those of the experts result in higher inter-annotator
                   agreement and better discrimination between human and
                   machine translations. In addition, we analyse the human
                   translations of the test set and identify important
                   translation issues. Finally, based on these findings, we
                   provide a set of recommendations for future human
                   evaluations of MT.",
  month         =  aug,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1808.10432"
}


@ARTICLE{Laubli2018-sn,
  title         = "Has Machine Translation Achieved Human Parity? A Case for
                   Document-level Evaluation",
  author        = "L{\"a}ubli, Samuel and Sennrich, Rico and Volk, Martin",
  abstract      = "Recent research suggests that neural machine translation
                   achieves parity with professional human translation on the
                   WMT Chinese--English news translation task. We empirically
                   test this claim with alternative evaluation protocols,
                   contrasting the evaluation of single sentences and entire
                   documents. In a pairwise ranking experiment, human raters
                   assessing adequacy and fluency show a stronger preference
                   for human over machine translation when evaluating documents
                   as compared to isolated sentences. Our findings emphasise
                   the need to shift towards document-level evaluation as
                   machine translation improves to the degree that errors which
                   are hard or impossible to spot at the sentence-level become
                   decisive in discriminating quality of different translation
                   outputs.",
  month         =  aug,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1808.07048"
}


@ARTICLE{Dobbe2019-ms,
  title         = "Hard Choices in Artificial Intelligence: Addressing
                   Normative Uncertainty through Sociotechnical Commitments",
  author        = "Dobbe, Roel and Gilbert, Thomas Krendl and Mintz, Yonatan",
  abstract      = "As AI systems become prevalent in high stakes domains such
                   as surveillance and healthcare, researchers now examine how
                   to design and implement them in a safe manner. However, the
                   potential harms caused by systems to stakeholders in complex
                   social contexts and how to address these remains unclear. In
                   this paper, we explain the inherent normative uncertainty in
                   debates about the safety of AI systems. We then address this
                   as a problem of vagueness by examining its place in the
                   design, training, and deployment stages of AI system
                   development. We adopt Ruth Chang's theory of intuitive
                   comparability to illustrate the dilemmas that manifest at
                   each stage. We then discuss how stakeholders can navigate
                   these dilemmas by incorporating distinct forms of dissent
                   into the development pipeline, drawing on Elizabeth
                   Anderson's work on the epistemic powers of democratic
                   institutions. We outline a framework of sociotechnical
                   commitments to formal, substantive and discursive challenges
                   that address normative uncertainty across stakeholders, and
                   propose the cultivation of related virtues by those
                   responsible for development.",
  month         =  nov,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "1911.09005"
}


@MISC{Buolamwini_undated-dd,
  title        = "Gender shades: Intersectional accuracy disparities in
                  commercial gender classification",
  author       = "Buolamwini, Joy and Friedler, Sorelle A and Wilson, Christo",
  abstract     = "Recent studies demonstrate that machine learning algorithms
                  can discriminate based on classes like race and gender. In
                  this work, we present an approach to evaluate bias present in
                  automated facial analysis algorithms and datasets with
                  respect to phenotypic subgroups. Using the dermatologist
                  approved Fitzpatrick Skin Type classification system, we
                  characterize the gender and skin type distribution of two
                  facial analysis benchmarks, IJB-A and Adience. We find that
                  these datasets are overwhelmingly composed of lighter-skinned
                  subjects (79.6\% for IJB-A and 86.2\% for Adience) and
                  introduce a new facial analysis dataset which is balanced by
                  gender and skin type. We evaluate 3 commercial gender
                  classification systems using our dataset and show that
                  darker-skinned females are the most misclassified group (with
                  error rates of up to 34.7\%). The maximum error rate for
                  lighter-skinned males is 0.8\%. The substantial disparities
                  in the accuracy of classifying darker females, lighter
                  females, darker males, and lighter males in gender
                  classification systems require urgent attention if commercial
                  companies are to build genuinely fair, transparent and
                  accountable facial analysis algorithms.",
  howpublished = "\url{http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf}",
  note         = "Accessed: 2022-1-12"
}


@MISC{Snow2018-vw,
  title        = "Amazon's Face Recognition Falsely Matched 28 Members of
                  Congress With Mugshots",
  booktitle    = "American Civil Liberties Union",
  author       = "Snow, Jacob",
  abstract     = "Amazon's face surveillance technology is the target of
                  growing opposition nationwide, and today, there are 28 more
                  causes for concern. In a test the ACLU recently conducted of
                  the facial recognition tool, called ``Rekognition,'' the
                  software incorrectly matched 28 members of Congress,
                  identifying them as other people who have been arrested for a
                  crime. The members of Congress",
  month        =  jul,
  year         =  2018,
  howpublished = "\url{https://www.aclu.org/blog/privacy-technology/surveillance-technologies/amazons-face-recognition-falsely-matched-28}",
  note         = "Accessed: 2022-1-12",
  language     = "en"
}


@MISC{Wood_undated-ek,
  title        = "Thoughts On Machine Learning Accuracy",
  author       = "Wood, Matt",
  howpublished = "\url{https://aws.amazon.com/blogs/aws/thoughts-on-machine-learning-accuracy/}"
}


@MISC{aclu_response_response_fr,
  title        = "{ACLU} Comment on New Amazon Statement Responding to Face
                  Recognition Technology Test",
  booktitle    = "American Civil Liberties Union",
  abstract     = "SAN FRANCISCO -- Amazon today issued an additional statement
                  in response to the American Civil Liberties Union Foundation
                  of Northern California test of Rekognition, the company's
                  face recognition technology. The test revealed that
                  Rekognition falsely matched 28 current members of Congress
                  with images in an arrest photo database.",
  howpublished = "\url{https://www.aclu.org/press-releases/aclu-comment-new-amazon-statement-responding-face-recognition-technology-test}",
  note         = "Accessed: 2022-1-12",
  language     = "en",
  year = 2018,
  month = jul,
  author = {ACLU}
}


@MISC{Ross2018-nn,
  title        = "{IBM's} Watson supercomputer recommended 'unsafe and
                  incorrect' cancer treatments, internal documents show",
  booktitle    = "{STAT}",
  author       = "Ross, Casey and Swetlitz, Ike and Cohrs, Rachel and
                  Dillingham, Ian and {STAT Staff} and Florko, Nicholas and
                  Bender, Maddie",
  abstract     = "Slide decks presented last summer by an IBM Watson Health
                  executive largely blame the problems on the training of
                  Watson for Oncology by IBM engineers and doctors at the
                  renowned Memorial Sloan Kettering Cancer Center.",
  month        =  jul,
  year         =  2018,
  howpublished = "\url{https://www.statnews.com/2018/07/25/ibm-watson-recommended-unsafe-incorrect-treatments/?utm_source=STAT+Newsletters&utm_campaign=beb06f048d-MR_COPY_08&utm_medium=email&utm_term=0_8cab1d7961-beb06f048d-150085821}",
  note         = "Accessed: 2022-1-13",
  language     = "en"
}

 
@ARTICLE{md_anderson_benches_watson,
  title    = "{MD} Anderson Benches {IBM} Watson In Setback For Artificial
              Intelligence In Medicine",
  author   = "Herper, Matthew",
  abstract = "MD Anderson has placed a much-ballyhooed 'Watson for cancer'
              product it was developing with IBM on hold -- and is looking for
              a new partner.",
  journal  = "Forbes Magazine",
  month    =  feb,
  year     =  2017,
  language = "en"
}


@MISC{Wojcik_undated-nb,
  title        = "{IBM's} Watson `is a joke,' says Social Capital {CEO}
                  Palihapitiya",
  booktitle    = "{CNBC}",
  author       = "Wojcik, Natalia",
  howpublished = "\url{https://www.cnbc.com/2017/05/08/ibms-watson-is-a-joke-says-social-capital-ceo-palihapitiya.html}",
  note         = "Accessed: 2022-1-13"
}


% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Simon2019-ed,
  title    = "Applying Artificial Intelligence to Address the Knowledge Gaps in
              Cancer Care",
  author   = "Simon, George and DiNardo, Courtney D and Takahashi, Koichi and
              Cascone, Tina and Powers, Cynthia and Stevens, Rick and Allen,
              Joshua and Antonoff, Mara B and Gomez, Daniel and Keane, Pat and
              Suarez Saiz, Fernando and Nguyen, Quynh and Roarty, Emily and
              Pierce, Sherry and Zhang, Jianjun and Hardeman Barnhill, Emily
              and Lakhani, Kate and Shaw, Kenna and Smith, Brett and Swisher,
              Stephen and High, Rob and Futreal, P Andrew and Heymach, John and
              Chin, Lynda",
  abstract = "BACKGROUND: Rapid advances in science challenge the timely
              adoption of evidence-based care in community settings. To bridge
              the gap between what is possible and what is practiced, we
              researched approaches to developing an artificial intelligence
              (AI) application that can provide real-time patient-specific
              decision support. MATERIALS AND METHODS: The Oncology Expert
              Advisor (OEA) was designed to simulate peer-to-peer consultation
              with three core functions: patient history summarization,
              treatment options recommendation, and management advisory.
              Machine-learning algorithms were trained to construct a dynamic
              summary of patients cancer history and to suggest approved
              therapy or investigative trial options. All patient data used
              were retrospectively accrued. Ground truth was established for
              approximately 1,000 unique patients. The full Medline database of
              more than 23 million published abstracts was used as the
              literature corpus. RESULTS: OEA's accuracies of searching
              disparate sources within electronic medical records to extract
              complex clinical concepts from unstructured text documents
              varied, with F1 scores of 90\%-96\% for non-time-dependent
              concepts (e.g., diagnosis) and F1 scores of 63\%-65\% for
              time-dependent concepts (e.g., therapy history timeline). Based
              on constructed patient profiles, OEA suggests approved therapy
              options linked to supporting evidence (99.9\% recall; 88\%
              precision), and screens for eligible clinical trials on
              ClinicalTrials.gov (97.9\% recall; 96.9\% precision). CONCLUSION:
              Our results demonstrated technical feasibility of an AI-powered
              application to construct longitudinal patient profiles in context
              and to suggest evidence-based treatment and trial options. Our
              experience highlighted the necessity of collaboration across
              clinical and AI domains, and the requirement of clinical
              expertise throughout the process, from design to training to
              testing. IMPLICATIONS FOR PRACTICE: Artificial intelligence
              (AI)-powered digital advisors such as the Oncology Expert Advisor
              have the potential to augment the capacity and update the
              knowledge base of practicing oncologists. By constructing dynamic
              patient profiles from disparate data sources and organizing and
              vetting vast literature for relevance to a specific patient, such
              AI applications could empower oncologists to consider all therapy
              options based on the latest scientific evidence for their
              patients, and help them spend less time on information ``hunting
              and gathering'' and more time with the patients. However,
              realization of this will require not only AI technology
              maturation but also active participation and leadership by
              clincial experts.",
  journal  = "Oncologist",
  volume   =  24,
  number   =  6,
  pages    = "772--782",
  month    =  jun,
  year     =  2019,
  keywords = "Artificial intelligence application in medicine; Clinical
              decision support; Closing the cancer care gap; Democratization of
              evidence‐based care; Virtual expert advisor",
  language = "en"
}


@MISC{Strickland_undated-ng,
  title        = "{IBM} Watson Heal Thyself: How {IBM} Watson Overpromised And
                  Underdeliverd On {AI} Health Care",
  author       = "Strickland, Eliza",
  abstract     = "After its triumph on Jeopardy!, IBM's AI seemed poised to
                  revolutionize medicine. Doctors are still waiting",
  howpublished = "\url{https://spectrum.ieee.org/how-ibm-watson-overpromised-and-underdelivered-on-ai-health-care}",
  note         = "Accessed: 2022-1-13"
}


@ARTICLE{Gianfrancesco2018-vl,
  title    = "Potential Biases in Machine Learning Algorithms Using Electronic
              Health Record Data",
  author   = "Gianfrancesco, Milena A and Tamang, Suzanne and Yazdany, Jinoos
              and Schmajuk, Gabriela",
  abstract = "A promise of machine learning in health care is the avoidance of
              biases in diagnosis and treatment; a computer algorithm could
              objectively synthesize and interpret the data in the medical
              record. Integration of machine learning with clinical decision
              support tools, such as computerized alerts or diagnostic support,
              may offer physicians and others who provide health care targeted
              and timely information that can improve clinical decisions.
              Machine learning algorithms, however, may also be subject to
              biases. The biases include those related to missing data and
              patients not identified by algorithms, sample size and
              underestimation, and misclassification and measurement error.
              There is concern that biases and deficiencies in the data used by
              machine learning algorithms may contribute to socioeconomic
              disparities in health care. This Special Communication outlines
              the potential biases that may be introduced into machine
              learning-based clinical decision support tools that use
              electronic health record data and proposes potential solutions to
              the problems of overreliance on automation, algorithms based on
              biased data, and algorithms that do not provide information that
              is clinically meaningful. Existing health care disparities should
              not be amplified by thoughtless or excessive reliance on
              machines.",
  journal  = "JAMA Intern. Med.",
  volume   =  178,
  number   =  11,
  pages    = "1544--1547",
  month    =  nov,
  year     =  2018,
  language = "en"
}


@INPROCEEDINGS{Jacobs2021-rk,
  title      = "Measurement and Fairness",
  booktitle  = "Proceedings of the 2021 {ACM} Conference on Fairness,
                Accountability, and Transparency",
  author     = "Jacobs, Abigail Z and Wallach, Hanna",
  publisher  = "ACM",
  month      =  mar,
  year       =  2021,
  address    = "New York, NY, USA",
  conference = "FAccT '21: 2021 ACM Conference on Fairness, Accountability, and
                Transparency",
  location   = "Virtual Event Canada"
}


@ARTICLE{Alexandrova_undated-mx,
  title   = "Democratising Measurement: Or Why Thick Concepts Call for
             Coproduction",
  author  = "Alexandrova, Anna and Fabian, Mark",
  journal = "Eur. J. Philos. Sci."
}



@ARTICLE{Jacobs2021-og,
  title         = "Measurement as governance in and for responsible {AI}",
  author        = "Jacobs, Abigail Z",
  abstract      = "Measurement of social phenomena is everywhere, unavoidably,
                   in sociotechnical systems. This is not (only) an academic
                   point: Fairness-related harms emerge when there is a
                   mismatch in the measurement process between the thing we
                   purport to be measuring and the thing we actually measure.
                   However, the measurement process -- where social, cultural,
                   and political values are implicitly encoded in
                   sociotechnical systems -- is almost always obscured.
                   Furthermore, this obscured process is where important
                   governance decisions are encoded: governance about which
                   systems are fair, which individuals belong in which
                   categories, and so on. We can then use the language of
                   measurement, and the tools of construct validity and
                   reliability, to uncover hidden governance decisions. In
                   particular, we highlight two types of construct validity,
                   content validity and consequential validity, that are useful
                   to elicit and characterize the feedback loops between the
                   measurement, social construction, and enforcement of social
                   categories. We then explore the constructs of fairness,
                   robustness, and responsibility in the context of governance
                   in and for responsible AI. Together, these perspectives help
                   us unpack how measurement acts as a hidden governance
                   process in sociotechnical systems. Understanding measurement
                   as governance supports a richer understanding of the
                   governance processes already happening in AI -- responsible
                   or otherwise -- revealing paths to more effective
                   interventions.",
  month         =  sep,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CY",
  eprint        = "2109.05658"
}


@MISC{Mayson_dangdefendants,
  title        = "Dangerous Defendants",
  author       = "Mayson, Sandra G",
  abstract     = "Bail reformers aspire to untether pretrial detention from
                  wealth and condition it instead on the risk that a defendant
                  will commit crime if released. In setting this risk
                  threshold, this Article argues that there is no clear
                  constitutional, moral, or practical basis for distinguishing
                  between equally dangerous defendants and non-defendants.",
  howpublished = "\url{https://www.yalelawjournal.org/article/dangerous-defendants}",
  note         = "Accessed: 2022-1-15"
}


@ARTICLE{Lum2016-hz,
  title     = "To predict and serve?",
  author    = "Lum, Kristian and Isaac, William",
  abstract  = "Predictive policing systems are used increasingly by law
               enforcement to try to prevent crime before it occurs. But what
               happens when these systems are trained using biased data?
               Kristian Lum and William Isaac consider the evidence ? and the
               social consequences",
  journal   = "Signif. (Oxf.)",
  publisher = "Wiley",
  volume    =  13,
  number    =  5,
  pages     = "14--19",
  month     =  oct,
  year      =  2016,
  language  = "en"
}


% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Ferguson2016-bs,
  title     = "Policing predictive policing",
  author    = "Ferguson, A G",
  abstract  = "… This article examines predictive policing's evolution with the
               goal ofproviding the first practical and theoretical critique of
               this new policing … assessment throughout the criminal justice
               system, this article provides an analytical framework to police
               new predictive technologies. …",
  journal   = "Wash. UL Rev.",
  publisher = "HeinOnline",
  year      =  2016
}


@ARTICLE{Hoffman2013-ms,
  title    = "The use and misuse of biomedical data: is bigger really better?",
  author   = "Hoffman, Sharona and Podgurski, Andy",
  abstract = "Very large biomedical research databases, containing electronic
              health records (EHR) and genomic data from millions of patients,
              have been heralded recently for their potential to accelerate
              scientific discovery and produce dramatic improvements in medical
              treatments. Research enabled by these databases may also lead to
              profound changes in law, regulation, social policy, and even
              litigation strategies. Yet, is ``big data'' necessarily better
              data? This paper makes an original contribution to the legal
              literature by focusing on what can go wrong in the process of
              biomedical database research and what precautions are necessary
              to avoid critical mistakes. We address three main reasons for
              approaching such research with care and being cautious in relying
              on its outcomes for purposes of public policy or litigation.
              First, the data contained in biomedical databases is surprisingly
              likely to be incorrect or incomplete. Second, systematic biases,
              arising from both the nature of the data and the preconceptions
              of investigators, are serious threats to the validity of research
              results, especially in answering causal questions. Third, data
              mining of biomedical databases makes it easier for individuals
              with political, social, or economic agendas to generate
              ostensibly scientific but misleading research findings for the
              purpose of manipulating public opinion and swaying policymakers.
              In short, this paper sheds much-needed light on the problems of
              credulous and uninformed acceptance of research results derived
              from biomedical databases. An understanding of the pitfalls of
              big data analysis is of critical importance to anyone who will
              rely on or dispute its outcomes, including lawyers, policymakers,
              and the public at large. The Article also recommends technical,
              methodological, and educational interventions to combat the
              dangers of database errors and abuses.",
  journal  = "Am. J. Law Med.",
  volume   =  39,
  number   =  4,
  pages    = "497--538",
  year     =  2013,
  language = "en"
}


@ARTICLE{Hoffman2013-oa,
  title    = "Big bad data: law, public health, and biomedical databases",
  author   = "Hoffman, Sharona and Podgurski, Andy",
  abstract = "The accelerating adoption of electronic health record (EHR)
              systems will have far-reaching implications for public health
              research and surveillance, which in turn could lead to changes in
              public policy, statutes, and regulations. The public health
              benefits of EHR use can be significant. However, researchers and
              analysts who rely on EHR data must proceed with caution and
              understand the potential limitations of EHRs. Because of
              clinicians' workloads, poor user-interface design, and other
              factors, EHR data can be erroneous, miscoded, fragmented, and
              incomplete. In addition, public health findings can be tainted by
              the problems of selection bias, confounding bias, and measurement
              bias. These flaws may become all the more troubling and important
              in an era of electronic ``big data,'' in which a massive amount
              of information is processed automatically, without human checks.
              Thus, we conclude the paper by outlining several regulatory and
              other interventions to address data analysis difficulties that
              could result in invalid conclusions and unsound public health
              policies.",
  journal  = "J. Law Med. Ethics",
  volume   = "41 Suppl 1",
  pages    = "56--60",
  month    =  mar,
  year     =  2013,
  language = "en"
}



@ARTICLE{Agrawal2020-rs,
  title    = "Big data in digital healthcare: lessons learnt and
              recommendations for general practice",
  author   = "Agrawal, Raag and Prabakaran, Sudhakaran",
  abstract = "Big Data will be an integral part of the next generation of
              technological developments-allowing us to gain new insights from
              the vast quantities of data being produced by modern life. There
              is significant potential for the application of Big Data to
              healthcare, but there are still some impediments to overcome,
              such as fragmentation, high costs, and questions around data
              ownership. Envisioning a future role for Big Data within the
              digital healthcare context means balancing the benefits of
              improving patient outcomes with the potential pitfalls of
              increasing physician burnout due to poor implementation leading
              to added complexity. Oncology, the field where Big Data
              collection and utilization got a heard start with programs like
              TCGA and the Cancer Moon Shot, provides an instructive example as
              we see different perspectives provided by the United States (US),
              the United Kingdom (UK) and other nations in the implementation
              of Big Data in patient care with regards to their centralization
              and regulatory approach to data. By drawing upon global
              approaches, we propose recommendations for guidelines and
              regulations of data use in healthcare centering on the creation
              of a unique global patient ID that can integrate data from a
              variety of healthcare providers. In addition, we expand upon the
              topic by discussing potential pitfalls to Big Data such as the
              lack of diversity in Big Data research, and the security and
              transparency risks posed by machine learning algorithms.",
  journal  = "Heredity",
  volume   =  124,
  number   =  4,
  pages    = "525--534",
  month    =  apr,
  year     =  2020,
  language = "en"
}


@ARTICLE{Ensign2017-vi,
  title         = "Runaway Feedback Loops in Predictive Policing",
  author        = "Ensign, Danielle and Friedler, Sorelle A and Neville, Scott
                   and Scheidegger, Carlos and Venkatasubramanian, Suresh",
  abstract      = "Predictive policing systems are increasingly used to
                   determine how to allocate police across a city in order to
                   best prevent crime. Discovered crime data (e.g., arrest
                   counts) are used to help update the model, and the process
                   is repeated. Such systems have been empirically shown to be
                   susceptible to runaway feedback loops, where police are
                   repeatedly sent back to the same neighborhoods regardless of
                   the true crime rate. In response, we develop a mathematical
                   model of predictive policing that proves why this feedback
                   loop occurs, show empirically that this model exhibits such
                   problems, and demonstrate how to change the inputs to a
                   predictive policing system (in a black-box manner) so the
                   runaway feedback loop does not occur, allowing the true
                   crime rate to be learned. Our results are quantitative: we
                   can establish a link (in our model) between the degree to
                   which runaway feedback causes problems and the disparity in
                   crime rates between areas. Moreover, we can also demonstrate
                   the way in which \textbackslashemph\{reported\} incidents of
                   crime (those reported by residents) and
                   \textbackslashemph\{discovered\} incidents of crime (i.e.
                   those directly observed by police officers dispatched as a
                   result of the predictive policing algorithm) interact: in
                   brief, while reported incidents can attenuate the degree of
                   runaway feedback, they cannot entirely remove it without the
                   interventions we suggest.",
  month         =  jun,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CY",
  eprint        = "1706.09847"
}


@UNPUBLISHED{Richardson2019-cn,
  title    = "Dirty Data, Bad Predictions: How Civil Rights Violations Impact
              Police Data, Predictive Policing Systems, and Justice",
  author   = "Richardson, Rashida and Schultz, Jason and Crawford, Kate",
  abstract = "Law enforcement agencies are increasingly using predictive
              policing systems to forecast criminal activity and allocate
              police resources. Yet in numerous jurisdictions, these systems
              are built on data produced during documented periods of flawed,
              racially biased, and sometimes unlawful practices and policies
              (``dirty policing''). These policing practices and policies shape
              the environment and the methodology by which data is created,
              which raises the risk of creating inaccurate, skewed, or
              systemically biased data (``dirty data''). If predictive policing
              systems are informed by such data, they cannot escape the
              legacies of the unlawful or biased policing practices that they
              are built on. Nor do current claims by predictive policing
              vendors provide sufficient assurances that their systems
              adequately mitigate or segregate this data.In our research, we
              analyze thirteen jurisdictions that have used or developed
              predictive policing tools while under government commission
              investigations or federal court monitored settlements, consent
              decrees, or memoranda of agreement stemming from corrupt,
              racially biased, or otherwise illegal policing practices. In
              particular, we examine the link between unlawful and biased
              police practices and the data available to train or implement
              these systems. We highlight three case studies: (1) Chicago, an
              example of where dirty data was ingested directly into the city's
              predictive system; (2) New Orleans, an example where the
              extensive evidence of dirty policing practices and recent
              litigation suggests an extremely high risk that dirty data was or
              could be used in predictive policing; and (3) Maricopa County,
              where despite extensive evidence of dirty policing practices, a
              lack of public transparency about the details of various
              predictive policing systems restricts a proper assessment of the
              risks. The implications of these findings have widespread
              ramifications for predictive policing writ large. Deploying
              predictive policing systems in jurisdictions with extensive
              histories of unlawful police practices presents elevated risks
              that dirty data will lead to flawed or unlawful predictions,
              which in turn risk perpetuating additional harm via feedback
              loops throughout the criminal justice system. The use of
              predictive policing must be treated with high levels of caution
              and mechanisms for the public to know, assess, and reject such
              systems are imperative.",
  month    =  feb,
  year     =  2019,
  keywords = "Policing, Predictive Policing, Civil Rights, Bias, Justice, Data,
              AI, Machine Learning"
}


@UNPUBLISHED{Stevenson2021-fr,
  title    = "Pretrial detention and the value of liberty",
  author   = "Stevenson, Megan T and Mayson, Sandra G",
  abstract = "How dangerous must a person be to justify the state in locking
              her up for the greater good? The bail reform movement, which
              aspires to limit pretrial detention to the truly dangerous---and
              which has looked to algorithmic risk assessments to quantify
              danger---has brought this question to the fore. Constitutional
              doctrine authorizes pretrial detention when the government's
              interest in safety ``outweighs'' an individual's interest in
              liberty, but it does not specify how to balance these goods. If
              detaining ten presumptively innocent people for three months is
              projected to prevent one robbery, is it worth it?This Article
              confronts the question of what degree of risk justifies pretrial
              preventive detention if one takes the consequentialist approach
              of current law seriously. Surveying the law, we derive two
              principles: 1) detention must avert greater harm (by preventing
              crime) than it inflicts (by depriving a person of liberty) and 2)
              prohibitions against pretrial punishment mean that the harm
              experienced by the detainee cannot be discounted in the
              cost-benefit calculus. With this conceptual framework in place,
              we develop a novel empirical method for estimating the relative
              harms of incarceration and crime victimization that we call
              ``Rawlsian cost-benefit analysis'': a survey method that asks
              respondents to choose between being the victim of certain crimes
              or being jailed for varying time periods. The results suggest
              that even short periods of incarceration impose grave harms, such
              that a person must pose an extremely high risk of serious crime
              in order for detention to be justified. No existing risk
              assessment tool is sufficient to identify individuals who warrant
              detention. The empirical results demonstrate that the stated
              consequentialist rationale for pretrial detention cannot begin to
              justify our current detention rates, and suggest that the
              existing system veers uncomfortably close to pretrial punishment.
              The degree of discord between theory and practice demands a
              rethinking of pretrial law and policy.",
  month    =  feb,
  year     =  2021,
  keywords = "pretrial detention, consequentialism, risk assessments, bail
              reform"
}


@MISC{Gouldin_undated-oc,
  title        = "Defining flight risk",
  author       = "Gouldin, Lauryn P and Appleman, Laura and Baughman, Shima
                  Baradaran and Berger, Todd and Bybee, Keith and Cahill,
                  Michael and Commandeur, Nicolas and Eaglin, Jessica and
                  Futrell, Nicole Smith and Godsoe, Cynthia and Gold, Russell
                  and Kohn, Nina and Lain, Corinna and Levine, Kate and Mayson,
                  Sandy and Moore, Janet and Ouziel, Lauren and Podgor, Ellen
                  and Roberts, Anna and Sacharoff, Laurent and Schnacke, Tim
                  and Simonson, Jocelyn and True-Frost, Cora",
  abstract     = "Our illogical and too-well-traveled paths to pretrial
                  detention have created staggering costs for defendants who
                  spend unnecessary time in pretrial detention and for
                  taxpayers who fund a broken system. These problems remain
                  recalcitrant even as a third generation of reform efforts
                  makes impressive headway. They are likely to remain so until
                  judges, attorneys, legislators, and scholars address a
                  fundamental definitional problem: the collapsing of very
                  different types of behavior that result in failures to appear
                  in court into a single, undifferentiated category of
                  nonappearance risk. That single category muddies critical
                  distinctions that this Article's new taxonomy of pretrial
                  nonappearance risks clarifies. This taxonomy (i) isolates
                  true flight risk (the risk that a defendant will flee the
                  jurisdiction) from other forms of ``local'' nonappearance
                  risk and (ii) distinguishes between local nonappearance risks
                  based on persistence, willfulness, amenability to
                  intervention, and cost. Upon examination, it is clear that
                  flight and nonappearance are not simply interchangeable names
                  for the same concept, nor are they merely different degrees
                  of the same type of risk. In the context of measuring and
                  managing risks, many defendants who merely fail to appear
                  differ in important ways from their fugitive cousins.
                  Precision about these distinctions is constitutionally
                  mandated and statutorily required. It is also essential for
                  current reform efforts that are aimed at identifying less
                  intrusive and lower-cost interventions that can effectively
                  manage the full range of nonappearance and flight risks.
                  These distinctions are not reflected in the pretrial
                  risk-assessment tools that are increasingly being employed
                  across the country. But they should be. A more nuanced
                  understanding of these differences",
  howpublished = "\url{https://lawreview.uchicago.edu/sites/lawreview.uchicago.edu/files/02\%20Gouldin_ART_SA\%20\%28JPM\%29.pdf}",
  note         = "Accessed: 2022-1-14"
}


@ARTICLE{Slobogin2003-ou,
  title     = "A jurisprudence of dangerousness",
  author    = "Slobogin, Christopher",
  abstract  = "This article addresses the state's police power authority to
               deprive people of liberty based on predictions of antisocial
               behavior. Most conspicuously exercised against so-called
               ``sexual predators,'' this authority purportedly justifies a
               wide array of other state interventions as well, ranging from
               police stops to executions. Yet there still is no general theory
               of preventive detention. This article is a preliminary effort in
               that regard. The article first surveys the various objections to
               preventive detention: the unreliability objection; the
               punishment-in-disguise objection; the legality objection; and
               the dehumanization objection. None of these objections justifies
               a complete prohibition on the state's power to detain people
               based on dangerousness. But they do suggest significant
               limitations on that power regarding acceptable methods of
               prediction, the nature and duration of preventive detention, the
               threshold conduct that can trigger such detention, and the
               extent to which it can replace punishment as the official
               response to antisocial behavior. On the latter issue, the
               central conclusion is that preventive detention which functions
               as a substitute for punishment, as in the case of sexual
               predator statutes, is only permissible if certain psychological
               and predictive criteria are met. The rest of the paper develops
               these criteria. It argues that the psychological criterion
               should be undeterrability, defined as the characteristic
               ignorance that one's criminal activity is criminal or a
               characteristic willingness to commit crime despite certain and
               significant punishment, a definition that differs from both the
               usual academic stance and the Supreme Court's
               inability-to-control formulation. The paper next argues that
               selection of a prediction criterion should be informed by two
               principles, the proportionality principle (which varies the
               legally requisite level of dangerousness with the nature and
               duration of the state's intervention) and the consistency
               principle (which takes as a reference point the implicit
               dangerousness assessments in the law of crimes). Finally, the
               paper explores some of the implications of the latter principle
               for the criminal law, including the possibility that some crimes
               - in particular various possession offenses, reckless
               endangerment and vagrancy - violate the fundamental norms of the
               police power authority.",
  journal   = "SSRN Electron. J.",
  publisher = "Elsevier BV",
  year      =  2003,
  language  = "en"
}




@INPROCEEDINGS{Akpinar2021-fb,
  title     = "The effect of differential victim crime reporting on predictive
               policing systems",
  booktitle = "Proceedings of the 2021 {ACM} Conference on Fairness,
               Accountability, and Transparency",
  author    = "Akpinar, Nil-Jana and De-Arteaga, Maria and Chouldechova,
               Alexandra",
  abstract  = "Police departments around the world have been experimenting with
               forms of place-based data-driven proactive policing for over two
               decades. Modern incarnations of such systems are commonly known
               as hot spot predictive policing. These systems predict where
               future crime is likely to concentrate such that police can
               allocate patrols to these areas and deter crime before it
               occurs. Previous research on fairness in predictive policing has
               concentrated on the feedback loops which occur when models are
               trained on discovered crime data, but has limited implications
               for models trained on victim crime reporting data. We
               demonstrate how differential victim crime reporting rates across
               geographical areas can lead to outcome disparities in common
               crime hot spot prediction models. Our analysis is based on a
               simulation1 patterned after district-level victimization and
               crime reporting survey data for Bogot{\'a}, Colombia. Our
               results suggest that differential crime reporting rates can lead
               to a displacement of predicted hotspots from high crime but low
               reporting areas to high or medium crime and high reporting
               areas. This may lead to misallocations both in the form of
               over-policing and under-policing.",
  publisher = "Association for Computing Machinery",
  pages     = "838--849",
  series    = "FAccT '21",
  month     =  mar,
  year      =  2021,
  address   = "New York, NY, USA",
  location  = "Virtual Event, Canada"
}



@article{vinsel_critihype,
  title        = "You’re Doing It Wrong: Notes on Criticism and Technology Hype",
  author       = "Vinsel, Lee",
  url = {https://sts-news.medium.com/youre-doing-it-wrong-notes-on-criticism-and-technology-hype-18b08b4307e5}
}


@inbook{krafft_et_al,
author = {Krafft, P. M. and Young, Meg and Katell, Michael and Huang, Karen and Bugingo, Ghislain},
title = {Defining AI in Policy versus Practice},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375835},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {72–78},
numpages = {7}
}
  
@inproceedings{hidden_technical_debt,
 author = {Sculley, D. and Holt, Gary and Golovin, Daniel and Davydov, Eugene and Phillips, Todd and Ebner, Dietmar and Chaudhary, Vinay and Young, Michael and Crespo, Jean-Fran\c{c}ois and Dennison, Dan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Hidden Technical Debt in Machine Learning Systems},
 url = {https://proceedings.neurips.cc/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf},
 volume = {28},
 year = {2015}
}


@ARTICLE{stark_and_hutson,
  title    = "Physiognomic Artificial Intelligence",
  author   = "Stark, Luke and Hutson, Jevan",
  journal  = "forthcoming in Fordham Intellectual Property, Media \& Entertainment Law Journal XXXII",
  year = "2022",
  url = "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3927300"
}

@article{wired_criminality,
 author  = {Fussell, Sidney},
 date    = {2020-06-24},
 title   = {An Algorithm That ‘Predicts’ Criminality Based on a Face Sparks a Furor},
 journal = {Wired},
 url     = {https://www.wired.com/story/algorithm-predicts-criminality-based-face-sparks-furor/}
}

@article {ai_vs_clinicians,
	author = {Nagendran, Myura and Chen, Yang and Lovejoy, Christopher A and Gordon, Anthony C and Komorowski, Matthieu and Harvey, Hugh and Topol, Eric J and Ioannidis, John P A and Collins, Gary S and Maruthappu, Mahiben},
	title = {Artificial intelligence versus clinicians: systematic review of design, reporting standards, and claims of deep learning studies},
	volume = {368},
	elocation-id = {m689},
	year = {2020},
	doi = {10.1136/bmj.m689},
	publisher = {BMJ Publishing Group Ltd},
	URL = {https://www.bmj.com/content/368/bmj.m689},
	eprint = {https://www.bmj.com/content/368/bmj.m689.full.pdf},
	journal = {BMJ}
}

@article{sepsis_validation,
    author = {Wong, Andrew and Otles, Erkin and Donnelly, John P. and Krumm, Andrew and McCullough, Jeffrey and DeTroyer-Cooley, Olivia and Pestrue, Justin and Phillips, Marie and Konye, Judy and Penoza, Carleen and Ghous, Muhammad and Singh, Karandeep},
    title = "{External Validation of a Widely Implemented Proprietary Sepsis Prediction Model in Hospitalized Patients}",
    journal = {JAMA Internal Medicine},
    volume = {181},
    number = {8},
    pages = {1065-1070},
    year = {2021},
    month = {08},
    issn = {2168-6106},
    doi = {10.1001/jamainternmed.2021.2626},
    url = {https://doi.org/10.1001/jamainternmed.2021.2626},
    eprint = {https://jamanetwork.com/journals/jamainternalmedicine/articlepdf/2781307/jamainternal\_wong\_2021\_oi\_210027\_1627674961.11707.pdf},
}



@article{aclu_idaho,
 author  = {Stanley, Jay},
 date    = {2017-06-02},
 title   = {Pitfalls of Artificial Intelligence Decisionmaking Highlighted In Idaho ACLU Case
},
 journal = {ACLU Blogs},
 url     = {https://www.aclu.org/blog/privacy-technology/pitfalls-artificial-intelligence-decisionmaking-highlighted-idaho-aclu-case}
}

@article{verge_aclu_idaho,
 author  = {Lecher, Colin},
 date    = {2018-03-21},
 title   = {What Happens When an Algorithm Cuts Your Health Care
},
 journal = {The Verge},
 url     = {https://www.theverge.com/2018/3/21/17144260/healthcare-medicaid-algorithm-arkansas-cerebral-palsy}
}

@article{democratizing_cloud,
author = {Hasbe, Sudhir and Lippert, Ryan}
title = {The democratization of data and insights: Expanding machine learning access
}
date = {2020-11-16},
journal = {Google Cloud Blog},
url = {https://cloud.google.com/blog/products/data-analytics/democratization-of-ml-and-ai-with-google-cloud}}

@misc{democratizing_h20,
title = {H2O.ai is Democratizing Artificial Intelligence},
year = {2022},
howpublished={\url{https://www.h2o.ai/democratizing-ai/}}
}

@article{democratizing_deloitte,
title = {Democratizing data science to bridge the talent gap},
year = {2018},
author = {Schatsky, David and Chauhan, Rameeta and Muraskin, Craig},
url = {https://www2.deloitte.com/content/dam/insights/us/articles/4602_Democratizing-data-science/DI_Democratizing-data-science.pdf},
journal = {Deloitte Insights - Signals for Strategists}
}

@article{de_democratizing,
  author    = {Nur Ahmed and
               Muntasir Wahed},
  title     = {The De-democratization of {AI:} Deep Learning and the Compute Divide
               in Artificial Intelligence Research},
  journal   = {CoRR},
  volume    = {abs/2010.15581},
  year      = {2020},
  url       = {https://arxiv.org/abs/2010.15581},
  eprinttype = {arXiv},
  eprint    = {2010.15581},
  timestamp = {Tue, 03 Nov 2020 11:44:23 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2010-15581.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{wired_paperclips,
 author  = {Rogers, Adam},
 date    = {2017-10-21},
 title   = {The Way the World Ends: Not with a Bang But a Paperclip},
 journal = {Wired},
 url     = {https://www.wired.com/story/the-way-the-world-ends-not-with-a-bang-but-a-paperclip/}
}

@misc{coalition,
author = {Coalition for Critical Technology},
date = {2020-06-22},
title = {Abolish the \#TechToPrisonPipeline},
howpublished={\url{https://medium.com/@CoalitionForCriticalTechnology/abolish-the-techtoprisonpipeline-9b5b14366b16}}
}

@misc{google_nest_help,
title = {Wave control - Google Nest Help},
howpublished={\url{https://support.google.com/googlenest/answer/6294727?hl=en}}
}

@inproceedings{measuring_robustness_to_natural_distribution_shifts,
 author = {Taori, Rohan and Dave, Achal and Shankar, Vaishaal and Carlini, Nicholas and Recht, Benjamin and Schmidt, Ludwig},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {18583--18599},
 publisher = {Curran Associates, Inc.},
 title = {Measuring Robustness to Natural Distribution Shifts in Image Classification},
 url = {https://proceedings.neurips.cc/paper/2020/file/d8330f857a17c53d217014ee776bfd50-Paper.pdf},
 volume = {33},
 year = {2020}
}

@misc{arvind-reproducibility,
	title = {({Ir}){Reproducible} {Machine} {Learning}: {A} {Case} {Study}},
	language = {en},
	author = {Kapoor, Sayash and Narayanan, Arvind},
	pages = {6},
	year = {2021},
	howpublished = {\url{https://reproducible.cs.princeton.edu/}},
	url = {https://reproducible.cs.princeton.edu/},
	urldate = {2021-07-28}
}

@inproceedings{fairness_tradeoffs_neurips,
 author = {Wick, Michael and panda, swetasudha and Tristan, Jean-Baptiste},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Unlocking Fairness: a Trade-off Revisited},
 url = {https://proceedings.neurips.cc/paper/2019/file/373e4c5d8edfa8b74fd4b6791d0cf6dc-Paper.pdf},
 volume = {32},
 year = {2019}
}

@article{nest,
title={Nest Labs Stops Selling Its Smoke Detector}, url={https://www.nytimes.com/2014/04/04/technology/nest-labs-citing-flaw-halts-smoke-detector-sales.html}, journal={The New York Times},
author={Wingfield, Nick},
year={2014}, month={Apr}}


@misc{catherine_olsson,
	title = {Unsolved research problems vs. real-world threat models},
	language = {en},
	author = {Olsson, Catherine},
	year = {2019},
	howpublished = {\url{https://medium.com/@catherio/unsolved-research-problems-vs-real-world-threat-models-e270e256bc9e}},
	url = {https://medium.com/@catherio/unsolved-research-problems-vs-real-world-threat-models-e270e256bc9e},
}


@article{3d_printed_masks,
 author  = {Peters, Jay},
 date    = {2019-12-13},
 title   = {Researchers fooled Chinese facial recognition terminals with just a mask},
 journal = {The Verge},
 url     = {https://www.theverge.com/2019/12/13/21020575/china-facial-recognition-terminals-fooled-3d-mask-kneron-research-fallibility}
}

@article{makeup,
  author    = {Nitzan Guetta and
               Asaf Shabtai and
               Inderjeet Singh and
               Satoru Momiyama and
               Yuval Elovici},
  title     = {Dodging Attack Using Carefully Crafted Natural Makeup},
  journal   = {CoRR},
  volume    = {abs/2109.06467},
  year      = {2021},
  url       = {https://arxiv.org/abs/2109.06467},
  eprinttype = {arXiv},
  eprint    = {2109.06467},
  timestamp = {Tue, 21 Sep 2021 17:46:04 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2109-06467.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{vegas_pd,
 author  = {Feathers, Todd},
 date    = {2020-08-07},
 title   = {Las Vegas Cops Used ‘Unsuitable’ Facial Recognition Photos To Make Arrests},
 journal = {Vice},
 url     = {https://www.vice.com/en/article/pkyxwv/las-vegas-cops-used-unsuitable-facial-recognition-photos-to-make-arrests}
}

@inproceedings{
LiaoAreWe2021,
title={Are We Learning Yet? A Meta Review of Evaluation Failures Across Machine Learning},
author={Thomas Liao and Rohan Taori and Inioluwa Deborah Raji and Ludwig Schmidt},
booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Pre-Proceedings)},
year={2021},
url={https://openreview.net/forum?id=mPducS1MsEK}
}



@inproceedings{friedler2019comparative,
  title={A comparative study of fairness-enhancing interventions in machine learning},
  author={Friedler, Sorelle A and Scheidegger, Carlos and Venkatasubramanian, Suresh and Choudhary, Sonam and Hamilton, Evan P and Roth, Derek},
  booktitle={Proceedings of the conference on fairness, accountability, and transparency},
  pages={329--338},
  year={2019}
}

@inproceedings{fish2016confidence,
  title={A confidence-based approach for balancing fairness and accuracy},
  author={Fish, Benjamin and Kun, Jeremy and Lelkes, {\'A}d{\'a}m D},
  booktitle={Proceedings of the 2016 SIAM International Conference on Data Mining},
  pages={144--152},
  year={2016},
  organization={SIAM}
}

@article{impossibility_of_fairness,
author = {Friedler, Sorelle A. and Scheidegger, Carlos and Venkatasubramanian, Suresh},
title = {The (Im)Possibility of Fairness: Different Value Systems Require Different Mechanisms for Fair Decision Making},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {64},
number = {4},
issn = {0001-0782},
url = {https://doi.org/10.1145/3433949},
doi = {10.1145/3433949},
abstract = {What does it mean to be fair?},
journal = {Commun. ACM},
month = {mar},
pages = {136–143},
numpages = {8}
}

@inproceedings{disparate_impact_suresh,
author = {Feldman, Michael and Friedler, Sorelle A. and Moeller, John and Scheidegger, Carlos and Venkatasubramanian, Suresh},
title = {Certifying and Removing Disparate Impact},
year = {2015},
isbn = {9781450336642},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2783258.2783311},
doi = {10.1145/2783258.2783311},
booktitle = {Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {259–268},
numpages = {10},
keywords = {disparate impact, fairness, machine learning},
location = {Sydney, NSW, Australia},
series = {KDD '15}
}


@article{adversarial_examples,
  title={Explaining and harnessing adversarial examples},
  author={Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
  journal={arXiv preprint arXiv:1412.6572},
  year={2014}
}


@ARTICLE{De_Mauro2018-mi,
  title    = "Human resources for Big Data professions: A systematic
              classification of job roles and required skill sets",
  author   = "De Mauro, Andrea and Greco, Marco and Grimaldi, Michele and
              Ritala, Paavo",
  journal  = "Inf. Process. Manag.",
  volume   =  54,
  number   =  5,
  pages    = "807--817",
  month    =  sep,
  year     =  2018
}


@book{virginia_eubanks,
  title     = "Automating Inequality: How High-Tech Tools Profile, Police, and Punish the Poor",
  author    = "Eubanks, Virginia",
  year      = 2018,
  publisher = "St. Martin's Press",
  address   = "New York"
}

@book{green2019smart,
  title={The smart enough city: putting technology in its place to reclaim our urban future},
  author={Green, Ben},
  year={2019},
  publisher={MIT Press}
}


@article{haibe_kains,
	Author = {Haibe-Kains, Benjamin and Adam, George Alexandru and Hosny, Ahmed and Khodakarami, Farnoosh and Shraddha, Thakkar and Kusko, Rebecca and Sansone, Susanna-Assunta and Tong, Weida and Wolfinger, Russ D. and Mason, Christopher E. and Jones, Wendell and Dopazo, Joaquin and Furlanello, Cesare and Waldron, Levi and Wang, Bo and McIntosh, Chris and Goldenberg, Anna and Kundaje, Anshul and Greene, Casey S. and Broderick, Tamara and Hoffman, Michael M. and Leek, Jeffrey T. and Korthauer, Keegan and Huber, Wolfgang and Brazma, Alvis and Pineau, Joelle and Tibshirani, Robert and Hastie, Trevor and Ioannidis, John P. A. and Quackenbush, John and Aerts, Hugo J. W. L. and Massive Analysis Quality Control (MAQC) Society Board of Directors},
	Da = {2020/10/01},
	Date-Added = {2022-01-21 23:46:00 +0000},
	Date-Modified = {2022-01-21 23:46:00 +0000},
	Doi = {10.1038/s41586-020-2766-y},
	Id = {Haibe-Kains2020},
	Isbn = {1476-4687},
	Journal = {Nature},
	Number = {7829},
	Pages = {E14--E16},
	Title = {Transparency and reproducibility in artificial intelligence},
	Ty = {JOUR},
	Url = {https://doi.org/10.1038/s41586-020-2766-y},
	Volume = {586},
	Year = {2020},
	Bdsk-Url-1 = {https://doi.org/10.1038/s41586-020-2766-y},
	Bdsk-Url-2 = {http://dx.doi.org/10.1038/s41586-020-2766-y}}



@ARTICLE{mit_replication,
  title    = "AI is wrestling with a replication crisis",
  author   = "Douglas Heaven, Will",
  journal  = "MIT Technology Review",
  month    =  nov,
  year     =  2020
}
