
@misc{mazumder_dataperf_2022,
	title = {{DataPerf}: {Benchmarks} for {Data}-{Centric} {AI} {Development}},
	shorttitle = {{DataPerf}},
	url = {http://arxiv.org/abs/2207.10062},
	doi = {10.48550/arXiv.2207.10062},
	abstract = {Machine learning (ML) research has generally focused on models, while the most prominent datasets have been employed for everyday ML tasks without regard for the breadth, difficulty, and faithfulness of these datasets to the underlying problem. Neglecting the fundamental importance of datasets has caused major problems involving data cascades in real-world applications and saturation of dataset-driven criteria for model quality, hindering research growth. To solve this problem, we present DataPerf, a benchmark package for evaluating ML datasets and dataset-working algorithms. We intend it to enable the "data ratchet," in which training sets will aid in evaluating test sets on the same problems, and vice versa. Such a feedback-driven strategy will generate a virtuous loop that will accelerate development of data-centric AI. The MLCommons Association will maintain DataPerf.},
	urldate = {2023-02-14},
	publisher = {arXiv},
	author = {Mazumder, Mark and Banbury, Colby and Yao, Xiaozhe and Karlaš, Bojan and Rojas, William Gaviria and Diamos, Sudnya and Diamos, Greg and He, Lynn and Kiela, Douwe and Jurado, David and Kanter, David and Mosquera, Rafael and Ciro, Juan and Aroyo, Lora and Acun, Bilge and Eyuboglu, Sabri and Ghorbani, Amirata and Goodman, Emmett and Kane, Tariq and Kirkpatrick, Christine R. and Kuo, Tzu-Sheng and Mueller, Jonas and Thrush, Tristan and Vanschoren, Joaquin and Warren, Margaret and Williams, Adina and Yeung, Serena and Ardalani, Newsha and Paritosh, Praveen and Zhang, Ce and Zou, James and Wu, Carole-Jean and Coleman, Cody and Ng, Andrew and Mattson, Peter and Reddi, Vijay Janapa},
	month = jul,
	year = {2022},
	note = {arXiv:2207.10062 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{mcgregor_participation_2022,
	title = {Participation {Interfaces} for {Human}-{Centered} {AI}},
	url = {http://arxiv.org/abs/2211.08419},
	doi = {10.48550/arXiv.2211.08419},
	abstract = {Emerging artificial intelligence (AI) applications often balance the preferences and impacts among diverse and contentious stakeholder groups. Accommodating these stakeholder groups during system design, development, and deployment requires tools for the elicitation of disparate system interests and collaboration interfaces supporting negotiation balancing those interests. This paper introduces interactive visual "participation interfaces" for Markov Decision Processes (MDPs) and collaborative ranking problems as examples restoring a human-centered locus of control.},
	urldate = {2022-12-13},
	publisher = {arXiv},
	author = {McGregor, Sean},
	month = nov,
	year = {2022},
	note = {arXiv:2211.08419 [cs]},
	keywords = {Computer Science - Computers and Society, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning},
}

@misc{munich_re_insure_2022,
	title = {Insure {AI} – {Guarantee} the performance of your {Artificial} {Intelligence} systems},
	url = {https://www.munichre.com/en/solutions/for-industry-clients/insure-ai.html},
	abstract = {Artificial intelligence will turn many areas of the economy upside down: It offers us the chance to reduce costs, improve quality and increase profits, whether in fraud management, production processes or agriculture.},
	language = {en},
	urldate = {2022-11-21},
	journal = {Munich Re},
	author = {{Munich Re}},
	month = nov,
	year = {2022},
}

@inproceedings{mcgregor_indexing_2022,
	title = {Indexing {AI} {Risks} with {Incidents}, {Issues}, and {Variants}},
	url = {http://arxiv.org/abs/2211.10384},
	abstract = {Two years after publicly launching the AI Incident Database (AIID) as a collection of harms or near harms produced by AI in the world, a backlog of "issues" that do not meet its incident ingestion criteria have accumulated in its review queue. Despite not passing the database's current criteria for incidents, these issues advance human understanding of where AI presents the potential for harm. Similar to databases in aviation and computer security, the AIID proposes to adopt a two-tiered system for indexing AI incidents (i.e., a harm or near harm event) and issues (i.e., a risk of a harm event). Further, as some machine learning-based systems will sometimes produce a large number of incidents, the notion of an incident "variant" is introduced. These proposed changes mark the transition of the AIID to a new version in response to lessons learned from editing 2,000+ incident reports and additional reports that fall under the new category of "issue."},
	urldate = {2022-11-21},
	booktitle = {{NeurIPS} {Workshop} on {Human}-{Centered} {AI}},
	publisher = {arXiv},
	author = {McGregor, Sean and Paeth, Kevin and Lam, Khoa},
	month = nov,
	year = {2022},
	note = {arXiv:2211.10384 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Machine Learning},
}

@book{underwriters_laboratories_engineering_2016,
	title = {Engineering {Progress}},
	url = {https://www.ul.com/about/download-engineering-progress-ebook},
	abstract = {“Engineering Progress” is a comprehensive, historical account of UL Solutions.},
	language = {en},
	urldate = {2022-11-21},
	publisher = {Selby Marketing Associates},
	author = {{Underwriters Laboratories}},
	year = {2016},
}

@misc{ml_commons_mlcommons_2022,
	title = {{MLCommons}},
	url = {https://mlcommons.org/},
	abstract = {MLCommons aims to accelerate machine learning innovation to benefit everyone.},
	language = {en},
	urldate = {2022-11-21},
	journal = {MLCommons},
	author = {{ML Commons}},
	month = nov,
	year = {2022},
}

@misc{appen_launch_2022,
	title = {Launch {World}-{Class} {AI} and {ML} {Projects} with {Confidence}},
	url = {https://s40188.p1443.sites.pressdns.com/platform-5/},
	abstract = {Access high-quality structureed or unstructured data to train models for unique use cases through a world-class technology platform},
	language = {en-GB},
	urldate = {2022-11-21},
	journal = {Appen},
	author = {{Appen}},
	month = nov,
	year = {2022},
}

@misc{gitlab_cicd_2022,
	title = {{CI}/{CD} concepts {\textbar} {GitLab}},
	url = {https://docs.gitlab.com/ee/ci/introduction/},
	abstract = {An overview of Continuous Integration, Continuous Delivery, and Continuous Deployment, as well as an introduction to GitLab CI/CD.},
	language = {en-us},
	urldate = {2022-11-20},
	journal = {GitLab Documentation},
	author = {{GitLab}},
	month = nov,
	year = {2022},
}

@misc{hugging_face_distilbert-base-uncased-finetuned-sst-2-english_2022,
	title = {distilbert-base-uncased-finetuned-sst-2-english · {Hugging} {Face}},
	url = {https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2022-11-20},
	journal = {Hugging Face - The AI Community Building the Future},
	author = {{Hugging Face}},
	month = nov,
	year = {2022},
}

@article{anonymous_incident_2016,
	title = {Incident 37: {Female} {Applicants} {Down}-{Ranked} by {Amazon} {Recruiting} {Tool}},
	url = {https://incidentdatabase.ai/cite/37},
	journal = {AI Incident Database},
	author = {{Anonymous}},
	editor = {McGregor, Sean},
	year = {2016},
	note = {Publisher: Responsible AI Collaborative},
}

@article{colmer_incident_2018,
	title = {Incident 361: {Amazon} {Echo} {Mistakenly} {Recorded} and {Sent} {Private} {Conversation} to {Random} {Contact}},
	url = {https://incidentdatabase.ai/cite/361},
	journal = {AI Incident Database},
	author = {Colmer, Devon},
	editor = {Lam, Khoa},
	year = {2018},
	note = {Publisher: Responsible AI Collaborative},
}

@article{lam_incident_2021,
	title = {Incident 171: {Traffic} {Camera} {Misread} {Text} on {Pedestrian}'s {Shirt} as {License} {Plate}, {Causing} {UK} {Officials} to {Issue} {Fine} to an {Unrelated} {Person}},
	url = {https://incidentdatabase.ai/cite/171},
	journal = {AI Incident Database},
	author = {Lam, Khoa},
	editor = {McGregor, Sean},
	year = {2021},
	note = {Publisher: Responsible AI Collaborative},
}

@article{anonymous_incident_2021,
	title = {Incident 160: {Alexa} {Recommended} {Dangerous} {TikTok} {Challenge} to {Ten}-{Year}-{Old} {Girl}},
	url = {https://incidentdatabase.ai/cite/160},
	journal = {AI Incident Database},
	author = {{Anonymous}},
	editor = {McGregor, Sean},
	year = {2021},
	note = {Publisher: Responsible AI Collaborative},
}

@article{anonymous_incident_2019,
	title = {Incident 159: {Tesla} {Autopilot}’s {Lane} {Recognition} {Allegedly} {Vulnerable} to {Adversarial} {Attacks}},
	url = {https://incidentdatabase.ai/cite/159},
	journal = {AI Incident Database},
	author = {{Anonymous}},
	editor = {McGregor, Sean},
	year = {2019},
	note = {Publisher: Responsible AI Collaborative},
}

@article{anonymous_incident_2021-1,
	title = {Incident 149: {Zillow} {Shut} {Down} {Zillow} {Offers} {Division} {Allegedly} {Due} to {Predictive} {Pricing} {Tool}'s {Insufficient} {Accuracy}},
	url = {https://incidentdatabase.ai/cite/149},
	journal = {AI Incident Database},
	author = {{Anonymous}},
	editor = {McGregor, Sean},
	year = {2021},
	note = {Publisher: Responsible AI Collaborative},
}

@article{hall_incident_2020,
	title = {Incident 134: {Robot} in {Chinese} {Shopping} {Mall} {Fell} off the {Escalator}, {Knocking} down {Passengers}},
	url = {https://incidentdatabase.ai/cite/134},
	journal = {AI Incident Database},
	author = {Hall, Patrick},
	editor = {McGregor, Sean},
	year = {2020},
	note = {Publisher: Responsible AI Collaborative},
}

@article{xie_incident_2018,
	title = {Incident 114: {Amazon}'s {Rekognition} {Falsely} {Matched} {Members} of {Congress} to {Mugshots}},
	url = {https://incidentdatabase.ai/cite/114},
	journal = {AI Incident Database},
	author = {Xie, Fabio},
	editor = {McGregor, Sean},
	year = {2018},
	note = {Publisher: Responsible AI Collaborative},
}

@article{anonymous_incident_2020,
	title = {Incident 102: {Personal} voice assistants struggle with black voices, new study shows},
	url = {https://incidentdatabase.ai/cite/102},
	journal = {AI Incident Database},
	author = {{Anonymous}},
	editor = {McGregor, Sean},
	year = {2020},
	note = {Publisher: Responsible AI Collaborative},
}

@article{anonymous_incident_2017,
	title = {Incident 68: {Security} {Robot} {Drowns} {Itself} in a {Fountain}},
	url = {https://incidentdatabase.ai/cite/68},
	journal = {AI Incident Database},
	author = {{Anonymous}},
	editor = {McGregor, Sean},
	year = {2017},
	note = {Publisher: Responsible AI Collaborative},
}

@article{yampolskiy_incident_2016,
	title = {Incident 55: {Alexa} {Plays} {Pornography} {Instead} of {Kids} {Song}},
	url = {https://incidentdatabase.ai/cite/55},
	journal = {AI Incident Database},
	author = {Yampolskiy, Roman},
	editor = {McGregor, Sean},
	year = {2016},
	note = {Publisher: Responsible AI Collaborative},
}

@article{aiaaic_incident_2016,
	title = {Incident 53: {Biased} {Google} {Image} {Results}},
	url = {https://incidentdatabase.ai/cite/53},
	journal = {AI Incident Database},
	author = {{AIAAIC}},
	editor = {McGregor, Sean},
	year = {2016},
	note = {Publisher: Responsible AI Collaborative},
}

@article{mcgregor_incident_2016,
	title = {Incident 51: {Security} {Robot} {Rolls} {Over} {Child} in {Mall}},
	url = {https://incidentdatabase.ai/cite/51},
	journal = {AI Incident Database},
	author = {McGregor, Sean},
	editor = {McGregor, Sean},
	year = {2016},
	note = {Publisher: Responsible AI Collaborative},
}

@article{olsson_incident_2018,
	title = {Incident 36: {Picture} of {Woman} on {Side} of {Bus} {Shamed} for {Jaywalking}},
	url = {https://incidentdatabase.ai/cite/36},
	journal = {AI Incident Database},
	author = {Olsson, Catherine},
	editor = {McGregor, Sean},
	year = {2018},
	note = {Publisher: Responsible AI Collaborative},
}

@article{yampolskiy_incident_2015,
	title = {Incident 34: {Amazon} {Alexa} {Responding} to {Environmental} {Inputs}},
	url = {https://incidentdatabase.ai/cite/34},
	journal = {AI Incident Database},
	author = {Yampolskiy, Roman},
	editor = {McGregor, Sean},
	year = {2015},
	note = {Publisher: Responsible AI Collaborative},
}

@article{olsson_incident_2017,
	title = {Incident 22: {Waze} {Navigates} {Motorists} into {Wildfires}},
	url = {https://incidentdatabase.ai/cite/22},
	journal = {AI Incident Database},
	author = {Olsson, Catherine},
	editor = {McGregor, Sean},
	year = {2017},
	note = {Publisher: Responsible AI Collaborative},
}

@article{anonymous_incident_2015,
	title = {Incident 16: {Images} of {Black} {People} {Labeled} as {Gorillas}},
	url = {https://incidentdatabase.ai/cite/16},
	journal = {AI Incident Database},
	author = {{Anonymous}},
	editor = {McGregor, Sean},
	year = {2015},
	note = {Publisher: Responsible AI Collaborative},
}

@article{olsson_incident_2017-1,
	title = {Incident 13: {High}-{Toxicity} {Assessed} on {Text} {Involving} {Women} and {Minority} {Groups}},
	url = {https://incidentdatabase.ai/cite/13},
	journal = {AI Incident Database},
	author = {Olsson, Catherine},
	editor = {McGregor, Sean},
	year = {2017},
	note = {Publisher: Responsible AI Collaborative},
}

@techreport{gartner_predicts_2018,
	title = {Predicts 2019: {Artificial} {Intelligence} {Core} {Technologies}},
	shorttitle = {Predicts 2019},
	url = {https://www.gartner.com/en/documents/3894131},
	abstract = {AI adoption in organizations has increased nearly threefold since last year, raising the chances of misaligned core technologies and AI initiatives. I\&O leaders must use a combination of buy, build and outsource to accelerate productivity in AI initiatives.},
	language = {en},
	urldate = {2022-11-18},
	institution = {Gartner},
	author = {{Gartner}},
	month = nov,
	year = {2018},
}

@misc{the_linux_foundation_models_2022,
	title = {Models and pre-trained weights — {Torchvision} 0.14 documentation},
	url = {https://pytorch.org/vision/stable/models.html#using-the-pre-trained-models},
	urldate = {2022-11-15},
	author = {{The Linux Foundation}},
	month = nov,
	year = {2022},
}

@misc{goog-411team_goodbye_2010,
	title = {Goodbye to an old friend: 1-800-{GOOG}-411},
	url = {https://googleblog.blogspot.com/2010/10/goodbye-to-old-friend-1-800-goog-411.html},
	publisher = {Google},
	author = {{GOOG-411Team}},
	month = oct,
	year = {2010},
	note = {Publication Title: Google Blog},
}

@misc{perez_google_2007,
	title = {Google wants your phonemes},
	url = {https://www.infoworld.com/article/2642023/google-wants-your-phonemes.html},
	publisher = {Infoworld},
	author = {Perez, Juan Carlos},
	month = oct,
	year = {2007},
	note = {Publication Title: Infoworld},
}

@misc{google_how_2022,
	title = {How one team turned the dream of speech recognition into a reality},
	url = {https://careers.google.com/stories/how-one-team-turned-the-dream-of-speech-recognition-into-a-reality/},
	publisher = {Google},
	author = {{Google}},
	month = mar,
	year = {2022},
	note = {Publication Title: Google Careers Blog},
}

@misc{grafana_labs_grafana_2022,
	title = {Grafana {\textbar} {Query}, visualize, alerting observability platform},
	url = {https://grafana.com/grafana/},
	abstract = {Grafana feature overview, screenshots, videos, and feature tours.},
	language = {en},
	urldate = {2022-11-09},
	journal = {Grafana Labs},
	author = {{Grafana Labs}},
	month = nov,
	year = {2022},
}

@article{strathern_improving_1997,
	title = {‘{Improving} ratings’: {Audit} in the {British} {University} system},
	volume = {5},
	issn = {1474-0575, 1062-7987},
	shorttitle = {‘{Improving} ratings’},
	url = {https://archive.org/details/ImprovingRatingsAuditInTheBritishUniversitySystem/mode/2up},
	doi = {10.1002/(SICI)1234-981X(199707)5:3<305::AID-EURO184>3.0.CO;2-4},
	abstract = {This paper gives an anthropological comment on what has been called the ‘audit explosion’, the proliferation of procedures for evaluating performance. In higher education the subject of audit (in this sense) is not so much the education of the students as the institutional provision for their education. British universities, as institutions, are increasingly subject to national scrutiny for teaching, research and administrative competence. In the wake of this scrutiny comes a new cultural apparatus of expectations and technologies. While the metaphor of financial auditing points to the important values of accountability, audit does more than monitor—it has a life of its own that jeopardizes the life it audits. The runaway character of assessment practices is analysed in terms of cultural practice. Higher education is intimately bound up with the origins of such practices, and is not just the latter day target of them. © 1997 by John Wiley \& Sons, Ltd.},
	language = {en},
	number = {3},
	urldate = {2022-11-07},
	journal = {European Review},
	author = {Strathern, Marilyn},
	month = jul,
	year = {1997},
	note = {Publisher: Cambridge University Press},
	pages = {305--321},
}

@inproceedings{recht_imagenet_2019,
	title = {Do imagenet classifiers generalize to {ImageNet}?},
	url = {https://proceedings.mlr.press/v97/recht19a.html},
	abstract = {We build new test sets for the CIFAR-10 and ImageNet datasets. Both benchmarks have been the focus of intense research for almost a decade, raising the danger of overfitting to excessively re-used test sets. By closely following the original dataset creation processes, we test to what extent current classification models generalize to new data. We evaluate a broad range of models and find accuracy drops of 3\% - 15\% on CIFAR-10 and 11\% - 14\% on ImageNet. However, accuracy gains on the original test sets translate to larger gains on the new test sets. Our results suggest that the accuracy drops are not caused by adaptivity, but by the models’ inability to generalize to slightly "harder" images than those found in the original test sets.},
	language = {en},
	urldate = {2022-10-21},
	booktitle = {International {Conference} on {Machine} {Learning} ({ICML})},
	publisher = {PMLR},
	author = {Recht, Benjamin and Roelofs, Rebecca and Schmidt, Ludwig and Shankar, Vaishaal},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {5389--5400},
}

@misc{piorkowski_quantitative_2022,
	title = {Quantitative {AI} risk assessments: {Opportunities} and challenges},
	shorttitle = {Quantitative ai risk assessments},
	url = {http://arxiv.org/abs/2209.06317},
	abstract = {Although AI-based systems are increasingly being leveraged to provide value to organizations, individuals, and society, signiﬁcant attendant risks have been identiﬁed [1]–[5]. These risks have led to proposed regulations, litigation, and general societal concerns.},
	language = {en},
	urldate = {2022-10-30},
	publisher = {arXiv},
	author = {Piorkowski, David and Hind, Michael and Richards, John},
	month = sep,
	year = {2022},
	note = {arXiv:2209.06317 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@misc{van_looveren_alibi_2022,
	title = {Alibi {Detect}: {Algorithms} for outlier, adversarial and drift detection},
	url = {https://github.com/SeldonIO/alibi-detect},
	author = {Van Looveren, Arnaud and Klaise, Janis and Vacanti, Giovanni and Cobb, Oliver and Scillitoe, Ashley and Samoilescu, Robert and Athorne, Alex},
	month = oct,
	year = {2022},
}

@misc{seldon_core_seldon_2022,
	title = {Seldon {Core}},
	url = {https://www.seldon.io/solutions/open-source-projects/core},
	abstract = {Open-source platform for rapidly deploying machine learning models on Kubernetes The de facto standard open-source platform for rapidly deploying machine learning models […]},
	language = {en-US},
	urldate = {2022-11-09},
	journal = {Seldon Core},
	author = {{Seldon Core}},
	month = nov,
	year = {2022},
}

@misc{national_institute_of_standards_and_technology_what_2022,
	title = {What is {Dioptra}? — {Dioptra} 0.0.0 documentation},
	url = {https://pages.nist.gov/dioptra/},
	urldate = {2022-11-09},
	journal = {National Institute of Standards and Technology},
	author = {{National Institute of Standards and Technology}},
	month = nov,
	year = {2022},
}

@misc{czakon_best_2021,
	title = {Best {Tools} to {Do} {ML} {Model} {Monitoring}},
	url = {https://neptune.ai/blog/ml-model-monitoring-best-tools},
	abstract = {If you deploy models to production sooner or later, you will start looking for ML model monitoring tools. When your ML models impact the business (and they should), you just need visibility into “how things work”. The first moment you really feel this is when things stop working. With no model monitoring set up, you […]},
	language = {en-US},
	urldate = {2022-11-09},
	journal = {neptune.ai},
	author = {Czakon, Jakub},
	month = mar,
	year = {2021},
}

@misc{manheim_categorizing_2019,
	title = {Categorizing {Variants} of {Goodhart}'s {Law}},
	url = {http://arxiv.org/abs/1803.04585},
	doi = {10.48550/arXiv.1803.04585},
	abstract = {There are several distinct failure modes for overoptimization of systems on the basis of metrics. This occurs when a metric which can be used to improve a system is used to an extent that further optimization is ineffective or harmful, and is sometimes termed Goodhart's Law. This class of failure is often poorly understood, partly because terminology for discussing them is ambiguous, and partly because discussion using this ambiguous terminology ignores distinctions between different failure modes of this general type. This paper expands on an earlier discussion by Garrabrant, which notes there are "(at least) four different mechanisms" that relate to Goodhart's Law. This paper is intended to explore these mechanisms further, and specify more clearly how they occur. This discussion should be helpful in better understanding these types of failures in economic regulation, in public policy, in machine learning, and in Artificial Intelligence alignment. The importance of Goodhart effects depends on the amount of power directed towards optimizing the proxy, and so the increased optimization power offered by artificial intelligence makes it especially critical for that field.},
	urldate = {2022-11-06},
	publisher = {arXiv},
	author = {Manheim, David and Garrabrant, Scott},
	month = feb,
	year = {2019},
	note = {arXiv:1803.04585 [cs, q-fin, stat]},
	keywords = {91E45, Computer Science - Artificial Intelligence, Quantitative Finance - General Finance, Statistics - Machine Learning},
}

@book{stuart_russell_artificial_2009,
	title = {Artificial {Intelligence}: {A} {Modern} {Approach}, 3rd {US} ed.},
	isbn = {0-13-604259-7},
	url = {http://aima.cs.berkeley.edu/},
	language = {English},
	urldate = {2022-11-06},
	publisher = {Prentice Hall},
	author = {{Stuart Russell} and {Peter Norvig}},
	year = {2009},
}

@book{knauss_detecting_2012,
	title = {Detecting and {Classifying} {Patterns} of {Requirements} {Clarifications}},
	abstract = {In current project environments, requirements often evolve throughout the project and are worked on by stakeholders in large and distributed teams. Such teams often use online tools such as mailing lists, bug tracking systems or online discussion forums to communicate, clarify or coordinate work on requirements. In this kind of environment, the expected evolution from initial idea, through clarification, to a stable requirement, often stagnates. When project managers are not aware of underlying problems, development may pro-ceed before requirements are fully understood and stabilized, leading to numerous implementation issues and often resulting in the need for early redesign and modification. In this paper, we present an approach to analyzing online requirements communication and a method for the detection and classification of clarification events in requirement discus-sions. We used our approach to analyze online requirements communication in the IBM R Rational Team Concert R (RTC) project and identified a set of six clarification patterns. Since a predominant amount of clarifications through the lifetime of a requirement is often indicative of problematic requirements, our approach lends support to project managers to assess, in real-time, the state of discussions around a requirement and promptly react to requirements problems.},
	publisher = {20th IEEE International Requirements Engineering Conference},
	author = {Knauss, Eric and Damian, Daniela and Poo-Caamaño, Germán and Cleland-Huang, Jane},
	month = sep,
	year = {2012},
	doi = {10.1109/RE.2012.6345811},
	note = {Journal Abbreviation: 2012 20th IEEE International Requirements Engineering Conference, RE 2012 - Proceedings
Publication Title: 2012 20th IEEE International Requirements Engineering Conference, RE 2012 - Proceedings},
}

@misc{michael_sayre_significance_2019,
	title = {The significance of “edge cases” and the cost of imperfection as it pertains to {AI} adoption},
	url = {https://medium.com/@livewithai/the-significance-of-edge-cases-and-the-cost-of-imperfection-as-it-pertains-to-ai-adoption-dc1cebeef72c},
	abstract = {With the advent of new tools and technologies, it’s tempting to think that the rules of work have changed or that old problems can be…},
	language = {en},
	urldate = {2022-11-05},
	journal = {Medium},
	author = {{Michael Sayre}},
	month = apr,
	year = {2019},
}

@misc{erin_mulvaney_nyc_2021,
	type = {News},
	title = {{NYC} {Targets} {Artificial} {Intelligence} {Bias} in {Hiring} {Under} {New} {Law}},
	url = {https://news.bloomberglaw.com/daily-labor-report/nyc-targets-artificial-intelligence-bias-in-hiring-under-new-law},
	abstract = {New York City has a new law on the books—one of the boldest measures of its kind in the country—that aims to curb hiring bias that can occur when businesses use artificial intelligence tools to screen out job candidates.},
	language = {en},
	urldate = {2022-11-05},
	journal = {Bloomberg Law},
	author = {{Erin Mulvaney}},
	month = dec,
	year = {2021},
}

@techreport{jared_dunnmon_responsible_2021,
	title = {Responsible {AI} {Guidelines} in {Practice}},
	url = {https://assets.ctfassets.net/3nanhbfkr0pc/acoo1Fj5uungnGNPJ3QWy/6ec382b3b5a20ec7de6defdb33b04dcd/2021_RAI_Report.pdf},
	urldate = {2022-11-04},
	institution = {Defense Innovation Unit},
	author = {{Jared Dunnmon} and {Bryce Goodman} and {Peter Kirechu} and {Carol Smith} and {Alexandrea Van Deusen}},
	month = mar,
	year = {2021},
}

@misc{european_union_ai_2021,
	title = {The {AI} {Act}},
	url = {https://artificialintelligenceact.eu/the-act/},
	language = {en-US},
	urldate = {2022-11-05},
	author = {{European Union}},
	month = feb,
	year = {2021},
}

@article{mokander_operationalising_2022,
	title = {Operationalising {AI} governance through ethics-based auditing: an industry case study},
	issn = {2730-5953, 2730-5961},
	shorttitle = {Operationalising {AI} governance through ethics-based auditing},
	url = {https://link.springer.com/10.1007/s43681-022-00171-7},
	doi = {10.1007/s43681-022-00171-7},
	abstract = {Ethics-based auditing (EBA) is a structured process whereby an entity’s past or present behaviour is assessed for consistency with moral principles or norms. Recently, EBA has attracted much attention as a governance mechanism that may help to bridge the gap between principles and practice in AI ethics. However, important aspects of EBA—such as the feasibility and effectiveness of different auditing procedures—have yet to be substantiated by empirical research. In this article, we address this knowledge gap by providing insights from a longitudinal industry case study. Over 12 months, we observed and analysed the internal activities of AstraZeneca, a biopharmaceutical company, as it prepared for and underwent an ethicsbased AI audit. While previous literature concerning EBA has focussed on proposing or analysing evaluation metrics or visualisation techniques, our findings suggest that the main difficulties large multinational organisations face when conducting EBA mirror classical governance challenges. These include ensuring harmonised standards across decentralised organisations, demarcating the scope of the audit, driving internal communication and change management, and measuring actual outcomes. The case study presented in this article contributes to the existing literature by providing a detailed description of the organisational context in which EBA procedures must be integrated to be feasible and effective.},
	language = {en},
	urldate = {2022-11-05},
	journal = {AI and Ethics},
	author = {Mökander, Jakob and Floridi, Luciano},
	month = may,
	year = {2022},
}

@techreport{information_commisioners_office_guidance_2022,
	title = {Guidance on the {AI} auditing framework {Draft} guidance for consultation},
	url = {https://ico.org.uk/media/2617219/guidance-on-the-ai-auditing-framework-draft-for-consultation.pdf},
	language = {en},
	number = {20200214},
	urldate = {2022-11-04},
	institution = {Information Commisioner's Office},
	author = {{Information Commisioner's Office}},
	month = nov,
	year = {2022},
	pages = {105},
}

@misc{office_of_the_director_of_national_intelligence_artificial_2022,
	title = {Artificial {Intelligence} {Ethics} {Framework} for the {Intelligence} {Community}},
	url = {https://www.intelligence.gov/images/AI/AI_Ethics_Framework_for_the_Intelligence_Community_1.0.pdf},
	urldate = {2022-11-04},
	publisher = {Office of the Director of National Intelligence},
	author = {{Office of the Director of National Intelligence}},
	month = nov,
	year = {2022},
}

@misc{ibm_ai_2022,
	title = {{AI} {Ethics}},
	url = {https://www.ibm.com/artificial-intelligence/ethics},
	abstract = {IBM’s multidisciplinary, multidimensional approach to AI ethics.},
	language = {en-us},
	urldate = {2022-11-05},
	author = {{IBM}},
	month = nov,
	year = {2022},
}

@misc{google_building_2022,
	title = {Building responsible {AI} for everyone},
	url = {https://ai.google/responsibilities/},
	language = {en},
	urldate = {2022-11-05},
	journal = {Google AI},
	author = {{Google}},
	month = nov,
	year = {2022},
}

@misc{hosseini_deceiving_2017,
	title = {Deceiving {Google}'s {Perspective} {API} {Built} for {Detecting} {Toxic} {Comments}},
	url = {http://arxiv.org/abs/1702.08138},
	abstract = {Social media platforms provide an environment where people can freely engage in discussions. Unfortunately, they also enable several problems, such as online harassment. Recently, Google and Jigsaw started a project called Perspective, which uses machine learning to automatically detect toxic language. A demonstration website has been also launched, which allows anyone to type a phrase in the interface and instantaneously see the toxicity score [1].},
	language = {en},
	urldate = {2022-10-31},
	publisher = {arXiv},
	author = {Hosseini, Hossein and Kannan, Sreeram and Zhang, Baosen and Poovendran, Radha},
	month = feb,
	year = {2017},
	note = {arXiv:1702.08138 [cs]},
	keywords = {Computer Science - Computers and Society, Computer Science - Machine Learning, Computer Science - Social and Information Networks},
}

@article{buslaev_albumentations_2020,
	title = {Albumentations: {Fast} and {Flexible} {Image} {Augmentations}},
	volume = {11},
	issn = {2078-2489},
	url = {https://www.mdpi.com/2078-2489/11/2/125},
	doi = {10.3390/info11020125},
	number = {2},
	journal = {Information},
	author = {Buslaev, Alexander and Iglovikov, Vladimir I. and Khvedchenya, Eugene and Parinov, Alex and Druzhinin, Mikhail and Kalinin, Alexandr A.},
	year = {2020},
}

@inproceedings{blum_ladder_2015,
	title = {The ladder: {A} reliable leaderboard for machine learning competitions},
	shorttitle = {The ladder},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Blum, Avrim and Hardt, Moritz},
	year = {2015},
	pages = {1006--1014},
}

@article{damour_underspecification_2020,
	title = {Underspecification presents challenges for credibility in modern machine learning},
	journal = {arXiv preprint arXiv:2011.03395},
	author = {D'Amour, Alexander and Heller, Katherine and Moldovan, Dan and Adlam, Ben and Alipanahi, Babak and Beutel, Alex and Chen, Christina and Deaton, Jonathan and Eisenstein, Jacob and Hoffman, Matthew D and {others}},
	year = {2020},
}

@article{deng_imagenet_2009,
	title = {{ImageNet}: {A} {Large}-{Scale} {Hierarchical} {Image} {Database}},
	abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a largescale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 5001000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classiﬁcation and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
	language = {en},
	journal = {IEEE Conference on Computer Vision and Pattern Recognition},
	author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
	year = {2009},
	pages = {8},
}

@misc{tencent_keen_security_lab_experimental_2019,
	title = {Experimental security research of {Tesla} autopilot},
	url = {https://keenlab.tencent.com/en/whitepapers/Experimental_Security_Research_of_Tesla_Autopilot.pdf},
	publisher = {Tencent},
	author = {Tencent Keen Security Lab},
	month = mar,
	year = {2019},
}

@inproceedings{peng_mitigating_2021,
	title = {Mitigating dataset harms requires stewardship: {Lessons} from 1000 papers},
	shorttitle = {Mitigating dataset harms requires stewardship},
	url = {http://arxiv.org/abs/2108.02922},
	abstract = {Machine learning datasets have elicited concerns about privacy, bias, and unethical applications, leading to the retraction of prominent datasets such as DukeMTMC, MS-Celeb-1M, and Tiny Images. In response, the machine learning community has called for higher ethical standards in dataset creation. To help inform these efforts, we studied three inﬂuential but ethically problematic face and person recognition datasets—Labeled Faces in the Wild (LFW), MS-Celeb-1M, and DukeMTMC—by analyzing nearly 1000 papers that cite them. We found that the creation of derivative datasets and models, broader technological and social change, the lack of clarity of licenses, and dataset management practices can introduce a wide range of ethical concerns. We conclude by suggesting a distributed approach to harm mitigation that considers the entire life cycle of a dataset.},
	language = {en},
	urldate = {2022-10-23},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} ({NeurIPS})},
	publisher = {arXiv},
	author = {Peng, Kenny and Mathur, Arunesh and Narayanan, Arvind},
	month = nov,
	year = {2021},
	note = {arXiv:2108.02922 [cs]},
	keywords = {Computer Science - Computers and Society, Computer Science - Machine Learning},
}

@book{barocas_fairness_2019,
	title = {Fairness and machine learning},
	url = {https://fairmlbook.org/},
	publisher = {fairmlbook.org},
	author = {Barocas, Solon and Hardt, Moritz and Narayanan, Arvind},
	year = {2019},
}

@inproceedings{lee_landscape_2021,
	title = {The landscape and gaps in open source fairness toolkits},
	booktitle = {Proceedings of the 2021 {CHI} conference on human factors in computing systems},
	author = {Lee, Michelle Seng Ah and Singh, Jat},
	year = {2021},
	pages = {1--13},
}

@article{saleiro_aequitas_2018,
	title = {Aequitas: {A} bias and fairness audit toolkit},
	shorttitle = {Aequitas},
	journal = {arXiv preprint arXiv:1811.05577},
	author = {Saleiro, Pedro and Kuester, Benedict and Hinkson, Loren and London, Jesse and Stevens, Abby and Anisfeld, Ari and Rodolfa, Kit T. and Ghani, Rayid},
	year = {2018},
}

@article{bellamy_ai_2019,
	title = {{AI} {Fairness} 360: {An} extensible toolkit for detecting and mitigating algorithmic bias},
	volume = {63},
	shorttitle = {{AI} {Fairness} 360},
	number = {4/5},
	journal = {IBM Journal of Research and Development},
	author = {Bellamy, Rachel KE and Dey, Kuntal and Hind, Michael and Hoffman, Samuel C. and Houde, Stephanie and Kannan, Kalapriya and Lohia, Pranay and Martino, Jacquelyn and Mehta, Sameep and Mojsilović, Aleksandra},
	year = {2019},
	note = {Publisher: IBM},
	pages = {4--1},
}

@article{ntoutsi_bias_2020,
	title = {Bias in data-driven artificial intelligence systems—{An} introductory survey},
	volume = {10},
	issn = {1942-4795},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.1356},
	doi = {10.1002/widm.1356},
	abstract = {Artificial Intelligence (AI)-based systems are widely employed nowadays to make decisions that have far-reaching impact on individuals and society. Their decisions might affect everyone, everywhere, and anytime, entailing concerns about potential human rights issues. Therefore, it is necessary to move beyond traditional AI algorithms optimized for predictive performance and embed ethical and legal principles in their design, training, and deployment to ensure social good while still benefiting from the huge potential of the AI technology. The goal of this survey is to provide a broad multidisciplinary overview of the area of bias in AI systems, focusing on technical challenges and solutions as well as to suggest new research directions towards approaches well-grounded in a legal frame. In this survey, we focus on data-driven AI, as a large part of AI is powered nowadays by (big) data and powerful machine learning algorithms. If otherwise not specified, we use the general term bias to describe problems related to the gathering or processing of data that might result in prejudiced decisions on the bases of demographic features such as race, sex, and so forth. This article is categorized under: Commercial, Legal, and Ethical Issues {\textgreater} Fairness in Data Mining Commercial, Legal, and Ethical Issues {\textgreater} Ethical Considerations Commercial, Legal, and Ethical Issues {\textgreater} Legal Issues},
	language = {en},
	number = {3},
	urldate = {2022-10-22},
	journal = {WIREs Data Mining and Knowledge Discovery},
	author = {Ntoutsi, Eirini and Fafalios, Pavlos and Gadiraju, Ujwal and Iosifidis, Vasileios and Nejdl, Wolfgang and Vidal, Maria-Esther and Ruggieri, Salvatore and Turini, Franco and Papadopoulos, Symeon and Krasanakis, Emmanouil and Kompatsiaris, Ioannis and Kinder-Kurlanda, Katharina and Wagner, Claudia and Karimi, Fariba and Fernandez, Miriam and Alani, Harith and Berendt, Bettina and Kruegel, Tina and Heinze, Christian and Broelemann, Klaus and Kasneci, Gjergji and Tiropanis, Thanassis and Staab, Steffen},
	year = {2020},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/widm.1356},
	keywords = {fairness, fairness-aware AI, fairness-aware machine learning, interpretability, responsible AI},
	pages = {e1356},
}

@article{mehrabi_survey_2021,
	title = {A survey on bias and fairness in machine learning},
	volume = {54},
	number = {6},
	journal = {ACM Computing Surveys (CSUR)},
	author = {Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
	year = {2021},
	note = {Publisher: ACM New York, NY, USA},
	pages = {1--35},
}

@inproceedings{suresh_framework_2021,
	address = {-- NY USA},
	title = {A {Framework} for {Understanding} {Sources} of {Harm} throughout the {Machine} {Learning} {Life} {Cycle}},
	isbn = {978-1-4503-8553-4},
	url = {https://dl.acm.org/doi/10.1145/3465416.3483305},
	doi = {10.1145/3465416.3483305},
	language = {en},
	urldate = {2022-10-22},
	booktitle = {Equity and {Access} in {Algorithms}, {Mechanisms}, and {Optimization}},
	publisher = {ACM},
	author = {Suresh, Harini and Guttag, John},
	month = oct,
	year = {2021},
	pages = {1--9},
}

@article{zendel_how_2017,
	title = {How good is my test data? {Introducing} safety analysis for computer vision},
	volume = {125},
	issn = {1573-1405},
	shorttitle = {How good is my test data?},
	url = {https://doi.org/10.1007/s11263-017-1020-z},
	doi = {10.1007/s11263-017-1020-z},
	abstract = {Good test data is crucial for driving new developments in computer vision (CV), but two questions remain unanswered: which situations should be covered by the test data, and how much testing is enough to reach a conclusion? In this paper we propose a new answer to these questions using a standard procedure devised by the safety community to validate complex systems: the hazard and operability analysis (HAZOP). It is designed to systematically identify possible causes of system failure or performance loss. We introduce a generic CV model that creates the basis for the hazard analysis and—for the first time—apply an extensive HAZOP to the CV domain. The result is a publicly available checklist with more than 900 identified individual hazards. This checklist can be utilized to evaluate existing test datasets by quantifying the covered hazards. We evaluate our approach by first analyzing and annotating the popular stereo vision test datasets Middlebury and KITTI. Second, we demonstrate a clearly negative influence of the hazards in the checklist on the performance of six popular stereo matching algorithms. The presented approach is a useful tool to evaluate and improve test datasets and creates a common basis for future dataset designs.},
	language = {en},
	number = {1},
	urldate = {2022-10-22},
	journal = {International Journal of Computer Vision},
	author = {Zendel, Oliver and Murschitz, Markus and Humenberger, Martin and Herzner, Wolfgang},
	month = dec,
	year = {2017},
	keywords = {Hazard analysis, Safety analysis, Stereo vision, Test data, Testing, Validation},
	pages = {95--109},
}

@misc{van_etten_spacenet_2019,
	title = {{SpaceNet}: {A} {Remote} {Sensing} {Dataset} and {Challenge} {Series}},
	shorttitle = {{SpaceNet}},
	url = {http://arxiv.org/abs/1807.01232},
	abstract = {Foundational mapping remains a challenge in many parts of the world, particularly in dynamic scenarios such as natural disasters when timely updates are critical. Updating maps is currently a highly manual process requiring a large number of human labelers to either create features or rigorously validate automated outputs. We propose that the frequent revisits of earth imaging satellite constellations may accelerate existing efforts to quickly update foundational maps when combined with advanced machine learning techniques. Accordingly, the SpaceNet partners (CosmiQ Works, Radiant Solutions, and NVIDIA), released a large corpus of labeled satellite imagery on Amazon Web Services (AWS) called SpaceNet. The SpaceNet partners also launched a series of public prize competitions to encourage improvement of remote sensing machine learning algorithms. The first two of these competitions focused on automated building footprint extraction, and the most recent challenge focused on road network extraction. In this paper we discuss the SpaceNet imagery, labels, evaluation metrics, prize challenge results to date, and future plans for the SpaceNet challenge series.},
	urldate = {2022-10-21},
	publisher = {arXiv},
	author = {Van Etten, Adam and Lindenbaum, Dave and Bacastow, Todd M.},
	month = jul,
	year = {2019},
	note = {arXiv:1807.01232 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{andrea_brennen_ai_2022,
	title = {{AI} {Assurance}: {What} happened when we audited a deepfake detection tool called {FakeFinder}},
	shorttitle = {{AI} {Assurance}},
	url = {https://www.iqt.org/ai-assurance-what-happened-when-we-audited-a-deepfake-detection-tool-called-fakefinder/},
	abstract = {IQT Labs recently audited an open-source deep learning tool called FakeFinder that predicts whether or not a video is a […]},
	language = {en-US},
	urldate = {2022-10-21},
	journal = {In-Q-Tel},
	author = {{Andrea Brennen} and {Ryan Ashley}},
	month = jan,
	year = {2022},
}

@article{dolhansky_deepfake_2019,
	title = {The {DeepFake} {Detection} {Challenge} ({DFDC}) {Dataset}},
	journal = {arXiv e-prints},
	author = {Dolhansky, B. and Bitton, J. and Pflaum, B. and Lu, J. and Howes, R. and Wang, M. and Ferrer, C. Canton},
	year = {2019},
}

@misc{leibowicz_deepfake_2021,
	title = {The {Deepfake} {Detection} {Dilemma}: {A} {Multistakeholder} {Exploration} of {Adversarial} {Dynamics} in {Synthetic} {Media}},
	shorttitle = {The {Deepfake} {Detection} {Dilemma}},
	url = {http://arxiv.org/abs/2102.06109},
	abstract = {Synthetic media detection technologies label media as either synthetic or non-synthetic and are increasingly used by journalists, web platforms, and the general public to identify misinformation and other forms of problematic content. As both well-resourced organizations and the non-technical general public generate more sophisticated synthetic media, the capacity for purveyors of problematic content to adapt induces a {\textbackslash}newterm\{detection dilemma\}: as detection practices become more accessible, they become more easily circumvented. This paper describes how a multistakeholder cohort from academia, technology platforms, media entities, and civil society organizations active in synthetic media detection and its socio-technical implications evaluates the detection dilemma. Specifically, we offer an assessment of detection contexts and adversary capacities sourced from the broader, global AI and media integrity community concerned with mitigating the spread of harmful synthetic media. A collection of personas illustrates the intersection between unsophisticated and highly-resourced sponsors of misinformation in the context of their technical capacities. This work concludes that there is no "best" approach to navigating the detector dilemma, but derives a set of implications from multistakeholder input to better inform detection process decisions and policies, in practice.},
	urldate = {2022-10-21},
	publisher = {arXiv},
	author = {Leibowicz, Claire and McGregor, Sean and Ovadya, Aviv},
	month = feb,
	year = {2021},
	note = {arXiv:2102.06109 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computers and Society, Computer Science - Machine Learning},
}

@misc{robert_stojnic_papers_2022,
	title = {Papers with {Code} - {ImageNet} {Benchmark} ({Image} {Classification})},
	url = {https://paperswithcode.com/sota/image-classification-on-imagenet},
	abstract = {The current state-of-the-art on ImageNet is CoCa (finetuned). See a full comparison of 756 papers with code.},
	language = {en},
	urldate = {2022-10-21},
	journal = {Papers with Code},
	author = {{Robert Stojnic} and {Taylor, Ross} and {Kardas, Marcin} and {Scialom, Scialom}},
	month = oct,
	year = {2022},
}

@misc{munroe_tasks_2014,
	title = {Tasks},
	url = {https://xkcd.com/1425/},
	urldate = {2022-10-21},
	journal = {xkcd},
	author = {Munroe, Randall},
	month = sep,
	year = {2014},
}

@misc{simonite_why_2015,
	title = {Why and {How} {Baidu} {Cheated} an {Artificial} {Intelligence} {Test}},
	url = {https://www.technologyreview.com/2015/06/04/72951/why-and-how-baidu-cheated-an-artificial-intelligence-test/},
	abstract = {Machine learning gets its first cheating scandal.},
	language = {en},
	urldate = {2022-10-21},
	journal = {MIT Technology Review},
	author = {Simonite, Tom},
	month = jun,
	year = {2015},
}

@inproceedings{krizhevsky_imagenet_2012,
	address = {Red Hook, NY, USA},
	series = {{NIPS}'12},
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overriding in the fully-connected layers we employed a recently-developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
	booktitle = {Proceedings of the 25th {International} {Conference} on {Neural} {Information} {Processing} {Systems} - {Volume} 1},
	publisher = {Curran Associates Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	year = {2012},
	note = {event-place: Lake Tahoe, Nevada},
	pages = {1097--1105},
}

@misc{noauthor_experimental_security_research_of_tesla_autopilotpdf_nodate,
	title = {Experimental\_Security\_Research\_of\_Tesla\_Autopilot.pdf},
	url = {https://keenlab.tencent.com/en/whitepapers/Experimental_Security_Research_of_Tesla_Autopilot.pdf},
	urldate = {2022-10-21},
}

@misc{floridi_capai_2022,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {{capAI} - {A} procedure for conducting conformity assessment of {AI} systems in line with the {EU} {Artificial} {Intelligence} {Act}},
	url = {https://papers.ssrn.com/abstract=4064091},
	doi = {10.2139/ssrn.4064091},
	abstract = {We have developed capAI, a conformity assessment procedure for AI systems, to provide an independent, comparable, quantifiable, and accountable assessment of AI systems that conforms with the proposed AIA regulation. By building on the AIA, capAI provides organisations with practical guidance on how high-level ethics principles can be translated into verifiable criteria that help shape the design, development, deployment and use of ethical AI. The main purpose of capAI is to serve as a governance tool that ensures and demonstrates that the development and operation of an AI system are trustworthy – i.e., legally compliant, ethically sound, and technically robust – and thus conform to the AIA.},
	language = {en},
	urldate = {2022-10-21},
	author = {Floridi, Luciano and Holweg, Matthias and Taddeo, Mariarosaria and Amaya Silva, Javier and Mökander, Jakob and Wen, Yuni},
	month = mar,
	year = {2022},
	keywords = {AI, AI Act, AI Auditing, AI legislation, Conformity Assessment, EU, Trustworthy AI},
}

@article{varoquaux_machine_2022,
	title = {Machine learning for medical imaging: {Methodological} failures and recommendations for the future},
	volume = {5},
	copyright = {2022 The Author(s)},
	issn = {2398-6352},
	shorttitle = {Machine learning for medical imaging},
	url = {https://www.nature.com/articles/s41746-022-00592-y},
	doi = {10.1038/s41746-022-00592-y},
	abstract = {Research in computer analysis of medical images bears many promises to improve patients’ health. However, a number of systematic challenges are slowing down the progress of the field, from limitations of the data, such as biases, to research incentives, such as optimizing for publication. In this paper we review roadblocks to developing and assessing methods. Building our analysis on evidence from the literature and data challenges, we show that at every step, potential biases can creep in. On a positive note, we also discuss on-going efforts to counteract these problems. Finally we provide recommendations on how to further address these problems in the future.},
	language = {en},
	number = {1},
	urldate = {2022-10-21},
	journal = {npj Digital Medicine},
	author = {Varoquaux, Gaël and Cheplygina, Veronika},
	month = apr,
	year = {2022},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Computer science, Medical research, Research data, ai/medicine, ai/testing},
	pages = {1--8},
}

@inproceedings{wang_towards_2020,
	address = {Seattle, WA, USA},
	title = {Towards {Fairness} in {Visual} {Recognition}: {Effective} {Strategies} for {Bias} {Mitigation}},
	isbn = {978-1-72817-168-5},
	shorttitle = {Towards {Fairness} in {Visual} {Recognition}},
	url = {https://ieeexplore.ieee.org/document/9156668/},
	doi = {10.1109/CVPR42600.2020.00894},
	abstract = {Computer vision models learn to perform a task by capturing relevant statistics from training data. It has been shown that models learn spurious age, gender, and race correlations when trained for seemingly unrelated tasks like activity recognition or image captioning. Various mitigation techniques have been presented to prevent models from utilizing or learning such biases. However, there has been little systematic comparison between these techniques. We design a simple but surprisingly effective visual recognition benchmark for studying bias mitigation. Using this benchmark, we provide a thorough analysis of a wide range of techniques. We highlight the shortcomings of popular adversarial training approaches for bias mitigation, propose a simple but similarly effective alternative to the inference-time Reducing Bias Ampliﬁcation method of Zhao et al., and design a domain-independent training technique that outperforms all other methods. Finally, we validate our ﬁndings on the attribute classiﬁcation task in the CelebA dataset, where attribute presence is known to be correlated with the gender of people in the image, and demonstrate that the proposed technique is effective at mitigating real-world gender bias.},
	language = {en},
	urldate = {2022-10-20},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Wang, Zeyu and Qinami, Klint and Karakozis, Ioannis Christos and Genova, Kyle and Nair, Prem and Hata, Kenji and Russakovsky, Olga},
	month = jun,
	year = {2020},
	keywords = {ai/fairness, ai/vision},
	pages = {8916--8925},
}

@inproceedings{hendrycks_deep_2019,
	title = {Deep {Anomaly} {Detection} with {Outlier} {Exposure}},
	abstract = {It is important to detect anomalous inputs when deploying machine learning systems. The use of larger and more complex inputs in deep learning magniﬁes the difﬁculty of distinguishing between anomalous and in-distribution examples. At the same time, diverse image and text data are available in enormous quantities. We propose leveraging these data to improve deep anomaly detection by training anomaly detectors against an auxiliary dataset of outliers, an approach we call Outlier Exposure (OE). This enables anomaly detectors to generalize and detect unseen anomalies. In extensive experiments on natural language processing and small- and large-scale vision tasks, we ﬁnd that Outlier Exposure signiﬁcantly improves detection performance. We also observe that cutting-edge generative models trained on CIFAR-10 may assign higher likelihoods to SVHN images than to CIFAR-10 images; we use OE to mitigate this issue. We also analyze the ﬂexibility and robustness of Outlier Exposure, and identify characteristics of the auxiliary dataset that improve performance.},
	language = {en},
	booktitle = {International {Conference} on {Learning} {Representations} ({ICLR})},
	author = {Hendrycks, Dan and Mazeika, Mantas and Dietterich, Thomas},
	year = {2019},
	keywords = {ai/open-world, ai/trust},
	pages = {18},
}

@inproceedings{ribeiro_beyond_2020,
	address = {Online},
	title = {Beyond {Accuracy}: {Behavioral} {Testing} of {NLP} {Models} with {CheckList}},
	shorttitle = {Beyond {Accuracy}},
	url = {https://www.aclweb.org/anthology/2020.acl-main.442},
	doi = {10.18653/v1/2020.acl-main.442},
	abstract = {Although measuring held-out accuracy has been the primary approach to evaluate generalization, it often overestimates the performance of NLP models, while alternative approaches for evaluating models either focus on individual tasks or on speciﬁc behaviors. Inspired by principles of behavioral testing in software engineering, we introduce CheckList, a taskagnostic methodology for testing NLP models. CheckList includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation, as well as a software tool to generate a large and diverse number of test cases quickly. We illustrate the utility of CheckList with tests for three tasks, identifying critical failures in both commercial and state-of-art models. In a user study, a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model. In another user study, NLP practitioners with CheckList created twice as many tests, and found almost three times as many bugs as users without it.},
	language = {en},
	urldate = {2022-10-13},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Ribeiro, Marco Tulio and Wu, Tongshuang and Guestrin, Carlos and Singh, Sameer},
	year = {2020},
	keywords = {ai/nlp, ai/testing},
	pages = {4902--4912},
}

@misc{denton_bringing_2020,
	title = {Bringing the people back in: {Contesting} benchmark machine learning datasets},
	shorttitle = {Bringing the people back in},
	url = {http://arxiv.org/abs/2007.07399},
	abstract = {In response to algorithmic unfairness embedded in sociotechnical systems, significant attention has been focused on the contents of machine learning datasets which have revealed biases towards white, cisgender, male, and Western data subjects. In contrast, comparatively less attention has been paid to the histories, values, and norms embedded in such datasets. In this work, we outline a research program - a genealogy of machine learning data - for investigating how and why these datasets have been created, what and whose values influence the choices of data to collect, the contextual and contingent conditions of their creation. We describe the ways in which benchmark datasets in machine learning operate as infrastructure and pose four research questions for these datasets. This interrogation forces us to "bring the people back in" by aiding us in understanding the labor embedded in dataset construction, and thereby presenting new avenues of contestation for other researchers encountering the data.},
	urldate = {2022-10-07},
	publisher = {arXiv},
	author = {Denton, Emily and Hanna, Alex and Amironesei, Razvan and Smart, Andrew and Nicole, Hilary and Scheuerman, Morgan Klaus},
	month = jul,
	year = {2020},
	note = {arXiv:2007.07399 [cs]},
}

@misc{beyer_are_2020,
	title = {Are we done with {ImageNet}?},
	url = {http://arxiv.org/abs/2006.07159},
	abstract = {Yes, and no. We ask whether recent progress on the ImageNet classiﬁcation benchmark continues to represent meaningful generalization, or whether the community has started to overﬁt to the idiosyncrasies of its labeling procedure. We therefore develop a signiﬁcantly more robust procedure for collecting human annotations of the ImageNet validation set. Using these new labels, we reassess the accuracy of recently proposed ImageNet classiﬁers, and ﬁnd their gains to be substantially smaller than those reported on the original labels. Furthermore, we ﬁnd the original ImageNet labels to no longer be the best predictors of this independently-collected set, indicating that their usefulness in evaluating vision models may be nearing an end. Nevertheless, we ﬁnd our annotation procedure to have largely remedied the errors in the original labels, reinforcing ImageNet as a powerful benchmark for future research in visual recognition3.},
	language = {en},
	urldate = {2022-10-10},
	publisher = {arXiv},
	author = {Beyer, Lucas and Hénaff, Olivier J. and Kolesnikov, Alexander and Zhai, Xiaohua and Oord, Aäron van den},
	month = jun,
	year = {2020},
	note = {arXiv:2006.07159 [cs]},
}

@inproceedings{cubuk_randaugment_2020,
	title = {Randaugment: {Practical} automated data augmentation with a reduced search space},
	shorttitle = {Randaugment},
	booktitle = {Proceedings of the {IEEE}/{CVF} conference on computer vision and pattern recognition workshops},
	author = {Cubuk, Ekin D. and Zoph, Barret and Shlens, Jonathon and Le, Quoc V.},
	year = {2020},
	keywords = {ai/data-augmentation},
	pages = {702--703},
}

@inproceedings{koh_wilds_2021,
	title = {Wilds: {A} benchmark of in-the-wild distribution shifts},
	shorttitle = {Wilds},
	booktitle = {International {Conference} on {Machine} {Learning} ({ICML})},
	publisher = {PMLR},
	author = {Koh, Pang Wei and Sagawa, Shiori and Marklund, Henrik and Xie, Sang Michael and Zhang, Marvin and Balsubramani, Akshay and Hu, Weihua and Yasunaga, Michihiro and Phillips, Richard Lanas and Gao, Irena and Lee, Tony and David, Etienne and Stavness, Ian and Guo, Wei and Earnshaw, Berton A. and Haque, Imran S. and Beery, Sara and Leskovec, Jure and Kundaje, Anshul and Pierson, Emma and Levine, Sergey and Finn, Chelsea and Liang, Percy},
	year = {2021},
	keywords = {ai/datasets, ai/domain-shift},
	pages = {5637--5664},
}

@inproceedings{mcgregor_preventing_2021,
	title = {Preventing repeated real world {AI} failures by cataloging incidents: {The} {AI} {Incident} {Database}},
	volume = {35},
	shorttitle = {Preventing repeated real world {AI} failures by cataloging incidents},
	booktitle = {{AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {McGregor, Sean},
	year = {2021},
	note = {Issue: 17},
	pages = {15458--15463},
}

@techreport{david_defense_2016,
	title = {Defense {Science} {Board} summer study on autonomy},
	url = {https://apps.dtic.mil/sti/citations/AD1017790},
	abstract = {At the request of the Under Secretary of Defense for Acquisition, Technology, and Logistics USDAT and L, the Defense Science Board DSB conducted a study on the applicability of autonomy to Department of Defense DoD missions. The study concluded that there are both substantial operational benefits and potential perils associated with the use of autonomy. Autonomy delivers significant military value, including opportunities to reduce the number of warfighters in harms way, increase the quality and speed of decisions in time-critical operations, and enable new missions that would otherwise be impossible. Autonomy is by no means new to the DoD. Fielded capabilities demonstrate ongoing progress in embedding autonomous functionality into systems, and many development programs already underway include an increasingly sophisticated use of autonomy. Autonomy also delivers significant value across a diverse array of global markets. Both enabling technologies and commercial applications are advancing rapidly in response to market opportunities. Autonomy is becoming a ubiquitous enabling capability for products spanning a spectrum from expert advisory systems to autonomous vehicles. Commercial market forces are accelerating progress, providing opportunities for DoD to leverage the investments of others, while also providing substantial capabilities to potential adversaries. This study concluded that DoD must accelerate its exploitation of autonomy both to realize the potential military value and to remain ahead of adversaries who also will exploit its operational benefits.},
	language = {en},
	number = {AD1017790},
	urldate = {2022-10-06},
	author = {David, Ruth A. and Nielsen, Paul},
	month = jun,
	year = {2016},
	note = {Section: Technical Reports},
	keywords = {ai/audit, ai/trust, government},
}

@article{noauthor_executive_2019,
	title = {Executive {Order} 13859. {Maintaining} {American} leadership in artificial intelligence.},
	volume = {84},
	url = {https://www.federalregister.gov/documents/2019/02/14/2019-02544/maintaining-american-leadership-in-artificial-intelligence},
	number = {31},
	journal = {Federal Register},
	month = feb,
	year = {2019},
	keywords = {ai/trust, government},
	pages = {3967--3972},
}

@inproceedings{zhou_non-vacuous_2019,
	title = {Non-vacuous generalization bounds at the {ImageNet} scale: {A} {PAC}-{Bayesian} compression approach},
	shorttitle = {Non-vacuous generalization bounds at the imagenet scale},
	url = {http://arxiv.org/abs/1804.05862},
	abstract = {Modern neural networks are highly overparameterized, with capacity to substantially overﬁt to training data. Nevertheless, these networks often generalize well in practice. It has also been observed that trained networks can often be “compressed” to much smaller representations. The purpose of this paper is to connect these two empirical observations. Our main technical result is a generalization bound for compressed networks based on the compressed size that, combined with off-theshelf compression algorithms, leads to state-of-the-art generalization guarantees. In particular, we provide the ﬁrst non-vacuous generalization guarantees for realistic architectures applied to the ImageNet classiﬁcation problem. Additionally, we show that compressibility of models that tend to overﬁt is limited. Empirical results show that an increase in overﬁtting increases the number of bits required to describe a trained network.},
	language = {en},
	urldate = {2022-10-10},
	booktitle = {International {Conference} on {Learning} {Representations} ({ICLR})},
	publisher = {arXiv},
	author = {Zhou, Wenda and Veitch, Victor and Austern, Morgane and Adams, Ryan P. and Orbanz, Peter},
	month = feb,
	year = {2019},
	note = {arXiv:1804.05862 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{zhao_bias_2018,
	title = {Bias and generalization in deep generative models: {An} empirical study},
	abstract = {In high dimensional settings, density estimation algorithms rely crucially on their inductive bias. Despite recent empirical success, the inductive bias of deep generative models is not well understood. In this paper we propose a framework to systematically investigate bias and generalization in deep generative models of images. Inspired by experimental methods from cognitive psychology, we probe each learning algorithm with carefully designed training datasets to characterize when and how existing models generate novel attributes and their combinations. We identify similarities to human psychology and verify that these patterns are consistent across commonly used models and architectures.},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} ({NeurIPS})},
	author = {Zhao, Shengjia and Ren, Hongyu and Yuan, Arianna and Song, Jiaming and Goodman, Noah and Ermon, Stefano},
	year = {2018},
	pages = {10},
}

@inproceedings{tsipras_imagenet_2020,
	title = {From {ImageNet} to image classiﬁcation: {Contextualizing} progress on benchmarks},
	abstract = {Building rich machine learning datasets in a scal-
able manner often necessitates a crowd-sourced
data collection pipeline. In this work, we use hu-
man studies to investigate the consequences of em-
ploying such a pipeline, focusing on the popular
ImageNet dataset. We study how specific design
choices in the ImageNet creation process impact
the fidelity of the resulting dataset—including the
introduction of biases that state-of-the-art models
exploit. Our analysis pinpoints how a noisy data
collection pipeline can lead to a systematic mis-
alignment between the resulting benchmark and
the real-world task it serves as a proxy for. Finally,
our findings emphasize the need to augment our
current model training and evaluation toolkit to
take such misalignments into account.},
	language = {en},
	booktitle = {International {Conference} on {Machine} {Learning} ({ICML})},
	author = {Tsipras, Dimitris and Santurkar, Shibani and Engstrom, Logan and Ilyas, Andrew and Ma, Aleksander},
	year = {2020},
	pages = {11},
}

@inproceedings{toneva_empirical_2019,
	title = {An empirical study of example forgetting during deep neural network learning},
	url = {http://arxiv.org/abs/1812.05159},
	abstract = {Inspired by the phenomenon of catastrophic forgetting, we investigate the learning dynamics of neural networks as they train on single classiﬁcation tasks. Our goal is to understand whether a related phenomenon occurs when data does not undergo a clear distributional shift. We deﬁne a “forgetting event” to have occurred when an individual training example transitions from being classiﬁed correctly to incorrectly over the course of learning. Across several benchmark data sets, we ﬁnd that: (i) certain examples are forgotten with high frequency, and some not at all; (ii) a data set’s (un)forgettable examples generalize across neural architectures; and (iii) based on forgetting dynamics, a signiﬁcant fraction of examples can be omitted from the training data set while still maintaining state-of-the-art generalization performance.},
	language = {en},
	urldate = {2022-10-10},
	booktitle = {International {Conference} on {Learning} {Representations} ({ICLR})},
	publisher = {arXiv},
	author = {Toneva, Mariya and Sordoni, Alessandro and Combes, Remi Tachet des and Trischler, Adam and Bengio, Yoshua and Gordon, Geoffrey J.},
	year = {2019},
	note = {arXiv:1812.05159 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{shankar_evaluating_2020,
	title = {Evaluating machine accuracy on {ImageNet}},
	abstract = {We evaluate a wide range of ImageNet models with ﬁve trained human labelers. In our year-long experiment, trained humans ﬁrst annotated 40,000 images from the ImageNet and ImageNetV2 test sets with multi-class labels to enable a semantically coherent evaluation. Then we measured the classiﬁcation accuracy of the ﬁve trained humans on the full task with 1,000 classes. Only the latest models from 2020 are on par with our best human labeler, and human accuracy on the 590 object classes is still 4\% and 11\% higher than the best model on ImageNet and ImageNetV2, respectively. Moreover, humans achieve the same accuracy on ImageNet and ImageNetV2, while all models see a consistent accuracy drop. Overall, our results show that there is still substantial room for improvement on ImageNet and direct accuracy comparisons between humans and machines may overstate machine performance.},
	language = {en},
	booktitle = {International {Conference} on {Machine} {Learning} ({ICML})},
	author = {Shankar, Vaishaal and Roelofs, Rebecca and Mania, Horia and Fang, Alex and Recht, Benjamin and Schmidt, Ludwig},
	year = {2020},
	pages = {11},
}

@techreport{sayler_artificial_nodate,
	title = {Artificial intelligence and national security},
	url = {https://crsreports.congress.gov/product/pdf/R/R45178/5},
	abstract = {Artificial intelligence (AI) is a rapidly growing field of technology with potentially significant implications for national security. As such, the U.S. Department of Defense (DOD) and other nations are developing AI applications for a range of military functions. AI research is underway in the fields of intelligence collection and analysis, logistics, cyber operations, information operations, command and control, and in a variety of semiautonomous and autonomous vehicles. Already, AI has been incorporated into military operations in Iraq and Syria. Congressional action has the potential to shape the technology’s development further, with budgetary and legislative decisions influencing the growth of military applications as well as the pace of their adoption.},
	language = {en},
	number = {R45178},
	author = {Sayler, Kelley M},
	keywords = {ai/explainability, ai/trust, government},
	pages = {41},
}

@inproceedings{minderer_revisiting_2021,
	title = {Revisiting the calibration of modern neural networks},
	url = {https://proceedings.neurips.cc/paper/2021/file/8420d359404024567b5aefda1231af24-Paper.pdf},
	abstract = {Accurate estimation of predictive uncertainty (model calibration) is essential for the safe application of neural networks. Many instances of miscalibration in modern neural networks have been reported, suggesting a trend that newer, more accurate models produce poorly calibrated predictions. Here, we revisit this question for recent state-of-the-art image classiﬁcation models. We systematically relate model calibration and accuracy, and ﬁnd that the most recent models, notably those not using convolutions, are among the best calibrated. Trends observed in prior model generations, such as decay of calibration with distribution shift or model size, are less pronounced in recent architectures. We also show that model size and amount of pretraining do not fully explain these differences, suggesting that architecture is a major determinant of calibration properties.},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} ({NeurIPS})},
	author = {Minderer, Matthias and Djolonga, Josip and Romijnders, Rob and Hubis, Frances and Zhai, Xiaohua and Houlsby, Neil and Tran, Dustin and Lucic, Mario},
	year = {2021},
	pages = {13},
}

@inproceedings{kok_classification_2007,
	address = {Berlin, Heidelberg},
	title = {Classification of anti-learnable biological and synthetic data},
	volume = {4702},
	isbn = {978-3-540-74975-2 978-3-540-74976-9},
	url = {http://link.springer.com/10.1007/978-3-540-74976-9_19},
	abstract = {We demonstrate a binary classiﬁcation problem in which standard supervised learning algorithms such as linear and kernel SVM, naive Bayes, ridge regression, k-nearest neighbors, shrunken centroid, multilayer perceptron and decision trees perform in an unusual way. On certain data sets they classify a randomly sampled training subset nearly perfectly, but systematically perform worse than random guessing on cases unseen in training. We demonstrate this phenomenon in classiﬁcation of a natural data set of cancer genomics microarrays using crossvalidation test. Additionally, we generate a range of synthetic datasets, the outcomes of 0-sum games, for which we analyse this phenomenon in the i.i.d. setting.},
	language = {en},
	urldate = {2022-10-10},
	booktitle = {Knowledge {Discovery} in {Databases} ({KDD})},
	publisher = {Springer Berlin Heidelberg},
	author = {Kowalczyk, Adam},
	editor = {Kok, Joost N. and Koronacki, Jacek and Lopez de Mantaras, Ramon and Matwin, Stan and Mladenič, Dunja and Skowron, Andrzej},
	year = {2007},
	doi = {10.1007/978-3-540-74976-9_19},
	note = {ISSN: 0302-9743, 1611-3349
Series Title: Lecture Notes in Computer Science},
	pages = {176--187},
}

@inproceedings{hardt_ladder_2015,
	title = {The {Ladder}: a reliable leaderboard for machine learning competitions},
	abstract = {The organizer of a machine learning competi-
tion faces the problem of maintaining an accurate
leaderboard that faithfully represents the quality
of the best submission of each competing team.
What makes this estimation problem particularly
challenging is its sequential and adaptive nature.
As participants are allowed to repeatedly evaluate
their submissions on the leaderboard, they may
begin to overfit to the holdout data that supports
the leaderboard. Few theoretical results give ac-
tionable advice on how to design a reliable leader-
board. Existing approaches therefore often resort
to poorly understood heuristics such as limiting
the bit precision of answers and the rate of re-
submission.
In this work, we introduce a notion of leader-
board accuracy tailored to the format of a com-
petition. We introduce a natural algorithm called
the Ladder and demonstrate that it simultaneously
supports strong theoretical guarantees in a fully
adaptive model of estimation, withstands practical
adversarial attacks, and achieves high utility on
real submission files from an actual competition
hosted by Kaggle.
Notably, we are able to sidestep a powerful recent
hardness result for adaptive risk estimation that
rules out algorithms such as ours under a seem-
ingly very similar notion of accuracy. On a practi-
cal note, we provide a completely parameter-free
variant of our algorithm that can be deployed in a
real competition with no tuning required whatso-
ever.},
	language = {en},
	booktitle = {International {Conference} on {Machine} {Learning} ({ICML})},
	author = {Hardt, Moritz and Blum, Avrim},
	year = {2015},
	pages = {9},
}

@inproceedings{guo_calibration_2017,
	title = {On calibration of modern neural networks},
	abstract = {Conﬁdence calibration – the problem of predicting probability estimates representative of the true correctness likelihood – is important for classiﬁcation models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors inﬂuencing calibration. We evaluate the performance of various post-processing calibration methods on state-ofthe-art architectures with image and document classiﬁcation datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling – a singleparameter variant of Platt Scaling – is surprisingly effective at calibrating predictions.},
	language = {en},
	booktitle = {International {Conference} on {Machine} {Learning} ({ICML})},
	author = {Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q},
	year = {2017},
	pages = {10},
}

@inproceedings{engstrom_identifying_2020,
	title = {Identifying statistical bias in dataset replication},
	abstract = {Dataset replication is a useful tool for assessing whether improvements in test accuracy on a speciﬁc benchmark correspond to improvements in models’ ability to generalize reliably. In this work, we present unintuitive yet signiﬁcant ways in which standard approaches to dataset replication introduce statistical bias, skewing the resulting observations. We study ImageNet-v2, a replication of the ImageNet dataset on which models exhibit a signiﬁcant (11-14\%) drop in accuracy, even after controlling for selection frequency, a human-in-the-loop measure of data quality. We show that after remeasuring selection frequencies and correcting for statistical bias, only an estimated 3.6\%±1.5\% of the original 11.7\%±1.0\% accuracy drop remains unaccounted for. We conclude with concrete recommendations for recognizing and avoiding bias in dataset replication. Code for our study is publicly available1.},
	language = {en},
	booktitle = {International {Conference} on {Machine} {Learning} ({ICML})},
	author = {Engstrom, Logan and Ilyas, Andrew and Santurkar, Shibani and Tsipras, Dimitris and Steinhardt, Jacob and Madry, Aleksander},
	year = {2020},
	pages = {11},
}

@inproceedings{dwork_generalization_2015,
	title = {Generalization in adaptive data analysis and holdout reuse},
	abstract = {Overﬁtting is the bane of data analysts, even when data are plentiful. Formal approaches to understanding this problem focus on statistical inference and generalization of individual analysis procedures. Yet the practice of data analysis is an inherently interactive and adaptive process: new analyses and hypotheses are proposed after seeing the results of previous ones, parameters are tuned on the basis of obtained results, and datasets are shared and reused. An investigation of this gap has recently been initiated by the authors in [7], where we focused on the problem of estimating expectations of adaptively chosen functions.},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} ({NeurIPS})},
	author = {Dwork, Cynthia and Feldman, Vitaly and Hardt, Moritz and Pitassi, Toni and Reingold, Omer and Roth, Aaron},
	year = {2015},
	pages = {9},
}

@inproceedings{bendale_towards_2016,
	address = {Las Vegas, NV, USA},
	title = {Towards open set deep networks},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780542/},
	doi = {10.1109/CVPR.2016.173},
	abstract = {Deep networks have produced signiﬁcant gains for various visual recognition problems, leading to high impact academic and commercial applications. Recent work in deep networks highlighted that it is easy to generate images that humans would never classify as a particular object class, yet networks classify such images high conﬁdence as that given class – deep network are easily fooled with images humans do not consider meaningful. The closed set nature of deep networks forces them to choose from one of the known classes leading to such artifacts. Recognition in the real world is open set, i.e. the recognition system should reject unknown/unseen classes at test time. We present a methodology to adapt deep networks for open set recognition, by introducing a new model layer, OpenMax, which estimates the probability of an input being from an unknown class. A key element of estimating the unknown probability is adapting Meta-Recognition concepts to the activation patterns in the penultimate layer of the network. OpenMax allows rejection of “fooling” and unrelated open set images presented to the system; OpenMax greatly reduces the number of obvious errors made by a deep network. We prove that the OpenMax concept provides bounded open space risk, thereby formally providing an open set recognition solution. We evaluate the resulting open set deep networks using pre-trained networks from the Caffe Model-zoo on ImageNet 2012 validation data, and thousands of fooling and open set images. The proposed OpenMax model significantly outperforms open set recognition accuracy of basic deep networks as well as deep networks with thresholding of SoftMax probabilities.},
	language = {en},
	urldate = {2022-10-11},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Bendale, Abhijit and Boult, Terrance E.},
	month = jun,
	year = {2016},
	pages = {1563--1572},
}

@inproceedings{bendale_towards_2015,
	address = {Boston, MA, USA},
	title = {Towards open world recognition},
	isbn = {978-1-4673-6964-0},
	url = {http://ieeexplore.ieee.org/document/7298799/},
	doi = {10.1109/CVPR.2015.7298799},
	abstract = {With the of advent rich classiﬁcation models and high computational power visual recognition systems have found many operational applications. Recognition in the real world poses multiple challenges that are not apparent in controlled lab environments. The datasets are dynamic and novel categories must be continuously detected and then added. At prediction time, a trained system has to deal with myriad unseen categories. Operational systems require minimal downtime, even to learn. To handle these operational issues, we present the problem of Open World Recognition and formally deﬁne it. We prove that thresholding sums of monotonically decreasing functions of distances in linearly transformed feature space can balance “open space risk” and empirical risk. Our theory extends existing algorithms for open world recognition. We present a protocol for evaluation of open world recognition systems. We present the Nearest Non-Outlier (NNO) algorithm that evolves model efﬁciently, adding object categories incrementally while detecting outliers and managing open space risk. We perform experiments on the ImageNet dataset with 1.2M+ images to validate the effectiveness of our method on large scale visual recognition tasks. NNO consistently yields superior results on open world recognition.},
	language = {en},
	urldate = {2022-10-11},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Bendale, Abhijit and Boult, Terrance},
	month = jun,
	year = {2015},
	pages = {1893--1902},
}

@inproceedings{arpit_closer_2017,
	title = {A closer look at memorization in deep networks},
	abstract = {We examine the role of memorization in deep learning, drawing connections to capacity, generalization, and adversarial robustness. While deep networks are capable of memorizing noise data, our results suggest that they tend to prioritize learning simple patterns ﬁrst. In our experiments, we expose qualitative differences in gradient-based optimization of deep neural networks (DNNs) on noise vs. real data. We also demonstrate that for appropriately tuned explicit regularization (e.g., dropout) we can degrade DNN training performance on noise datasets without compromising generalization on real data. Our analysis suggests that the notions of effective capacity which are dataset independent are unlikely to explain the generalization performance of deep networks when trained with gradient based methods because training data itself plays an important role in determining the degree of memorization.},
	language = {en},
	booktitle = {International {Conference} on {Machine} {Learning} ({ICML})},
	author = {Arpit, Devansh and Jastrzebski, Stanisław and Ballas, Nicolas and Krueger, David and Bengio, Emmanuel and Kanwal, Maxinder S and Maharaj, Tegan and Fischer, Asja and Courville, Aaron and Bengio, Yoshua and Lacoste-Julien, Simon},
	year = {2017},
	pages = {10},
}

@inproceedings{morcos_insights_2018,
	title = {Insights on representational similarity in neural networks with canonical correlation},
	abstract = {Comparing different neural network representations and determining how representations evolve over time remain challenging open questions in our understanding of the function of neural networks. Comparing representations in neural networks is fundamentally difﬁcult as the structure of representations varies greatly, even across groups of networks trained on identical tasks, and over the course of training. Here, we develop projection weighted CCA (Canonical Correlation Analysis) as a tool for understanding neural networks, building off of SVCCA, a recently proposed method [22]. We ﬁrst improve the core method, showing how to differentiate between signal and noise, and then apply this technique to compare across a group of CNNs, demonstrating that networks which generalize converge to more similar representations than networks which memorize, that wider networks converge to more similar solutions than narrow networks, and that trained networks with identical topology but different learning rates converge to distinct clusters with diverse representations. We also investigate the representational dynamics of RNNs, across both training and sequential timesteps, ﬁnding that RNNs converge in a bottom-up pattern over the course of training and that the hidden state is highly variable over the course of a sequence, even when accounting for linear transforms. Together, these results provide new insights into the function of CNNs and RNNs, and demonstrate the utility of using CCA to understand representations.},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} ({NeurIPS})},
	author = {Morcos, Ari and Raghu, Maithra and Bengio, Samy},
	year = {2018},
	pages = {10},
}

@inproceedings{ovadia_can_2019,
	title = {Can you trust your model's uncertainty?  {Evaluating} predictive uncertainty under dataset shift},
	abstract = {Modern machine learning methods including deep learning have achieved great success in predictive accuracy for supervised learning tasks, but may still fall short in giving useful estimates of their predictive uncertainty. Quantifying uncertainty is especially critical in real-world settings, which often involve input distributions that are shifted from the training distribution due to a variety of factors including sample bias and non-stationarity. In such settings, well calibrated uncertainty estimates convey information about when a model’s output should (or should not) be trusted. Many probabilistic deep learning methods, including Bayesian-and nonBayesian methods, have been proposed in the literature for quantifying predictive uncertainty, but to our knowledge there has not previously been a rigorous largescale empirical comparison of these methods under dataset shift. We present a largescale benchmark of existing state-of-the-art methods on classiﬁcation problems and investigate the effect of dataset shift on accuracy and calibration. We ﬁnd that traditional post-hoc calibration does indeed fall short, as do several other previous methods. However, some methods that marginalize over models give surprisingly strong results across a broad spectrum of tasks.},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} ({NeurIPS})},
	author = {Ovadia, Yaniv and Fertig, Emily and Ren, Jie and Nado, Zachary and Sculley, D and Nowozin, Sebastian and Dillon, Joshua and Lakshminarayanan, Balaji and Snoek, Jasper},
	year = {2019},
	pages = {12},
}

@inproceedings{papadopoulos_inductive_2002,
	title = {Inductive confidence machines for regression},
	booktitle = {European {Conference} on {Machine} {Learning} ({ECML})},
	publisher = {Springer},
	author = {Papadopoulos, Harris and Proedrou, Kostas and Vovk, Volodya and Gammerman, Alex},
	year = {2002},
	pages = {345--356},
}

@inproceedings{romano_conformalized_2019,
	title = {Conformalized quantile regression},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/file/5103c3584b063c431bd1268e9b5e76fb-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} ({NeurIPS})},
	author = {Romano, Yaniv and Patterson, Evan and Candes, Emmanuel},
	year = {2019},
}

@inproceedings{saunders_transduction_1999,
	title = {Transduction with confidence and credibility},
	booktitle = {International {Joint} {Conference} on {Artificial} {Intelligence} ({IJCAI})},
	author = {Saunders, Craig and Gammerman, Alexander and Vovk, Volodya},
	year = {1999},
}

@inproceedings{torralba_unbiased_2011,
	address = {Colorado Springs, CO, USA},
	title = {Unbiased look at dataset bias},
	isbn = {978-1-4577-0394-2},
	url = {http://ieeexplore.ieee.org/document/5995347/},
	doi = {10.1109/CVPR.2011.5995347},
	abstract = {Datasets are an integral part of contemporary object recognition research. They have been the chief reason for the considerable progress in the ﬁeld, not just as source of large amounts of training data, but also as means of measuring and comparing performance of competing algorithms. At the same time, datasets have often been blamed for narrowing the focus of object recognition research, reducing it to a single benchmark performance number. Indeed, some datasets, that started out as data capture efforts aimed at representing the visual world, have become closed worlds onto themselves (e.g. the Corel world, the Caltech101 world, the PASCAL VOC world). With the focus on beating the latest benchmark numbers on the latest dataset, have we perhaps lost sight of the original purpose? The goal of this paper is to take stock of the current state of recognition datasets. We present a comparison study using a set of popular datasets, evaluated based on a number of criteria including: relative data bias, cross-dataset generalization, effects of closed-world assumption, and sample value. The experimental results, some rather surprising, suggest directions that can improve dataset collection as well as algorithm evaluation protocols. But more broadly, the hope is to stimulate discussion in the community regarding this very important, but largely neglected issue.},
	language = {en},
	urldate = {2022-10-10},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Torralba, Antonio and Efros, Alexei A.},
	month = jun,
	year = {2011},
	pages = {1521--1528},
}

@inproceedings{vovk_machine-learning_1999,
	title = {Machine-learning applications of algorithmic randomness},
	booktitle = {International {Conference} on {Machine} {Learning} ({ICML})},
	author = {Vovk, Volodya and Gammerman, Alexander and Saunders, Craig},
	year = {1999},
}

@article{dietterich_familiarity_2022,
	title = {The familiarity hypothesis: {Explaining} the behavior of deep open set methods},
	volume = {132},
	shorttitle = {The familiarity hypothesis},
	journal = {Pattern Recognition},
	author = {Dietterich, Thomas G. and Guyer, Alex},
	year = {2022},
	note = {Publisher: Elsevier},
	pages = {108931},
}

@incollection{papadopoulos_inductive_2008,
	title = {Inductive conformal prediction: {Theory} and application to neural networks},
	shorttitle = {Inductive conformal prediction},
	booktitle = {Tools in artificial intelligence},
	publisher = {Citeseer},
	author = {Papadopoulos, Harris},
	year = {2008},
}

@inproceedings{thagaard_can_2020,
	title = {Can you trust predictive uncertainty under real dataset shifts in digital pathology?},
	booktitle = {International {Conference} on {Medical} {Image} {Computing} and {Computer}-{Assisted} {Intervention}},
	publisher = {Springer},
	author = {Thagaard, Jeppe and Hauberg, Søren and Vegt, Bert van der and Ebstrup, Thomas and Hansen, Johan D. and Dahl, Anders B.},
	year = {2020},
	pages = {824--833},
}

@article{zhang_understanding_2021,
	title = {Understanding deep learning (still) requires rethinking generalization},
	volume = {64},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/3446776},
	doi = {10.1145/3446776},
	abstract = {Despite their massive size, successful deep artificial ­neural networks can exhibit a remarkably small gap between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family or to the regularization techniques used during training.},
	language = {en},
	number = {3},
	urldate = {2022-10-10},
	journal = {Communications of the ACM},
	author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
	month = mar,
	year = {2021},
	pages = {107--115},
}

@misc{zhang_understanding_2017,
	title = {Understanding deep learning requires rethinking generalization},
	url = {http://arxiv.org/abs/1611.03530},
	abstract = {Despite their massive size, successful deep artiﬁcial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training.},
	language = {en},
	urldate = {2022-10-10},
	publisher = {arXiv},
	author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
	month = feb,
	year = {2017},
	note = {arXiv:1611.03530 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@techreport{mattis_summary_2018,
	title = {Summary of the 2018 {National} {Defense} {Strategy}},
	language = {en},
	author = {Mattis, Jim},
	year = {2018},
	keywords = {government, us-dod},
	pages = {14},
}

@techreport{blackburn_summary_2019,
	title = {Summary of the 2018 {Department} of {Defense} {Artificial} {Intelligence} {Strategy}},
	url = {https://media.defense.gov/2019/Feb/12/2002088963/-1/-1/1/SUMMARY-OF-DOD-AI-STRATEGY.PDF},
	language = {en},
	author = {Blackburn, R Alan},
	month = feb,
	year = {2019},
	pages = {17},
}
