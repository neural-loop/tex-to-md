% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{mum}
\BIBentryALTinterwordspacing
(2021) Mum: A new ai milestone for understanding information. [Online].
  Available: \url{https://blog.google/products/search/introducing-mum/}
\BIBentrySTDinterwordspacing

\bibitem{meb}
(2021) Make every feature binary: A 135b parameter sparse neural network for
  massively improved search relevance.

\bibitem{team2022NoLL}
N.~team, M.~R. Costa-juss{\`a}, J.~Cross, O.~cCelebi, M.~Elbayad, K.~Heafield,
  K.~Heffernan, E.~Kalbassi, J.~Lam, D.~Licht, J.~Maillard, A.~Sun, S.~Wang,
  G.~Wenzek, A.~Youngblood, B.~Akula, L.~Barrault, G.~M. Gonzalez, P.~Hansanti,
  J.~Hoffman, S.~Jarrett, K.~R. Sadagopan, D.~Rowe, S.~L. Spruit, C.~Tran,
  P.~Y. Andrews, N.~F. Ayan, S.~Bhosale, S.~Edunov, A.~Fan, C.~Gao, V.~Goswami,
  F.~Guzm'an, P.~Koehn, A.~Mourachko, C.~Ropers, S.~Saleem, H.~Schwenk, and
  J.~Wang, ``No language left behind: Scaling human-centered machine
  translation,'' \emph{ArXiv}, vol. abs/2207.04672, 2022.

\bibitem{palm}
\BIBentryALTinterwordspacing
A.~Chowdhery, S.~Narang, J.~Devlin, M.~Bosma, G.~Mishra, A.~Roberts, P.~Barham,
  H.~W. Chung, C.~Sutton, S.~Gehrmann, P.~Schuh, K.~Shi, S.~Tsvyashchenko,
  J.~Maynez, A.~Rao, P.~Barnes, Y.~Tay, N.~Shazeer, V.~Prabhakaran, E.~Reif,
  N.~Du, B.~Hutchinson, R.~Pope, J.~Bradbury, J.~Austin, M.~Isard, G.~Gur-Ari,
  P.~Yin, T.~Duke, A.~Levskaya, S.~Ghemawat, S.~Dev, H.~Michalewski, X.~Garcia,
  V.~Misra, K.~Robinson, L.~Fedus, D.~Zhou, D.~Ippolito, D.~Luan, H.~Lim,
  B.~Zoph, A.~Spiridonov, R.~Sepassi, D.~Dohan, S.~Agrawal, M.~Omernick, A.~M.
  Dai, T.~S. Pillai, M.~Pellat, A.~Lewkowycz, E.~Moreira, R.~Child, O.~Polozov,
  K.~Lee, Z.~Zhou, X.~Wang, B.~Saeta, M.~Diaz, O.~Firat, M.~Catasta, J.~Wei,
  K.~Meier-Hellstern, D.~Eck, J.~Dean, S.~Petrov, and N.~Fiedel, ``Palm:
  Scaling language modeling with pathways,'' 2022. [Online]. Available:
  \url{https://arxiv.org/abs/2204.02311}
\BIBentrySTDinterwordspacing

\bibitem{lee2022coauthor}
M.~Lee, P.~Liang, and Q.~Yang, ``Coauthor: Designing a human-ai collaborative
  writing dataset for exploring language model capabilities,'' in
  \emph{Proceedings of the 2022 CHI Conference on Human Factors in Computing
  Systems}, 2022, pp. 1--19.

\bibitem{gibson2019efficiency}
E.~Gibson, R.~Futrell, S.~P. Piantadosi, I.~Dautriche, K.~Mahowald, L.~Bergen,
  and R.~Levy, ``How efficiency shapes human language,'' \emph{Trends in
  cognitive sciences}, vol.~23, no.~5, pp. 389--407, 2019.

\bibitem{liptak2017amazon}
A.~Liptak, ``Amazon’s alexa started ordering people dollhouses after hearing
  its name on tv,'' \emph{The Verge}, vol.~7, 2017.

\bibitem{GoogleHome}
\BIBentryALTinterwordspacing
Google home. [Online]. Available: \url{https://home.google.com/}
\BIBentrySTDinterwordspacing

\bibitem{wolf2017we}
M.~J. Wolf, K.~Miller, and F.~S. Grodzinsky, ``Why we should have seen that
  coming: comments on microsoft's tay" experiment," and wider implications,''
  \emph{Acm Sigcas Computers and Society}, vol.~47, no.~3, pp. 54--64, 2017.

\bibitem{abdi2019more}
N.~Abdi, K.~M. Ramokapane, and J.~M. Such, ``More than smart speakers: Security
  and privacy perceptions of smart home personal assistants.'' in \emph{SOUPS@
  USENIX Security Symposium}, 2019.

\bibitem{rae2021scaling}
J.~W. Rae, S.~Borgeaud, T.~Cai, K.~Millican, J.~Hoffmann, F.~Song,
  J.~Aslanides, S.~Henderson, R.~Ring, S.~Young \emph{et~al.}, ``Scaling
  language models: Methods, analysis \& insights from training gopher,''
  \emph{arXiv preprint arXiv:2112.11446}, 2021.

\bibitem{jin2021good}
Z.~Jin, G.~Chauhan, B.~Tse, M.~Sachan, and R.~Mihalcea, ``How good is nlp? a
  sober look at nlp tasks through the lens of social impact,'' in
  \emph{Findings of the Association for Computational Linguistics: ACL-IJCNLP
  2021}, 2021, pp. 3099--3113.

\bibitem{schuster2020limitations}
T.~Schuster, R.~Schuster, D.~J. Shah, and R.~Barzilay, ``The limitations of
  stylometry for detecting machine-generated fake news,'' \emph{Computational
  Linguistics}, vol.~46, no.~2, pp. 499--510, 2020.

\bibitem{liang2021towards}
P.~P. Liang, C.~Wu, L.-P. Morency, and R.~Salakhutdinov, ``Towards
  understanding and mitigating social biases in language models,'' in
  \emph{International Conference on Machine Learning}.\hskip 1em plus 0.5em
  minus 0.4em\relax PMLR, 2021, pp. 6565--6576.

\bibitem{weidinger2021ethical}
L.~Weidinger, J.~Mellor, M.~Rauh, C.~Griffin, J.~Uesato, P.-S. Huang, M.~Cheng,
  M.~Glaese, B.~Balle, A.~Kasirzadeh \emph{et~al.}, ``Ethical and social risks
  of harm from language models,'' \emph{arXiv preprint arXiv:2112.04359}, 2021.

\bibitem{nadeem2021stereoset}
M.~Nadeem, A.~Bethke, and S.~Reddy, ``Stereoset: Measuring stereotypical bias
  in pretrained language models,'' in \emph{Proceedings of the 59th Annual
  Meeting of the Association for Computational Linguistics and the 11th
  International Joint Conference on Natural Language Processing (Volume 1: Long
  Papers)}, 2021, pp. 5356--5371.

\bibitem{kirk2021bias}
H.~R. Kirk, Y.~Jun, F.~Volpin, H.~Iqbal, E.~Benussi, F.~Dreyer, A.~Shtedritski,
  and Y.~Asano, ``Bias out-of-the-box: An empirical analysis of intersectional
  occupational biases in popular generative language models,'' \emph{Advances
  in neural information processing systems}, vol.~34, pp. 2611--2624, 2021.

\bibitem{carlini2021extracting}
N.~Carlini, F.~Tramer, E.~Wallace, M.~Jagielski, A.~Herbert-Voss, K.~Lee,
  A.~Roberts, T.~B. Brown, D.~Song, U.~Erlingsson \emph{et~al.}, ``Extracting
  training data from large language models.'' in \emph{USENIX Security
  Symposium}, vol.~6, 2021.

\bibitem{wei2022ai}
M.~Wei and Z.~Zhou, ``Ai ethics issues in real world: Evidence from ai incident
  database,'' \emph{arXiv preprint arXiv:2206.07635}, 2022.

\bibitem{perez2022red}
E.~Perez, S.~Huang, F.~Song, T.~Cai, R.~Ring, J.~Aslanides, A.~Glaese,
  N.~McAleese, and G.~Irving, ``Red teaming language models with language
  models,'' \emph{arXiv preprint arXiv:2202.03286}, 2022.

\bibitem{henderson2018ethical}
P.~Henderson, K.~Sinha, N.~Angelard-Gontier, N.~R. Ke, G.~Fried, R.~Lowe, and
  J.~Pineau, ``Ethical challenges in data-driven dialogue systems,'' in
  \emph{Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and
  Society}, 2018, pp. 123--129.

\bibitem{lucy2021gender}
L.~Lucy and D.~Bamman, ``Gender and representation bias in gpt-3 generated
  stories,'' in \emph{Proceedings of the Third Workshop on Narrative
  Understanding}, 2021, pp. 48--55.

\bibitem{abid2021persistent}
A.~Abid, M.~Farooqi, and J.~Zou, ``Persistent anti-muslim bias in large
  language models,'' in \emph{Proceedings of the 2021 AAAI/ACM Conference on
  AI, Ethics, and Society}, 2021, pp. 298--306.

\bibitem{si2022so}
W.~M. Si, M.~Backes, J.~Blackburn, E.~De~Cristofaro, G.~Stringhini,
  S.~Zannettou, and Y.~Zhang, ``Why so toxic? measuring and triggering toxic
  behavior in open-domain chatbots,'' in \emph{Proceedings of the 2022 ACM
  SIGSAC Conference on Computer and Communications Security}, 2022, pp.
  2659--2673.

\bibitem{roller2020recipes}
S.~Roller, E.~Dinan, N.~Goyal, D.~Ju, M.~Williamson, Y.~Liu, J.~Xu, M.~Ott,
  K.~Shuster, E.~M. Smith \emph{et~al.}, ``Recipes for building an open-domain
  chatbot,'' \emph{arXiv preprint arXiv:2004.13637}, 2020.

\bibitem{miller2017parlai}
A.~H. Miller, W.~Feng, A.~Fisch, J.~Lu, D.~Batra, A.~Bordes, D.~Parikh, and
  J.~Weston, ``Parlai: A dialog research software platform,'' \emph{EMNLP
  2017}, p.~79, 2017.

\bibitem{liang2022holistic}
P.~Liang, R.~Bommasani, T.~Lee, D.~Tsipras, D.~Soylu, M.~Yasunaga, Y.~Zhang,
  D.~Narayanan, Y.~Wu, A.~Kumar \emph{et~al.}, ``Holistic evaluation of
  language models,'' \emph{arXiv preprint arXiv:2211.09110}, 2022.

\bibitem{khurana2022natural}
D.~Khurana, A.~Koli, K.~Khatter, and S.~Singh, ``Natural language processing:
  State of the art, current trends and challenges,'' \emph{Multimedia tools and
  applications}, pp. 1--32, 2022.

\bibitem{goldstein2023generative}
J.~A. Goldstein, G.~Sastry, M.~Musser, R.~DiResta, M.~Gentzel, and K.~Sedova,
  ``Generative language models and automated influence operations: Emerging
  threats and potential mitigations,'' \emph{arXiv preprint arXiv:2301.04246},
  2023.

\bibitem{taddeo2018ai}
M.~Taddeo and L.~Floridi, ``How ai can be a force for good,'' \emph{Science},
  vol. 361, no. 6404, pp. 751--752, 2018.

\bibitem{jobin2019global}
A.~Jobin, M.~Ienca, and E.~Vayena, ``The global landscape of ai ethics
  guidelines,'' \emph{Nature Machine Intelligence}, vol.~1, no.~9, pp.
  389--399, 2019.

\bibitem{higashinaka2018role}
R.~Higashinaka, M.~Mizukami, H.~Kawabata, E.~Yamaguchi, N.~Adachi, and
  J.~Tomita, ``Role play-based question-answering by real users for building
  chatbots with consistent personalities,'' in \emph{Proceedings of the 19th
  annual sigdial meeting on discourse and dialogue}, 2018, pp. 264--272.

\bibitem{reisenbichler2022frontiers}
M.~Reisenbichler, T.~Reutterer, D.~A. Schweidel, and D.~Dan, ``Frontiers:
  Supporting content marketing with natural language generation,''
  \emph{Marketing Science}, vol.~41, no.~3, pp. 441--452, 2022.

\bibitem{bartz2008natural}
K.~Bartz, C.~Barr, and A.~Aijaz, ``Natural language generation for
  sponsored-search advertisements,'' in \emph{Proceedings of the 9th ACM
  Conference on Electronic Commerce}, 2008, pp. 1--9.

\bibitem{cao2022ai}
L.~Cao, ``Ai in finance: challenges, techniques, and opportunities,'' \emph{ACM
  Computing Surveys (CSUR)}, vol.~55, no.~3, pp. 1--38, 2022.

\bibitem{zhang2018deep}
L.~Zhang, S.~Wang, and B.~Liu, ``Deep learning for sentiment analysis: A
  survey,'' \emph{Wiley Interdisciplinary Reviews: Data Mining and Knowledge
  Discovery}, vol.~8, no.~4, p. e1253, 2018.

\bibitem{minaee2021deep}
S.~Minaee, N.~Kalchbrenner, E.~Cambria, N.~Nikzad, M.~Chenaghlu, and J.~Gao,
  ``Deep learning--based text classification: a comprehensive review,''
  \emph{ACM computing surveys (CSUR)}, vol.~54, no.~3, pp. 1--40, 2021.

\bibitem{abbasiantaeb2021text}
Z.~Abbasiantaeb and S.~Momtazi, ``Text-based question answering from
  information retrieval and deep neural network perspectives: A survey,''
  \emph{Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  vol.~11, no.~6, p. e1412, 2021.

\bibitem{talat2022you}
Z.~Talat, A.~N{\'e}v{\'e}ol, S.~Biderman, M.~Clinciu, M.~Dey, S.~Longpre,
  S.~Luccioni, M.~Masoud, M.~Mitchell, D.~Radev \emph{et~al.}, ``You reap what
  you sow: On the challenges of bias evaluation under multilingual settings,''
  in \emph{Proceedings of BigScience Episode\# 5--Workshop on Challenges \&
  Perspectives in Creating Large Language Models}, 2022, pp. 26--41.

\bibitem{alzantot2018generating}
M.~Alzantot, Y.~S. Sharma, A.~Elgohary, B.-J. Ho, M.~Srivastava, and K.-W.
  Chang, ``Generating natural language adversarial examples,'' in
  \emph{Proceedings of the 2018 Conference on Empirical Methods in Natural
  Language Processing}, 2018.

\bibitem{li2020bert}
L.~Li, R.~Ma, Q.~Guo, X.~Xue, and X.~Qiu, ``Bert-attack: Adversarial attack
  against bert using bert,'' in \emph{Proceedings of the 2020 Conference on
  Empirical Methods in Natural Language Processing (EMNLP)}, 2020, pp.
  6193--6202.

\bibitem{mcguffie2020radicalization}
K.~McGuffie and A.~Newhouse, ``The radicalization risks of gpt-3 and advanced
  neural language models,'' \emph{arXiv preprint arXiv:2009.06807}, 2020.

\bibitem{jang2021towards}
J.~Jang, S.~Ye, S.~Yang, J.~Shin, J.~Han, G.~Kim, S.~J. Choi, and M.~Seo,
  ``Towards continual knowledge learning of language models,'' \emph{arXiv
  preprint arXiv:2110.03215}, 2021.

\bibitem{gehman2020realtoxicityprompts}
S.~Gehman, S.~Gururangan, M.~Sap, Y.~Choi, and N.~A. Smith,
  ``Realtoxicityprompts: Evaluating neural toxic degeneration in language
  models,'' in \emph{Findings of the Association for Computational Linguistics:
  EMNLP 2020}, 2020, pp. 3356--3369.

\bibitem{solaiman2021process}
I.~Solaiman and C.~Dennison, ``Process for adapting language models to society
  (palms) with values-targeted datasets,'' \emph{Advances in Neural Information
  Processing Systems}, vol.~34, pp. 5861--5873, 2021.

\bibitem{parrish2022bbq}
A.~Parrish, A.~Chen, N.~Nangia, V.~Padmakumar, J.~Phang, J.~Thompson, P.~M.
  Htut, and S.~Bowman, ``Bbq: A hand-built bias benchmark for question
  answering,'' in \emph{Findings of the Association for Computational
  Linguistics: ACL 2022}, 2022, pp. 2086--2105.

\bibitem{dhamala2021bold}
J.~Dhamala, T.~Sun, V.~Kumar, S.~Krishna, Y.~Pruksachatkun, K.-W. Chang, and
  R.~Gupta, ``Bold: Dataset and metrics for measuring biases in open-ended
  language generation,'' in \emph{Proceedings of the 2021 ACM conference on
  fairness, accountability, and transparency}, 2021, pp. 862--872.

\bibitem{brown2020language}
T.~Brown, B.~Mann, N.~Ryder, M.~Subbiah, J.~D. Kaplan, P.~Dhariwal,
  A.~Neelakantan, P.~Shyam, G.~Sastry, A.~Askell \emph{et~al.}, ``Language
  models are few-shot learners,'' \emph{Advances in neural information
  processing systems}, vol.~33, pp. 1877--1901, 2020.

\bibitem{post-2018-call}
\BIBentryALTinterwordspacing
M.~Post, ``A call for clarity in reporting {BLEU} scores,'' in
  \emph{Proceedings of the Third Conference on Machine Translation: Research
  Papers}.\hskip 1em plus 0.5em minus 0.4em\relax Belgium, Brussels:
  Association for Computational Linguistics, Oct. 2018, pp. 186--191. [Online].
  Available: \url{https://www.aclweb.org/anthology/W18-6319}
\BIBentrySTDinterwordspacing

\bibitem{goyal2022flores}
N.~Goyal, C.~Gao, V.~Chaudhary, P.-J. Chen, G.~Wenzek, D.~Ju, S.~Krishnan,
  M.~Ranzato, F.~Guzm{\'a}n, and A.~Fan, ``The flores-101 evaluation benchmark
  for low-resource and multilingual machine translation,'' \emph{Transactions
  of the Association for Computational Linguistics}, vol.~10, pp. 522--538,
  2022.

\bibitem{popovic2015chrf}
M.~Popovi{\'c}, ``chrf: character n-gram f-score for automatic mt evaluation,''
  in \emph{Proceedings of the tenth workshop on statistical machine
  translation}, 2015, pp. 392--395.

\bibitem{kim2022prosocialdialog}
H.~Kim, Y.~Yu, L.~Jiang, X.~Lu, D.~Khashabi, G.~Kim, Y.~Choi, and M.~Sap,
  ``Prosocialdialog: A prosocial backbone for conversational agents,''
  \emph{arXiv preprint arXiv:2205.12688}, 2022.

\bibitem{maas2011learning}
A.~Maas, R.~E. Daly, P.~T. Pham, D.~Huang, A.~Y. Ng, and C.~Potts, ``Learning
  word vectors for sentiment analysis,'' in \emph{Proceedings of the 49th
  annual meeting of the association for computational linguistics: Human
  language technologies}, 2011, pp. 142--150.

\bibitem{clark2019boolq}
C.~Clark, K.~Lee, M.-W. Chang, T.~Kwiatkowski, M.~Collins, and K.~Toutanova,
  ``Boolq: Exploring the surprising difficulty of natural yes/no questions,''
  in \emph{Proceedings of the 2019 Conference of the North American Chapter of
  the Association for Computational Linguistics: Human Language Technologies,
  Volume 1 (Long and Short Papers)}, 2019, pp. 2924--2936.

\bibitem{dhole2021nl}
K.~D. Dhole, V.~Gangal, S.~Gehrmann, A.~Gupta, Z.~Li, S.~Mahamood,
  A.~Mahendiran, S.~Mille, A.~Srivastava, S.~Tan \emph{et~al.}, ``Nl-augmenter:
  A framework for task-sensitive natural language augmentation,'' \emph{arXiv
  preprint arXiv:2112.02721}, 2021.

\bibitem{gardner2020evaluating}
M.~Gardner, Y.~Artzi, V.~Basmov, J.~Berant, B.~Bogin, S.~Chen, P.~Dasigi,
  D.~Dua, Y.~Elazar, A.~Gottumukkala \emph{et~al.}, ``Evaluating models’
  local decision boundaries via contrast sets,'' in \emph{Findings of the
  Association for Computational Linguistics: EMNLP 2020}, 2020, pp. 1307--1323.

\bibitem{zeng2022glm}
A.~Zeng, X.~Liu, Z.~Du, Z.~Wang, H.~Lai, M.~Ding, Z.~Yang, Y.~Xu, W.~Zheng,
  X.~Xia \emph{et~al.}, ``Glm-130b: An open bilingual pre-trained model,''
  \emph{arXiv preprint arXiv:2210.02414}, 2022.

\bibitem{ethical-constraint}
\BIBentryALTinterwordspacing
(2022) Chatgpt has a handful of ethical constraints that are currently being
  tested. [Online]. Available:
  \url{https://ordinary-times.com/2022/12/02/chatgpt-has-a-handful-of-ethical-constraints-that-are-currently-being-tested/}
\BIBentrySTDinterwordspacing

\bibitem{mihaylov2018can}
T.~Mihaylov, P.~Clark, T.~Khot, and A.~Sabharwal, ``Can a suit of armor conduct
  electricity? a new dataset for open book question answering,'' in
  \emph{Proceedings of the 2018 Conference on Empirical Methods in Natural
  Language Processing}, 2018, pp. 2381--2391.

\bibitem{lin2022truthfulqa}
S.~Lin, J.~Hilton, and O.~Evans, ``Truthfulqa: Measuring how models mimic human
  falsehoods,'' in \emph{Proceedings of the 60th Annual Meeting of the
  Association for Computational Linguistics (Volume 1: Long Papers)}, 2022, pp.
  3214--3252.

\bibitem{openai-chatgpt}
\BIBentryALTinterwordspacing
(2022) Chatgpt: Optimizing language models for dialoguechatgpt: Optimizing
  language models for dialogue. [Online]. Available:
  \url{https://openai.com/blog/chatgpt/}
\BIBentrySTDinterwordspacing

\bibitem{ji2022survey}
Z.~Ji, N.~Lee, R.~Frieske, T.~Yu, D.~Su, Y.~Xu, E.~Ishii, Y.~Bang, A.~Madotto,
  and P.~Fung, ``Survey of hallucination in natural language generation,''
  \emph{ACM Computing Surveys}.

\bibitem{Gokaslan2019OpenWeb}
\BIBentryALTinterwordspacing
A.~Gokaslan and V.~Cohen, ``Openwebtext corpus,'' 2019. [Online]. Available:
  \url{http://Skylion007.github.io/OpenWebTextCorpus}
\BIBentrySTDinterwordspacing

\bibitem{raffel2020exploring}
C.~Raffel, N.~Shazeer, A.~Roberts, K.~Lee, S.~Narang, M.~Matena, Y.~Zhou,
  W.~Li, and P.~J. Liu, ``Exploring the limits of transfer learning with a
  unified text-to-text transformer,'' \emph{The Journal of Machine Learning
  Research}, vol.~21, no.~1, pp. 5485--5551, 2020.

\bibitem{borji2023categorical}
A.~Borji, ``A categorical archive of chatgpt failures,'' \emph{arXiv preprint
  arXiv:2302.03494}, 2023.

\bibitem{armengol2021multilingual}
J.~Armengol-Estap{\'e}, O.~d.~G. Bonet, and M.~Melero, ``On the multilingual
  capabilities of very large-scale english language models,'' \emph{arXiv
  preprint arXiv:2108.13349}, 2021.

\bibitem{chatgpt-multilingual}
\BIBentryALTinterwordspacing
(2022) Chatgpt is multilingual but monocultural, and it’s learning your
  values. [Online]. Available:
  \url{https://jilltxt.net/right-now-chatgpt-is-multilingual-but-monocultural-but-its-learning-your-values/}
\BIBentrySTDinterwordspacing

\bibitem{sawhney2021empirical}
R.~Sawhney, A.~Aggarwal, and R.~Shah, ``An empirical investigation of bias in
  the multimodal analysis of financial earnings calls,'' in \emph{Proceedings
  of the 2021 Conference of the North American Chapter of the Association for
  Computational Linguistics: Human Language Technologies}, 2021, pp.
  3751--3757.

\bibitem{llm-remarks}
\BIBentryALTinterwordspacing
Y.~Goldberg. (2023) Some remarks on large language model. [Online]. Available:
  \url{https://gist.github.com/yoavg/59d174608e92e845c8994ac2e234c8a9}
\BIBentrySTDinterwordspacing

\bibitem{mitchellfast}
E.~Mitchell, C.~Lin, A.~Bosselut, C.~Finn, and C.~D. Manning, ``Fast model
  editing at scale,'' in \emph{International Conference on Learning
  Representations}.

\bibitem{DeCao2021EditingFK}
N.~D. Cao, W.~Aziz, and I.~Titov, ``Editing factual knowledge in language
  models,'' in \emph{Conference on Empirical Methods in Natural Language
  Processing}, 2021.

\bibitem{Zhu2020ModifyingMI}
C.~Zhu, A.~S. Rawat, M.~Zaheer, S.~Bhojanapalli, D.~Li, F.~X. Yu, and S.~Kumar,
  ``Modifying memories in transformer models,'' \emph{ArXiv}, vol.
  abs/2012.00363, 2020.

\bibitem{kandpal2022deduplicating}
N.~Kandpal, E.~Wallace, and C.~Raffel, ``Deduplicating training data mitigates
  privacy risks in language models,'' in \emph{International Conference on
  Machine Learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 2022, pp.
  10\,697--10\,707.

\bibitem{scao2022bloom}
T.~L. Scao, A.~Fan, C.~Akiki, E.~Pavlick, S.~Ili{\'c}, D.~Hesslow,
  R.~Castagn{\'e}, A.~S. Luccioni, F.~Yvon, M.~Gall{\'e} \emph{et~al.},
  ``Bloom: A 176b-parameter open-access multilingual language model,''
  \emph{arXiv preprint arXiv:2211.05100}, 2022.

\bibitem{allal2023santacoder}
L.~B. Allal, R.~Li, D.~Kocetkov, C.~Mou, C.~Akiki, C.~M. Ferrandis,
  N.~Muennighoff, M.~Mishra, A.~Gu, M.~Dey \emph{et~al.}, ``Santacoder: don't
  reach for the stars!'' \emph{arXiv preprint arXiv:2301.03988}, 2023.

\bibitem{yang2022diffsound}
D.~Yang, J.~Yu, H.~Wang, W.~Wang, C.~Weng, Y.~Zou, and D.~Yu, ``Diffsound:
  Discrete diffusion model for text-to-sound generation,'' \emph{arXiv preprint
  arXiv:2207.09983}, 2022.

\bibitem{kreuk2022audiogen}
F.~Kreuk, G.~Synnaeve, A.~Polyak, U.~Singer, A.~D{\'e}fossez, J.~Copet,
  D.~Parikh, Y.~Taigman, and Y.~Adi, ``Audiogen: Textually guided audio
  generation,'' \emph{arXiv preprint arXiv:2209.15352}, 2022.

\bibitem{borsos2022audiolm}
Z.~Borsos, R.~Marinier, D.~Vincent, E.~Kharitonov, O.~Pietquin, M.~Sharifi,
  O.~Teboul, D.~Grangier, M.~Tagliasacchi, and N.~Zeghidour, ``Audiolm: a
  language modeling approach to audio generation,'' \emph{arXiv preprint
  arXiv:2209.03143}, 2022.

\bibitem{zhang2021vinvl}
P.~Zhang, X.~Li, X.~Hu, J.~Yang, L.~Zhang, L.~Wang, Y.~Choi, and J.~Gao,
  ``Vinvl: Revisiting visual representations in vision-language models,'' in
  \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
  Recognition}, 2021, pp. 5579--5588.

\bibitem{zhou2022learning}
K.~Zhou, J.~Yang, C.~C. Loy, and Z.~Liu, ``Learning to prompt for
  vision-language models,'' \emph{International Journal of Computer Vision},
  vol. 130, no.~9, pp. 2337--2348, 2022.

\bibitem{he2022protecting}
X.~He, Q.~Xu, L.~Lyu, F.~Wu, and C.~Wang, ``Protecting intellectual property of
  language generation apis with lexical watermark,'' in \emph{Proceedings of
  the AAAI Conference on Artificial Intelligence}, vol.~36, no.~10, 2022, pp.
  10\,758--10\,766.

\bibitem{kirchenbauer2023watermark}
J.~Kirchenbauer, J.~Geiping, Y.~Wen, J.~Katz, I.~Miers, and T.~Goldstein, ``A
  watermark for large language models,'' \emph{arXiv preprint
  arXiv:2301.10226}, 2023.

\bibitem{hecater}
X.~He, Q.~Xu, Y.~Zeng, L.~Lyu, F.~Wu, J.~Li, and R.~Jia, ``Cater: Intellectual
  property protection on text generation apis via conditional watermarks,'' in
  \emph{Advances in Neural Information Processing Systems}.

\bibitem{kaplan2020scaling}
J.~Kaplan, S.~McCandlish, T.~Henighan, T.~B. Brown, B.~Chess, R.~Child,
  S.~Gray, A.~Radford, J.~Wu, and D.~Amodei, ``Scaling laws for neural language
  models,'' \emph{arXiv preprint arXiv:2001.08361}, 2020.

\bibitem{aghajanyan2023scaling}
A.~Aghajanyan, L.~Yu, A.~Conneau, W.-N. Hsu, K.~Hambardzumyan, S.~Zhang,
  S.~Roller, N.~Goyal, O.~Levy, and L.~Zettlemoyer, ``Scaling laws for
  generative mixed-modal language models,'' \emph{arXiv preprint
  arXiv:2301.03728}, 2023.

\bibitem{bommasani2021opportunities}
R.~Bommasani, D.~A. Hudson, E.~Adeli, R.~Altman, S.~Arora, S.~von Arx, M.~S.
  Bernstein, J.~Bohg, A.~Bosselut, E.~Brunskill \emph{et~al.}, ``On the
  opportunities and risks of foundation models,'' \emph{arXiv preprint
  arXiv:2108.07258}, 2021.

\bibitem{weiemergent}
J.~Wei, Y.~Tay, R.~Bommasani, C.~Raffel, B.~Zoph, S.~Borgeaud, D.~Yogatama,
  M.~Bosma, D.~Zhou, D.~Metzler \emph{et~al.}, ``Emergent abilities of large
  language models,'' \emph{Transactions on Machine Learning Research}.

\bibitem{villalobos2022will}
P.~Villalobos, J.~Sevilla, L.~Heim, T.~Besiroglu, M.~Hobbhahn, and A.~Ho,
  ``Will we run out of data? an analysis of the limits of scaling datasets in
  machine learning,'' \emph{arXiv preprint arXiv:2211.04325}, 2022.

\bibitem{northcutt2021confident}
C.~Northcutt, L.~Jiang, and I.~Chuang, ``Confident learning: Estimating
  uncertainty in dataset labels,'' \emph{Journal of Artificial Intelligence
  Research}, vol.~70, pp. 1373--1411, 2021.

\bibitem{treviso2022efficient}
M.~Treviso, T.~Ji, J.-U. Lee, B.~van Aken, Q.~Cao, M.~R. Ciosici, M.~Hassid,
  K.~Heafield, S.~Hooker, P.~H. Martins \emph{et~al.}, ``Efficient methods for
  natural language processing: a survey,'' \emph{arXiv preprint
  arXiv:2209.00099}, 2022.

\bibitem{mishra2020we}
S.~Mishra and B.~S. Sachdeva, ``Do we need to create big datasets to learn a
  task?'' in \emph{Proceedings of SustaiNLP: Workshop on Simple and Efficient
  Natural Language Processing}, 2020, pp. 169--173.

\bibitem{lee2022deduplicating}
K.~Lee, D.~Ippolito, A.~Nystrom, C.~Zhang, D.~Eck, C.~Callison-Burch, and
  N.~Carlini, ``Deduplicating training data makes language models better,'' in
  \emph{Proceedings of the 60th Annual Meeting of the Association for
  Computational Linguistics (Volume 1: Long Papers)}, 2022, pp. 8424--8445.

\bibitem{Bengio2009CurriculumL}
Y.~Bengio, J.~Louradour, R.~Collobert, and J.~Weston, ``Curriculum learning,''
  in \emph{International Conference on Machine Learning}, 2009.

\bibitem{Ren2020ASO}
P.~Ren, Y.~Xiao, X.~Chang, P.-Y. Huang, Z.~Li, X.~Chen, and X.~Wang, ``A survey
  of deep active learning,'' \emph{ACM Computing Surveys (CSUR)}, vol.~54, pp.
  1 -- 40, 2020.

\bibitem{Brown2020LanguageMA}
T.~B. Brown, B.~Mann, N.~Ryder, M.~Subbiah, J.~Kaplan, P.~Dhariwal,
  A.~Neelakantan, P.~Shyam, G.~Sastry, A.~Askell, S.~Agarwal, A.~Herbert-Voss,
  G.~Krueger, T.~J. Henighan, R.~Child, A.~Ramesh, D.~M. Ziegler, J.~Wu,
  C.~Winter, C.~Hesse, M.~Chen, E.~Sigler, M.~Litwin, S.~Gray, B.~Chess,
  J.~Clark, C.~Berner, S.~McCandlish, A.~Radford, I.~Sutskever, and D.~Amodei,
  ``Language models are few-shot learners,'' \emph{ArXiv}, vol. abs/2005.14165,
  2020.

\bibitem{thompson2020computational}
N.~C. Thompson, K.~Greenewald, K.~Lee, and G.~F. Manso, ``The computational
  limits of deep learning,'' \emph{arXiv preprint arXiv:2007.05558}, 2020.

\bibitem{wu2022sustainable}
C.-J. Wu, R.~Raghavendra, U.~Gupta, B.~Acun, N.~Ardalani, K.~Maeng, G.~Chang,
  F.~Aga, J.~Huang, C.~Bai \emph{et~al.}, ``Sustainable ai: Environmental
  implications, challenges and opportunities,'' \emph{Proceedings of Machine
  Learning and Systems}, vol.~4, pp. 795--813, 2022.

\end{thebibliography}
