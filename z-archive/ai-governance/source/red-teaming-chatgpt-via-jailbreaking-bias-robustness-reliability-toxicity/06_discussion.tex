\section{Discussions}

\subsection{Summary of Evaluation}
Our evaluation empirically red teams a few ethical perspectives of ChatGPT, from bias to toxicity, unveiling the model performance under major ethical risks. Through these studies, we tend to answer the main research question ``How responsible is ChatGPT?''. One of our main findings is that predominant benchmarks for language model evaluation are insufficient for ChatGPT. We consistently observe that ChatGPT performs comparably or even better, among SOTA LMs on those benchmarks, which indicates the nontrivial progress in the recent development of AI. The fact partially confirms OpenAI's claim of mitigating the bias and toxicity from the previous LLM, GPT-3. In contrast, motivated by the community, we illustrate several shortcomings of ChatGPT via small-scale case studies. Some of the issues are later covered by \citep{borji2023categorical}. We summarize them as follows:

\paragraph{Bias}
\hfill

\textbf{Lack of Multilingual Understanding:} ChatGPT appears to not fully understand diverse languages. This drawback was also identified in the prototype of GPT-3~\cite{armengol2021multilingual}, though daily users claim that ChatGPT is more like a multilingual communicator~\cite{chatgpt-multilingual}. Due to the poor capability of multilingual understanding, ChatGPT can be biased in decision-making and creative generation. We expect that the bias in multilingualism will potentially imply the bias in multicultural understanding, leading to an unethical impact on underrepresented groups in society.


\textbf{Multimodality:} Besides the natural language, ChatGPT could be biased in code generation due to the logical fallacy of program oversimplification. The bias in multimodality~\cite{sawhney2021empirical} could be an unethical threat to the daily programming practice, resulting in huge flaws in real-world productions where the programs are usually more sophisticated.

\paragraph{Robustness \& Toxicity} Prompt injection is an effective approach to breaking the model constraints. Although ChatGPT is likely to be trained safely, it can easily bypass due to the emergent risks with prompt injections. With the emergent ability in LLMs, models are easy to be manipulated for harmful behaviors.

\paragraph{Reliability} ChatGPT does not encode enough knowledge, especially factual one. This greatly reduces the reliability of the model, as the majority of daily usage relies on factual justification. Due to the hallucination, the model can be wrong for spreading misinformation and disinformation and advising unethical decisions in the domains like clinics and law. Another unmeasured but inevitable shortcoming is that the knowledge encoded by ChatGPT and all other LLMs is limited by the amount and time of training data. Without the constant update in model weights, language models are expected to be out-of-date and hence provide incorrect information. This will also degrade the model's reliability.

\subsection{Towards Responsible Language Models}
% Applications of language models are greatly changing influence operations of the society~\cite{}. 
The empirical findings on the AI ethics and risks of ChatGPT serve to further underscore the importance of providing a comprehensive outlook on the ethics of language models more broadly. Our examination of the diagnosed risks inherent in ChatGPT supports the conjecture that similar ethical considerations are likely to pertain to other language models, as discussed in prior literature~\cite{goldstein2023generative, weidinger2021ethical}. Despite the challenges, it is clear that the development of safe and ethical language models represents a crucial long-term objective for the advancement of responsible artificial general intelligence. In this section, we aim to provide valuable insights into this endeavor, with a focus on both Internal Ethics and External Ethics, as inspired by the seminal work of~\cite{llm-remarks}.

\paragraph{Internal Ethics --- Modeling} We believe there should be an evolution of current learning strategies. We argue that the current main focus of language modeling is more on effectiveness (on prototypical benchmarks) and efficiency, instead of reliability and practical efficacy. For instance, there are few modeling approaches to avoid the miscorrelation at the learning stage. Regardless of the size, language models more or less encode wrong beliefs (e.g. biases, stereotypes, and misunderstanding), though these beliefs may not necessarily appear in the training data. Furthermore, general language models do not have good senses of time or temporal knowledge. The facts and knowledge learned by language models could be changed due to a matter of time, while the parameters of language models still stay unchanged. It is foreseen that the reliability of trained-once language models will constantly decrease as time goes by. Constant updates on data and models would definitely mitigate the issues, though it can not be afforded by the majority of people. We kindly mention that some existing works of weight editing~\cite{mitchellfast,DeCao2021EditingFK,Zhu2020ModifyingMI} could partially address the problem, but impractically. Practitioners
who seek for weight editing need to predesign the mapping of knowledge updates.

\paragraph{External Ethics --- Usage} We define external ethics as the responsibility of producers and users. From the production perspective, the training data should be responsibly constructed. We emphasize on the privacy of data usage. Without privacy protection, LLMs can easily leak private information in generation~\cite{carlini2021extracting}. One ethical practice is to filter the personally identifiable information, which has been adopted by some recent LLMs~\cite{kandpal2022deduplicating,scao2022bloom,allal2023santacoder}. Secondly, language models for release should be systematically evaluated on various scenarios and large-scale test samples. We suggest that the benchmarks like HELM could be set as the practice inside the future supply chain of language models. However, we also argue that most tasks of HELM only measure in the modality of natural language, which is insufficient for multimodal LLMs, such as audio LLMs~\cite{yang2022diffsound,kreuk2022audiogen,borsos2022audiolm} and vision LLMs~\cite{zhang2021vinvl,borsos2022audiolm,zhou2022learning}. Despite the rising benchmarks on multimodal tasks, the ones for multimodal AI ethics have not yet been seriously considered. At the deployment stage, we note that LLMs could be attacked to output malicious content or decisions, by unethical users~\cite{goldstein2023generative, weidinger2021ethical}. Thus, even internally ethical language models can be used unethically by third parties. Existing strategies~\cite{he2022protecting,kirchenbauer2023watermark} have demonstrated the effectiveness of preventing LLM abuse, though they can be invalid via attacks~\cite{hecater}. We, therefore, encourage future works to explore more feasible protections for language models. From the daily usage perspective, the users should be fully aware of the shortcomings of the language model's application, and not abuse or attack language models for performing unethical tasks. Most of the unethical behaviors towards language models are deemed a great challenge for the LLM producers, as they are almost unpredictable. Consequently, we would like to call for the education and policy of model usage in the community. Specifically, courses for proper machine learning model usage should be developed for guiding users to learn `Dos' and Dont' in AI. Detailed policies could also be proposed to list all user's responsibilities before the model access.

\subsection{Language Models Beyond ChatGPT}
The examination of ethical implications associated with language models necessitates a comprehensive examination of the broader challenges that arise within the domain of language models, in light of recent advancements in the field of artificial intelligence. The last decade has seen a rapid evolution of AI techniques, characterized by an exponential increase in the size and complexity of AI models, and a concomitant scale-up of model parameters. The scaling laws that govern the development of language models, as documented in recent literature~\cite{kaplan2020scaling,aghajanyan2023scaling}, suggest that we can expect to encounter even more expansive models that incorporate multiple modalities in the near future. Efforts to integrate multiple modalities into a single model are driven by the ultimate goal of realizing the concept of foundation models~\cite{bommasani2021opportunities}. In the following sections, we will outline some of the most pressing challenges that must be addressed in order to facilitate further progress in the development of language models.

\paragraph{Emergent Ability}
As described in the previous work~\cite{weiemergent}, emergent ability is defined as \textit{An ability is emergent if it is not present in smaller models but is present in larger models.}. From our diagnosis, we successfully identify a few unethical behaviors in ChatGPT that were inadequately discussed in previous works, which could be potentially be viewed as emergent risks. Kaplan et al.~\cite{kaplan2020scaling} has confirmed that risks inside small language models can be further expanded in large ones due to the model scales. On the basis of this finding, we add that the model scales and the current trend of prompting training can exacerbate risks from all dimensions. The main reason is that LLMs could be too feasible from the learning perspective. Firstly, these models are more context-dependent, meaning that they are easily manipulated by prompt injections. Although we agree that some injected scenarios can be temporarily mitigated with ad-hoc parameter tuning, there is no silver bullet to avoid all risk concerns brought by prompting. Meanwhile, we urge up-to-date benchmarks for measuring unforeseen behaviors inside large language models. Without benchmarking the emergent abilities, it could be hard to mitigate the risks and problems at scale. Secondly, we note that larger language models are generally trained with more data. Assuming the data is completely clean and informatively correct, language models will still fail to learn all information and knowledge, and also may wrongly correlate information to each other. Furthermore, under the scope of the foundation models, multimodal data could bring the possibility of miscorrelation between different modalities.

\paragraph{Machine Learning Data}
Our discussion lies in the collection and usage of machine learning data. Previous study~\cite{villalobos2022will} suggests that high-quality language data is likely exhausted before 2026, and low-quality language and image data could be run out by 2060. This implies that the limited progress of data collection and construction could be constraints of future LLM development. Furthermore, as better-quality data is assumed to train language models with better performances, companies and independent researchers are spending more time on data curation. However, this can not be done easily under the low-resource and low-budget scenarios. Even if we pay much effort to design comprehensive human annotation frameworks, the data could still contain inaccurate or misleading information due to the natural biases in crowdsourcing. In fact, we notice that prior constructed datasets have experienced multiple rounds of filtering across time~\cite{northcutt2021confident}. On the other hand, current findings suggest that the usage of data for language models may not be optimized~\cite{treviso2022efficient}. Specifically, recent works on data deduplication and reduction~\cite{mishra2020we,lee2022deduplicating} have shown that data in high quality by low quantity can improve the model performance. Besides, we consider the design of training data as a crucial factor to the efficient data usage. For example, experiments show that curriculum learning~\cite{Bengio2009CurriculumL}, active learning~\cite{Ren2020ASO} and prompting~\cite{Brown2020LanguageMA} could improve the data efficiency. However, most of these strategies are still at the early stage and need the further investigation.

\paragraph{Computational Resource}
As LLMs are growing bigger and bigger, the deployment and training of these models are getting more and more costly. Daily practitioners in NLP and deep learning will find it hard to install the LLMs on their own devices. Previous study~\cite{thompson2020computational} also show that the computational resource requirements for strong model scaling clearly outpaces that of
system hardware. We argue that model scaling may be inevitable, which is determined by the scaling law. However, recent attempts among model design, tuning strategy and compression could possibly mitigate the extreme consumption of the computational resources. As Wu et al.~\cite{wu2022sustainable} have summarized most works around this topic, we do not tend to elaborate the introduction of these approaches and designs. In addition, the increasing demand of computational resources is leading to the energy consumption and carbon emission, negatively impacting the environment~\cite{wu2022sustainable}. Hence, we encourage more advanced hardware-software co-designs in computation to optimize the carbon footprint in LLMs.


\section{Conclusion}
We present a comprehensive diagnosis on the AI ethics encoded by ChatGPT, including bias, robustness, reliability and toxicity. By measuring on a number of benchmarks and case studies, we find that ChatGPT may perform slightly better than current SOTA language models, while showing the evidence of ethical risks. Concretely, we reveal that ChatGPT is sensible to prompt injections for unethical behaviors. We further provide an outlook of ethical challenges to develop advance language models. Then, we provide suggestions on the directions and strategies to design ethical language models. We believe that our research can inspire researchers to focus more effort on language models and their evaluations.


\section{Limitations}
The primary limitation of the study pertains to the validity of our empirical analysis of ChatGPT. It is acknowledged that the reported results may be inconsistent as the hyperparameters of ChatGPT remain undisclosed. Moreover, it is feasible that ChatGPT underwent iteration in three versions (initial version, version from December 15th and version from January 9th) over the course of two months and was trained with new data in each version. Despite these limitations, our study endeavors to highlight the potential ethical risks associated with future language models by addressing a comprehensive set of topics in AI ethics.

Additionally, the evaluation settings of our study may be criticized for its lack of rigor. Although our diagnostic study employed a diverse range of evaluation methods through a AI ethics lens, there may exist additional datasets that could enhance its validity. Moreover, the zero-shot performance of ChatGPT was intuitively prompted, and the prompt design could be further scrutinized to attain better results. Given the proprietary nature of the data and model of ChatGPT, it is possible that it has already been trained on some of the evaluated samples. Nonetheless, our objective is to highlight that many ethical concerns have yet to be thoroughly discussed or quantified.